var tipuesearch = {
  "pages": [
    {
      "title": "How to help?",
      "text": "How to help PyPy development?\nHere are some ideas to help PyPy development:\n\nuse pypy for your projects and provide detailed feedback\ntalk to us about how to support Python 3.x\nwrite blog posts or tweets about your experiences\nhelp porting to new platforms\ncontact us and get involved\ndonate some money to enable others to help",
      "tags": "",
      "url": "https://www.pypy.org/howtohelp.html"
    },
    {
      "title": "PyPy v7.3.7: bugfix release of python 3.7 and 3.8",
      "text": "PyPy v7.3.7: bug-fix release of 3.7, 3.8\nWe are releasing a PyPy 7.3.7 to fix the recent 7.3.6 release's binary\nincompatibility with the previous 7.3.x releases. We mistakenly added fields\nto PyFrameObject and PyDateTime_CAPI that broke the promise of binary\ncompatibility, which means that c-extension wheels compiled for 7.3.5 will not\nwork with 7.3.6 and via-versa. Please do not use 7.3.6.\nWe have added a cursory test for binary API breakage to the\nhttps://github.com/pypy/binary-testing repo which hopefully will prevent such\nmistakes in the future.\nAdditionally, a few smaller bugs were fixed:\n\nUse uint for the request argument of fcntl.ioctl (issue 3568)\nFix incorrect tracing of while True` body in 3.8 (issue 3577)\nProperly close resources when using a conncurrent.futures.ProcessPool\n(issue 3317)\nFix the value of LIBDIR in _sysconfigdata in 3.8 (issue 3582)\n\nYou can find links to download the v7.3.7 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog site via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a CFFI / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\n3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 64 bits, OpenBSD, FreeBSD)\n64-bit ARM machines running Linux.\ns390x running Linux\n\n\nPyPy does support ARM 32 bit and PPC64 processors, but does not release binaries.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/10/pypy-v737-release.html"
    },
    {
      "title": "PyPy v7.3.6: release of python 2.7, 3.7, and 3.8",
      "text": "PyPy v7.3.6: release of python 2.7, 3.7, and 3.8-beta\nThe PyPy team is proud to release version 7.3.6 of PyPy, which includes\nthree different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.7,  which is an interpreter supporting the syntax and the features of\nPython 3.7, including the stdlib for CPython 3.7.12.\nPyPy3.8, which is an interpreter supporting the syntax and the features of\nPython 3.8, including the stdlib for CPython 3.8.12. Since this is our\nfirst release of the interpreter, we relate to this as \"beta\" quality. We\nwelcome testing of this version, if you discover incompatibilites, please\nreport them so we can gain confidence in the version.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. Highlights of the release, since the release of 7.3.5 in May 2021,\ninclude:\n\n\nWe have merged a backend for HPy, the better C-API interface. The backend\nimplements HPy version 0.0.3.\nTranslation of PyPy into a binary, known to be slow, is now about 40%\nfaster. On a modern machine, PyPy3.8 can translate in about 20 minutes.\nPyPy Windows 64 is now available on conda-forge, along with nearly 700\ncommonly used binary packages. This new offering joins the more than 1000\nconda packages for PyPy on Linux and macOS. Many thanks to the conda-forge\nmaintainers for pushing this forward over the past 18 months.\nSpeed improvements were made to io, sum, _ssl and more. These\nwere done in response to user feedback.\nThe 3.8 version of the release contains a beta-quality improvement to the\nJIT to better support compiling huge Python functions by breaking them\nup into smaller pieces.\nThe release of Python3.8 required a concerted effort. We were greatly\nhelped by @isidentical (Batuhan Taskaya) and other new contributors.\nThe 3.8 package now uses the same layout as CPython, and many of the\nPyPy-specific changes to sysconfig, distutils.sysconfig, and\ndistutils.commands.install.py have been removed. The stdlib now\nis located in <base>/lib/pypy3.8 on posix systems, and in\n<base>/Lib on Windows. The include files on windows remain the same.\nOn posix they are in <base>/include/pypy3.8. Note we still use the\npypy prefix to prevent mixing the files with CPython (which uses\npython.\n\n\nWe recommend updating. You can find links to download the v7.3.6 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better. Since the\nprevious release, we have accepted contributions from 7 new contributors,\nthanks for pitching in, and welcome to the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a CFFI / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\nsoon 3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 64 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\nPyPy does support Windows 32-bit and ARM 32 bit processors, but does not\nrelease binaries. Please reach out to us if you wish to sponsor releases for\nthose platforms.\n\n\nWhat else is new?\nFor more information about the 7.3.6 release, see the full changelog.\nPlease update, and continue to help us make PyPy better.\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/10/pypy-v736-release.html"
    },
    {
      "title": "Better JIT Support for Auto-Generated Python Code",
      "text": "Performance Cliffs\nA common bad property of many different JIT compilers is that of a \"performance\ncliff\": A seemingly reasonable code change, leading to massively reduced\nperformance due to hitting some weird property of the JIT compiler that's not\neasy to understand for the programmer (e.g. here's a blog post about the fix of\na performance cliff when running React on\nV8). Hitting a performance cliff as a\nprogrammer can be intensely frustrating and turn people off from using PyPy\naltogether. Recently we've been working on trying to remove some of PyPy's\nperformance cliffs, and this post describes one such effort.\nThe problem showed up in an issue\nwhere somebody found the performance\nof their website using Tornado a lot\nworse than what various benchmarks suggested. It took some careful digging to\nfigure out what caused the problem: The slow performance was caused by the huge\nfunctions that the Tornado templating engine creates. These functions lead the\nJIT to behave in unproductive ways. In this blog post I'll describe why the\nproblem occurs and how we fixed it.\nProblem\nAfter quite a bit of debugging we narrowed down the problem to the following\nreproducer: If you render a big HTML template\n(example)\nusing the Tornado templating engine, the template rendering is really not any\nfaster than CPython. A small template doesn't show this behavior, and other\nparts of Tornado seem to perform well. So we looked into how the templating\nengine works, and it turns out that the templates are compiled into Python\nfunctions. This means that a big template can turn into a really enormous Python\nfunction (Python version of the\nexample).\nFor some reason really enormous Python functions aren't handled particularly\nwell by the JIT, and in the next section I'll explain some the background that's\nnecessary to understand why this happens.\nTrace Limits and Inlining\nTo understand why the problem occurs, it's necessary to understand how PyPy's\ntrace limit and inlining works. The tracing JIT has a maximum trace length built\nin, the reason for that is some limitation in the compact encoding of traces in\nthe JIT. Another reason is that we don't want to generate arbitrary large chunks\nof machine code. Usually, when we hit the trace limit, it is due to inlining.\nWhile tracing, the JIT will inline many of the functions called from the\noutermost one. This is usually good and improves performance greatly, however,\ninlining can also lead to the trace being too long. If that happens, we\nwill mark a called function as uninlinable. The next time we trace the outer\nfunction we won't inline it, leading to a shorter trace, which hopefully fits\nthe trace limit.\n\nIn the diagram above we trace a function f, which calls a function g, which\nis inlined into the trace. The trace ends up being too long, so the JIT\ndisables inlining of g. The next time we try to trace f the trace will\ncontain a call to g instead of inlining it. The trace ends up being not too\nlong, so we can turn it into machine code when tracing finishes.\nNow we know enough to understand what the problem with automatically generated\ncode is: sometimes, the outermost function itself\ndoesn't fit the trace limit, without any inlining going on at all. This is\nusually not the case for normal, hand-written Python functions. However, it can\nhappen for automatically generated Python code, such as the code that the\nTornado templating engine produces.\nSo, what happens when the JIT hits such a huge function? The function is traced\nuntil the trace is too long. Then the trace limits stops further tracing. Since\nnothing was inlined, we cannot make the trace shorter the next time by disabling\ninlining. Therefore, this happens again and again, the next time we trace the\nfunction we run into exactly the same problem. The net effect is that the\nfunction is even slowed down: we spend time tracing it, then stop tracing and\nthrow the trace away. Therefore, that effort is never useful, so the resulting\nexecution can be slower than not using the JIT at all!\nSolution\nTo get out of the endless cycle of useless retracing we first had the idea of\nsimply disabling all code generation for such huge functions, that produce too long\ntraces even if there is no inlining at all. However, that lead to disappointing\nperformance in the example Tornado program, because important parts of the code\nremain always interpreted.\nInstead, our solution is now as follows: After we have hit the trace limit and\nno inlining has happened so far, we mark the outermost function as a source of huge\ntraces. The next time we trace such a function, we do so in a special mode. In\nthat mode, hitting the trace limit behaves differently: Instead of stopping the\ntracer and throwing away the trace produced so far, we will use the unfinished\ntrace to produce machine code. This trace corresponds to the first part of the\nfunction, but stops at a basically arbitrary point in the middle of the\nfunction.\nThe question is what should happen when execution\nreaches the end of this unfinished trace. We want to be able to cover more of\nthe function with machine code and therefore need to extend the trace\nfrom that point on. But we don't want to do that too\neagerly to prevent lots and lots of machine code being generated. To achieve\nthis behaviour we add a guard to the end of the unfinished trace, which will\nalways fail. This has the right behaviour: a failing guard will transfer control\nto the interpreter, but if it fails often enough, we can patch it to jump to\nmore machine code, that starts from this position. In that way, we can slowly\nexplore the full gigantic function and add all those parts of the control flow\ngraph that are actually commonly executed at runtime.\n\nIn the diagram we are trying to trace a huge function f, which leads to\nhitting the trace limit. However, nothing was inlined into the trace, so\ndisabling inlining won't ensure a successful trace attempt the next time.\nInstead, we mark f as \"huge\". This has the effect that when we trace it again\nand are about to hit the trace limit, we end the trace at an arbitrary point by\ninserting a guard that always fails.\n\nIf this guard failure is executed often enough, we might patch the guard and\nadd a jump to a further part of the function f. This can continue potentially\nseveral times, until the trace really hits and end points (for example by\nclosing the loop and jumping back to trace 1, or by returning from f).\nEvaluation\nSince this is a performance cliff that we didn't observe in any of our\nbenchmarks ourselves, it's pointless to look at the\neffect that this improvement has on existing benchmarks \u2013 there shouldn't and\nindeed there isn't any.\nInstead, we are going to look at a micro-benchmark that came out of the\noriginal bug report, one that simply renders a big artificial Tornado template\n200 times. The code of the micro-benchmark can be found\nhere.\nAll benchmarks were run 10 times in new processes. The means and standard\ndeviations of the benchmark runs are:\n\n\n\nImplementation\nTime taken (lower is better)\n\n\n\n\nCPython 3.9.5\n14.19 \u00b1 0.35s\n\n\nPyPy3 without JIT\n59.48 \u00b1 5.41s\n\n\nPyPy3 JIT old\n14.47 \u00b1 0.35s\n\n\nPyPy3 JIT new\n4.89 \u00b1 0.10s\n\n\n\nWhat we can see is that while the old JIT is very helpful for this\nmicro-benchmark, it only brings the performance up to CPython levels, not\nproviding any extra benefit. The new JIT gives an almost 3x speedup.\nAnother interesting number we can look at is how often the JIT started a trace,\nand for how many traces we produced actual machine code:\n\n\n\nImplementation\nTraces Started\nTraces sent to backend\nTime spent in JIT\n\n\n\n\nPyPy3 JIT old\n216\n24\n0.65s\n\n\nPyPy3 JIT new\n30\n25\n0.06s\n\n\n\nHere we can clearly see the problem: The old JIT would try tracing the\nauto-generated templating code again and again, but would never actually produce\nany machine code, wasting lots of time in the process. The new JIT still traces a\nfew times uselessly, but then eventually converges and stops emitting machine\ncode for all the paths through the auto-generated Python code.\n\n\nRelated Work\nTim Felgentreff pointed me to the fact that\nTruffle also has a\nmechanism\nto slice huge methods into smaller compilation units (and I am sure other JITs\nhave such mechanisms as well).\nConclusion\nIn this post we've described a performance cliff in PyPy's JIT, that of really\nbig auto-generated functions which hit the trace limit without inlining, that we\nstill want to generate machine code for. We achieve this by chunking up the\ntrace into several smaller traces, which we compile piece by piece. This is not\na super common thing to be happening \u2013 otherwise we would have run into and\nfixed it earlier \u2013 but it's still good to have a fix now.\nThe work\ndescribed in this post tiny bit experimental still, but we will release it as\npart of the upcoming 3.8 beta release, to get some more experience with it.\nPlease grab a 3.8 release\ncandidate,\ntry it out and let us know your observations, good and bad!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/09/jit-auto-generated-code.html"
    },
    {
      "title": "#pypy IRC moves to Libera.Chat",
      "text": "Following the example of many other FOSS projects, the PyPy team has\ndecided to move its official #pypy IRC channel from Freenode to\nLibera.Chat: irc.libera.chat/pypy\nThe core devs will no longer be present on the Freenode channel, so we recommend to\njoin the new channel as soon as possible.\nwikimedia.org has a\nnice guide on\nhow to setup your client to migrate from Freenode to Libera.Chat.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/05/pypy-irc-moves-to-libera-chat.html"
    },
    {
      "title": "PyPy v7.3.5: bugfix release of python 2.7 and 3.7",
      "text": "PyPy v7.3.5: release of 2.7 and 3.7\nWe are releasing a PyPy 7.3.5 with bugfixes for PyPy 7.3.4, released April 4.\nPyPy 7.3.4 was the first release that runs on windows 64-bit, so that support\nis still \"beta\". We are releasing it in the hopes that we can garner momentum\nfor its continued support, but are already aware of some problems, for instance\nit errors in the NumPy test suite (issue 3462). Please help out with testing\nthe releae and reporting successes and failures, financially supporting our\nongoing work, and helping us find the source of these problems.\n\nThe new windows 64-bit builds improperly named c-extension modules\nwith the same extension as the 32-bit build (issue 3443)\nUse the windows-specific PC/pyconfig.h rather than the posix one\nFix the return type for _Py_HashDouble which impacts 64-bit windows\nA change to the python 3.7 sysconfig.get_config_var('LIBDIR') was wrong,\nleading to problems finding libpypy3-c.so for embedded PyPy (issue 3442).\nInstantiate distutils.command.install schema for PyPy-specific\nimplementation_lower\nDelay thread-checking logic in greenlets until the thread is actually started\n(continuation of issue 3441)\nFour upstream (CPython) security patches were applied:\n\nBPO 42988 to remove pydoc.getfile\nBPO 43285 to not trust the PASV response in ftplib.\nBPO 43075 to remove a possible ReDoS in urllib AbstractBasicAuthHandler\nBPO 43882 to sanitize urls containing ASCII newline and tabs in\nurllib.parse\n\n\nFix for json-specialized dicts (issue 3460)\nSpecialize ByteBuffer.setslice which speeds up binary file reading by a\nfactor of 3\nWhen assigning the full slice of a list, evaluate the rhs before clearing the\nlist (issue 3440)\nOn Python2, PyUnicode_Contains accepts bytes as well as unicode.\nFinish fixing _sqlite3 - untested _reset() was missing an argument\n(issue 3432)\nUpdate the packaged sqlite3 to 3.35.5 on windows. While not a bugfix, this\nseems like an easy win.\n\nWe recommend updating. These fixes are the direct result of end-user bug\nreports, so please continue reporting issues as they crop up.\nYou can find links to download the v7.3.5 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our renovated blog site via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a CFFI / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\nsoon 3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32/64 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\nPyPy does support ARM 32 bit processors, but does not release binaries.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/05/pypy-v735-release.html"
    },
    {
      "title": "Checksums",
      "text": "Here are the checksums\npypy3.8-v3.7.7 sha256:\ncbd44e0a9146b3c03a9d14b265774a848f387ed846316c3e984847e278d0efd3  pypy3.8-v7.3.7-aarch64.tar.bz2\ndfb9d005f0fc917edc60fd618143e4934c412f9168b55166f5519ba0a3b1a835  pypy3.8-v7.3.7-linux32.tar.bz2\n5dee37c7c3cb8b160028fbde3a5901c68043dfa545a16794502b897d4bc40d7e  pypy3.8-v7.3.7-linux64.tar.bz2\n1f044fe7bbdd443b7913ecf554683dab6dade5dcd7f47d4e6d01f4bb4cf84836  pypy3.8-v7.3.7-osx64.tar.bz2\nae7d6a76490b317a74b87788d596610c7ffd0ae2d3ffa2433d5bb5300f6b4b77  pypy3.8-v7.3.7-s390x.tar.bz2\n21ae339f4f5016d6ca7300305f3e3b554373835cb3c39a9041fe30e6811c80c6  pypy3.8-v7.3.7-src.tar.bz2\naa9aa0a800d06048d301fbafa7892ff8978e2d63b23cc23a147f2fd1fd288baf  pypy3.8-v7.3.7-src.zip\n8ceb03d2f7b73c6ce0758290bc42ba366a45c46e033eda36f1779d957a905735  pypy3.8-v7.3.7-win64.zip\npypy3.7-v3.7.7 sha256:\na1a84882525dd574c4b051b66e9b7ef0e132392acc2f729420d7825f96835216  pypy3.7-v7.3.7-aarch64.tar.bz2\n0ab9e2e8ae1ac463bb811b9d3ba24d138f41f7378c17ca9e2d8dee51bf151d19  pypy3.7-v7.3.7-linux32.tar.bz2\n8332f923755441fedfe4767a84601c94f4d6f8475384406cb5f259ad8d0b2002  pypy3.7-v7.3.7-linux64.tar.bz2\nedc9df7d0f7c56f7ee05b24117bdb6c03aa65e768471e210c05ccdbbfd11a866  pypy3.7-v7.3.7-osx64.tar.bz2\n7f91efc65a69e727519cc885ca6351f4bfdd6b90580dced2fdcc9ae1bf10013b  pypy3.7-v7.3.7-s390x.tar.bz2\n2ed02ac9e710859c41bc82deafb08619792bb9a27eeaa1676c741ededd214dd7  pypy3.7-v7.3.7-src.tar.bz2\n240ecf56c50b190cc7b728b07fc535be4b3d70a65406d0d8440edc02df4cce17  pypy3.7-v7.3.7-src.zip\n53505dc0b57590290efd7656117ee5384bcd036f7f7c4f0bc3f5cd10299037d1  pypy3.7-v7.3.7-win64.zip\npypy3.8-v3.7.6 sha256:\n704d5303096e8a3173e73435f3bb204e31a8bf02ed5ba617a4a0f1e7491edf50  pypy3.8-v7.3.6-aarch64.tar.bz2\ne857a04a76285f0ef5bae84f6f5e9943ca415d499204c531b1c33fe8f015b48d  pypy3.8-v7.3.6-linux32.tar.bz2\n8579ea990e95d2b7e101ef47fd9ebf25a9500d5086e8f708c43f9bae83306ece  pypy3.8-v7.3.6-linux64.tar.bz2\n8195e52a20cf2a4f42c2d7e4969fbf44fe349c1f80f758e20525dd0f8c134bec  pypy3.8-v7.3.6-osx64.tar.bz2\na36208d5e950ec4b630b33d0aede8ca3da383d973fc5ca387082c7e5bad8d245  pypy3.8-v7.3.6-s390x.tar.bz2\nf234c56eb0d4ab0afb196232fb38cd1ca8e19b1c65cf7b65eb691695499be259  pypy3.8-v7.3.6-src.tar.bz2\n055caaab4171e29915aaad602c9a49fa46e2b50a3f56c650772e31467c541858  pypy3.8-v7.3.6-src.zip\n1b216fd75f8f0a48633cc21dce7d6f25ba65016142df758842e1df661269b458  pypy3.8-v7.3.6-win64.zip\n\n\n59c299e9657334d651e2154c77490a743cb507f4f39344f934b2975ca91b4b2f  pypy3.8-v7.3.6rc3-aarch64.tar.bz2\n6cd36eb9857d6f7022099300c70666eb706f1e06b404234ea929a341fee40b68  pypy3.8-v7.3.6rc3-linux32.tar.bz2\nacdbc39ade2ef2cf2b4bcf0eb387ec0ef0d257175751d32e9d730886405439d0  pypy3.8-v7.3.6rc3-linux64.tar.bz2\n18fdba4a6c54c7df6fe2521858046ba865261c0e89557c4b53ef37eb7e562806  pypy3.8-v7.3.6rc3-osx64.tar.bz2\n128ede0f5565b626431755d58eb632362c748508e53777d32184eba5da8fdb6d  pypy3.8-v7.3.6rc3-s390x.tar.bz2\n0cb9c517a96850c4fba0494ee10b35e87861d71d8b1387e0588c316fa21230ee  pypy3.8-v7.3.6rc3-src.tar.bz2\n54704168785a6b22580d46a4a39f5a2c3f81e5d9f0c8e5ba906ac01603d42cbf  pypy3.8-v7.3.6rc3-src.zip\n1bd65ab6c82a696f2dcecd9b37679b474eadd149d96aab30438642236a1f7136  pypy3.8-v7.3.6rc3-win64.zip\n\n8ec2b28c6f1558a6abd0ce0a6fb504253b43b013a750c08c1e74470631afc1dd  pypy3.8-v7.3.6rc2-aarch64.tar.bz2\n008e9a9336108821f0080011aafe54a71e42ffffb7223d5183e610f689a0f8aa  pypy3.8-v7.3.6rc2-linux32.tar.bz2\nb1069fc7b08c2a230630f55f155c3ea016038471490ff0be020f850c5a8ec0cc  pypy3.8-v7.3.6rc2-linux64.tar.bz2\n4298d6b1a8333746c43dd313eb6ccd64f11b3dde795921d07f02c8e32d1ac44b  pypy3.8-v7.3.6rc2-osx64.tar.bz2\n9f3f7bb2842e626a85c8b314a3af959f98dc4a57fc0169c98b566b6fe645ea39  pypy3.8-v7.3.6rc2-s390x.tar.bz2\na9c3835e37e84a7667e3e548a176986a77663612d30594c7c4877ce0e712c6c9  pypy3.8-v7.3.6rc2-src.tar.bz2\ncae1f0a13b0da3b9db87141e662c3db73564f8fa4e4f1dab2d838341bf8bacc1  pypy3.8-v7.3.6rc2-src.zip\n6415bfd8afb6cef9cd7666de60f58d7fbbabae92042a9c1f3ce5e8ffe9ba4a26  pypy3.8-v7.3.6rc2-win64.zip\n\n18308f227c02ecb84ad21ed4a51bba8472acafe20386caef7ada0058d2d5a243  pypy3.8-v7.3.6rc1-aarch64.tar.bz2\n9b16a894477cbdb1275ab253d7bc71e8d64ad7d12dd61c835242fdac2cdf6cc7  pypy3.8-v7.3.6rc1-linux32.tar.bz2\n2abcd2a21f17216613c941a6bf6e26b395b089b9aa8f227af9e1b55c86d6d732  pypy3.8-v7.3.6rc1-linux64.tar.bz2\nd3aebc5c862e223606e3a79c245a748da7b9aa7d0206a2400e6c7d906676ef34  pypy3.8-v7.3.6rc1-osx64.tar.bz2\ne5013c21d21ca0eb16bc2e12c4093ec3095150b606830fb10f0c588629412b37  pypy3.8-v7.3.6rc1-s390x.tar.bz2\n999747cb4eacbc23c14e9f71d42c784c35cf45b52a7de9113c6db0811300e526  pypy3.8-v7.3.6rc1-src.tar.bz2\n3c9010fb3d1074c1ac350f0dbc8b215c53b2ab8ca3440d9ca4e903800e2ef1ce  pypy3.8-v7.3.6rc1-src.zip\ncef32837d4ab2cd9fbb6173472b633c6996f6a7915d89c66f87f0f0c69edcda2  pypy3.8-v7.3.6rc1-win64.zip\npypy3.7-v7.3.6 sha256:\nd446b6987eeaa03d706603863e83d6b99df69232cf1e06d3ee5706add6a84cd6  pypy3.7-v7.3.6-aarch64.tar.bz2\n459e77c845b31fa9367f7b1b1122155f0ba7888b1d4ce4455c35d2111eeeb275  pypy3.7-v7.3.6-linux32.tar.bz2\nc41d07063b1d002a91ad2a0763b4baaca2b306ec635889c2e4826e706cc7f9ca  pypy3.7-v7.3.6-linux64.tar.bz2\n26f0c5c2a5f4a2ce35281d2fa760aa10715300dd110387eac43699a78ed32365  pypy3.7-v7.3.6-osx64.tar.bz2\n3659bf96a177a53426ffc38d3619c6ee307e600c80e924edc9cee604680c141d  pypy3.7-v7.3.6-s390x.tar.bz2\n9252ccaa130094205b3c7f0a2cad5adc0d9dfba31658ff3172f788dec1fdb348  pypy3.7-v7.3.6-src.tar.bz2\nc2385436004d7d8d8978650efff1c22512ed9f9808c83ddfd68fe8fe812eb879  pypy3.7-v7.3.6-src.zip\n341e69a369da5a1f4f69dbbd47e7dff5e745439b203e28c7afcf98308a24b003  pypy3.7-v7.3.6-win64.zip\n\n742fc6fa7bdc377e8a8c976f57ef643a9068a0427a5ffbb50f8ba32aa6986392  pypy3.7-v7.3.6rc3-aarch64.tar.bz2\nb5382404935dd09b8a7ac160b593729151c9c907e6df029e3a7f312c53b5038a  pypy3.7-v7.3.6rc3-linux32.tar.bz2\n33db78a3c9c9f78eaaf7f52c9c174b1e4c795e5d3294e8364002470a3ced0986  pypy3.7-v7.3.6rc3-linux64.tar.bz2\n3218ef597290ec2983c692a01a6fe9ba5ebf05b8e95fed5e8431b750ec588544  pypy3.7-v7.3.6rc3-osx64.tar.bz2\n4f555251083f633bf044a1bc68d6c50629a374d90f1bee66e245cfac0fdd86f5  pypy3.7-v7.3.6rc3-s390x.tar.bz2\nf0f047f046bec43e433ee08db460c267518eb5b7df1f4d4d6bc3fd735c06a3bc  pypy3.7-v7.3.6rc3-src.tar.bz2\na27d35e75c2486029502590ee862e02af2a3453fa685b42916d618cdbc250fd0  pypy3.7-v7.3.6rc3-src.zip\n67c2e0676b04bbb3bbcf13f5c1f6c97a420b576e362c4948bed0fcbbf64419ee  pypy3.7-v7.3.6rc3-win64.zip\n\n7c5877b27ece045af7603436d64c8589eadc920045341bb16c9a773b924b1dfc  pypy3.7-v7.3.6rc2-aarch64.tar.bz2\n1afe2650a79ea2f234576986e599d504c1f4ab7928a50e3360cdac3b900c04b3  pypy3.7-v7.3.6rc2-linux32.tar.bz2\nd590359ea1a674b51ea13c2a79d883db38b21c43494c986f90af1f34053111a6  pypy3.7-v7.3.6rc2-linux64.tar.bz2\nbd9a96b9c5c542ef36e1e01f0e1987140d54f7bf04f0434bf3a3b9efe166c912  pypy3.7-v7.3.6rc2-osx64.tar.bz2\n22cab4d077f39dc2ff74ebb0d4505e5e3a5b88f2b909643181f57d7b810391da  pypy3.7-v7.3.6rc2-s390x.tar.bz2\n064e4f9fa408bacb67829782d95e2206b20319ae5b15e85993c76532350f57e8  pypy3.7-v7.3.6rc2-src.tar.bz2\n4071597a7450fb0d886005c82c52ed7773e9b0c2015bc93968850071d3195f6d  pypy3.7-v7.3.6rc2-src.zip\n6c6ac71a616882a53648d49e3b20dd1991c08e39a422e650cd58e2f12eecf19c  pypy3.7-v7.3.6rc2-win64.zip\n\n7cfb96afb7aa7478516c1747da77616edf92b46fda56570bcc3117bed46364c1  pypy3.7-v7.3.6rc1-aarch64.tar.bz2\n8079707602a24ab1b61f8982c8ef858f2780e60c08e02354c377d428326f57dd  pypy3.7-v7.3.6rc1-linux32.tar.bz2\nc40b7859933e14ca398e4eba0f70f9dbd521def5279acb4fc7c897d41ac0ac60  pypy3.7-v7.3.6rc1-linux64.tar.bz2\n8d9fde2810f84564902cb37d2d8f7294e5c3ea1fd664ab186864c71edb517d83  pypy3.7-v7.3.6rc1-osx64.tar.bz2\n8c4db2df86239c3e1fa5fb8a4efa5f5ec1f4d55f48ea92a01bd73bdce7fdf9bb  pypy3.7-v7.3.6rc1-s390x.tar.bz2\n25b980da5a5ca89a67e3752dfb1bb6ee3cd0804b7961d0a12e2f9180afe5bd07  pypy3.7-v7.3.6rc1-src.tar.bz2\nc2d21937db476d9c2d86f1e8622998278599f0cadda43a6335c6c7ada5403fec  pypy3.7-v7.3.6rc1-src.zip\na8d8a861dbff630f902d167da202b654e700b802b1c77643723cd246cef0b2ff  pypy3.7-v7.3.6rc1-win64.zip\npypy2.7-v7.3.6 sha256:\n90e9aafb310314938f54678d4d6d7db1163b57c9343e640b447112f74d7f9151  pypy2.7-v7.3.6-aarch64.tar.bz2\n7a1145f3a278ffab4da0e2d4c4bd024ab8d67106a502e4bb7f6d67337e7af2b7  pypy2.7-v7.3.6-linux32.tar.bz2\n82127f43fae6ce75d47d6c4539f8c1ea372e9c2dbfa40fae8b58351d522793a4  pypy2.7-v7.3.6-linux64.tar.bz2\n9a97de82037d4be1949ec0c35a4d638ba635e8b34948549ae2fa08abd2cbaa8c  pypy2.7-v7.3.6-osx64.tar.bz2\nbb29ecbe1f4a05045f0804b3e741267fc2db742249747b36cdbbd18866c15f04  pypy2.7-v7.3.6-s390x.tar.bz2\n0114473c8c57169cdcab1a69c60ad7fef7089731fdbe6f46af55060b29be41e4  pypy2.7-v7.3.6-src.tar.bz2\ncd88f99eccce3b9921a3c7fa452b25d7b60d87ff580bb03237bb1cd0fe2dd031  pypy2.7-v7.3.6-src.zip\nfcc8f6b3b472a77eaa754951f288fe234b4953bfba845888dd839b9b862cb891  pypy2.7-v7.3.6-win64.zip\n\n\ne92e4ba12a62f053e70799e463c7fcb2663b9fa270a16764250385024180cde4  pypy2.7-v7.3.6rc3-aarch64.tar.bz2\n918cf465e1339adcc66d9829b711e30d6a78d764ce74d79407ce35222f24e569  pypy2.7-v7.3.6rc3-linux32.tar.bz2\n21d9ed5a80aee8c320321b32eb3ca0bc89d630646a7371ee560c15296e68e4aa  pypy2.7-v7.3.6rc3-linux64.tar.bz2\ndcb0f049626b47d0bef1ff4f6d19c43b92f7c99a2cf2032afcbf3456b0e00425  pypy2.7-v7.3.6rc3-osx64.tar.bz2\n648e6e02e31d0ee17428f90da7fc938c2b6d0a8bd790ca73887c94a1016013d7  pypy2.7-v7.3.6rc3-s390x.tar.bz2\n0b868fe3b6c5a1a498b558395876a5d9cd3f0add649d5c281542db31a086c16b  pypy2.7-v7.3.6rc3-src.tar.bz2\neec6ec44cb9e4da0a29118fe98d4c289374af617e5279a77f6759a9713b68d2d  pypy2.7-v7.3.6rc3-src.zip\n47f9003c5909271c3ee4ce81de3703e2f17e20d7eba7d7328e8dc29407107b3d  pypy2.7-v7.3.6rc3-win64.zip\n\n9de5474ae55d31b02b9d43be26d7b3ea70e24e6e8a24bdc1d2ee396e191f315d  pypy2.7-v7.3.6rc2-aarch64.tar.bz2\n85a57d385a0e6072dfcf979654160fecb3f7d3d7a43352a28dff2c9dd63c7b01  pypy2.7-v7.3.6rc2-linux32.tar.bz2\n5e5800b1dcc705476bdc1bb6a195e857390d3fafc6406ba27513bff461cfadf7  pypy2.7-v7.3.6rc2-linux64.tar.bz2\nc6cb5bc6107bdbbf18a18db5b143a9d0476c6578f2d35792c49274d14f6f55ab  pypy2.7-v7.3.6rc2-osx64.tar.bz2\na490ab50a846c5587d525aba6ec6cbaeca758e9c6c6941ea0a1738bb78d32b22  pypy2.7-v7.3.6rc2-s390x.tar.bz2\n1e3870ba5ca5567e4808893ca3361e79f1ba02424059e4459936810ff304ba63  pypy2.7-v7.3.6rc2-src.tar.bz2\n38d18c15a64950822a404e98b9fba8aac671671e4d51553a60923de5992a6ddd  pypy2.7-v7.3.6rc2-src.zip\n965f3581e53de1d55f150d78aa9d90b7717a243be494b78d9b88b30ab4a1a8be  pypy2.7-v7.3.6rc2-win64.zip\n\nb2957fc3a3fe3957529fdb3e0e85965d46f4b7c09e4101237869f34ddfe5f0d4  pypy2.7-v7.3.6rc1-aarch64.tar.bz2\n37b9c8d41b5ba85b8ab9defd86da98b842f975d72c473bf92c3c1143a9c293cf  pypy2.7-v7.3.6rc1-linux32.tar.bz2\nb83967849db84c6e7b7c80b2135788da9c235a89a689729fd044b58d1d92c12f  pypy2.7-v7.3.6rc1-linux64.tar.bz2\n63a57129987f54ee692129b53fdf13d635cb6097dc0a1c8cd77f255fc95edda4  pypy2.7-v7.3.6rc1-osx64.tar.bz2\n187e9de4fc4d7edc332275031a40f0de8dc882050b14d5e9b588808c51efedf9  pypy2.7-v7.3.6rc1-s390x.tar.bz2\nbe979c8742181d5646ee1b78eac467612cf61484713ae6862e2b3475b4325b98  pypy2.7-v7.3.6rc1-src.tar.bz2\nc746176c507128e8e5aca14e5a0eaa101955b7cc860ceeba8b20f4f011da4061  pypy2.7-v7.3.6rc1-src.zip\nc515b46bccf1b56fd2f7761a9e3984aa6d56843e848eae67a28fd58fb158a5a9  pypy2.7-v7.3.6rc1-win64.zip\npypy3.7-v7.3.5 sha256:\n85d83093b3ef5b863f641bc4073d057cc98bb821e16aa9361a5ff4898e70e8ee  pypy3.7-v7.3.5-aarch64.tar.bz2\n3dd8b565203d372829e53945c599296fa961895130342ea13791b17c84ed06c4  pypy3.7-v7.3.5-linux32.tar.bz2\n9000db3e87b54638e55177e68cbeb30a30fe5d17b6be48a9eb43d65b3ebcfc26  pypy3.7-v7.3.5-linux64.tar.bz2\nb3a7d3099ad83de7c267bb79ae609d5ce73b01800578ffd91ba7e221b13f80db  pypy3.7-v7.3.5-osx64.tar.bz2\ndffdf5d73613be2c6809dc1a3cf3ee6ac2f3af015180910247ff24270b532ed5  pypy3.7-v7.3.5-s390x.tar.bz2\nd920fe409a9ecad9d074aa8568ca5f3ed3581be66f66e5d8988b7ec66e6d99a2  pypy3.7-v7.3.5-src.tar.bz2\n61bb9740eaac5dd93577e6b76e8bb1a998daa1df5314bc3b192e6803552e12ea  pypy3.7-v7.3.5-src.zip\n072bd22427178dc4e65d961f50281bd2f56e11c4e4d9f16311c703f69f46ae24  pypy3.7-v7.3.5-win64.zip\n\ndbf579f7eb5c527d37ecd43da88cbad02920881b608eb7486d70b4fa31bfc146  pypy3.7-v7.3.5rc3-aarch64.tar.bz2\nd2daf8b1966497d09be703b939bd0020394e0738095243396b3d5f87cef0d815  pypy3.7-v7.3.5rc3-linux32.tar.bz2\n1f9712fa86a50b1de00eb776f3e99033c2a7911dceaa8bc9daf77aa3d2a95842  pypy3.7-v7.3.5rc3-linux64.tar.bz2\nff1d1ce25f60d9474a950ccc90c5c4af376cba2b8af83b4e30cf33de97611c7e  pypy3.7-v7.3.5rc3-osx64.tar.bz2\n8e1c4035ba05161083105f452dfcd463c657085405444afc0acf26ceedb1e8a3  pypy3.7-v7.3.5rc3-s390x.tar.bz2\n9f7215f77106a6df0c201b6025dffdc605cd0731d60ee85a81343a51e64edc76  pypy3.7-v7.3.5rc3-src.tar.bz2\n21cae47ec47bead5d0c5e7a902a1bec85cab1eb30bf7190bd140309c20602110  pypy3.7-v7.3.5rc3-src.zip\n8e40ddc6e4360602597bed44f3ae227d20f8eaa0adfb6a728d10805f76456b74  pypy3.7-v7.3.5rc3-win64.zip\n\n\nc01e59167a26976e764f7b230f6febe0af59982911cd727c551191aed0a843c4  pypy3.7-v7.3.5rc2-aarch64.tar.bz2\n7f8e55f34bf9422576a501c22ae8b82d5d6ffcbf40251a9daf53b5d8d96c2f43  pypy3.7-v7.3.5rc2-linux32.tar.bz2\n93f9ccf44ec92145cf2fe17ac98a07f0adc08866b001c7f023b64a3729ed9710  pypy3.7-v7.3.5rc2-linux64.tar.bz2\n4902ac65329447f2451d2b2b264a12fb95d97a4bb734c75410d2b5abc6e6de52  pypy3.7-v7.3.5rc2-osx64.tar.bz2\nf0d4bbbe4000c836c17168cc709b233b6184039aad69bc9929c415a92bc462a9  pypy3.7-v7.3.5rc2-s390x.tar.bz2\nb1ac30e5e7cd8d04c4472b5c4a71a414d6b0cf08a2026fd1bfc84994598abfda  pypy3.7-v7.3.5rc2-src.tar.bz2\nc6c004550444c2f8749d7e34bcdfe404333b5f4bdf08af7745e28371c8358050  pypy3.7-v7.3.5rc2-src.zip\nea41d9e5cb94c7b9e7df2652b74fcc1018ce3e786c9636791b70e46d90e7e8ac  pypy3.7-v7.3.5rc2-win64.zip\n\n8dcd20e35e26bf92ce08fc8c97350acb4c773e19a78a89d3b4f28a8be63006d3  pypy3.7-v7.3.5rc1-aarch64.tar.bz2\n04573fd71618d5c26b0828dd306fa02e9eece8a33a020081e55b60d9a6bc6240  pypy3.7-v7.3.5rc1-linux32.tar.bz2\n97c1142f7ac99af03b2c56eb379af6e9ed4eef7d0d37675f4ca5ec33c841d62f  pypy3.7-v7.3.5rc1-linux64.tar.bz2\nf4893667f0b978deb891b0b7d91a1117e25299f19c65b31281c40e87dea523d3  pypy3.7-v7.3.5rc1-osx64.tar.bz2\n2880cfa6349aebc5c28aff5df06cabb8c8733dc7090f7f36410eb9ff3def37bc  pypy3.7-v7.3.5rc1-s390x.tar.bz2\nddccb7e8b24523f3f0e31e6c34b3a61c260b895ac9c7567f560f8ceda675fef8  pypy3.7-v7.3.5rc1-src.tar.bz2\nf39baa99eb0cb4d1505cd43676f86c54cae142f88b9b875542520b8596368ba7  pypy3.7-v7.3.5rc1-src.zip\nab8c5e6bf756f6dda2eba5c2e8d65d8d5de9b3a2c54f2f7a3dfb4f111e40ba0d  pypy3.7-v7.3.5rc1-win64.zip\npypy2.7-7.3.5 sha256:\n8dc2c753f8a94eca1a304d7736c99b439c09274f492eaa3446770c6c32ed010e  pypy2.7-v7.3.5-aarch64.tar.bz2\n35bb5cb1dcca8e05dc58ba0a4b4d54f8b4787f24dfc93f7562f049190e4f0d94  pypy2.7-v7.3.5-linux32.tar.bz2\n4858b347801fba3249ad90af015b3aaec9d57f54d038a58d806a1bd3217d5150  pypy2.7-v7.3.5-linux64.tar.bz2\n8b10442ef31c3b28048816f858adde6d6858a190d9367001a49648e669cbebb6  pypy2.7-v7.3.5-osx64.tar.bz2\nb91aaa5819ba8af90799eed8eaaba87ceca1fd4dbcbcdb2defc6d313d663b5dd  pypy2.7-v7.3.5-s390x.tar.bz2\nc0444fd9873058c1c0d99e13a934e92285cb05992c9968bf523c32bf9bec0a9d  pypy2.7-v7.3.5-src.tar.bz2\nc67214acee357d383bb2716269663406611e17cee580026d6d7baa7891afa85b  pypy2.7-v7.3.5-src.zip\n0b90eded11ba89a526c4288f17fff7e75000914ac071bd6d67912748ae89d761  pypy2.7-v7.3.5-win64.zip\n\n0f83212202d51835dcedfdfe607fe157d1111a368f7f28738792417acd987c37  pypy2.7-v7.3.5rc3-aarch64.tar.bz2\n6dc2fec9894121cc75500c84509c869648e6fa95c8e8084c81bf17191d80ba8c  pypy2.7-v7.3.5rc3-linux32.tar.bz2\n8a918307a51a02ae222e71e2973a4d0dc520a3bae2d510a6571aaf53cf7cead7  pypy2.7-v7.3.5rc3-linux64.tar.bz2\n9376ba404009ce435e7b04a3c194f783b841464031607081081429f079797faa  pypy2.7-v7.3.5rc3-osx64.tar.bz2\nc95f5d5cef6181fe08f54824872c94f27177feb5d156fa6dae279a5b8228b13c  pypy2.7-v7.3.5rc3-s390x.tar.bz2\nb643dd908e6d07d703f388798e0355e3378a8157833680cbea55c3cf3e4256e2  pypy2.7-v7.3.5rc3-src.tar.bz2\nbaeafa81e445a5b6c8da8ec92c8587a11104f7e125478d669d9eaa45492b7b90  pypy2.7-v7.3.5rc3-src.zip\n21b21873124572043749bb5b19cc33a14ffbf6d8ea5e538006689cc4e3af3d5a  pypy2.7-v7.3.5rc3-win64.zip\n\n8250c8db8f227aec3d85f8866f8ad78d925ed338a5622f64c22d6a7fb0963b5a  pypy2.7-v7.3.5rc2-aarch64.tar.bz2\n978ed1e445809adbaa0ca593abd445384c28d72344bf67184b5cee5e0f76fc3c  pypy2.7-v7.3.5rc2-linux32.tar.bz2\na933976a2adc840d07be9ed4ac1dc1b1986fd68f875c4258ed214a2ce9f5f659  pypy2.7-v7.3.5rc2-linux64.tar.bz2\ncbdfe3f9e49cb96b5b182b19ce257a086dbb7204ba01c178db13b4e6272a3260  pypy2.7-v7.3.5rc2-osx64.tar.bz2\nda2bf8e5e8f03f10ffd8c7e970e20ff702a91fc44a6bd0de51f1a79401804e79  pypy2.7-v7.3.5rc2-s390x.tar.bz2\nb47ce66e8d716b22e7b78f1ec0e2d212a27afd355adcb94e00b6d76ffa9a513f  pypy2.7-v7.3.5rc2-src.tar.bz2\nb031352443dff2202fcc0ee131887a232214363af1d87ba35886dc683b18eb85  pypy2.7-v7.3.5rc2-src.zip\n47a355033a4c61e679f5ed34274a320adda8df2c27ed313bda0841dc8e11a354  pypy2.7-v7.3.5rc2-win64.zip\n\n4431bc2193f76b97add9726420c6d6ab14b46178e9cfeade5f596016b66b6549  pypy2.7-v7.3.5rc1-aarch64.tar.bz2\nb0d2432bf50bfeeb00e91e048db6df1bba40ca54b0d19d9f61db0f3a4e6e2bf5  pypy2.7-v7.3.5rc1-linux32.tar.bz2\n5a81b1e5733351a1e27e8072f474c60d24ab987dc1355873861b69961da425f5  pypy2.7-v7.3.5rc1-linux64.tar.bz2\nd2e3077b6c0a84e07af5e4c5eb9c883e54bf649ef982dd5310b3e8e68dfffc0e  pypy2.7-v7.3.5rc1-osx64.tar.bz2\n5d6a52bbed77855303dadf10a44c1f5e07920ad28948ecf6f13c57eed0c95f8b  pypy2.7-v7.3.5rc1-s390x.tar.bz2\n45639e3b398f1dbac54f35e2aebc4770432519dd8838e0190708f1dcfa945356  pypy2.7-v7.3.5rc1-src.tar.bz2\n67329cae37163b4838bb5768dd04ebc75ce1bbb0a62b74da404587f7344d80fc  pypy2.7-v7.3.5rc1-src.zip\n6d36595d6cf6f61c33c0e36ae47d9f84abe1ab99cee6cb910a2517d4d3db6cb0  pypy2.7-v7.3.5rc1-win64.zip\npypy3.7-7.3.4 sha256:\na4148fa73b74a091e004e1f378b278c0b8830984cbcb91e10fa31fd915c43efe  pypy3.7-v7.3.4-aarch64.tar.bz2\n04de1a2e80530f3d74abcf133ec046a0fb12d81956bc043dee8ab4799f3b77eb  pypy3.7-v7.3.4-linux32.tar.bz2\n09d7298b44a38648a87995ec06e1e093761644e50f547c8bb0b2d7f4fe433548  pypy3.7-v7.3.4-linux64.tar.bz2\n8a4f0e6c7e3845820202bf7f46b48e36886ceb820ff0767963fd74091c4f5d13  pypy3.7-v7.3.4-osx64.tar.bz2\n7d6fb180c359a66a158ef6e81eeca88fbabbb62656a1700f425a70db18de2a0f  pypy3.7-v7.3.4-s390x.tar.bz2\n74d3c1e79f3fc7d384ffb32d3d2a95c2d5f61b81091eccce12ac76030d96ad08  pypy3.7-v7.3.4-src.tar.bz2\n80d4da3aaeb8b4cc5e4e4ea747f2e468e9f448da549aa7ada4d59c24380cda43  pypy3.7-v7.3.4-src.zip\n0ff4e4653f1ff0653f105680eb101c64c857fa8f828a54a61b02f65c94b5d262  pypy3.7-v7.3.4-win64.zip\n\n647e34857d181e7560205eb877915b787836237929c7bd52860de626d5e85e9d  pypy3.7-v7.3.4rc2-aarch64.tar.bz2\ncfc661034347d79ba907078b4e3acea4f09d0de0eaf474c5bde173666319780c  pypy3.7-v7.3.4rc2-linux32.tar.bz2\ndcf1fa6dd5da4076f040ed4302a22c8da3838335e64cd118c29d69eb7d443d6b  pypy3.7-v7.3.4rc2-linux64.tar.bz2\nc9ecc213cdc3169ef230d85e49d9d073ffc1ba0a36bc1d8483f724e31b9d9d12  pypy3.7-v7.3.4rc2-osx64.tar.bz2\nfcc5c02382f67c7ee6f267b459131519b6a72e60ae370d6e398d54c0e07080f9  pypy3.7-v7.3.4rc2-s390x.tar.bz2\nf1257d4d8a3d84e84ff85c83f4f5bc2e126727d7595c536ccbe1a03a280c0df6  pypy3.7-v7.3.4rc2-src.tar.bz2\ndfab9881e2c42ae61115aa6ed77389f835094fd783dc08cf4dee1ebfdd4c1d47  pypy3.7-v7.3.4rc2-src.zip\nb62b7aad962a8c42895a13b08d68b32254934d6d1b1f5f1f02f762cbe111b035  pypy3.7-v7.3.4rc2-win64.zip\n\n958a562528d24fdb33b9fd12f2076f4b546dc218e0793324558560823234adb1  pypy3.7-v7.3.4rc1-aarch64.tar.bz2\nd05299744ac8c6f12bb3587541ce106f3a93d9ed64b0529c46e79b56efd27b24  pypy3.7-v7.3.4rc1-linux32.tar.bz2\nbb7ee16bdf7c1bbbca45d1228502a5c276be33e27e849525aa5a61c0eaec5b4a  pypy3.7-v7.3.4rc1-linux64.tar.bz2\n6d3aea12b744413c874e33ff456f6591049e12dc1a356d975dc0e29a047a151e  pypy3.7-v7.3.4rc1-osx64.tar.bz2\n8deb01eb54b95e480d2ee03ee9148ba0c1684b410165c198e9f68a015656246e  pypy3.7-v7.3.4rc1-src.tar.bz2\nbf247839954a4518327d5cbc9ab1a1b4296982c2fe78671d59a58373239e675e  pypy3.7-v7.3.4rc1-src.zip\n0819de5a5212bddef0f615f7ced03dfd9f5d4ee115ec3564119d45b6b447843f  pypy3.7-v7.3.4rc1-win64.zip\npypy2.7-7.3.4 sha256:\n9e741162ce486b14fbcf5aa377796d26b0529a9352fb602ee8b66c005f8420d1  pypy2.7-v7.3.4-aarch64.tar.bz2\n653cc3f0612399e494021027f4463d62639dffa4345736a16d0704f3f8a61d5f  pypy2.7-v7.3.4-linux32.tar.bz2\nd3f7b0625e770d9be62201765d7d2316febc463372fba9c93a12969d26ae03dd  pypy2.7-v7.3.4-linux64.tar.bz2\nee7bf42ce843596521e02c763408a5164d18f23c9617f1b8e032ce0675686582  pypy2.7-v7.3.4-osx64.tar.bz2\nf19b70ca5bd918d1349444be775bc2194c8165b0140e6e8b87c3ee101765a5ba  pypy2.7-v7.3.4-s390x.tar.bz2\nff9b928237767efe08ccfba79dae489519b3c768fb6e3af52d39c2a8a1c21ca4  pypy2.7-v7.3.4-src.tar.bz2\ne0811ecc272fee58e01b95c4c12f23b115a3e64075a1b50dcefe8faaa6cca869  pypy2.7-v7.3.4-src.zip\n1080012d7a3cea65182528259b51d52b1f61a3717377c2d9ba11ef36e06162d5  pypy2.7-v7.3.4-win64.zip\n\nf0a11bd48a01b27595e659c3a1b7fb936ac6e0a21574f1fc2f57fd032830342a  pypy2.7-v7.3.4rc2-aarch64.tar.bz2\n81dd5ac16b11f6f9ba0ff2536306dd85997a6cad86aa4e7971e7805264d61716  pypy2.7-v7.3.4rc2-linux32.tar.bz2\n077acdb14e797878341fc6f50d87a2f0c9b7d25215c6b2f73541bacb7730f64d  pypy2.7-v7.3.4rc2-linux64.tar.bz2\n6a220785a962c56db26dd56245aacb7cb6658879ecaad9ada04d26df56da172c  pypy2.7-v7.3.4rc2-osx64.tar.bz2\na3201493550457f932ddf743118635a7e8ff6b5c5fd69d0b8596dfeabcc5bffd  pypy2.7-v7.3.4rc2-s390x.tar.bz2\n1965dfc3de6fdae83bd954fed206111a020898708d8754705fb1312473be35bf  pypy2.7-v7.3.4rc2-src.tar.bz2\n1072727a4a948b16ccebb165015e43716ffc586f5249356c97c454b24aacb2dd  pypy2.7-v7.3.4rc2-src.zip\ne20f206ba8751d2c17ad80c66b7f4bd63c2f500cbfa9e8a3906cd7d77955e00f  pypy2.7-v7.3.4rc2-win64.zip\n\nee4894169260d3e4c55e06232c96d690e41d13e9f82f1512edcf6b8d960b695d  pypy2.7-v7.3.4rc1-aarch64.tar.bz2\nfd736003d5a7f5f2744269d67dc9a96005a5a2ceac8987007bd27ab57681c0f2  pypy2.7-v7.3.4rc1-linux32.tar.bz2\nec1cd67c28416c359dbe1caddf7ae7a0be10e3fbe6435150d39d4b7492469852  pypy2.7-v7.3.4rc1-linux64.tar.bz2\ncce4e360b31010e415e397ce8982535db482e36c0f13934eaa6d9e1e30eb2bc3  pypy2.7-v7.3.4rc1-osx64.tar.bz2\n84930e433a81f16dcf81b678c12167ef951cd74534ee1ee8e6b0b27b0a128e1d  pypy2.7-v7.3.4rc1-src.tar.bz2\n7bdc1e5431a7429bd2ec2853c86a68f09069f080b9765a87084904f52adab789  pypy2.7-v7.3.4rc1-src.zip\n02befc534dbcc2da6ad4c7e60735d977dc8b4f6901630eb599d1684cb86a58c7  pypy2.7-v7.3.4rc1-win64.zip\npypy3.7-7.3.3 sha256:\nee4aa041558b58de6063dd6df93b3def221c4ca4c900d6a9db5b1b52135703a8  pypy3.7-v7.3.3-aarch64.tar.bz2\n7d81b8e9fcd07c067cfe2f519ab770ec62928ee8787f952cadf2d2786246efc8  pypy3.7-v7.3.3-linux32.tar.bz2\n37e2804c4661c86c857d709d28c7de716b000d31e89766599fdf5a98928b7096  pypy3.7-v7.3.3-linux64.tar.bz2\nd72b27d5bb60813273f14f07378a08822186a66e216c5d1a768ad295b582438d  pypy3.7-v7.3.3-osx64.tar.bz2\n92000d90b9a37f2e9cb7885f2a872adfa9e48e74bf7f84a8b8185c8181f0502d  pypy3.7-v7.3.3-s390x.tar.bz2\nf6c96401f76331e474cca2d14437eb3b2f68a0f27220a6dcbc537445fe9d5b78  pypy3.7-v7.3.3-src.tar.bz2\n9e4756903b14c5f971989a2f5a4de6ee19b21a59f2a798b3ad2ad0e71b2582a5  pypy3.7-v7.3.3-src.zip\na282ce40aa4f853e877a5dbb38f0a586a29e563ae9ba82fd50c7e5dc465fb649  pypy3.7-v7.3.3-win32.zip\n\n54a1697d39f136c3e3961afbd58a049e10a5ed10e6d230e6729d696c226d5185  pypy3.7-v7.3.3rc2-aarch64.tar.bz2\n796c0b57b28850f9a212593f30baf7c241c0ed3fe857048d2ea50b3e13b9773b  pypy3.7-v7.3.3rc2-linux32.tar.bz2\nbe427afe0434ac42b4da997c841250c499286c57f1c1e9a764d49787bbeeda38  pypy3.7-v7.3.3rc2-linux64.tar.bz2\ne670772077ea400c8f276f8bea301a0c3fa0f037f7e174ae08b34d46e43ce433  pypy3.7-v7.3.3rc2-osx64.tar.bz2\nb230bfd935d6a4ecfaf890c91431b56cb53325ad988899542b178610f94d5970  pypy3.7-v7.3.3rc2-s390x.tar.bz2\nc4a7f8c8a00073de1f987562bed486c372005e021505d3847562966541e0ea6f  pypy3.7-v7.3.3rc2-src.tar.bz2\n26ba0babe260fbc9264c15070b129593ca871c7658a661eacf4c5e27507542f7  pypy3.7-v7.3.3rc2-src.zip\n53959607ea55de6ec5cf15227c195e3356d56629e91279ce26744cb3e392a863  pypy3.7-v7.3.3rc2-win32.zip\n\n45357c23a05bc4e4828c0c0964142a7c45f0bcc6653cae67837ff00a02ececb2  pypy3.7-v7.3.3rc1-aarch64.tar.bz2\n22c04f6984c986895999c73d845e57957d86ab788137e482b60f83aa4983e278  pypy3.7-v7.3.3rc1-linux32.tar.bz2\n2069912448749295537c2b381957c5e07dec103fc9a3322f2ce8a57b3fa6e60c  pypy3.7-v7.3.3rc1-linux64.tar.bz2\n9fbbf9cfb9ca699e00ea08aaec6248625541998c251033aa3e6d8c592c0a6ff9  pypy3.7-v7.3.3rc1-osx64.tar.bz2\nf502ed792c9da1531a413cd8a7c4c8158c649d7820cb4a910a5852866579c365  pypy3.7-v7.3.3rc1-s390x.tar.bz2\n6780d79e205768a5b2c1d6ecc9e1c4a8c05811cc6b130ed728ba1a53088e0406  pypy3.7-v7.3.3rc1-src.tar.bz2\nedaed54347b69d2a3037e427c60eb88050226cf082d26fff594221cbedab9cd8  pypy3.7-v7.3.3rc1-src.zip\n3c82f4569293dcff5085f0c61af1ba2671217256c58b6e6092629a406eee4fc5  pypy3.7-v7.3.3rc1-win32.zip\npypy3.6-7.3.3 sha256:\nbc82cf7f0182b942a2cfad4a0d167f364bfbf18f434e100a2fe62bc88547ac9b  pypy3.6-v7.3.3-aarch64.tar.bz2\nf183c61e66fd2c536a65695bd7ff770748c2884c235a589b9c6ac63690770c69  pypy3.6-v7.3.3-linux32.tar.bz2\n4fb85fdd516482cab727bb9473b066ff8fb672940dedf7ccc32bf92957d29e0a  pypy3.6-v7.3.3-linux64.tar.bz2\n84126fcb957f260de221244222152c981643144df1d817329781f555daa52e35  pypy3.6-v7.3.3-osx64.tar.bz2\n0de9c33ff3500c6e7fd273d0a6d341bc839b0298f697c4d6fe141f2b54c5c3e2  pypy3.6-v7.3.3-s390x.tar.bz2\na23d21ca0de0f613732af4b4abb0b0db1cc56134b5bf0e33614eca87ab8805af  pypy3.6-v7.3.3-src.tar.bz2\ndf534213c27c6ecc8e7d4f2a6950305301711ea3e132ec7a836959146761c9d8  pypy3.6-v7.3.3-src.zip\nb935253877b703d29b1b11f79e66944f1f88adb8a76f871abf765d4de9d25f8a  pypy3.6-v7.3.3-win32.zip\n\n58a35d069bc887c09f8106aec1c0da18241f887dc227bd9e31bd2819496b8256  pypy3.6-v7.3.3rc2-aarch64.tar.bz2\ne171477f56ada45ce64df6f91ad4961c13b674d268b8b16850d1bae5eda43393  pypy3.6-v7.3.3rc2-linux32.tar.bz2\ndf2f421c3782e09ca304f00afd79d7ac24224c3346b41ddae9ab919f4b243538  pypy3.6-v7.3.3rc2-linux64.tar.bz2\n1b2715c8bdf97bbe2135a13562aaeab3408c1459d714412a0b0c607309c5c48b  pypy3.6-v7.3.3rc2-osx64.tar.bz2\nd1eaa8ea52f8ce7b02ddc08cff56a64405cfdc7f657edd9bfbb8788484ab9c01  pypy3.6-v7.3.3rc2-s390x.tar.bz2\n3c91a1e911eee1baf9093dcb66899bd06a9ddc095ee60c51c2bca1626497148f  pypy3.6-v7.3.3rc2-src.tar.bz2\ne9e5dc879afcddc7ffea09500a092fe00c9070d8fd5008ef0342e0b77c9f9161  pypy3.6-v7.3.3rc2-src.zip\n7bfdc3544216003b96e76f133073084f2918c5cd29642211735c8507142d107a  pypy3.6-v7.3.3rc2-win32.zip\n\n9e65dff7a5bc34d32ea88b9436a9f9629542dd3eb8f948f49ecce40112530199  pypy3.6-v7.3.3rc1-aarch64.tar.bz2\n13a67079e78eaa01dcc2a8aa986a50944bc4bf42469c3c39e3ecb0f0cee31439  pypy3.6-v7.3.3rc1-linux32.tar.bz2\n17fb6dff3a5fd9d9e791ce1cd8ae9076e5f47b8b463b7575e4403f01656b0735  pypy3.6-v7.3.3rc1-linux64.tar.bz2\n2f62a9c9876d83a2bf04d8e5e1373aa7e0dcd1e523a58216e60f20329a536b9b  pypy3.6-v7.3.3rc1-osx64.tar.bz2\na652572f3c783c4c9cfae477a6a64584f2df39e4df75773131ab512e486d61f3  pypy3.6-v7.3.3rc1-s390x.tar.bz2\nbd5e6d6ba3bd9bc1a233c2dd77b518fd1d337a37670fe0e23edf837852254ee7  pypy3.6-v7.3.3rc1-src.tar.bz2\ne26c8c95e2d131507a08c3e8b8010e6dd366e8e9bf6e77db6844bc5145be1932  pypy3.6-v7.3.3rc1-src.zip\n773ffcabddc3bdc626318f24f0ba256153eca517775425b618c1c7b8b10f1680  pypy3.6-v7.3.3rc1-win32.zip\npypy2.7-7.3.3 sha256:\n23b145b7cfbaeefb6ee76fc8216c83b652ab1daffac490558718edbbd60082d8  pypy2.7-v7.3.3-aarch64.tar.bz2\nbfbc81874b137837a8ba8c517b97de29f5a336f7ec500c52f2bfdbd3580d1703  pypy2.7-v7.3.3-linux32.tar.bz2\nf412b602ccd6912ddee0e7523e0e38f4b2c7a144449c2cad078cffbdb66fd7b1  pypy2.7-v7.3.3-linux64.tar.bz2\nf34dc4f5ded1f6bcea05841aa9781b9307329e3ab755607917148568824ae0b0  pypy2.7-v7.3.3-osx64.tar.bz2\n8254a7fb98ea66c33324a403d06ccb052d616a4176ce0130591693ceeb011cf7  pypy2.7-v7.3.3-s390x.tar.bz2\nf63488051ba877fd65840bf8d53822a9c6423d947839023b8720139f4b6e2336  pypy2.7-v7.3.3-src.tar.bz2\n5ce67ea6afb0cf1a3e20bbd4bbd375e375f572d5325524f9c7760edf8521f029  pypy2.7-v7.3.3-src.zip\nb3e660dae8d25d8278fd6a0db77e76a16ac9a8c1dca22e7e103d39ed696dc69e  pypy2.7-v7.3.3-win32.zip\n\n4f2eee1d8ae2571d6fde76141237cf7717324dd6b6a1aa50036c42266d92cbce  pypy2.7-v7.3.3rc2-aarch64.tar.bz2\n79c741bd28f293820382f4ecd81414a327745956fa402a5dcfe38900e7520214  pypy2.7-v7.3.3rc2-linux32.tar.bz2\nb227698c4797170b7fdb427a56632fa7733695dd3b31fd404ce4c0939505f918  pypy2.7-v7.3.3rc2-linux64.tar.bz2\n451fca86c965e498ce2ada9474c36d316a627bd6aeeeb808b952a447c938c936  pypy2.7-v7.3.3rc2-osx64.tar.bz2\n83147a40ecc2ab39679129f7898756febd09422ee63a0074fb7f844964c189d8  pypy2.7-v7.3.3rc2-s390x.tar.bz2\n1d60d7f9662278ba59f34cd20c0332993c0bb117009309bc06bd3cb651318c36  pypy2.7-v7.3.3rc2-src.tar.bz2\n4810fb6761eccf6f3e6a14f7a8e4010548e551928fef27fb9482b0c7e3e501d5  pypy2.7-v7.3.3rc2-src.zip\n72a43db2c5bd639023adad2a5c9fd7d4db639c5269dcfeb19ef5b0576771ea9b  pypy2.7-v7.3.3rc2-win32.zip\n\n061be51e14fc5f16ce38a61b3873239a0a74b02af51be5930b52941bbb3e6eb2  pypy2.7-v7.3.3rc1-aarch64.tar.bz2\n395113ae0a9d1e352e5aef22b1d9e272b029b186d5e1c7e204dd6df044647fc1  pypy2.7-v7.3.3rc1-linux32.tar.bz2\n1e160ff884fdcdc3388b3c88a00ee54d0b11e7b3c94c4787a217eeea76da63e3  pypy2.7-v7.3.3rc1-linux64.tar.bz2\n761b6e9485dd218e63d231f351f908e74c6cc6bb38cc3b61992b92a0e5384f02  pypy2.7-v7.3.3rc1-osx64.tar.bz2\n72d62a3d0bfcb1693f44d5bc3601d528188838df9fbb885e3e18770f81f97e5a  pypy2.7-v7.3.3rc1-s390x.tar.bz2\n39fa3f6f0921785c4b44ab2e47777d64480737c710672f09913b2306a1430281  pypy2.7-v7.3.3rc1-src.tar.bz2\n6b5b466e74505e59985ff9583587a417a200ab2d41829b8c72c74daef4c0d44c  pypy2.7-v7.3.3rc1-src.zip\n403bce17882ca7f305fedd9f604f5657364e4ef76086064bbed0a31dfbf47155  pypy2.7-v7.3.3rc1-win32.zip\npypy3.6-7.3.2 sha256:\n164d6a0503c83dd328e1a6bf7fcb2b2e977c1d27c6fcc491a7174fd37bc32a12  pypy3.6-v7.3.2-aarch64.tar.bz2\n6fa871dedf5e60372231362d2ccb0f28f623d42267cabb49be11a3e10bee2726  pypy3.6-v7.3.2-linux32.tar.bz2\nd7a91f179076aaa28115ffc0a81e46c6a787785b2bc995c926fe3b02f0e9ad83  pypy3.6-v7.3.2-linux64.tar.bz2\nfd457bfeaf54aa69417b6aa4817df40e702dc8aaaf7e83ba005d391a1bddfa96  pypy3.6-v7.3.2-osx64.tar.bz2\n16afbaa245c016c054d9300c19433efcc76c50664ff2c86d913ff76ed0a729dc  pypy3.6-v7.3.2-s390x.tar.bz2\nfd6175fed63ff9fccd7886068078853078948d98afae9bd4f5554c6f7873c10d  pypy3.6-v7.3.2-src.tar.bz2\nedcbcd3598a91de3115f86550d1bc76ac46fc0a3e86a1e951769a993f6fbcbf0  pypy3.6-v7.3.2-src.zip\n13a39d46340afed20f11de24e9068968386e4bb7c8bd168662711916e2bf1da6  pypy3.6-v7.3.2-win32.zip\n\n62e525c6c71c8264c8476e2c4afe11d2aa07b71f9bcf6d694fc4aae27bfcbb66  pypy3.6-v7.3.2rc2-aarch64.tar.bz2\ne9de7036c663f08f06f760340c5d165d8bdecad159abd14d0d93d1bde714ed38  pypy3.6-v7.3.2rc2-linux32.tar.bz2\ne3ac3cf1560f8aee41e542bd999214cbbe0645a4786e4d8a5dc3d58b219429f3  pypy3.6-v7.3.2rc2-linux64.tar.bz2\n7995b74b190f619feb3f393620f63dd0f7cae9e8e298c0616bd184090c356c90  pypy3.6-v7.3.2rc2-osx64.tar.bz2\n9c09100e3302221dbe9776bb3f99e870a8404a2f6afd7a056fa3b7116f5ab013  pypy3.6-v7.3.2rc2-s390x.tar.bz2\nb7d4b3cf3ba7e7749421b1eb857be32d8e5fede124cb2a1d1e1bc606a437b4c5  pypy3.6-v7.3.2rc2-src.tar.bz2\nf5c4f219a974c69b949221082b789a455a67f9f6a37c173cb48a6246ab57f05c  pypy3.6-v7.3.2rc2-src.zip\n0555340fdd2e2fcbf114d1f2b57d798269dfccddf1b6419dbe3ce937927b0504  pypy3.6-v7.3.2rc2-win32.zip\n\n1c69cca7292e3c3ffcb7a09f5cdeb51d45e24dc75510b2c9bb410b8ffc57a579  pypy3.6-v7.3.2rc1-aarch64.tar.bz2\nd5738cffc11b364b5f0bf4883c2e1fd46431822f3bd126c7d8c83e9b5f0e6543  pypy3.6-v7.3.2rc1-linux32.tar.bz2\n41cab069841cfc713cc2d0526034f04fcbd741d67d70212926a3ff90754a39f5  pypy3.6-v7.3.2rc1-linux64.tar.bz2\nafabd1ea5a7da31df547c1d4b7028caef1dfaad0ba7e9dda81da2884dfe3062c  pypy3.6-v7.3.2rc1-osx64.tar.bz2\n9202fa080d821cca5fe788acfdee3020449e3c36df720ede89ef7389ad6d4a37  pypy3.6-v7.3.2rc1-src.tar.bz2\n8dc4d906720208d590133d580bc7976f7aca1fedf49c3dec1eba1fccb39e0bdc  pypy3.6-v7.3.2rc1-src.zip\n29d47b72cf417d12b23161d898dae38f48e48788733623ffb09807e913fbeb44  pypy3.6-v7.3.2rc1-win32.zip\npypy3.7-7.3.2 sha256:\nc5c35a37917f759c19e2a6b3df3b4d56298faa2fae83c143469bcbda42ca5dd2  pypy3.7-v7.3.2-aarch64.tar.bz2\n34c7e1c7bd06e437ad43cc90a20f9444be1f0a264d0955e32098294c30274784  pypy3.7-v7.3.2-linux32.tar.bz2\na285ddcbc909d68c648585fae4f33b0ba24961bb4e8fafe5874cf725d6e83df6  pypy3.7-v7.3.2-linux64.tar.bz2\n337dd4d9e529d2f221e0beb092236c18430e0564ab835c6bba425a1daf7c9958  pypy3.7-v7.3.2-osx64.tar.bz2\nd4ce71ebba148bf83c24fc963e8282c9b7f0c81fcf6b612301b8efe6bd7658d1  pypy3.7-v7.3.2-s390x.tar.bz2\n9274186eb0c28716a8c6134803b1df857bc3f496e25e50e605c4d95201c8817d  pypy3.7-v7.3.2-src.tar.bz2\n23363123c607058dac29995cf281c4609a8d8d278841a8f05ea8559bdb1678a8  pypy3.7-v7.3.2-src.zip\ne3c589be07760bc3042981c379b7fd1603e832a4db426075f09e090473846a96  pypy3.7-v7.3.2-win32.zip\n\n78fe46fa8706e325bd0bdb81d6f0865b7dae0ffb22a77c533a24fa960e885b1b  pypy3.7-v7.3.2rc2-aarch64.tar.bz2\n2ed3489e1ea42b1807e79ba46a2dfb2c763bdd4d15efac0fd8ba9cf05ab436bb  pypy3.7-v7.3.2rc2-linux32.tar.bz2\n6c67701914b7885e67d282c1286e9109fc79e73ab65b5c164492fb024b8deb7f  pypy3.7-v7.3.2rc2-linux64.tar.bz2\n28b48a691276a806bcf0009df5e367d90159b9b4a4161ad9857454999e6915ec  pypy3.7-v7.3.2rc2-osx64.tar.bz2\n544023b22670be740970bfc8d67a102dfa045cb229e40271a4197a9e8d3bc5da  pypy3.7-v7.3.2rc2-s390x.tar.bz2\n9a3f29338340ab5e006300b68369745bd16f99943a7d48d8440c5a0ad67a5c68  pypy3.7-v7.3.2rc2-src.tar.bz2\n73a6c2241d0a5ce7741a15f8cfd205a6f1eb10310799d912c069d6be58907ba7  pypy3.7-v7.3.2rc2-src.zip\n9a44c694f9c642a7a127241466f72ca58f303d3e148bf5488e34a162c7d7a55b  pypy3.7-v7.3.2rc2-win32.zip\n\na7e2376f5e64256aa2e3cf3d403b4c48753c9c2588c57e0fc6bddebefacb3a9d  pypy3.7-v7.3.2rc1-aarch64.tar.bz2\ne2b2fa3f83f4a3cc138eb88c3bbf4fde395faec6bc04cd72721623865a366d96  pypy3.7-v7.3.2rc1-linux32.tar.bz2\n8173935a5d1cae7238cb27e35bf881ab0ed0d8bd978d3cf6c80311ed596324ba  pypy3.7-v7.3.2rc1-linux64.tar.bz2\ne730cf9e5be8566544a478bf2da4bc4ab84428ac4f4a7bb8e001ea4516a3f3be  pypy3.7-v7.3.2rc1-osx64.tar.bz2\n209c2136654ea116c316c6d5305659e8e33d49b9f9f61eee36c06330bb3214ba  pypy3.7-v7.3.2rc1-src.tar.bz2\n419020e81793030cb6d011e7c0b75183163a7586a31ae88a6a52689e9c45926e  pypy3.7-v7.3.2rc1-src.zip\na6fc9d568c05504759e945e70b94fc55f5e99748eb01da4fb5192231238fa1d7  pypy3.7-v7.3.2rc1-win32.zip\npypy2.7-7.3.2 sha256:\nfce1f06f20ab8bcacb9ac1c33572d6425033de53c3a93fbd5391189cc3e106cb  pypy2.7-v7.3.2-aarch64.tar.bz2\n78f30ac17abe3cc077fc2456ef55adb51b052c5126011b2a32bacc858acaca7d  pypy2.7-v7.3.2-linux32.tar.bz2\n8d4f08116a97153a0f739de8981874d544b564cbc87dd064cca33f36c29da13b  pypy2.7-v7.3.2-linux64.tar.bz2\n10ca57050793923aea3808b9c8669cf53b7342c90c091244e9660bf797d397c7  pypy2.7-v7.3.2-osx64.tar.bz2\n042d5e99f660de098de979c4b27f7f8c1332d904db379bb2bf2c3402729749bb  pypy2.7-v7.3.2-s390x.tar.bz2\n8189480d8350ad6364d05c2b39fd7d832644d4b1cd018f785126389df45928d1  pypy2.7-v7.3.2-src.tar.bz2\nd891c55f4e657b5e3fe609cee02b2288790abb5554a544ca047f088310d129c4  pypy2.7-v7.3.2-src.zip\n0fd62265e0421a02432f10a294a712a5e784a8e061375e6d8ea5fd619be1be62  pypy2.7-v7.3.2-win32.zip\n\nfa76bfc65200eeb3b32253e674a9339a417aef23f5a5c54e0c519bbbfefcdc7e  pypy2.7-v7.3.2rc2-aarch64.tar.bz2\n40ff311202eca98ef3d6edeac4171470135087a8de34296f486c17ec376ebe51  pypy2.7-v7.3.2rc2-linux32.tar.bz2\n379d458c1a9d38c2b3a6a32bd805786fc584739548a697a4ef7b683bcfdfda3e  pypy2.7-v7.3.2rc2-linux64.tar.bz2\n3d515a233c83cbc833bcdd0b75354b20dc79b9f6ca892a5db9cadaea36c6bb5b  pypy2.7-v7.3.2rc2-osx64.tar.bz2\n41344e1e4d27d774780e9cace6e70c5025b510c82de708ea55b64d21ed0c2f40  pypy2.7-v7.3.2rc2-s390x.tar.bz2\n144bfc9607e6319ba950de9a4d1587020e3f1311cc25a79d1711de78c5992f4f  pypy2.7-v7.3.2rc2-src.tar.bz2\nf9de3fe464ca11dfcdd6816b64051f03bdba7c66755b17ddd4f071c4d08cc0fb  pypy2.7-v7.3.2rc2-src.zip\n01a9b5b266fde443698cb01c7bac843cc0ed8747f47f1e8930666a4303bf83b2  pypy2.7-v7.3.2rc2-win32.zip\n\n925543a3161153d9b15df49000e96ce2625bf4371619667b5f37616b699acc21  pypy2.7-v7.3.2rc1-linux32.tar.bz2\n6216e1bbac3b86bfd38d16f0685c34c8c9c7aaf908ebd00388844ec295b89c17  pypy2.7-v7.3.2rc1-linux64.tar.bz2\na6fcdb44f12379eb1a547750322bd4c154b6e0c5ee30f9de2d9e2b86b2f2f319  pypy2.7-v7.3.2rc1-osx64.tar.bz2\n9f58b5bacab010d945d9c31e8b7a2539034858f4cdf048f016d8d04430688cc6  pypy2.7-v7.3.2rc1-src.tar.bz2\n0c86b52f6ad09dce1275427c18a216a0cbb5cf0db89eba2389e97ae81416eef7  pypy2.7-v7.3.2rc1-src.zip\nbbb737f4ce714af0e7797fc951f5231b26ee10f8bca3d969c5b732982f952957  pypy2.7-v7.3.2rc1-win32.zip\npypy2.7-7.3.1 sha256:\n094f23ab262e666d8740bf27459a6b1215a628dad9b6c2a88f1ed5c793fab267  pypy2.7-v7.3.1-aarch64.tar.bz2\ncd155d06cd0956d9de4a16e8a6bdf0722cb45b5bc4bbf805825d393ebd6690ad  pypy2.7-v7.3.1-linux32.tar.bz2\nbe74886547df7bf7094096a11fc0a48496779d0d1b71901797b0c816f92caca3  pypy2.7-v7.3.1-linux64.tar.bz2\ndfd4651243441d2f8f1c348e9ecc09848642d0c31bb323aa8ac320e5b9f232f0  pypy2.7-v7.3.1-osx64.tar.bz2\n1b65e085118e44ac57d38a9ba79516c68bf1fdcd65c81c66b5b5ffff06b4463b  pypy2.7-v7.3.1-ppc64.tar.bz2\nd81c7177e25bd8b1c99081e32362a29ee467ccd310b17a11161f4a9b96222b20  pypy2.7-v7.3.1-ppc64le.tar.bz2\n71ad5132a6fd32af0b538c17ebd1e0bfe5f5dfa74b129bce242bd28357bf35fc  pypy2.7-v7.3.1-s390x.tar.bz2\nfa3771514c8a354969be9bd3b26d65a489c30e28f91d350e4ad2f4081a9c9321  pypy2.7-v7.3.1-src.tar.bz2\n71d764c94f467f9dd75b6af086e2b69e0d520bf6227bcb39055c24c799c135be  pypy2.7-v7.3.1-src.zip\ne3c0dfb385d9825dd7723f26576d55d43ed92f1178f2399ab39e9fa11621a47b  pypy2.7-v7.3.1-win32.zip\npypy3.6-7.3.1 sha256:\n0069bc3c1570b935f1687f5e128cf050cd7229309e48fad2a2bf2140d43ffcee  pypy3.6-v7.3.1-aarch64.tar.bz2\n2e7a818c67f3ac0708e4d8cdf1961f30cf9586b3f3ca2f215d93437c5ea4567b  pypy3.6-v7.3.1-linux32.tar.bz2\nf67cf1664a336a3e939b58b3cabfe47d893356bdc01f2e17bc912aaa6605db12  pypy3.6-v7.3.1-linux64.tar.bz2\nd9c1778cd1ba37e129b495ea0f35ccdd9b68f5cd9d33ef0ce24e955c16d8840b  pypy3.6-v7.3.1-osx64.tar.bz2\nee02b3e65f0ca49dc09850b57835c2b65d1234f26f7991027ca6d65fadbaa4d9  pypy3.6-v7.3.1-ppc64.tar.bz2\n089fd806629ebf79cb0cb4b0c303d8665f360903b79f0df9214b58dbc42e8231  pypy3.6-v7.3.1-ppc64le.tar.bz2\n147592888e25678c1ae1c2929dc7420b3a0990117fdb25f235cb22476b4e4b5a  pypy3.6-v7.3.1-s390x.tar.bz2\n0c2cc3229da36c6984baee128c8ff8bb4516d69df1d73275dc4622bf249afa83  pypy3.6-v7.3.1-src.tar.bz2\n91e7ba30519f2c4c1833280acfb660b48392ef57c5ed0fa4e8af78587a7b8f20  pypy3.6-v7.3.1-src.zip\n752fbe8c4abee6468e5ce22af82818f821daded36faa65f3d69423f9c217007a  pypy3.6-v7.3.1-win32.zip\npypy2.7-7.3.0 sha256:\na3dd8d5e2a656849fa344dce4679d854a19bc4a096a0cf62b46a1be127a5d56c  pypy2.7-v7.3.0-aarch64.tar.bz2\neac1308b7d523003a5f6d20f58406d52ab14611bcec750122ae513a5a35110db  pypy2.7-v7.3.0-linux32.tar.bz2\nf4950a54378ac637da2a6defa52d6ffed96af12fcd5d74e1182fb834883c9826  pypy2.7-v7.3.0-linux64.tar.bz2\nca7b056b243a6221ad04fa7fc8696e36a2fb858396999dcaa31dbbae53c54474  pypy2.7-v7.3.0-osx64.tar.bz2\n82e62869812aa2953a4f83e96c813cbc52973dfa5e42605e72b6610ac13f2481  pypy2.7-v7.3.0-ppc64.tar.bz2\n592a6db77270b922ffa13cbeced9eabbc36c532ded9fc145f6a19073d3e78499  pypy2.7-v7.3.0-ppc64le.tar.bz2\nd254b82a00021339762198e41ba7f72316010d0f9bd4dcd7b0755185da9c005e  pypy2.7-v7.3.0-s390x.tar.bz2\nb0b25c7f8938ab0fedd8dedf26b9e73c490913b002b484c1b2f19d5844a518de  pypy2.7-v7.3.0-src.tar.bz2\n42dc84a277e7a5e635fe39bbd745f06135902c229a257123332b7555800d915b  pypy2.7-v7.3.0-src.zip\na9e3c5c983edba0313a41d3c1ab55b080816c4129e67a6c272c53b9dbcdd97ec  pypy2.7-v7.3.0-win32.zip\npypy3.6-7.3.0 sha256:\nb900241bca7152254c107a632767f49edede99ca6360b9a064141267b47ef598  pypy3.6-v7.3.0-aarch64.tar.bz2\n7045b295d38ba0b5ee65bd3f078ca249fcf1de73fedeaab2d6ad78de2eab0f0e  pypy3.6-v7.3.0-linux32.tar.bz2\nd3d549e8f43de820ac3385b698b83fa59b4d7dd6cf3fe34c115f731e26ad8856  pypy3.6-v7.3.0-linux64.tar.bz2\n87b2545dad75fe3027b4b2108aceb9fdadcdd24e61ae312ac48b449fdd452bf3  pypy3.6-v7.3.0-osx64.tar.bz2\ne2587e8da2abb12a86bf75941ce739124d2a1156367a9a3d729ac31d0841c300  pypy3.6-v7.3.0-ppc64.tar.bz2\nd6f3b701313df69483b43ebdd21b9652ae5e808b2eea5fbffe3b74b82d2e7433  pypy3.6-v7.3.0-ppc64le.tar.bz2\n0fe2f7bbf42ea88b40954d7de773a43179a44f40656f2f58201524be70699544  pypy3.6-v7.3.0-s390x.tar.bz2\n48d12c15fbcbcf4a32882a883195e1f922997cde78e7a16d4342b9b521eefcfa  pypy3.6-v7.3.0-src.tar.bz2\n8ae9efd0a2aadb19e892bbd07eca8ef51536296a3ef93964149aceba511e79ca  pypy3.6-v7.3.0-src.zip\n30e6870c4f3d8ef91890a6556a98080758000ba7c207cccdd86a8f5d358998c1  pypy3.6-v7.3.0-win32.zip\npypy2.7-7.2.0 sha256:\n57b0be053c6a5f069e23b843f38863cf7920f5eef7bc89f2e086e5c3a28a2ba9  pypy2.7-v7.2.0-aarch64.tar.bz2\n76d666e5aee54b519d6ec1af4ef0cbdc85f7f9276dd554e97deb026adfd0c936  pypy2.7-v7.2.0-linux32.tar.bz2\n05acf28e6a243026ecad933b9361d8f74b41f00818071b76b38c4694cc4c9599  pypy2.7-v7.2.0-linux64.tar.bz2\n36aa2f2440e762333569118dd0b3d5371d575c40966effa194d116c5453ddb52  pypy2.7-v7.2.0-osx64.tar.bz2\nfb51150a4ce94b0ca8587899ba69c41fc58a6b35c5340ea6926376ecb9cfcac4  pypy2.7-v7.2.0-ppc64.tar.bz2\n5c4224525657c29b815cb2c6b3f9bc5a267368cc6adf0fedb235a6052929f65f  pypy2.7-v7.2.0-ppc64le.tar.bz2\nbb7ae585ecb4d904c890e28a2c5b6bd379f57cc3d9e38ff45597ff54fa935eaa  pypy2.7-v7.2.0-s390x.tar.bz2\n55cb7757784fbe3952102447f65b27d80e6c885a464a7af1a9ce264492439dcc  pypy2.7-v7.2.0-src.tar.bz2\n897038550614d558f9f6718409b107e27903ef2b2b57ec250939d1b1ebdf0aba  pypy2.7-v7.2.0-src.zip\n956eeaaaac053e5d0917e77a3d2ad1933ab5561eb3e6e71235780b5aa5fd2bb7  pypy2.7-v7.2.0-win32.zip\npypy2.7-7.1.1 sha256:\n41ca390a76ca0d47b8353a0d6a20d5aab5fad8b0bb647b960d8c33e873d18ef5  pypy2.7-v7.1.1-linux32.tar.bz2\n73b09ef0860eb9ad7997af3030b22909806a273d90786d78420926df53279d66  pypy2.7-v7.1.1-linux64.tar.bz2\n31a17294dec96c2191885c776b4ee02112957dc874f7ba03e570537a77b78c35  pypy2.7-v7.1.1-osx64.tar.bz2\n1ef94c3a9c67c2335cee0b21753036b4696ed588b9d54b7b8036a6ae47f7001d  pypy2.7-v7.1.1-s390x.tar.bz2\n5f06bede6d71dce8dfbfe797aab26c8e35cb990e16b826914652dc093ad74451  pypy2.7-v7.1.1-src.tar.bz2\nd9b07a2954ad6dbde94feffd848311e2b5169563d33e3e9f17969579b01a4158  pypy2.7-v7.1.1-src.zip\n9c59226311f216a181e70ee7b5aa4d9665a15d00f24ae02acec9af7d96355f63  pypy2.7-v7.1.1-win32.zip\npypy2.7-7.1.0 sha256:\n44ec91e8cb01caab289d8763c203f3aaf288d14325a6c42692bd1ac4e870d758  pypy2.7-v7.1.0-linux32.tar.bz2\nfef176a29a2ef068c00c8098e59dab935ca6e956f089672b3f7351da95a034f5  pypy2.7-v7.1.0-linux64.tar.bz2\n8be43685ce718b0768387450fc6dc395d60809b778b6146c353ef67826022153  pypy2.7-v7.1.0-osx64.tar.bz2\nb065f55741bcb37863f1eca30ce91c9d79159371a6994100930cdc2ede3237bc  pypy2.7-v7.1.0-s390x.tar.bz2\nb051a71ea5b4fa27d0a744b28e6054661adfce8904dcc82500716b5edff5ce4b  pypy2.7-v7.1.0-src.tar.bz2\ne60ce30f9947844da43daaa7658adc0c05330681305225954114772f42df06ec  pypy2.7-v7.1.0-src.zip\n76658c9ad679d562b8b6a09d006caa666406337b9834ff56db16980c5e549f20  pypy2.7-v7.1.0-win32.zip\npypy3.6-7.2.0 sha256:\nf82dc9dc6c692417ee9727f23beae75364a5757ebdc657a2a1d0010ac3ad17ab  pypy3.6-v7.2.0-aarch64.tar.bz2\n45e99de197cb3e974cfc8d45e0076ad2066852e61e56b3eafd1237efafd2c43e  pypy3.6-v7.2.0-linux32.tar.bz2\naa128e555ad0fe5c4c15104ae0903052bd232b6e3a73f5fe023d27b8fd0d6089  pypy3.6-v7.2.0-linux64.tar.bz2\n836abb0ec303b90a684533711ed3b8269d3e8c64805b595e410920abdea678ac  pypy3.6-v7.2.0-osx64.tar.bz2\n14021d196e393b3a6d2395ab94ceec347753715e37223efe4c50b7c141b351a2  pypy3.6-v7.2.0-ppc64.tar.bz2\n6aef73a3b68e9a6c062cadd83d3db16790960cf97401ca6f2aad2195e9b05c35  pypy3.6-v7.2.0-ppc64le.tar.bz2\na11da8118064db102d159e9221319c428b298c4a87f26166fd6ae94be8d6ae0d  pypy3.6-v7.2.0-s390x.tar.bz2\n0d7c707df5041f1593fe82f29c40056c21e4d6cb66554bbd66769bd80bcbfafc  pypy3.6-v7.2.0-src.tar.bz2\n405ac35695dd374d5ea192cb44cb47231f9a65812cc7b6549df33df12ffe54db  pypy3.6-v7.2.0-src.zip\nc926f622bec24a8b348591d631717ace83b3a6c3c2dac02b157b622b97d1fc9c  pypy3.6-v7.2.0-win32.zip\npypy3.6-7.1.1 sha256:\ncb11ef4b0df569c28390b1ee93029159e1b90bfbad98df6abd629d5203b2abd9  pypy3.6-v7.1.1-linux32.tar.bz2\n8014f63b1a34b155548852c7bf73aab2d41ebddf2c8fb603dc9dd8509be93db0  pypy3.6-v7.1.1-linux64.tar.bz2\na5c2f2bfa2b4a4d29e8a67baab95699b169054066df218a14f171bb84a6df0c0  pypy3.6-v7.1.1-osx64.tar.bz2\n4a91bf2d9a142b6dbf82b5301cb510535ae9a54e1645546b2e0735a7b5ed85ba  pypy3.6-v7.1.1-s390x.tar.bz2\n6a3ef876e3691a54f4cff045028ec3be94ab9beb2e99f051b83175302c1899a8  pypy3.6-v7.1.1-src.tar.bz2\n4a3ebeb767740f2dc0b886d02797d21d7d69f154cf951bb991c19bd485e6cae1  pypy3.6-v7.1.1-src.zip\n8b513b254de5f31890f5956569de9aec3a0a91d7aba72fc89d66901f4a8ccf49  pypy3.6-v7.1.1-win32.zip\npypy 3.6-v7.1.0 sha256:\n031bfac61210a6e161bace0691b854dc15d01b0e624dc0588c544ee5e1621a83  pypy3.6-v7.1.0-linux32.tar.bz2\n270dd06633cf03337e6f815d7235e790e90dabba6f4b6345c9745121006925fc  pypy3.6-v7.1.0-linux64.tar.bz2\nd46e005ba095cb4a7006079ffbf4fe63c18cf5e9d8ce9ce8383efc1a4863ab5b  pypy3.6-v7.1.0-osx64.tar.bz2\n243cd0cc188a94c1f064f402ae72b8ba4303eb3137eac53c53826472b8005098  pypy3.6-v7.1.0-s390x.tar.bz2\nfaa81f469bb2a7cbd22c64f22d4b4ddc5a1f7c798d43b7919b629b932f9b1c6f  pypy3.6-v7.1.0-src.tar.bz2\n4858e7e8a0007bc3b381bd392208b28d30889a4e5a88a3c28e3d9dc4f25b654e  pypy3.6-v7.1.0-src.zip\n77a0576a3d518210467f0df2d0d9a1892c664566dc02f25d974c2dbc6b4749e7  pypy3.6-v7.1.0-win32.zip",
      "tags": "",
      "url": "https://www.pypy.org/checksums.html"
    },
    {
      "title": "Some Ways that PyPy uses Graphviz",
      "text": "Some way that PyPy uses Graphviz\nSomebody wrote this super cool thread on Twitter about using Graphviz to make\nsoftware visualize its internal state:\n\ud83e\uddf5 Make yours and everybody else's lives slightly less terrible by having all your programs print out their internal stuff as pictures; \u2728 a thread \u2728 pic.twitter.com/NjQ42bXN2E\u2014 Kate (@thingskatedid) April 24, 2021 PyPy is using this approach a lot too and I collected a few screenshots of that\ntechnique on Twitter and I thought it would make a nice blog post too!\nThe most important view early in the project, and the way that our Graphviz\nvisualizations got started was that we implemented a way to look at the control\nflow graphs of our RPython functions after type inference. They are in static\nsingle information form (SSI), a variant of SSA form. Hovering over the\nvariables shows the inferred types in the footer:\n\nThere's another view that shows the inferred call graph of the program:\n\nA related viewer shows the inferred class hierarchy (in this case the exception\nhierarchy) and you can focus on a single class, which will show you its base\nclasses and all the methods and instance attributes that were found:\n\n\nWe also have a view to show us the traces that are produced by the tracing JIT\ntests. this viewer doesn't really scale to the big traces that the full Python\ninterpreter produces, but it's really useful during testing:\n\nThen there are more traditional tree views, eg here is a parse tree for a small\npiece of Python source code:\n\nParsing-related we have visualized the DFAs of the parser in the past,\nthough the code is unfortunately lost.\nAll these visualizations are made by walking the relevant data structures and\nproducing a Graphviz input file using a bit of string manipulation, which is\nquite easy to do. Knowing a bit of Graphviz is a really useful skill, it's\nsuper easy to make throwaway visualizations.\nFor example here is a one-off thing I did when debugging our JSON parser to\nshow the properties of the objects used in a huge example json file:\n\nOn top of graphviz, we have a custom tool called the dotviewer, which is\nwritten in Python and uses Pygame to give you a zoomable, pannable, searchable\nway to look at huge Graphviz graphs. All the images in this post are\nscreenshots of that tool. In its simplest form it takes any .dot files as\ninput.\nHere's a small video dotviewer, moving around and searching in the json graph.\nBy writing a bit of extra Python code the dotviewer can also be extended to add\nhyperlinks in the graphs to navigate to different views (for example, we did\nthat for the callgraphs above).\nAll in all this is a really powerful approach to understand the behaviour of\nsome of code, or when debugging complicated problems and we have gotten a\nhuge amount of milage out of this over the years. It can be seen as an instance\nof moldable development (\"a way of programming through which you construct\ncustom tools for each problem\"). And it's really easy to get into! The Graphviz\nlanguage is quite a simple text-based language that can be applied to a huge\namount of different visualization situations.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/04/ways-pypy-graphviz.html"
    },
    {
      "title": "PyPy v7.3.4: release of python 2.7 and 3.7",
      "text": "PyPy v7.3.4: release of python 2.7 and 3.7\nThe PyPy team is proud to release the version 7.3.4 of PyPy, which includes\ntwo different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.7,  which is an interpreter supporting the syntax and the features of\nPython 3.7, including the stdlib for CPython 3.7.10. We no longer refer to\nthis as beta-quality as the last incompatibilities with CPython (in the\nre module) have been fixed.\n\n\nWe are no longer releasing a Python3.6 version, as we focus on updating to\nPython 3.8. We have begun streaming the advances towards this goal on Saturday\nevenings European time on https://www.twitch.tv/pypyproject. If Python3.6 is\nimportant to you, please reach out as we could offer sponsored longer term\nsupport.\nThe two interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. Highlights of the release include binary Windows 64 support,\nfaster numerical instance fields, and a preliminary HPy backend.\nA new contributor (Ondrej Baranovi\u010d - thanks!) took us up on the challenge to get\nwindows 64-bit support.  The work has been merged and for the first time we\nare releasing a 64-bit Windows binary package.\nThe release contains the biggest change to PyPy's implementation of the\ninstances of user-defined classes in many years. The optimization was\nmotivated by the report of performance problems running a numerical particle\nemulation. We implemented an optimization that stores int and float\ninstance fields in an unboxed way, as long as these fields are type-stable\n(meaning that the same field always stores the same type, using the principle\nof type freezing). This gives significant performance improvements on\nnumerical pure-Python code, and other code where instances store many integers\nor floating point numbers.\nThere were also a number of optimizations for methods around strings and bytes,\nfollowing user reported performance problems. If you are unhappy with PyPy's\nperformance on some code of yours, please report an issue!\nA major new feature is prelminary support for the Universal mode of HPy: a\nnew way of writing c-extension modules to totally encapsulate PyObject*.\nThe goal, as laid out in the HPy documentation and recent HPy blog post,\nis to enable a migration path\nfor c-extension authors who wish their code to be performant on alternative\ninterpreters like GraalPython (written on top of the Java virtual machine),\nRustPython, and PyPy. Thanks to Oracle and IBM for sponsoring work on HPy.\nSupport for the vmprof statistical profiler has been extended to ARM64 via a\nbuilt-in backend.\nSeveral issues exposed in the 7.3.3 release were fixed. Many of them came from the\ngreat work ongoing to ship PyPy-compatible binary packages in conda-forge.\nA big shout out to them for taking this on.\nDevelopment of PyPy takes place on https://foss.heptapod.net/pypy/pypy.\nWe have seen an increase in the number of drive-by contributors who are able to\nuse gitlab + mercurial to create merge requests.\nThe CFFI backend has been updated to version 1.14.5 and the cppyy backend\nto 1.14.2. We recommend using CFFI rather than C-extensions to interact with C,\nand using cppyy for performant wrapping of C++ code for Python.\nAs always, we strongly recommend updating to the latest versions. Many fixes\nare the direct result of end-user bug reports, so please continue reporting\nissues as they crop up.\nYou can find links to download the v7.3.4 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our renovated blog site via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better. Since the\nprevious release, we have accepted contributions from 10 new contributors,\nthanks for pitching in, and welcome to the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\nsoon 3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32/64 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\nPyPy does support ARM 32 bit processors, but does not release binaries.\n\n\nWhat else is new?\nFor more information about the 7.3.4 release, see the full changelog.\nPlease update, and continue to help us make PyPy better.\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/04/pypy-v734-release-of-python-27-and-37.html"
    },
    {
      "title": "New HPy blog",
      "text": "Regular readers of this blog\nalready know\nabout HPy, a project which aims to develop a new C\nAPI for Python to make it easier/faster to support C extensions on alternative\nPython implementations, including PyPy.\nThe HPy team just published the\nfirst post of HPy new\nblog, so if you are interested in its development, make sure to check it out!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/03/new-hpy-blog.html"
    },
    {
      "title": "PyPy's blog has moved",
      "text": "For many years, PyPy has been publishing blog posts at\nhttps://morepypy.blogspot.com. From now on,\nthe posts will be here, at https://pypy.org/blog. The\nRSS feed is https://pypy.org/rss.xml. The original\ncontent has been migrated to the newer site, including comments.\n\n\nAmong the motivations for the move were:\nOne site to rule them all\nAdding the blog posts here seems like a natural extension of the web site\nrather than outsourcing it to a third-party. Since the site is generated using\nthe static site generator nikola from the github repo\nhttps://github.com/pypy/pypy.org, we also\nhave good source control for the content.\nCI previews, and github\nThose of you who follow PyPy may note something new in the URL for the repo:\nuntil now PyPy has been using mercurial as hosted\non https://foss.heptapod.net.  While\nheptapod (a community driven effort to bring mercurial\nsupport to GitLab\u2122) does provide a GitLab CI runner for the open source\noffering, on github it is easier to integrate netlify\nfor previews. Hopefully the move to the more popular github platform will\nencourage new contributors to publish their success stories around using PyPy\nand the RPython toolchain.\nComments\nComments to blog posts are generated via the utterances\njavascript plugin. The comments appear as issues in the repo.\nWhen viewing the site, a query is made to fetch the comments to the issue with\nthat name. To comment, users must authorize the utterances app to post on their\nbehalf using the GitHub\nOAuth flow.\nAlternatively, users can comment on the GitHub issue directly. The interaction\nwith github for authentication and moderation seems more natural than the\nmanual moderation required on blogspot.\nPlease prove to us that the move is worth it\nHelp us with guest blog posts, and PRs to improve the styling of the site. One\nalready open issue is that the\nnavbar needlessly uses javascript, help to keep the responsive style in pure\nCSS is welcome. The theme could also use tweaking.\nBut more importantly, we want to hear from you.  Guest blog posts about\nPyPy are welcome. Just follow the directions in the repo's README to create a\nPR with your favorite PyPy story.\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/03/pypys-blog-has-moved.html"
    },
    {
      "title": "Mac meets Arm64",
      "text": "Looking for sponsorship\n\nApple now ships Macs which are running on an arm64 variant machine with the\nlatest version of MacOS, Big Sur M1.  We are getting requests for PyPy to\nsupport this new architecture.  Here is our position on this topic (or at least\nmine, Armin Rigo's), and how you can help.\n\nPorting PyPy is harder than just re-running the compiler, because PyPy contains\na few big architecture-dependent \"details\", like the JIT compiler and the\nforeign function interfaces (CFFI and ctypes).\n\nFixing the JIT compiler should not be too much work: we already support arm64,\njust the Linux one.  But Apple made various details different (like the calling\nconventions).  A few other parts need to be fixed too, notably CFFI and ctypes,\nagain because of the calling conventions.\n\nFixing that would be a reasonable amount of work.  I would do it myself for a\nsmall amount of money.  However, the story doesn't finish here.  Obviously, the\nstart of the story would be to get ssh access to a Big Sur M1 machine.  (If at\nthis point you're thinking \"sure, I can give you ssh access for three months\",\nthen please read on.)  The next part of the story is that we need a machine\navailable long term.  It can be either a machine provided and maintained by a\nthird party, or alternatively a pot of money big enough to support the\nacquision of a machine and ongoing work of one of us.\n\nIf we go with the provided-machine solution:  What we need isn't a lot of\nresources.  Our CI requires maybe 10 GB of disk space, and a few hours of CPU\nper run.  It should fit into 8 GB of RAM.  We normally do a run every night but\nwe can certainly lower the frequency a bit if that would help.  However, we'd\nideally like some kind of assurance that you are invested into maintaining the\nmachine for the next 3-5 years (I guess, see below).  We had far too many\nmachines that disappeared after a few months.\n\nIf we go with the money-supported solution: it's likely that after 3-5 years\nthe whole Mac base will have switched to arm64, we'll drop x86-64 support for\nMac, and we'll be back to the situation of the past where there was only one\nkind of Mac machine to care about.  In the meantime, we are looking at 3-5\nyears of lightweight extra maintenance.  We have someone that has said he would\ndo it, but not for free.\n\nIf either of these two solutions occurs, we'll still have, I quote, \"probably\nsome changes in distutils-type stuff to make python happy\", and then some\npackaging/deployment changes to support the  \"universal2\" architecture, i.e.\nincluding both versions inside a single executable (which will not be just an\nextra switch to clang, because the two versions need a different JIT backend\nand so must be translated separately).\n\nSo, now all the factors are on the table.  We won't do the minimal \"just the\nJIT compiler fixes\" if we don't have a plan that goes farther.  Either we get\nsufficient money, and maybe support, and then we can do it quickly; or PyPy\nwill just remain not natively available on M1 hardware for the next 3-5 years.\nWe are looking forward to supporting M1, and view resources contributed by\nthe community as a vote of confidence in assuring the future of PyPy on this\nhardware.  Contact us: pypy-dev@python.org, or our private mailing\nlist pypy-z@python.org.\n\nThanks for reading!\n\nArmin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/12/mac-meets-arm64-940822335619099039.html"
    },
    {
      "title": "PyPy 7.3.3 triple release: python 3.7, 3.6, and 2.7",
      "text": "The PyPy team is proud to release the version 7.3.3 of PyPy, which includes\nthree different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18 (updated from the\nprevious version)PyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.12 (updated from the\nprevious version).PyPy3.7 beta: which is our second release of an interpreter supporting the\nsyntax and the features of Python 3.7, including the stdlib for CPython\n3.7.9. We call this beta quality software, there may be issues about\ncompatibility with new and changed features in CPython 3.7.\nPlease let us know what is broken or missing. We have not implemented the\ndocumented changes in the re module, and a few other pieces are also\nmissing. For more information, see the PyPy 3.7 wiki page\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the 7.3\nreleases, but read on to find out what is new.\nSeveral issues found in the 7.3.2 release were fixed. Many of them came from the\ngreat work by conda-forge to ship PyPy binary packages.  A big shout out\nto them for taking this on.\nDevelopment of PyPy has moved to https://foss.heptapod.net/pypy/pypy.\nThis was covered more extensively in this blog post. We have seen an\nincrease in the number of drive-by contributors who are able to use gitlab +\nmercurial to create merge requests.\nThe CFFI backend has been updated to version 1.14.3. We recommend using CFFI\nrather than c-extensions to interact with C, and using cppyy for performant\nwrapping of C++ code for Python.\nA new contributor took us up on the challenge to get windows 64-bit support.\nThe work is proceeding on the win64 branch, more help in coding or\nsponsorship is welcome. In anticipation of merging this large change, we fixed\nmany test failures on windows.\nAs always, this release fixed several issues and bugs.  We strongly recommend\nupdating. Many of the fixes are the direct result of end-user bug reports, so\nplease continue reporting issues as they crop up.\nYou can find links to download the v7.3.3 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 2 new contributors,\nthanks for pitching in.\nIf you are a python library maintainer and use c-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.6, and\n3.7. It\u2019s fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)big- and little-endian variants of PPC64 running Linux,s390x running Linux64-bit ARM machines running Linux.\n\nPyPy does support ARM 32 bit processors, but does not release binaries.\u00a0\nWhat else is new?\nFor more information about the 7.3.3 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2020/11/pypy-733-triple-release-python-37-36-3446596804408262749.html"
    },
    {
      "title": "Download (advanced)",
      "text": "Contents\n\n\"JIT Compiler\" version\nLinux binaries and common distributions\nPyPy-STM 2.5.1\nOther versions\nInstalling\nInstalling more modules\nBuilding from source\nPackaging\nChecksums\n\n\nWe provide pre-compiled binaries for many platforms and OSes:\n\nthe Python2.7 compatible release \u2014 PyPy2.7 v7.3.6\nthe Python3.7 compatible release \u2014 PyPy3.7 v7.3.7\nthe Python3.8 compatible release \u2014 PyPy3.8 v7.3.7\n\n\nNote\nOur nightly binary builds have the most recent bugfixes and performance\nimprovements, though they can be less stable than the official releases. See\nthis link for older versions.\n\n\n\n\nPyPy latest\n\n\n\n\n\n\n\n\nOS\nPyPy3.8\nPyPy3.7\nPyPy2.7\nNotes\n\n\n\nLinux x86 64 bit\nDownload\nDownload\nDownload\ncompatible with CentOS6 and later\n\nWindows 64 bit\nDownload\nDownload\nDownload\ncompatible with any windows 64-bit\nyou might need the VC runtime library installer vcredist.x64.exe\n\n\nMacOS\nDownload\nDownload\nDownload\nHigh Sierra >= 10.13, not for Sierra and below. Not signed, for signed\npackages use Homebrew.\n\nLinux ARM64\nDownload\nDownload\nDownload\ncompatible with CentOS7 and later\n\n\n\n\n\nOther Platfoms\n\n\n\n\n\n\n\n\nOS\nPyPy3.8\nPyPy3.7\nPyPy2.7\nNotes\n\n\n\nLinux x86 32 bit\nDownload\nDownload\nDownload\ncompatible with CentOS6 and later\n\nPowerPC PPC64\nn/a\nn/a\n7.3.1\n64bit big-endian, Fedora 20 [1]\n\nPowerPC PPC64le\nn/a\nn/a\n7.3.1\n64bit little-endian, Fedora 21 [1]\n\nS390x\nDownload\nDownload\nDownload\nbuilt on Redhat Linux 7.2 [1]\n\n\n\n\n[1]\n(1,2,3)\nLinux binaries are provided for the\ndistributions listed here.  If your distribution is not exactly this\none, it won't work, you will probably see:\npypy: error while loading shared libraries: ....\n\n\n\"JIT Compiler\" version\nThe binaries above include a Just-in-Time compiler. On x86-32, they only work on\nCPUs that have the SSE2 instruction set (most of them do, nowadays).. They also\ncontain stackless extensions, like greenlets.\n\n\nLinux binaries and common distributions\nSince version 7.3, the linux x86 binaries ship with versions\nof OpenSSL, SQLite3, libffi, expat, and TCL/TK binary libraries linked in. This\nmake the binaries \"portable\" so that they should run on any current glibc-based\nlinux platform. The ideas were adopted from the portable-pypy package.\nThis solution to the portability problem means that the versions of the\npackaged libraries are frozen to the version shipped, so updating your system\nlibraries will not affect this installation of PyPy. Also see the note about\nSSL certificates below.\nFor s390x, and ppc64, the binaries target a specific operating system.\nThese binaries are dynamically linked, and thus might not be usable due to the\nsad story of linux binary compatibility.  This means that Linux binaries are\nonly usable on the distributions written next to them unless you're ready to\nhack your system by adding symlinks to the libraries it tries to open.  There\nare better solutions:\n\ndownload PyPy from your release vendor (usually an outdated\nversion): Ubuntu (PPA), Debian, Homebrew, MacPorts,\nFedora, Gentoo and Arch are known to package PyPy, with various\ndegrees of being up-to-date. FreshPorts packages for FreeBSD.\nuse conda, (for MacOS) which will also enable installing binary-compiled\npackages.\nrecompile the CFFI-based TCL/TK, OpenSSL, or sqlite3 modules, using system\nlibraries and the scripts in pypy/lib_pypy/pypy_tools. This solution will\nnot solve compatibility issues with libffi, since that is baked into PyPy.\nor translate your own PyPy.\n\n\n\nNote\nSSL Certificates\nWhile the linux binaries ship an OpenSSL library, they do not ship a\ncertificate store for SSL certificates. If you wish to use SSL module,\nyou will need a valid certificate store. You can use the certifi package\nand set SSL_CERT_FILE to certifi.where() or install your platform\ncertificates which should be discovered by the _ssl module.\n\n\nPrevious version can be downloaded from here, or directly from the buildbot's\nmirror.\nIf your CPU is really, really old, it may be a x86-32 without SSE2.\nThere is untested support for manually translating PyPy's JIT without\nSSE2 (--jit-backend=x86-without-sse2) but note that your machine\nis probably low-spec enough that running CPython on it is a better\nidea in the first place.\n\n\nPyPy-STM 2.5.1\nThis is a special version of PyPy!  See the Software Transactional\nMemory (STM) documentation.\n\nPyPy-STM Linux x86-64 binary (64bit, tar.bz2 built on Ubuntu 12.04 - 16.04)\n\n\n\nOther versions\nThe other versions of PyPy are:\n\nTry the most up-to-date nightly binary builds , if the official\nrelease is too old for what you want to do.\nReverse debugger: This version enables debugging your Python\nprograms by going forward and backward in time.  See the RevDB\ndocumentation.\n\n\nOld-style sandboxing: A special safe version.\nThis is NOT the version announced in-development during 2019!\nRead the docs about sandboxing.\nThis version is not supported and not actively maintained.  You\nwill likely have to fix some issues yourself, or checkout an old\nversion, or otherwise play around on your own.  We provide this\ndocumentation only for historical reasons.  Please do not use in\nproduction.  For reference, there are some very old, unmaintained\nbinaries for Linux (32bit, 64bit).\n\n\n\nInstalling\nAll binary versions are packaged in a tar.bz2 or zip file.  When\nuncompressed, they run in-place.  You can uncompress them\neither somewhere in your home directory or, say, in /opt.\nIf you want, put a symlink from somewhere like\n/usr/local/bin/pypy to /path/to/pypy_expanded/bin/pypy.  Do\nnot move or copy the executable pypy outside the tree --- put\na symlink to it, otherwise it will not find its libraries.\n\n\nInstalling more modules\nThere are as yet few distribution-ready packages. conda is one easy\nway to get packages with a minimum of compilation.\nWe recommend installing pip, which is the standard package\nmanager of Python.  It works like it does on CPython as explained in the\ninstallation documentation.\nIf you use your distribution's PyPy package we recommend you install packages\ninto a virtualenv. If you try to build a module and the build process complains\nabout \"missing Python.h\", you may need to install the pypy-dev package.\n\n\nBuilding from source\n(see more build instructions)\n\nGet the source code.  The preferred way is to checkout the current\ntrunk using Mercurial.  The trunk usually works and is of course\nmore up-to-date:\nhg clone https://foss.heptapod.net/pypy/pypy\nThe trunk contains PyPy 2.  For PyPy 3, switch to the correct branch:\n# switch to the branch that implements Python 3.7\nhg update py3.7\nAlternatively, get one of the following smaller packages for the source at\nthe same revision as the above binaries:\n\npypy2.7-v7.3.6-src.tar.bz2 (sources, PyPy 2.7 only)\npypy3.7-v7.3.7-src.tar.bz2 (sources, PyPy 3.7 only)\npypy3.8-v7.3.7-src.tar.bz2 (sources, PyPy 3.8 only)\n\n\nMake sure you installed the dependencies.  See the list here.\n\nEnter the goal directory:\ncd pypy/pypy/goal\n\nRun the rpython script.  Here are the common combinations\nof options (works also with python instead of pypy;\nrequires CPython 2.7 or PyPy 2, even to build PyPy 3):\n# get the JIT version\npypy ../../rpython/bin/rpython -Ojit targetpypystandalone\n# get the no-jit version\npypy ../../rpython/bin/rpython -O2 targetpypystandalone\n# get the sandbox version\npypy ../../rpython/bin/rpython -O2 --sandbox targetpypystandalone\n\nEnjoy Mandelbrot :-)  It takes on the order of half an hour to\nfinish the translation, and about 3GB of RAM on a 32-bit system\nand about 5GB on 64-bit systems.  (Do not start a translation on a\nmachine with insufficient RAM!  It will just swap forever.  See\nnotes below in that case.)\nIf you want to install this PyPy as root, please read the next section,\nPackaging.\n\nNotes:\n\nIt is recommended to use PyPy to do translations, instead of using CPython,\nbecause it is twice as fast.  You should just start by downloading an\nofficial release of PyPy (with the JIT).  If you really have to use CPython\nthen note that we are talking about CPython 2.7 here, not CPython 3.x.\n(Older versions like 2.6 are out.)\nOn some 32-bit systems, the address space limit of 2 or 3 GB of RAM\ncan be an issue.  More generally you may be just a little bit low of\nRAM.  First note that 2 GB is really not enough nowadays; on Windows\nyou first need to refer to the Windows build instructions.  More\nprecisely, translation on 32-bit takes at this point 2.7 GB if PyPy is\nused and 2.9 GB if CPython is used.  There are two workarounds:\n1. use PyPy, not CPython.  If you don't have any PyPy so far, not even\nan older version, then you need to build one first, with some parts\nremoved.  So, first translate with:\ncpython2 rpython -Ojit targetpypystandalone \\\n--withoutmod-micronumpy --withoutmod-cpyext\nthen copy pypy-c and libpypy_c.so somewhere else, and finally\ncall it with ...pypy-c ../../rpython/bin/rpython -Ojit.\n2. if even using PyPy instead of CPython is not enough, try to tweak\nsome internal parameters.  Example (slower but saves around 400MB):\nPYPY_DONT_RUN_SUBPROCESS=1 PYPY_GC_MAX_DELTA=200MB \\\npypy --jit loop_longevity=300 ../../rpython/bin/rpython \\\n-Ojit --source\n# then read the next point about --source\n\nYou can run translations with --source, which only builds the C\nsource files (and prints at the end where).  Then you can cd there\nand execute make.  This is another way to reduce memory usage.\nNote that afterwards, you have to run manually pypy-c\n.../pypy/tool/build_cffi_imports.py if you want to be able to import\nthe cffi-based modules.\nLike other JITs, PyPy doesn't work out of the box on some Linux\ndistributions that trade full POSIX compliance for extra security\nfeatures.  E.g. with PAX, you have to run PyPy with paxctl -cm.\nThis also applies to translation (unless you use CPython to run the\ntranslation and you specify --source).\n\n\n\nPackaging\nOnce PyPy is translated from source, a binary package similar to those\nprovided in the section Default (with a JIT Compiler) above can be\ncreated with the package.py script:\ncd ./pypy/pypy/tool/release/\npython package.py --help  # for information\npython package.py --archive-name pypy-my-own-package-name\nIt is recommended to use package.py because custom scripts will\ninvariably become out-of-date.  If you want to write custom scripts\nanyway, note an easy-to-miss point: some modules are written with CFFI,\nand require some compilation.  If you install PyPy as root without\npre-compiling them, normal users will get errors:\n\nPyPy 2.5.1 or earlier: normal users would see permission errors.\nInstallers need to run pypy -c \"import gdbm\" and other similar\ncommands at install time; the exact list is in package.py.  Users\nseeing a broken installation of PyPy can fix it after-the-fact if they\nhave sudo rights, by running once e.g. sudo pypy -c \"import gdbm.\nPyPy 2.6 and later: anyone would get ImportError: no module named\n_gdbm_cffi.  Installers need to run pypy _gdbm_build.py in the\nlib_pypy directory during the installation process (plus others;\nsee the exact list in package.py).  Users seeing a broken\ninstallation of PyPy can fix it after-the-fact, by running pypy\n/path/to/lib_pypy/_gdbm_build.py.  This command produces a file\ncalled _gdbm_cffi.pypy-41.so locally, which is a C extension\nmodule for PyPy.  You can move it at any place where modules are\nnormally found: e.g. in your project's main directory, or in a\ndirectory that you add to the env var PYTHONPATH.\n\n\n\nChecksums\nChecksums for the downloads are here",
      "tags": "",
      "url": "https://www.pypy.org/download_advanced.html"
    },
    {
      "title": "PyPy 7.3.2 triple release: python 2.7, 3.6, and 3.7",
      "text": "The PyPy team is proud to release version 7.3.2 of PyPy, which includes\nthree different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13PyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.PyPy3.7 alpha: which is our first release of an interpreter supporting the\nsyntax and the features of Python 3.7, including the stdlib for CPython\n3.7.9. We call this an alpha release since it is our first. It is based off PyPy 3.6 so\nissues should be around compatibility and not stability. Please try it out\nand let us know what is broken or missing. We have not implemented some of the\ndocumented changes in the re module, and other pieces are also\nmissing. For more information, see the PyPy 3.7 wiki page\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the 7.3.0 (Dec\n2019) and 7.3.1 (April 2020) releases, but read on to find out what is new.\nConda Forge now supports PyPy as a python interpreter. The support is quite\ncomplete for linux and macOS. This is the result of a lot of\nhard work and good will on the part of the Conda Forge team.  A big shout out\nto them for taking this on.\nDevelopment of PyPy has transitioning to https://foss.heptapod.net/pypy/pypy.\nThis move was covered more extensively in this blog post. We have seen an\nincrease in the number of drive-by contributors who are able to use gitlab +\nmercurial to create merge requests.\nThe CFFI backend has been updated to version 1.14.2. We recommend using CFFI\nrather than c-extensions to interact with C, and using cppyy for performant\nwrapping of C++ code for Python.\nNumPy has begun shipping wheels on PyPI for PyPy, currently for linux 64-bit\nonly.  Wheels for PyPy windows will be available from the next NumPy release. Thanks to NumPy for their support.\nA new contributor took us up on the challenge to get windows 64-bit support.\nThe work is proceeding on the win64 branch, more help in coding or\nsponsorship is welcome.\nAs always, this release fixed several issues and bugs.  We strongly recommend\nupdating. Many of the fixes are the direct result of end-user bug reports, so\nplease continue reporting issues as they crop up.You can find links to download the v7.3.2 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject. Please help support us at Open Collective. If PyPy is not yet good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 8 new contributors,\nthanks for pitching in.\nIf you are a python library maintainer and use c-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6, and 3.7. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)big- and little-endian variants of PPC64 running Linux,s390x running Linux64-bit ARM machines running Linux.\n\nPyPy does support ARM 32 bit processors, but does not release binaries.\n\n\n\n\nWhat else is new?\nFor more information about the 7.3.2 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2020/09/pypy-732-triple-release-python-27-36-3980901335490872787.html"
    },
    {
      "title": "PyPy is on Open Collective",
      "text": "Hi all,\n\nPyPy is now a member of Open Collective, a fiscal host.  We have been thinking about switching to this organization for a couple of years; we like it for various reasons, like the budget transparency and the lightweight touch.  We can now officially announce our membership!\n\nWith this, we are now again free to use PyPy for all financial issues, like receiving funds professionally, paying parts of sprint budgets as we like, and so on.  We will shortly be reintroducing buttons that link to Open Collective from the PyPy web site.\n\nAlthough the old donation buttons were removed last year, we believe that there are still a few people that send regularly money to the SFC, the not-for-profit charity we were affiliated with.  If you do, please stop doing it now (and, if you like to do so, please set up an equivalent donation to PyPy on Open Collective).\n\nAnd by the way, sorry for all of you who were getting mixed feelings from the previous blog post (co-written with the SFC).  PyPy is committed to continue being Open Source just like before.  This was never in question.  What these two blog posts mean is only that we switched to a different organization for our internal finances.\n\nWe're looking forward to how this new relationship will go!\n\nArmin Rigo, for the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/08/pypy-is-on-open-collective-5673322428814364737.html"
    },
    {
      "title": "A new chapter for PyPy",
      "text": "PyPy winds down its membership in the Software Freedom Conservancy\n\nConservancy and PyPy's great work together\n\nPyPy joined Conservancy in\nthe second half of 2010, shortly after the release of\nPyPy 1.2, the first version to contain a fully functional JIT. In 2013, PyPy\nstarted supporting ARM, bringing its just-in-time speediness to many more devices and began working toward supporting NumPy to help\nscientists crunch their numbers faster. Together, PyPy and Conservancy ran successful fundraising drives and facilitated payment\nand oversight for contractors and code sprints.\n\nConservancy supported PyPy's impressive growth as it expanded support for\ndifferent hardware platforms, greatly improved the performance of C extensions,\nand added support for Python 3 as the language itself evolved.\n\nThe road ahead\n  \nConservancy provides a fiscal and organizational home for projects that find the\nfreedoms and guardrails that come along with a charitable home advantageous for\ntheir community goals. While this framework was a great fit for the early PyPy\ncommunity, times change and all good things must come to an end.\n\nPyPy will remain a free and open source project, but the community's structure\nand organizational underpinnings will be changing and the PyPy community will be\nexploring options outside of the charitable realm for its next phase of growth\n(\"charitable\" in the legal sense -- PyPy will remain a community project).\n\nDuring the last year PyPy and Conservancy have worked together to properly\nutilise the generous donations made by stalwart PyPy enthusiats over the years\nand to wrap up PyPy's remaining charitable obligations. PyPy is grateful for\nthe Conservancy's help in shepherding the project toward its next chapter.\n\nThank yousFrom Conservancy: \"We are happy that Conservancy was able to help PyPy bring important software\nfor the public good during a critical time in its history. We wish the\ncommunity well and look forward to seeing it develop and succeed in new ways.\" \u2014 Karen Sandler, Conservancy's Executive DirectorFrom PyPy:\"PyPy would like to thank Conservancy for their decade long support in\nbuilding the community and wishes Conservancy continued success in their\njourney promoting, improving, developing and defending free and open source\nsofware.\" \u2014 Simon Cross & Carl Friedrich Bolz-Tereick, on behalf of PyPy.\n\n\nAbout\n\nPyPy is a multi-layer python interpreter with a built-in JIT compiler that runs\nPython quickly across different computing environments.\nSoftware Freedom Conservancy (Conservancy) is a charity that provides a home\nto over forty free and open source software projects.",
      "tags": "pypy",
      "url": "https://www.pypy.org/posts/2020/08/a-new-chapter-for-pypy-8388322709667328389.html"
    },
    {
      "title": "PyPy 7.3.1 released",
      "text": "The PyPy team is proud to release the version 7.3.1 of PyPy, which includes\ntwo different interpreters:\n\n\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13\nPyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.\n\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, no APIs have changed since the 7.3.0 release\nin December, but read on to find out what is new.\n\n\nConda Forge now supports PyPy as a Python interpreter. The support right now\nis being built out. After this release, many more c-extension-based\npackages can be successfully built and uploaded. This is the result of a lot of\nhard work and good will on the part of the Conda Forge team.  A big shout out\nto them for taking this on.\n\n\nWe have worked with the Python packaging group to support tooling around\nbuilding third party packages for Python, so this release updates the pip and\nsetuptools installed when executing pypy -mensurepip to pip>=20. This\ncompletes the work done to update the PEP 425 python tag from pp373 to\nmean \u201cPyPy 7.3 running python3\u201d to pp36 meaning \u201cPyPy running Python\n3.6\u201d (the format is recommended in the PEP). The tag itself was\nchanged in 7.3.0, but older pip versions build their own tag without querying\nPyPy. This means that wheels built for the previous tag format will not be\ndiscovered by pip from this version, so library authors should update their\nPyPy-specific wheels on PyPI.\n\n\nDevelopment of PyPy is transitioning to https://foss.heptapod.net/pypy/pypy.\nThis move was covered more extensively in the blog post from last month.\n\n\nThe CFFI backend has been updated to version 14.0. We recommend using CFFI\nrather than c-extensions to interact with C, and using cppyy for performant\nwrapping of C++ code for Python. The cppyy backend has been enabled\nexperimentally for win32, try it out and let use know how it works.\n\n\nEnabling cppyy requires a more modern C compiler, so win32 is now built\nwith MSVC160 (Visual Studio 2019). This is true for PyPy 3.6 as well as for 2.7.\n\n\nWe have improved warmup time by up to 20%, performance of io.StringIO to\nmatch if not be faster than CPython, and improved JIT code generation for\ngenerators (and generator expressions in particular) when passing them to\nfunctions like sum, map, and map that consume them. Performance of closures has also be improved in certain situations.\n\n\nAs always, this release fixed several issues and bugs raised by the growing\ncommunity of PyPy users.  We strongly recommend updating. Many of the fixes are\nthe direct result of end-user bug reports, so please continue reporting issues\nas they crop up.\n\nYou can find links to download the v7.3.1 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 13 new contributors,\nthanks for pitching in.\n\n\nIf you are a Python library maintainer and use c-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy wheels.\n\n\n\n\n\u00a0\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6, and soon 3.7. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\n\nThis PyPy release supports:\n\n\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\n\n\n\n\nWhat else is new?\nFor more information about the 7.3.1 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team\n\n\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2020/04/pypy-731-released-6266451647387657480.html"
    },
    {
      "title": "Leysin 2020 Sprint Report",
      "text": "At the end of February ten of us gathered in Leysin, Switzerland to work on\na variety of topics including HPy, PyPy Python 3.7 support and the PyPy\nmigration to Heptapod.\n\n\n\n\nWe had a fun and productive week. The snow was beautiful. There was skiing\nand lunch at the top of Berneuse, cooking together, some late nights at\nthe pub next door, some even later nights coding, and of course the\nobligatory cheese fondue outing.\n\nThere were a few of us participating in a PyPy sprint for the first time\nand a few familiar faces who had attended many sprints. Many different\nprojects were represented including PyPy, HPy, GraalPython,\nHeptapod, and rust-cpython. The atmosphere was relaxed and welcoming, so if\nyou're thinking of attending the next one -- please do!\n\nTopics worked on:\n\n\nHPy\nHPy is a new project to design and implement a better API for extending\nPython in C. If you're unfamiliar with it you can read more about it at\nHPy.\n\nA lot of attention was devoted to the Big HPy Design Discussion which\ntook up two full mornings. So much was decided that this will likely\nget its own detailed write-up, but bigger topics included:\n\nthe HPy GetAttr, SetAttr, GetItem and SetItem methods,\nHPy_FromVoidP and HPy_AsVoidP for passing HPy handles to C functions\nthat pass void* pointers to callbacks,\navoiding having va_args as part of the ABI,\nexception handling,\nsupport for creating custom types.\n\nQuite a few things got worked on too:\n\nimplemented support for writing methods that take keyword arguments with\nHPy_METH_KEYWORDS,\nimplemented HPy_GetAttr, HPy_SetAttr, HPy_GetItem, and HPy_SetItem,\nstarted implementing support for adding custom types,\nstarted implementing dumping JSON objects in ultrajson-hpy,\nrefactored the PyPy GIL to improve the interaction between HPy and\nPyPy's cpyext,\nexperimented with adding HPy support to rust-cpython.\n\nAnd there was some discussion of the next steps of the HPy initiative\nincluding writing documentation, setting up websites and funding, and\npossibly organising another HPy gathering later in the year.\n\n\nPyPy\n\nGeorges gave a presentation on the Heptapod topic and branch workflows\nand showed everyone how to use hg-evolve.\nWork was done on improving the PyPy CI buildbot post the move to\nheptapod, including a light-weight pre-merge CI and restricting\nwhen the full CI is run to only branch commits.\nA lot of work was done improving the -D tests. \n\n\n\nMiscellaneous\n\nArmin demoed VRSketch and NaN Industries in VR, including an implementation\nof the Game of Life within NaN Industries!\nSkiing!\n\n\n\nAftermath\nImmediately after the sprint large parts of Europe and the world were\nhit by the COVID-19 epidemic. It was good to spend time together before\ntravelling ceased to be a sensible idea and many gatherings were cancelled.\n\nKeep safe out there everyone.\n\nThe HPy & PyPy Team & Friends\n\nIn joke for those who attended the sprint: Please don't replace this blog post\nwith its Swedish translation (or indeed a translation to any other language :).",
      "tags": "cpyext,CPython,GraalPython,Heptapod,hpy,pypy,pypy3",
      "url": "https://www.pypy.org/posts/2020/03/leysin-2020-sprint-report-764567777353955897.html"
    },
    {
      "title": "PyPy and CFFI have moved to Heptapod",
      "text": "It has been a very busy month, not so much because of deep changes in the JIT of PyPy but more around the development, deployment, and packaging of the project.\n\n\n\u00a0\n\nHosting\nThe biggest news is that we have moved the center of our development off Bitbucket and to the new https://foss.heptapod.net/pypy. This is a friendly fork of Gitlab called heptapod that understands Mercurial and is hosted by Clever Cloud. When Atlassian decided to close down Mercurial hosting on bitbucket.org, PyPy debated what to do. Our development model is based on long-lived branches, and we want to keep the ability to immediately see which branch each commit came from. Mercurial has this, git does not (see our FAQ). Octobus, whose business is Mercurial, developed a way to use Mercurial with Gitlab called heptapod. The product is still under development, but quite usable (i.e., it doesn't get in the way). Octobus partnered with Clever Cloud hosting to offer community FOSS projects hosted on Bitbucket who wish to remain with Mercurial a new home. PyPy took them up on the offer, and migrated its repos to https://foss.heptapod.net/pypy. We were very happy with how smooth it was to import the repos to heptapod/GitLab, and are learning the small differences between Bitbucket and GitLab. All the pull requests, issues, and commits kept the same ids, but work is still being done to attribute the issues, pull requests, and comments to the correct users. So from now on, when you want to contribute to PyPy, you do so at the new home.\n\nCFFI, which previously was also hosted on Bitbucket, has joined the PyPy group at https://foss.heptapod.net/pypy/cffi.\n\n\n\u00a0\n\nWebsite\nSecondly, thanks to work by https://baroquesoftware.com/ in leading a redesign and updating the logo, the https://www.pypy.org website has undergone a facelift. It should now be easier to use on small-screen devices. Thanks also to the PSF for hosting the site.\n\n\n\u00a0\n\nPackaging\nAlso, building PyPy from source takes a fair amount of time. While we provide downloads in the form of tarballs or zipfiles, and some platforms such as debian and Homebrew provide packages, traditionally the downloads have only worked on a specific flavor of operating system. A few years ago squeaky-pl started providing portable builds. We have adopted that build system for our linux offerings, so the nightly downloads and release downloads should now work on any glibc platform that has not gone EndOfLife. So there goes another excuse not to use PyPy. And the \"but does it run scipy\" excuse also no longer holds, although \"does it speed up scipy\" still has the wrong answer. For that we are working on HPy, and will be sprinting soon.\nThe latest versions of pip, wheel, and setuptools, together with the manylinux2010 standard for linux wheels and tools such as multibuild or cibuildwheels (well, from the next version) make it easier for library developers to build binary wheels for PyPy. If you are having problems getting going with this, please reach out.\n\n\n\n\u00a0\n\nGive it a try\nThanks to all the folks who provide the infrastructure PyPy depends on. We hope the new look will encourage more involvement and engagement. Help prove us right!\n\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/02/pypy-and-cffi-have-moved-to-heptapod-5791595152472747032.html"
    },
    {
      "title": "Leysin Winter sprint 2020: Feb 29 - March 8th",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the fourteenth\ntime.  This is a fully public sprint: newcomers and topics other than\nthose proposed below are welcome.\n\n\n\n\nGoals and topics of the sprint\nThe list of topics is open.\u00a0 For reference, we would like to work at least partially on the following topics:\n\nHPy \nPython 3.7 support (buildbot status)\n\nAs usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off (for ski or anything else).\n\n\nTimes and accomodation\nThe sprint will occur for one week starting on Saturday, the 29th of February, to Sunday, the 8th of March 2020\u00a0(dates were pushed back one day!)\u00a0 It will occur in Les Airelles, a different bed-and-breakfast place from the traditional one in Leysin.\u00a0 It is a nice old house at the top of the village.\n\nWe have a 4- or 5-people room as well as up to three double-rooms.\u00a0 Please register early!\u00a0 These rooms are not booked for the sprint in advance, and might be already taken if you end up announcing yourself late.\u00a0 We have a big room for up to 7 people with nice view, which might be split in two or three sub-rooms; plus possibly separately-booked double rooms if needed. (But it is of course always possible to book at a different place in Leysin.)\n\nFor more information, see our repository or write to me directly at armin.rigo@gmail.com.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/01/leysin-winter-sprint-2020-feb-28-march-6349761524797409012.html"
    },
    {
      "title": "Python compatibility",
      "text": "The goal of this page is to point out some of the differences between running\npython with PyPy and with CPython\n\nTL;DR\nPure python code works, but there are a few differences with object lifetime\nmanagement. Modules that use the CPython C API will probably work, but will\nnot achieve a speedup via the JIT. We encourage library authors to use CFFI\ninstead.\nIf you are looking for how to use PyPy with the scientific python ecosystem,\nwe encourage you to use conda, since they repackage common libraries like\nscikit-learn and SciPy for PyPy.\n\n\nRefcounting, __del__, and resource use\nThe main difference in pure-python code that is not going to be fixed is that\nPyPy does\nnot support refcounting semantics for \"automatically\" releasing state when\nan object's __del__ is called. The following code won't fill the\nfile immediately, but only after a certain period of time, when the GC\ndoes a collection and flushes the output, since the file is only closed when\nthe __del__ method is called:\nopen(\"filename\", \"w\").write(\"stuff\")\nThe proper fix is\nwith open(\"filename\", \"w\") as f:\n    f.write(\"stuff\")\nThe same problem---not closing your files---can also show up if your\nprogram opens a large number of files without closing them explicitly.\nIn that case, you can easily hit the system limit on the number of file\ndescriptors that are allowed to be opened at the same time.\nPyPy can be run with the command-line option -X track-resources (as in,\npypy -X track-resources myprogram.py). This produces a ResourceWarning\nwhen the GC closes a non-closed file or socket.  The traceback for the place\nwhere the file or socket was allocated is given as well, which aids finding\nplaces where close() is missing.\nSimilarly, remember that you must close() a non-exhausted\ngenerator in order to have its pending finally or with\nclauses executed immediately:\ndef mygen():\n    with foo:\n        yield 42\n\nfor x in mygen():\n    if x == 42:\n        break    # foo.__exit__ is not run immediately!\n\n# fixed version:\ngen = mygen()\ntry:\n    for x in gen:\n        if x == 42:\n            break\nfinally:\n    gen.close()\nMore generally, __del__() methods are not executed as predictively\nas on CPython: they run \"some time later\" in PyPy (or not at all if\nthe program finishes running in the meantime).  See more details\nhere.\n\n\nWhy is memory usage so high?\nNote that PyPy returns unused memory to the operating system only after\na madvise() system call (at least Linux, OS X, BSD) or on Windows.  It is\nimportant to realize that you may not see this in top.  The unused\npages are marked with MADV_FREE, which tells the system \"if you\nneed more memory at some point, grab this page\".  As long as memory is\nplentiful, the RES column in top might remains high.  (Exceptions to\nthis rule are systems with no MADV_FREE, where we use\nMADV_DONTNEED, which forcefully lowers the RES.  This includes\nLinux <= 4.4.)\n\n\nMore info\nA more complete list of known differences is available at our dev site.",
      "tags": "",
      "url": "https://www.pypy.org/compat.html"
    },
    {
      "title": "Contact",
      "text": "irc: #pypy on irc.libera.chat\n\n\nNOTE: the official #pypy channel used to be on Freenode, but it is no\nlonger the case\n\n\n\nmailing list: pypy-dev at python.org\nfor security related issues, non-public funding enquiries etc. please contact pypy-z@python.org\nthe issue tracker (registration required to open new issues or to comment)\nmore on our dev site.\ncode on https://foss.heptapod.net/pypy/pypy",
      "tags": "",
      "url": "https://www.pypy.org/contact.html"
    },
    {
      "title": "Download and Install",
      "text": "We provide pre-compiled binaries for many platforms and OSes.\n\nNote\nOur nightly binary builds have the most recent bugfixes and performance\nimprovements, though they can be less stable than the official releases. See\nthese links for other versions or more information including other\nplatforms.\n\n\n\nPyPy latest\n\n\n\n\n\n\n\n\nOS\nPyPy3.8\nPyPy3.7\nPyPy2.7\nNotes\n\n\n\nLinux x86 64 bit\nDownload\nDownload\nDownload\ncompatible with CentOS6 and later\n\nWindows 64 bit\nDownload\nDownload\nDownload\ncompatible with any windows 64-bit\nyou might need the VC runtime library installer vcredist.x64.exe\n\n\nMacOS\nDownload\nDownload\nDownload\nHigh Sierra >= 10.13, not for Sierra and below. Not signed, for signed\npackages use Homebrew.\n\nLinux ARM64\nDownload\nDownload\nDownload\ncompatible with CentOS7 and later\n\n\n\n\n\nNote\nSSL Certificates\nWhile the linux binaries ship an OpenSSL library, they do not ship a\ncertificate store for SSL certificates. If you wish to use SSL module,\nyou will need a valid certificate store. You can use the certifi package\nand set SSL_CERT_FILE to certifi.where() or install your platform\ncertificates which should be discovered by the _ssl module.\n\n\n\nSource\n\n3.8 Source (tar.bz2); 3.8 Source (zip).\n3.7 Source (tar.bz2); 3.7 Source (zip).\n\n\n\nMore information\nVisit the more information page for other platforms, information about\nrunning PyPy, STM, instructions on building from source and more.\n\n\nChecksums\nChecksums for the downloads are here",
      "tags": "",
      "url": "https://www.pypy.org/download.html"
    },
    {
      "title": "PyPy - Features",
      "text": "PyPy is a replacement for CPython.  It is built using the RPython\nlanguage that was co-developed with it.  The main reason to use it\ninstead of CPython is speed: it runs generally faster (see next section).\nPyPy implements Python 2.7.18, and 3.7.10.\nIt supports all of the core language, passing the Python 2.7 test suite\nand almost all of the 3.7 test suite (with minor modifications) It supports most of\nthe commonly used Python standard library modules. For known differences with\nCPython, see our compatibility page.\nThe following CPU architectures are supported and maintained:\n\nx86 (IA-32) and x86_64\nARM platforms (ARMv6 or ARMv7, with VFPv3)\nAArch64\nPowerPC 64bit both little and big endian\nSystem Z (s390x)\n\nPyPy's x86 version runs on several operating systems, such as Linux\n(32/64 bits), Mac OS X (64 bits), Windows (32 bits), OpenBSD, FreeBSD.\nAll non-x86 versions are only supported on Linux.\nIf you are interested in helping, see our howtohelp page.\n\nThe main features of PyPy:\n\nSpeed\nOur main executable comes with a Just-in-Time compiler.  It is\nreally fast in running most benchmarks\u2014including very large and\ncomplicated Python applications, not just 10-liners.\nThere are two cases that you should be aware where PyPy will not be\nable to speed up your code:\n\nShort-running processes: if it doesn't run for at least a few seconds,\nthen the JIT compiler won't have enough time to warm up.\nIf all the time is spent in run-time libraries (i.e. in C functions),\nand not actually running Python code, the JIT compiler will not help.\n\nSo the case where PyPy works best is when executing long-running\nprograms where a significant fraction of the time is spent executing\nPython code.  This is the case covered by the majority of our\nbenchmarks, but not all of them --- the goal of PyPy is to get speed\nbut still support (ideally) any Python program.\n\n\nMemory usage\nMemory-hungry Python programs (several hundreds of MBs or more) might\nend up taking less space than they do in CPython.  It is not always\nthe case, though, as it depends on a lot of details.  Also note that\nthe baseline is higher than CPython's.\n\n\nStackless\nSupport for Stackless and greenlets are now integrated in the normal\nPyPy.  More detailed information is available here.\n\n\nOther features\nPyPy has many secondary features and semi-independent\nprojects.  We will mention here:\n\nOther languages:  we also implemented other languages that makes\nuse of our RPython toolchain: Prolog (almost complete), as\nwell as Smalltalk, JavaScript, Io, Scheme and Gameboy.\nThere is also a Ruby implementation called Topaz and a PHP implementation\ncalled HippyVM.\n\n\n\n\nSandboxing\nPyPy's sandboxing is a working prototype for the idea of running untrusted\nuser programs. Unlike other sandboxing approaches for Python, PyPy's does not\ntry to limit language features considered \"unsafe\". Instead we replace all\ncalls to external libraries (C or platform) with a stub that communicates\nwith an external process handling the policy.\n\nNote\nPlease be aware that it is a prototype only.  It needs work to become\nmore complete, and you are welcome to help.  In particular, almost none\nof the extension modules work (not even time ), and pypy_interact\nis merely a demo.  Also, a more complete system would include a way\nto do the same as pypy_interact from other languages than Python,\nto embed a sandboxed interpreter inside programs written in other\nlanguages.\n\nTo run the sandboxed process, you need to get the full sources and\nbuild pypy-sandbox from it (see Building from source).  These\ninstructions give you a pypy-c that you should rename to\npypy-sandbox to avoid future confusion.  Then run:\ncd pypy/sandbox\npypy_interact.py path/to/pypy-sandbox\n# don't confuse it with pypy/goal/pyinteractive.py!\nYou get a fully sandboxed interpreter, in its own filesystem hierarchy\n(try os.listdir('/')).  For example, you would run an untrusted\nscript as follows:\nmkdir virtualtmp\ncp untrusted.py virtualtmp/\npypy_interact.py --tmp=virtualtmp pypy-sandbox /tmp/untrusted.py\nNote that the path /tmp/untrusted.py is a path inside the sandboxed\nfilesystem.  You don't have to put untrusted.py in the real /tmp\ndirectory at all.\nTo read more about its features, try pypy_interact.py --help or go to\nour documentation site.",
      "tags": "",
      "url": "https://www.pypy.org/features.html"
    },
    {
      "title": "PyPy",
      "text": "A fast, compliant alternative implementation of Python\n\n    \n    Download PyPy\nWhat is PyPy ?\nDocumentation (external link)\n\n\n\nOn average, PyPy is 4.2 times faster than CPython\n\n\n\nPyPy (with JIT) benchmark times normalized to CPython. Smaller is\nbetter. Based on the geometric average of all benchmarks\n\n\n\"If you want your code to run faster,\nyou should probably just use PyPy.\"\n-- Guido van Rossum (creator of Python)\nAdvantages and distinct Features\n\nSpeed: thanks to its Just-in-Time compiler, Python programs\noften run faster on PyPy.  (What is a JIT compiler?)\nMemory usage: memory-hungry Python programs (several hundreds of\nMBs or more) might end up taking less space than they do in CPython.\nCompatibility: PyPy is highly compatible with existing python code.\nIt supports cffi, cppyy, and can run popular python libraries like\ntwisted, and django. It can also run NumPy, Scikit-learn and more via a\nc-extension compatibility layer.\nStackless: PyPy comes by default with support for stackless mode,\nproviding micro-threads for massive concurrency.\nAs well as other features.",
      "tags": "",
      "url": "https://www.pypy.org/"
    },
    {
      "title": "The PyPy Team (from 2008)",
      "text": "Armin Rigo\n\nArmin Rigo is a former researcher at the Heinrich-Heine Universitat\nD\u00fcsseldorf (Germany).  He studied Mathematics at the University\nof Lausanne (Switzerland), obtained his Ph.D. in Logic and Set\nTheory at the Free University of Brussels (Belgium) in 2002, and\nworked at the University of Southampton (UK) until 2005.  He is\nthe author of Psyco, the first just-in-time compiler for Python.\nHe is one of the founders and lead developers of the PyPy project\nwhich began in 2003.  He has taken part in all areas, from the Python\nlanguage definition to the RPython translation framework,\nincluding the garbage collector and the tracing just-in-time\ncompiler.\n\n\nMaciej Fija\u0142kowski\n\nMaciej is a freelancer working mostly on PyPy for the past several years.\nHe's a core developer since 2006, working on all kinds of parts in\nthe entire codebase including JIT, GC and assembler backends.\nMaciej has been going to many conferences, advertising PyPy to a broader\naudience for the past several years, including a keynote at Pycon 2010.\nHe's also the main maintainer of\njitviewer, a tool for analyzing performance of your python programs under\nPyPy.\n\n\nCarl Friedrich Bolz\n\nCarl Friedrich is a core developer since 2005, currently doing his PhD at the\nHeinrich-Heine Universit\u00e4t D\u00fcsseldorf (Germany). He has worked on most aspects\nof PyPy, from the core interpreter to the GC to the JIT. He has published\nseveral papers about the inner workings of PyPy, presenting them at various\nscientific conferences. Carl Friedrich is also interested in other dynamic\nlanguage implementation and was the original author of the Prolog\nimplementation.\nCarl Friedrich likes science fiction novels and sometimes plays the bassoon.\n\n\nAntonio Cuni\n\nAntonio Cuni loves skiing, mountains and programming languages.  He studied\nComputer Science at the University of Genova (Italy), and then at the same\nuniversity he obtained his Ph.D. in Computer Science in 2010, with a\ndissertation about the PyPy CLI JIT backend.  He has been a core PyPy\ndeveloper since 2006, working in various areas including the \"object oriented\nbackends\" for the CLI and JVM, the RPython translation framework, the Python\ninterpreter and the JIT compiler generator.  Apart from PyPy, he is the author of\nother popular tools such as pdb++.\n\n\nBenjamin Peterson\nBoth a PyPy and CPython core developer, Benjamin knows way too much about the\nnooks and cranies of the Python language. He is driven by a fascination with\ninterpreters and compilers of all shapes and sizes. Around the PyPy project, he\ntries to be generally useful and has taken on major projects including rewriting\nPyPy's Python compiler and porting PyPy to Python 2.7.\n\n\nAlex Gaynor\n\nAlex is software engineer living in Washington, DC. He's been a PyPy developer\nsince 2010, and has worked on many parts of the codebase, including the JIT\ncompiler's optimizers, the RPython translation toolchain, and the Python\ninterpreter. In addition to his work on PyPy, Alex is also the creator of\nTopaz, a Ruby VM built on RPython and a core developer of Django (a Python web\nframework) and CPython, as well as a retired member of the board of directors\nof the Python Software Foundation.\n\n\nH\u00e5kan Ard\u00f6\n\nH\u00e5kan Ard\u00f6 received his master of science degree in electrical\nengineering from Lund University in 2002. He specialized in\nVLSI-design and Image Processing. He worked as a software\nengineer at Axis Communications 2002-2003 before doing his\nPhD at the Centre for Mathematical Sciences of Lund University\n2003-2009 in the Mathematical Imaging Group. His thesis work consisted\nof designing image processing algorithms for traffic surveillance,\naiming for a system that automatically measures the safety of an\nintersection or road segment. He is currently working part-time as a\npostdoc at the Centre for Mathematical Sciences of Lund University\ncontinuing this work and part-time as CTO with a spinoff company\nCognimatics. His contributions to PyPy started 2010 and consists of\nthe array module as well as work on the JIT compiler's trace optimizers.\n\n\nHolger Krekel\n\nHolger Krekel is a founder of the PyPy project and has participated in\nPyPy core developement for several years as well as maintained much of\nits infrastructure.  He also is the author of the popular py.test and\ntox testing tools as well as execnet, a library for easily deploying\ndifferent interacting Python interpreters side by side.  He helped\nmanage multiple PyPy funding contracts through his company merlinux and was a\nPyPy representative within the Software Freedom Conservancy (SFC).  He\nholds a summa cum laude degree in computer science with a thesis about\nartificial intelligence applied to the game of Go.  As of 2011 he is on\nanother sabbatical-ish leave, caring for his newborn son, travelling\nand pondering what comes next.  Other than that he continues to care\nfor testing and some PyPy co-ordination bits behind the scene.\n\n\nSamuele Pedroni\nSamuele Pedroni got involved with PyPy almost at its inception in the\nspring of 2003. One of the design contributors to PyPy, his help has\nranged from infrastructure and processes, through building out\nRPython... optimizing the Python interpreter, to compressing resume\ndata in the last incarnation of the JIT compiler. Tempted away into the\napplication side of the software equation, these days he contributes\nsome words and wisdom to PyPy's paper writing.\n\n\nMany more people\nPyPy is and has always been an effort of many volunteers. Consult the LICENSE\nfile for details.",
      "tags": "",
      "url": "https://www.pypy.org/people.html"
    },
    {
      "title": "Performance",
      "text": "Contents\n\nProfiling: vmprof\nOptimization strategy\nMicro-tuning tips\n\n\nThis document collects strategies, tactics and tricks for making your\ncode run faster under PyPy.  Many of these are also useful hints for\nstock Python and other languages.  For contrast, we also describe some\nCPython (stock Python) optimizations that are not needed in PyPy.\n\n\nProfiling: vmprof\nAs a general rule, when considering performance issues, follow these\nthree points: first measure them (it is counter-productive to fight\nimaginary performance issues); then profile your code (it is useless\nto optimize the wrong parts).  Only optimize then.\nPyPy 2.6 introduced vmprof, a very-low-overhead statistical profiler.\nThe standard, non-statistical cProfile is also supported, and can be\nenabled without turning off the JIT.  We do recommend vmprof anyway\nbecause turning on cProfile can distort the result (sometimes massively,\nthough hopefully this should not be too common).\n\n\n\nOptimization strategy\nThese suggestions apply to all computer languages.  They're here as\nreminders of things to try before any Python or PyPy-specific tweaking.\n\nBuild a regression-test suite\nBefore you start tuning, build a regression-test suite for your code.\nThis front-loads a significant amount of work, but it means you can\ntry lots of optimizations without worrying so much about introducing\nfunctional bugs.\n\n\nMeasure, don't guess\nHuman beings are bad at guessing or intuiting where the hotspots in code are.\nMeasure, don't guess; use a profiler to pin down the 20% of the\ncode where the code is spending 80% of its time, then speed-tune that.\nMeasuring will save you a lot of effort wasted on tuning parts of the code\nthat aren't actually bottlenecks.\nAs you tune, re-profile frequently  so you can see how the hottest spots\nare shifting around.\n\n\nI/O-bound is different from compute-bound\nBe aware of the difference between code that is compute-bound (slow\nbecause it's doing a huge number of instructions) and code that is I/O\nbound (slow because of disk or network delays).\nExpect to get most of your gains from optimizing compute-bound code.\nIt's usually (though not always) a sign that you're near the end of\nworthwhile tuning when profiling shows that the bulk of the\napplication's time is spent on network and disk I/O.\n\n\nTune your algorithms first\nGenerally, when your code is doing things that are O(n**2) or larger\nin the size of your data set, the cost of those operations is going\nto swamp any small gains you can pick up with the tricks we describe\nhere.\nTune your algorithms first.  It's time to think about applying our\nlist of micro-tuning tips  after you think you've optimized out\nintrinsically expensive operations.\nThat said, be prepared for the possibility that you will discover\nbetter-hidden algorithmic problems as you micro-tune.  Likely\nyou will go through this cycle more than once.\n\n\nFocus on tight loops\nIt's extremely common for high time costs to lurk within some\ninnocuous-looking code inside a tight loop - especially in code\nthat does something like a searching/matching/lookup operation\nor any kind of graph traversal.\nProbably the most common kind of performance-killer in compute-bound\ncode is an O(n**2) operation that is disguised by being some sort of\nO(n) lookup or match inside an O(n) loop.\nAnother common time-sink is relatively expensive common-setup\noperations that are performed inside tight loops but could be moved\nto before they start.  (For a representative case of this, see the\nmicro-tuning tip on regexp compilation.)\n\n\nSmaller is faster\nModern computers have multiple levels of memory caching, some directly\non the processor chip.  Causing a cache miss at any level incurs a\nperformance penalty proportional to random-access time for the next\noutward (and much slower) layer of cache.\nAccordingly, smaller is faster.  Programs or routines with a small\nenough working set to fit inside a fast cache will be as fast as\nthat cache is. To make your code fast, reduce the length of the\nseries of Python or JIT-compiler opcodes it generates by making\nit simpler.\nThe tradeoff here is that algorithmic tuning often trades time for\nspace - that is, it increases the size of an algorithm's working set\nby including pre-computations or tables or reverse maps in order to\navoid O(n**2) operations.\nIt's impossible to predict in advance where the sweet spot in that\ntradeoff will be.  You have to try different things and measure -\nwhich takes us right back to \"Measure, don't guess\".  And another\nfunction of your regression test suite can be as a speed benchmark.\n\n\n\n\nMicro-tuning tips\nThese are in no particular order.\n\nKeep it simple\nSimple is better than complex. The PyPy JIT is not very smart; the\nsimpler your code is the better it will run. Here again, though, you face\na tradeoff: you may need to pay with more algorithmic complexity in order\nto avoid brute-force operations that are O(n**2) or worse.\nWrite plain-vanilla code in plain-vanilla ways. The PyPy JIT has many\nproductions that optimize a common usage pattern against an uncommon\nusage pattern.\n\n\nGlobal variables\nIn CPython, global variables and functions (including package imports)\nare much more expensive to reference than locals; avoid them.  (This\nis also good modularity practice).\nThe cost of CPython global references is high enough that, for example, if you\nhave code in a frequently-visited inner loop that uses int() a lot, it\nmay be worthwhile to create a local copy of the reference with \"int =\nint\" in an enclosing block.\nHowever, this in not true in JITted PyPy code. The \"int = int\" hack\nwon't buy you performance, it's just an extra copy.  The modularity\nreason for avoiding globals are still valid.\n\n\nRegular expressions\nRegular-expression compilation is expensive.  If the regexp pattern in\na search, match, or replace operation is static (doesn't mutate at\nruntime) refactor so it's only done once.\nIf the regexp compilation is in a class method, consider doing it as\nthe initializer of a regexp-valued static (shared) class member and\nusing that class member in your operation.\nIf the regexp compilation is in a free function, consider moving it\nto module level and referencing the resulting regexp object\n(but see the warning above about global variables).\n\n\nOld- vs. new-style classes\nNew-style classes allow faster attribute access and take up less core\nper instance than old-style classes.  Much of this advantage may be\nlost, however, if attribute names are not constant. For example: x.a\n= y or even setattr(x, 'a', y) will be much faster than a dynamic\nversion: setattr(x, 'a' + some_variable, y).\nClasses that inherit from both new- and old-style classes are\nextremely slow; avoid at all costs.\nIn PyPy, isinstance() called against an old-style class was very slow\nuntil 2.0.\n\n\nString concatenation is expensive\nIn CPython, you may want to replace:\ns = head + body + maybe + tail\nwith the admittedly less readable:\ns = \"%(head)s%(body)s%(maybe)s%(tail)s\" % locals()\nor even:\ns = \"{head}{body}{maybe}{tail}\".format(**locals())\nBoth of the latter forms avoid multiple-allocation overhead.\nBut PyPy's JIT makes the overhead of intermediate concatenations\ngo away in linear code that keeps the number of concatenations\nsmall, bound and constant.  (And locals() is rather slow\nwith PyPy's JIT.)\nOn the other hand, in code like this with a string-valued foo() function:\nfor x in mylist:\n    s += foo(x)\nthe JIT cannot optimize out intermediate copies.  This code is\nactually quadratic in the total size of the mylist strings due to\nrepeated string copies of ever-larger prefix segments.  (Such code\nis always fine for bytearrays, because in this case += is an\nin-place operation.)\nThis:\nparts = []\nfor x in mylist:\n    parts.append(foo(x))\ns = \"\".join(parts)\ncan be much faster because all the string concatenation in the last\nline creates exactly one new string object with one C-level copy\nsequence (and list operations are relatively cheap).\n\n\nFrame introspection and tracing are slow\nCertain function calls can disable PyPy's speed options over\nstretches of surrounding code called \"JIT scopes\".\nA JIT like PyPy's works based on the assumption that the only thing\nworth optimizing are loops that are executed often. Whenever the\ninterpreter enters a loop in the interpreted program, the JIT records\nwhat the interpreter does, creating a trace. This trace is optimized,\ncompiled to machine code and executed when the loop is hit with the\nconditions observed during tracing.  This trace is one kind of JIT scope.\nAnother kind of JIT scope that matters is a function, considered as\na unit for inlining.\nNote that a JIT scope is a run-time phenomenon, not a compile-time\none.  It's not confined by source-code module boundaries.  A library-\nor foreign-module call in a frequently-called loop or inlined function\nwill be part of its JIT scope.\nlocals(), globals(), sys._getframe(), sys.exc_info(), and sys.settrace\nwork in PyPy, but they incur a performance penalty that can be huge by\ndisabling the JIT over the enclosing JIT scope.\n(Thanks Eric S. Raymond for the text above)\n\nInsider's point of view\nThis section describes performance issues from the point of view of\ninsiders of the project; it should be particularly interesting if you\nplan to contribute in that area.\nOne of the goals of the PyPy project is to provide a fast and compliant\npython interpreter. Some of the ways we achieve this are by providing a\nhigh-performance garbage collector (GC) and a high-performance\nJust-in-Time compiler (JIT).  Results of comparing PyPy and CPython can\nbe found on the speed website. Those benchmarks are not a random\ncollection: they are a combination of real-world Python programs ---\nbenchmarks originally included with the (now dead) Unladen Swallow\nproject --- and benchmarks for which we found PyPy to be slow (and improved).\nConsult the descriptions of each for details.\nThe JIT, however, is not a magic bullet. There are several characteristics\nthat might surprise people who are not used to JITs in\ngeneral or to the PyPy JIT in particular.  The JIT is generally good at\nspeeding up straight-forward Python code that spends a lot of time in the\nbytecode dispatch loop, i.e., running actual Python code --- as opposed\nto running things that only are invoked by Python code.  Good\nexamples include numeric calculations or any kind of heavily\nobject-oriented program.  Bad examples include doing computations with\nlarge longs --- which is performed by unoptimizable support code.  When the\nJIT cannot help, PyPy is generally slower than CPython.\nMore specifically, the JIT is known not to work on:\n\nTests: The ideal unit tests execute each piece of tested code\nonce.  This leaves no time for the JIT to warm up.\nReally short-running scripts: A rule of thumb is if something runs below\n0.2s the JIT has no chance, but it depends a lot on the program in question.\nIn general, make sure you warm up your program before running benchmarks, if\nyou're measuring something long-running like a server.  The time required\nto warm up the JIT varies; give it at least a couple of seconds.  (PyPy's\nJIT takes an especially long time to warm up.)\nLong-running runtime functions: These are the functions provided\nby the runtime of PyPy that do a significant amount of work.\nPyPy's runtime is generally not as optimized as CPython's and we expect those\nfunctions to take somewhere between the same time as CPython to twice as long.\nThis includes, for example, computing with longs, or sorting large lists.\nA counterexample is regular expressions: although they take time, they\ncome with their own JIT.\n\nUnrelated things that we know PyPy to be slow at (note that we're probably\nworking on it):\n\nCPython C extension modules: Any C extension module recompiled\nwith PyPy takes a very large hit in performance.  PyPy supports C\nextension modules solely to provide basic functionality.\nIf the extension module is for speedup purposes only, then it\nmakes no sense to use it with PyPy at the moment.  Instead, remove it\nand use a native Python implementation, which also allows opportunities\nfor JIT optimization.  If the extension module is\nboth performance-critical and an interface to some C library, then it\nmight be worthwhile to consider rewriting it as a pure Python version\nthat uses CFFI for the interface.\nMissing RPython modules: A few modules of the standard library\n(like csv and cPickle) are written in C in CPython, but written\nnatively in pure Python in PyPy.  Sometimes the JIT is able to do a\ngood job on them, and sometimes not.  In most cases (like csv and\ncPickle), we're slower than CPython, with the notable exception of\njson and heapq.\nAbuse of itertools: The itertools module is often \"abused\" in the\nsense that it is used for the wrong purposes.  From our point of view,\nitertools is great if you have iterations over millions of items, but\nnot for most other cases.  It gives you 3 lines in functional style\nthat replace 10 lines of Python loops (longer but arguably much easier\nto read).  The pure Python version is generally not slower even on\nCPython, and on PyPy it allows the JIT to work much better --- simple\nPython code is fast.  The same argument also applies to filter(),\nreduce(), and to some extend map() (although the simple case\nis JITted), and to all usages of the operator module we can think\nof.\nCtypes: Ctypes is slower than on CPython.  Consider CFFI instead,\nwhich has special paths inside the JIT.\n\nWe generally consider things that are slower on PyPy than CPython to be bugs\nof PyPy.  If you find some issue that is not documented here,\nplease report it to our bug tracker for investigation.",
      "tags": "",
      "url": "https://www.pypy.org/performance.html"
    },
    {
      "title": "PyPy 7.3.0 released",
      "text": "The PyPy team is proud to release the version 7.3.0 of PyPy, which includes\ntwo different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13\nPyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.\n\n\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nWe have worked with the python packaging group to support tooling around\nbuilding third party packages for python, so this release changes the ABI tag\nfor PyPy.\n\n\nBased on the great work done in portable-pypy, the linux downloads we\nprovide are now built on top of the manylinux2010 CentOS6 docker image.\nThe tarballs include the needed shared objects to run on any platform that\nsupports manylinux2010 wheels, which should include all supported versions of\ndebian- and RedHat-based distributions (including Ubuntu, CentOS, and Fedora).\n\n\nThe CFFI backend has been updated to version 1.13.1. We recommend using CFFI\nrather than c-extensions to interact with C.\n\nThe built-in cppyy module was upgraded to 1.10.6, which\nprovides, among others, better template resolution, stricter enum handling,\nanonymous struct/unions, cmake fragments for distribution, optimizations for\nPODs, and faster wrapper calls. We reccomend using cppyy for performant\nwrapping of C++ code for Python.\n\n\nThe vendored pyrepl package for interaction inside the REPL was updated.\n\n\nSupport for codepage encoding and decoding was added for Windows.\n\n\nAs always, this release fixed several issues and bugs raised by the growing\ncommunity of PyPy users.  We strongly recommend updating. Many of the fixes are\nthe direct result of end-user bug reports, so please continue reporting issues\nas they crop up.\n\nYou can download the v7.3 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular packages to run\non pypy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 3 new contributors,\nthanks for pitching in.\n\nIf you are a python library maintainer and use c-extensions, please consider making a cffi / cppyy version of your library that would be performant on PyPy. If you are stuck with using the C-API, you can use docker images with PyPy built in or the multibuild system to build wheels.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bit, Mac OS X 64-bit, Windows 32-bit, OpenBSD, FreeBSD)\n\n\n\n\nbig- and little-endian variants of PPC64 running Linux \n\n\n\n\ns390x running Linux\n\n\n\n\n64-bit ARM machines running Linux\n\nUnfortunately at the moment of writing our ARM buildbots are out of service,\nso for now we are not releasing any binary for the ARM architecture (32-bit), although PyPy does support ARM 32-bit processors.\n\n\n\nWhat else is new?\nPyPy 7.2 was released in October, 2019.\nThere are many incremental improvements to RPython and PyPy, For more information about the 7.3.0 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/12/pypy-730-released-3614026620096963655.html"
    },
    {
      "title": "HPy kick-off sprint report",
      "text": "Recently Antonio, Armin and Ronan had a small internal sprint in the beautiful\ncity of Gda\u0144sk to kick-off the development of HPy. Here is a brief report of\nwhat was accomplished during the sprint.\n\nWhat is HPy?\nThe TL;DR answer is \"a better way to write C extensions for Python\".\nThe idea of HPy was born during EuroPython 2019 in Basel, where there was an\ninformal meeting which included core developers of PyPy, CPython (Victor\nStinner and Mark Shannon) and Cython (Stefan Behnel). The ideas were later also\ndiscussed with Tim Felgentreff of GraalPython, to make sure they would also be\napplicable to this very different implementation, Windel Bouwman of RustPython\nis following the project as well.\nAll of us agreed that the current design of the CPython C API is problematic\nfor various reasons and, in particular, because it is too tied to the current\ninternal design of CPython.  The end result is that:\n\n\nalternative implementations of Python (such as PyPy, but not only) have a\nhard time loading and executing existing C extensions;\nCPython itself is unable to change some of its internal implementation\ndetails without breaking the world. For example, as of today it would be\nimpossible to switch from using reference counting to using a real GC,\nwhich in turns make it hard for example to remove the GIL, as gilectomy\nattempted.\n\n\nHPy tries to address these issues by following two major design guidelines:\n\nobjects are referenced and passed around using opaque handles, which are\nsimilar to e.g., file descriptors in spirit. Multiple, different handles\ncan point to the same underlying object, handles can be duplicated and\neach handle must be released independently of any other duplicate.\nThe internal data structures and C-level layout of objects are not\nvisible nor accessible using the API, so each implementation if free to\nuse what fits best.\n\nThe other major design goal of HPy is to allow incremental transition and\nporting, so existing modules can migrate their codebase one method at a time.\nMoreover, Cython is considering to optionally generate HPy code, so extension\nmodule written in Cython would be able to benefit from HPy automatically.\nMore details can be found in the README of the official HPy repository.\n\n\nTarget ABI\nWhen compiling an HPy extension you can choose one of two different target ABIs:\n\n\nHPy/CPython ABI: in this case, hpy.h contains a set of macros and\nstatic inline functions. At compilation time this translates the HPy API\ninto the standard C-API. The compiled module will have no performance\npenalty, and it will have a \"standard\" filename like\nfoo.cpython-37m-x86_64-linux-gnu.so.\nUniversal HPy ABI: as the name implies, extension modules compiled\nthis way are \"universal\" and can be loaded unmodified by multiple Python\ninterpreters and versions.  Moreover, it will be possible to dynamically\nenable a special debug mode which will make it easy to find e.g., open\nhandles or memory leaks, without having to recompile the extension.\n\n\nUniversal modules can also be loaded on CPython, thanks to the\nhpy_universal module which is under development. An extra layer of\nindirection enables loading extensions compiled with the universal ABI. Users\nof hpy_universal will face a small performance penalty compared to the ones\nusing the HPy/CPython ABI.\nThis setup gives several benefits:\n\n\nExtension developers can use the extra debug features given by the\nUniversal ABI with no need to use a special debug version of Python.\nProjects which need the maximum level of performance can compile their\nextension for each relevant version of CPython, as they are doing now.\nProjects for which runtime speed is less important will have the choice of\ndistributing a single binary which will work on any version and\nimplementation of Python.\n\n\n\n\nA simple example\nThe HPy repo contains a proof of concept module. Here is a simplified\nversion which illustrates what a HPy module looks like:\n\n#include \"hpy.h\"\n\nHPy_DEF_METH_VARARGS(add_ints)\nstatic HPy add_ints_impl(HPyContext ctx, HPy self, HPy *args, HPy_ssize_t nargs)\n{\n    long a, b;\n    if (!HPyArg_Parse(ctx, args, nargs, \"ll\", &a, &b))\n        return HPy_NULL;\n    return HPyLong_FromLong(ctx, a+b);\n}\n\n\nstatic HPyMethodDef PofMethods[] = {\n    {\"add_ints\", add_ints, HPy_METH_VARARGS, \"\"},\n    {NULL, NULL, 0, NULL}\n};\n\nstatic HPyModuleDef moduledef = {\n    HPyModuleDef_HEAD_INIT,\n    .m_name = \"pof\",\n    .m_doc = \"HPy Proof of Concept\",\n    .m_size = -1,\n    .m_methods = PofMethods\n};\n\n\nHPy_MODINIT(pof)\nstatic HPy init_pof_impl(HPyContext ctx)\n{\n    HPy m;\n    m = HPyModule_Create(ctx, &moduledef);\n    if (HPy_IsNull(m))\n        return HPy_NULL;\n    return m;\n}\n\nPeople who are familiar with the current C-API will surely notice many\nsimilarities. The biggest differences are:\n\n\nInstead of PyObject *, objects have the type HPy, which as\nexplained above represents a handle.\nYou need to explicitly pass an HPyContext around: the intent is\nprimary to be future-proof and make it easier to implement things like\nsub- interpreters.\nHPy_METH_VARARGS is implemented differently than CPython's\nMETH_VARARGS: in particular, these methods receive an array of HPy\nand its length, instead of a fully constructed tuple: passing a tuple\nmakes sense on CPython where you have it anyway, but it might be an\nunnecessary burden for alternate implementations.  Note that this is\nsimilar to the new METH_FASTCALL which was introduced in CPython.\nHPy relies a lot on C macros, which most of the time are needed to support\nthe HPy/CPython ABI compilation mode. For example, HPy_DEF_METH_VARARGS\nexpands into a trampoline which has the correct C signature that CPython\nexpects (i.e., PyObject (*)(PyObject *self, *PyObject *args)) and\nwhich calls add_ints_impl.\n\n\n\n\nSprint report and current status\nAfter this long preamble, here is a rough list of what we accomplished during\nthe week-long sprint and the days immediatly after.\nOn the HPy side, we kicked-off the code in the repo: at the moment of writing\nthe layout of the directories is a bit messy because we moved things around\nseveral times, but we identified several main sections:\n\n\nA specification of the API which serves both as documentation and as an\ninput for parts of the projects which are automatically\ngenerated. Currently, this lives in public_api.h.\n\nA set of header files which can be used to compile extension modules:\ndepending on whether the flag -DHPY_UNIVERSAL_ABI is passed to the\ncompiler, the extension can target the HPy/CPython ABI or the HPy\nUniversal ABI\n\nA CPython extension module called hpy_universal which makes it\npossible to import universal modules on CPython\n\nA set of tests which are independent of the implementation and are meant\nto be an \"executable specification\" of the semantics.  Currently, these\ntests are run against three different implementations of the HPy API:\n\n\nthe headers which implements the \"HPy/CPython ABI\"\nthe hpy_universal module for CPython\nthe hpy_universal module for PyPy (these tests are run in the PyPy repo)\n\n\n\n\n\nMoreover, we started a PyPy branch in which to implement the\nhpy_univeral module: at the moment of writing PyPy can pass all the HPy\ntests apart the ones which allow conversion to and from PyObject *.\nAmong the other things, this means that it is already possible to load the\nvery same binary module in both CPython and PyPy, which is impressive on its\nown :).\nFinally, we wanted a real-life use case to show how to port a module to HPy\nand to do benchmarks.  After some searching, we choose ultrajson, for the\nfollowing reasons:\n\n\nit is a real-world extension module which was written with performance in\nmind\nwhen parsing a JSON file it does a lot of calls to the Python API to\nconstruct the various parts of the result message\nit uses only a small subset of the Python API\n\n\nThis repo contains the HPy port of ultrajson. This commit shows an example\nof what the porting looks like.\nujson_hpy is also a very good example of incremental migration: so far\nonly ujson.loads is implemented using the HPy API, while ujson.dumps\nis still implemented using the old C-API, and both can coexist nicely in the\nsame compiled module.\n\n\nBenchmarks\nOnce we have a fully working ujson_hpy module, we can finally run\nbenchmarks!  We tested several different versions of the module:\n\n\nujson: this is the vanilla implementation of ultrajson using the\nC-API. On PyPy this is executed by the infamous cpyext compatibility\nlayer, so we expect it to be much slower than on CPython\nujson_hpy: our HPy port compiled to target the HPy/CPython ABI. We\nexpect it to be as fast as ujson\nujson_hpy_universal: same as above but compiled to target the\nUniversal HPy ABI. We expect it to be slightly slower than ujson on\nCPython, and much faster on PyPy.\n\n\nFinally, we also ran the benchmark using the builtin json module. This is\nnot really relevant to HPy, but it might still be an interesting as a\nreference data point.\nThe benchmark is very simple and consists of parsing a big JSON file 100\ntimes. Here is the average time per iteration (in milliseconds) using the\nvarious versions of the module, CPython 3.7 and the latest version of the hpy\nPyPy branch:\n\n\n\n\n\n\n\n\u00a0\nCPython\nPyPy\n\nujson\n154.32\n633.97\n\nujson_hpy\n152.19\n\u00a0\n\nujson_hpy_universal\n168.78\n207.68\n\njson\n224.59\n135.43\n\n\n\nAs expected, the benchmark proves that when targeting the HPy/CPython ABI, HPy\ndoesn't impose any performance penalty on CPython. The universal version is\n~10% slower on CPython, but gives an impressive 3x speedup on PyPy! It it\nworth noting that the PyPy hpy module is not fully optimized yet, and we\nexpect to be able to reach the same performance as CPython for this particular\nexample (or even more, thanks to our better GC).\nAll in all, not a bad result for two weeks of intense hacking :)\nIt is also worth noting than PyPy's builtin json module does really\nwell in this benchmark, thanks to the recent optimizations that were described\nin an earlier blog post.\n\n\nConclusion and future directions\nWe think we can be very satisfied about what we have got so far. The\ndevelopment of HPy is quite new, but these early results seem to indicate that\nwe are on the right track to bring Python extensions into the future.\nAt the moment, we can anticipate some of the next steps in the development of\nHPy:\n\n\nThink about a proper API design: what we have done so far has\nbeen a \"dumb\" translation of the API we needed to run ujson. However,\none of the declared goal of HPy is to improve the design of the API. There\nwill be a trade-off between the desire of having a clean, fresh new API\nand the need to be not too different than the old one, to make porting\neasier.  Finding the sweet spot will not be easy!\nImplement the \"debug\" mode, which will help developers to find\nbugs such as leaking handles or using invalid handles.\nInstruct Cython to emit HPy code on request.\nEventually, we will also want to try to port parts of numpy to HPy to\nfinally solve the long-standing problem of sub-optimal numpy\nperformance in PyPy.\n\n\nStay tuned!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/12/hpy-kick-off-sprint-report-1840829336092490938.html"
    },
    {
      "title": "PyPy v7.2 released",
      "text": "The PyPy team is proud to release the version 7.2.0 of PyPy, which includes\ntwo different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13\n\n\nPyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.\n\n\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating. Many of the fixes are the direct result of\nend-user bug reports, so please continue reporting issues as they crop up.\n\n\nYou can download the v7.2 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWith the support of Arm Holdings Ltd. and Crossbar.io, this release supports\nthe 64-bit aarch64 ARM architecture. More about the work and the\nperformance data around this welcome development can be found in the blog\npost.\n\n\nThis release removes the \u201cbeta\u201d tag from PyPy3.6. While there may still be some\nsmall corner-case incompatibilities (around the exact error messages in\nexceptions and the handling of faulty codec errorhandlers) we are happy with\nthe quality of the 3.6 series and are looking forward to working on a Python\n3.7 interpreter.\n\n\nWe updated our benchmark runner at https://speed.pypy.org to a more modern\nmachine and updated the baseline python to CPython 2.7.11. Thanks to Baroque\nSoftware for maintaining the benchmark runner.\n\n\nThe CFFI-based _ssl module was backported to PyPy2.7 and updated to use\ncryptography version 2.7. Additionally, the _hashlib, and crypt (or\n_crypt on Python3) modules were converted to CFFI. This has two\nconsequences: end users and packagers can more easily update these libraries\nfor their platform by executing (cd lib_pypy; ../bin/pypy _*_build.py).\nMore significantly, since PyPy itself links to fewer system shared objects\n(DLLs), on platforms with a single runtime namespace like linux, different CFFI\nand c-extension modules can load different versions of the same shared object\ninto PyPy without collision (issue 2617).\n\n\nUntil downstream providers begin to distribute c-extension builds with PyPy, we\nhave made packages for some common packages available as wheels.\n\n\nThe CFFI backend has been updated to version 1.13.0. We recommend using CFFI\nrather than c-extensions to interact with C, and cppyy for interacting with\nC++ code.\n\n\nThanks to Anvil, we revived the PyPy Sandbox, (soon to be released) which allows total control\nover a Python interpreter\u2019s interactions with the external world.\n\n\nWe implemented a new JSON decoder that is much faster, uses less memory, and\nuses a JIT-friendly specialized dictionary. More about that in the recent blog post\n\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 27 new contributors,\nso thanks for pitching in.\n\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\n\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bit, Mac OS X 64-bit, Windows 32-bit, OpenBSD, FreeBSD)\n\n\n\n\nbig- and little-endian variants of PPC64 running Linux \n\n\n\n\ns390x running Linux\n\n\n\n\n64-bit ARM machines running Linux\n\n\n\n\n\nUnfortunately at the moment of writing our ARM buildbots are out of service,\nso for now we are not releasing any binary for the ARM architecture (32-bit), although PyPy does support ARM 32-bit processors.\n\n\n\nWhat else is new?\nPyPy 7.1 was released in March, 2019.\nThere are many incremental improvements to RPython and PyPy, For more information about the 7.2.0 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/10/pypy-v72-released-1090406556726313495.html"
    },
    {
      "title": "PyPy's new JSON parser",
      "text": "Introduction\nIn the last year or two I have worked on and off on making PyPy's\nJSON faster, particularly when parsing large\nJSON files. In this post I am going to document those techniques and\nmeasure their performance impact. Note that I am quite a lot more\nconstrained in what optimizations I can apply here, compared to some of\nthe much more advanced approaches like\nMison,\nSparser or\nSimdJSON because I don't want to\nchange the json.loads API that Python programs expect, and because I\ndon't want to only support CPUs with wide SIMD extensions. With a more\nexpressive API, more optimizations would be possible.\nThere are a number of problems of working with huge JSON files:\ndeserialization takes a long time on the one hand, and the resulting\ndata structures often take a lot of memory (usually they can be many\ntimes bigger than the size of the file they originated from). Of course\nthese problems are related, because allocating and initializing a big\ndata structure takes longer than a smaller data structure. Therefore I\nalways tried to attack both of these problems at the same time.\nOne common theme of the techniques I am describing is that of optimizing\nthe parser for how JSON files are typically used, not how they could\ntheoretically be used. This is a similar approach to the way dynamic\nlanguages are optimized more generally: most JITs will optimize for\ntypical patterns of usage, at the cost of less common usage patterns,\nwhich might even become slower as a result of the optimizations.\n\nMaps\nThe first technique I investigated is to use maps in the JSON parser.\nMaps, also called hidden classes or shapes, are a fairly common way to\n(generally, not just in the context of JSON parsing) optimize instances\nof\nclasses\nin dynamic language VMs. Maps exploit the fact that while it is in\ntheory possible to add arbitrary fields to an instance, in practice most\ninstances of a class are going to have the same set of fields (or one of\na small number of different sets). Since JSON dictionaries or objects\noften come from serialized instances of some kind, this property often\nholds in JSON files as well: dictionaries often have the same fields in\nthe same order, within a JSON file.\nThis property can be exploited in two ways: on the one hand, it can be\nused to again store the deserialized dictionaries in a more memory\nefficient way by not using a hashmap in most cases, but instead\nsplitting the dictionary into a shared description of the set of keys\n(the map) and an array of storage with the values. This makes the\ndeserialized dictionaries smaller if the same set of keys is repeated a\nlot. This is completely transparent to the Python programmer, the\ndictionary will look completely normal to the Python program but its\ninternal representation is different.\nOne downside of using maps is that sometimes files will contain many\ndictionaries that have unique key sets. Since maps themselves are quite\nlarge data structures and since dictionaries that use maps contain an\nextra level of indirection we want to fall back to using normal hashmaps\nto represent the dictionaries where that is the case. To prevent this we\nperform some statistics at runtime, how often every map (i.e. set of\nkeys) is used in the file. For uncommonly used maps, the map is\ndiscarded and the dictionaries that used the map converted into using a\nregular hashmap.\n\nUsing Maps to Speed up Parsing\nAnother benefit of using maps to store deserialized dictionaries is that\nwe can use them to speed up the parsing process itself. To see how this\nworks, we need to understand maps a bit better. All the maps produced as\na side-effect of parsing JSON form a tree. The tree root is a map that\ndescribes the object without any attributes. From every tree node we\nhave a number of edges going to other nodes, each edge for a specific\nnew attribute added:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n  \n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n\n\n\n  \n\n\n\n  \n  \n  \n  \n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\nThis map tree is the result of parsing a file that has dictionaries with\nthe keys a, b, c many times, the keys a, b, f less often, and also some\nobjects with the keys x, y.\nWhen parsing a dictionary we traverse this tree from the root, according\nto the keys that we see in the input file. While doing this, we\npotentially add new nodes, if we get key combinations that we have never\nseen before. The set of keys of a dictionary parsed so far are\nrepresented by the current tree node, while we can store the values into\nan array. We can use the tree of nodes to speed up parsing. A lot of the\nnodes only have one child, because after reading the first few keys of\nan object, the remaining ones are often uniquely determined in a given\nfile. If we have only one child map node, we can speculatively parse the\nnext key by doing a memcmp between the key that the map tree says is\nlikely to come next and the characters that follow the ',' that started\nthe next entry in the dictionary. If the memcmp returns true this\nmeans that the speculation paid off, and we can transition to the new map\nthat the edge points to, and parse the corresponding value. If not, we\nfall back to general code that parses the string, handles escaping rules\netc. This trick was explained to me by some V8 engineers, the same trick\nis supposedly used as part of the V8 JSON parser.\nThis scheme doesn't immediately work for map tree nodes that have more\nthan one child. However, since we keep statistics anyway about how often\neach map is used as the map of a parsed dictionary, we can speculate\nthat the most common map transition is taken more often than the others\nin the future, and use that as the speculated next node.\nSo for the example transition tree shown in the figure above the key\nspeculation would succeed for objects with keys a, b, c. For objects\nwith keys a, b, f the speculation would succeed for the first two\nkeys, but not for the third key f. For objects with the keys\nx, y the speculation would fail for the first key x but succeed\nfor the second key y.\nFor real-world datasets these transition trees can become a lot more\ncomplicated, for example here is a visualization of a part of the\ntransition tree generated for parsing a New York Times dataset:\n\n\n\n\nCaching Strings\nA rather obvious observation we can use to improve performance of the\nparser is the fact that string values repeat a lot in most JSON files.\nFor strings that are used as dictionary keys this is pretty obvious.\nHowever it happens also for strings that are used as values in\ndictionaries (or are stored in lists). We can use this fact to\nintern/memoize strings and save memory. This is an approach that many\nJSON parsers use, including\nCPython's.\nTo do this, I keep a dictionary of strings that we have seen so far\nduring parsing and look up new strings that are deserialized. If we have\nseen the string before, we can re-use the deserialized previous string.\nRight now I only consider utf-8 strings for caching that do not contain\nany escapes (whether stuff like \\\", \\n or escaped unicode chars).\nThis simple approach works extremely well for dictionary keys, but needs\na number of improvements to be a win in general. The first observation\nis that computing the hash to look up the string in the dictionary of\nstrings we've seen so far is basically free. We can compute the hash\nwhile scanning the input for the end of the string we are currently\ndeserializing. Computing the hash while scanning doesn't increase the\ntime spent scanning much. This is not a new idea, I am sure many other\nparsers do the same thing (but CPython doesn't seem to).\nAnother improvement follows from the observation that inserting every\nsingle deserialized non-key string into a hashmap is too expensive.\nInstead, we insert strings into the cache more conservatively, by\nkeeping a small ring buffer of hashes of recently deserialized strings.\nThe hash is looked for in the ring buffer, and only if the hash is\npresent we insert the string into the memoization hashmap. This has the\neffect of only inserting strings into the memoization hashmap that\nre-occur a second time not too far into the file. This seems to give a\ngood trade-off between still re-using a lot of strings but keeping the\ntime spent updating and the size of the memoization hashmap low.\nAnother twist is that in a lot of situations caching strings is not\nuseful at all, because it will almost never succeed. Examples of this\nare UUIDs (which are unique), or the content of a tweet in a JSON file\nwith many tweets (which is usually unique). However, in the same file it\nmight be useful to cache e.g. the user name of the Twitter user, because\nmany tweets from the same person could be in such a file. Therefore the\nusefulness of the string cache depends on which fields of objects we are\ndeserializing the value off. Therefore we keep statistics per map field\nand disable string memoization per individual field if the cache hit\nrate falls below a certain threshold. This gives the best of both\nworlds: in the cases where string values repeat a lot in certain fields\nwe use the cache to save time and memory. But for those fields that\nmostly contain unique strings we don't waste time looking up and adding\nstrings in the memoization table. Strings outside of dictionaries are\nquite rare anyway, so we just always try to use the cache for them.\nThe following pseudocode sketches the code to deserialize a string in\nthe input at a given position. The function also takes a map, which is\nthe point in the map tree that we are currently deserializing a field\noff (if we are deserializing a string in another context, some kind of\ndummy map can be used there).\n\ndef deserialize_string(pos, input, map):\n    # input is the input string, pos is the position of the starting \" of\n    # the string\n\n    # find end of string, check whether it contains escape codes,\n    # compute hash, all at the same time\n    end, escapes, hash = find_end_of_string(pos + 1, input)\n    if end == -1:\n        raise ParseError\n    if escapes:\n        # need to be much more careful with escaping\n        return deserialize_string_escapes(pos, input)\n    \n    # should we cache at all?\n    if map.cache_disabled():\n        return input[pos + 1:end]\n\n    # if string is in cache, return it\n    if hash in cache:\n        map.cache_hit += 1\n        return cache[hash]\n\n    result = input[pos + 1:end]\n    map.cache_miss += 1\n\n    # if hash is in the ring buffer of recently seen hashes,\n    # add the string to the cache\n    if hash in ring_buffer:\n        cache[hash] = result\n    else:\n        ring_buffer.write(hash)\n    return result\n\n\n\n\nEvaluation\nTo find out how much the various techniques help, I implemented a number\nof JSON parsers in PyPy with different combinations of the techniques\nenabled. I compared the numbers with the JSON parser of CPython 3.7.3\n(simplejson), with ujson, with the JSON parser of Node 12.11.1 (V8) and with\nRapidJSON (in DOM mode).\nI collected a number of medium-to-large JSON files to try the JSON\nparsers on:\n\nCensys: A subset of the Censys port and\nprotocol scan data for websites in the Alexa top million domains\nGharchive: Github activity from\nJanuary 15-23, 2015 from Github Archive\nReddit: Reddit\ncomments from May 2009\nRosie: The nested matches produced using the Rosie pattern\nlanguage all.things pattern on a log\nfile\nNytimes: Metadata of a collection of New York Times articles\nTpch: The TPC-H database benchmark's deals table as a JSON file\nTwitter: A JSON export of the @pypyproject Twitter account data\nWikidata: A file storing a subset of the Wikidata fact dump from Nov\n11, 2014\nYelp: A file of yelp\nbusinesses\n\nHere are the file sizes of the benchmarks:\n\n  \n\n      Benchmark\n      File Size [MiB]\n    \n\n  \n\n      Censys\n      898.45\n    \n\n      Gharchive\n      276.34\n    \n\n      NYTimes\n      12.98\n    \n\n      Reddit\n      931.65\n    \n\n      Rosie\n      388.88\n    \n\n      TPCH\n      173.86\n    \n\n      Wikidata\n      119.75\n    \n\n      Yelp\n      167.61\n    \n\n\nI measured the times of each benchmark with a number of variations\nof the improved PyPy algorithms:\n\nPyPyBaseline: The PyPy JSON parser as it was before my work with JSON\nparsing started (PyPy version 5.8)\nPyPyKeyStringCaching: Memoizing the key strings of dictionaries, but\nnot the other strings in a json file, and not using maps to represent\ndictionaries (this is the JSON parser that PyPy has been shipping since\nversion 5.9, in the benchmarks I used 7.1).\nPyPyMapNoCache: Like PyPyKeyStringCaching, but using maps to\nrepresent dictionaries. This includes speculatively parsing the next\nkey using memcmp, but does not use string caching of non-key strings.\nPyPyFull: Like PyPyMapNoCache but uses a string cache for all\nstrings, not just keys. This is equivalent to what will be released soon as part of PyPy 7.2\n\nIn addition to wall clock time of parsing, I also measured the increase\nin memory use of each implementation after the input string has been\ndeserialized, i.e. the size of the in-memory representation of every\nJSON file.\n\n\nContributions of Individual Optimizations\nLet's first look at the contributions of the individual optimizations to the\noverall performance and memory usage.\n\n\n\n\nAll the benchmarks were run 30 times in new processes, all the numbers are\nnormalized to PyPyFull.\nThe biggest individual improvement to both parsing time and memory used comes\nfrom caching just the keys in parsed dictionaries. This is the optimization in\nPyPy's JSON parser that has been implemented for a while already. To understand\nwhy this optimization is so useful, let's look at some numbers about each\nbenchmark, namely the number of total keys across all dictionaries in each\nfile, as well as the number of unique keys. As we can see, for all benchmarks\nthe number of unique keys is significantly smaller than the number of keys in\ntotal.\n\n  \n\n      Benchmark\n      Number of keys\n      Number of unique keys\n    \n\n  \n\n      Censys\n      14\u2009404\u2009234\n      163\n    \n\n      Gharchive\n      6\u2009637\u2009881\n      169\n    \n\n      NYTimes\n      417\u2009337\n      60\n    \n\n      Reddit\n      25\u2009226\u2009397\n      21\n    \n\n      Rosie\n      28\u2009500\u2009101\n      5\n    \n\n      TPCH\n      6\u2009700\u2009000\n      45\n    \n\n      Wikidata\n      6\u2009235\u2009088\n      1\u2009602\n    \n\n      Yelp\n      5\u2009133\u2009914\n      61\n    \n\n\nThe next big jump in deserialization time and memory comes from introducing\nmaps to represent deserialized dictionaries. With PyPyMapNoCache\ndeserialization time goes down because it's much cheaper to walk the tree\nof maps and store all deserialized objects into an array of values than to\nbuild hashmaps with the same keys again and again. Memory use goes down\nfor the same reason: it takes a lot less memory to store the shared\nstructure of each set of keys in the map, as opposed to repeating it again\nand again in every hashmap.\nWe can look at some numbers about every benchmark again. The table shows how\nmany map-based dictionaries are deserialized for every benchmark, and how many\nhashmap-backed dictionaries. We see that the number of hashmap-backed\ndictionaries is often zero, or at most a small percentage of all dictionaries\nin each benchmark. Yelp has the biggest number of hashmap-backed dictionaries.\nThe reason for this is that the input file contains hashmaps that store\ncombinations of various features of Yelp businesses, and a lot of these\ncombinations are totally unique to a business. Therefore the heuristics\ndetermine that it's better to store these using hashmaps.\n\n  \n    \n      Benchmark\n      Map Dicts\n      Regular Dicts\n      % Regular Dicts\n    \n  \n  \n    \n      Censys\n      4\u2009049\u2009235\n      1\u2009042\n      0.03\n    \n    \n      Gharchive\n      955\u2009301\n      0\n      0.00\n    \n    \n      NYTimes\n      80\u2009393\n      0\n      0.00\n    \n    \n      Reddit\n      1\u2009201\u2009257\n      0\n      0.00\n    \n    \n      Rosie\n      6\u2009248\u2009966\n      0\n      0.00\n    \n    \n      TPCH\n      1\u2009000\u2009000\n      0\n      0.00\n    \n    \n      Wikidata\n      1\u2009923\u2009460\n      46\u2009905\n      2.38\n    \n    \n      Yelp\n      443\u2009140\n      52\u2009051\n      10.51\n    \n  \n\n\nWe can also look at numbers about how often the memcmp-based speculative\nparsing of the next key of a given map succeeds. Looking at statistics\nabout each benchmark, we can see that the speculation of what key we\nexpect next pays off in a significant percentage of cases, between 63% for\nWikidata where the dictionary structures are quite irregular, and 99% for\nReddit, where all the dictionaries have the same set of keys.\n\n  \n\n      Benchmark\n      Number of Keys\n      Map Transitions\n      % Successful Speculation\n    \n\n  \n\n      Censys\n      14\u2009404\u2009234\n      14\u2009403\u2009243\n      65.79\n    \n\n      Gharchive\n      6\u2009637\u2009881\n      6\u2009637\u2009881\n      86.71\n    \n\n      NYTimes\n      417\u2009337\n      417\u2009337\n      79.85\n    \n\n      Reddit\n      25\u2009226\u2009397\n      25\u2009226\u2009397\n      100.00\n    \n\n      Rosie\n      28\u2009500\u2009101\n      28\u2009500\u2009101\n      90.37\n    \n\n      TPCH\n      6\u2009700\u2009000\n      6\u2009700\u2009000\n      86.57\n    \n\n      Wikidata\n      6\u2009235\u2009088\n      5\u2009267\u2009744\n      63.68\n    \n\n      Yelp\n      5\u2009133\u2009914\n      4\u2009593\u2009980\n      90.43\n    \n\n      geomean\n      \n      \n      82.04\n    \n\n\nGeneral string caching is the most unclear optimization. On the one hand its\nimpact on memory usage is quite substantial, leading to a 20% reduction for\nGharchive and Reddit, up to a 2\u00d7 improvement for Yelp. On the other hand, the\neffect on performance is less clear, since it even leads to a slowdown in\nGharchive and Reddit, and generally only a small improvement. Choosing the\nright heuristic for when to disable the cache also has somewhat unclear effects\nand is definitely a topic worthy of further investigation.\n\nComparison against other JSON Decoders\nTo get a more general feeling of the performance and memory usage of the\nimproved PyPy parser, we compare it against CPython's built-in json\nparser, ujson for CPython, Node's (V8) JSON parser and RapidJSON. For\nbetter context for the memory usage I also show the file size of the input\nfiles.\nThese benchmarks are not really an apples-to-apple comparison. All of the\nimplementations use different in-memory representations of strings in\nthe deserialized data-structure (Node uses two bytes per character in\na string, in CPython it\ndepends but 4 bytes on my\nmachine), PyPyBaseline uses four bytes, PyPy and RapidJSON use utf-8). But\nit's still interesting to get some ballpark numbers. The results are as\nfollows:\n\n\n\n\nAs we can see, PyPyFull handily beats CPython and ujson, with a geometric\nmean of the improvement of about 2.5\u00d7. The memory improvement can be even\nmore extreme, with an improvement of over 4\u00d7 against CPython/ujson in some\ncases (CPython gives better memory sizes, because its parser caches the\nkeys of dictionaries as well). Node is often more than 50% slower, whereas\nRapidJSON beats us easily, by a factor of 2\u00d7 on average.\n\nConclusions\nWhile the speedup I managed to achieve over the course of this project is\nnice and I am certainly happy to beat both CPython and Node, I am\nultimately still annoyed that RapidJSON manages to maintain such a clear\nlead over PyPyFull, and would like to get closer to it. One problem that\nPyPy suffers compared to RapidJSON is the overhead of garbage collection.\nDeserializing large JSON files is pretty much the worst case for the\ngenerational GC that PyPy uses, since none of the deserialized objects die\nyoung (and the GC expects that most objects do). That means that a lot of\nthe deserialization time of PyPy is wasted allocating the resulting\nobjects in the nursery, and then copying them into the old generation.\nSomehow, this should be done in better ways, but all my attempts to not\nhave to do the copy did not seem to help much. So maybe more improvements\nare possible, if I can come up with more ideas.\nOn the memory side of things, Node/V8 is beating PyPy clearly which might\nindicate more general problems in how we represent Python objects in\nmemory. On the other hand, I think it's cool that we are competitive with\nRapidJSON in terms of memory and often within 2\u00d7 of the file size.\nAn effect that I didn't consider at all in this blog post is the fact that\naccessing the deserialized objects with constants strings is also faster\nthan with regular dictionaries, due to them being represented with maps.\nMore benchmarking work to do in the future!\nIf you have your own programs that run on PyPy and use the json parser\na lot, please measure them on the new code and let me know whether you see\nany difference!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/10/pypys-new-json-parser-492911724084305501.html"
    },
    {
      "title": "A second life for the Sandbox",
      "text": "Hi all,\n\nAnvil is a UK-based company sponsoring one month of work to revive PyPy's\n\"sandbox\" mode and upgrade it to PyPy3.  Thanks to them, sandboxing will be\ngiven a second life!\n\nThe sandboxed PyPy is a special version of PyPy that runs\nfully isolated.  It gives a safe way to execute arbitrary Python\nprograms (whole programs, not small bits of code inside your larger Python\nprogram).  Such scripts can be fully untrusted, and they can try to do\nanything\u2014there are no syntax-based restrictions, for example\u2014but whatever\nthey do, any communication with the external world is not actually done but\ndelegated to the parent process.  This is similar but much more flexible than\nLinux's Seccomp approach, and it is more lightweight than setting up a full\nvirtual machine.  It also works without operating system support.\n\nHowever, during the course of the years the sandbox mode of PyPy has been\nmostly unmaintained and unsupported by the core developers, mostly because of\na lack of interest by users and because it took too much effort to maintain\nit.\n\nNow we have found that we have an actual user, Anvil.  As far as I can tell\nthey are still using a very old version of PyPy, the last one that supported\nsandboxing. This is where this contract comes from: the goal is to modernize sandboxing and port it to PyPy3.\n\nPart of my motivation for accepting this work is that I may have found a way to\ntweak the protocol on the pipe between the sandboxed PyPy and the parent\ncontroller process.  This should make the sandboxed PyPy more resilient against\nfuture developments and easier to maintain; at most, in the future some tweaks will be needed in the\ncontroller process but hopefully not deep inside the guts of the sandboxed\nPyPy.  Among the advantages, such a more robust solution should mean that we\ncan actually get a working sandboxed PyPy\u2014or sandboxed PyPy3 or sandboxed\nversion of any other interpreter written in RPython\u2014with just an extra\nargument when calling rpython to translate this interpreter.  If everything\nworks as planned, sandboxing may be given a second life.\n\nArmin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/08/a-second-life-for-sandbox-6848726729476245390.html"
    },
    {
      "title": "PyPy JIT for Aarch64",
      "text": "Hello everyone.\nWe are pleased to announce the availability of the new PyPy for AArch64. This\nport brings PyPy's high-performance just-in-time compiler to the AArch64\nplatform, also known as 64-bit ARM. With the addition of AArch64, PyPy now\nsupports a total of 6 architectures: x86 (32 & 64bit), ARM (32 & 64bit), PPC64,\nand s390x. The AArch64 work was funded by ARM Holdings Ltd. and Crossbar.io.\nPyPy has a good record of boosting the performance of Python programs on the\nexisting platforms. To show how well the new PyPy port performs, we compare the\nperformance of PyPy against CPython on a set of benchmarks. As a point of\ncomparison, we include the results of PyPy on x86_64.\nNote, however, that the results presented here were measured on a Graviton A1\nmachine from AWS, which comes with a very serious word of warning: Graviton A1's\nare virtual machines, and, as such, they are not suitable for benchmarking. If\nsomeone has access to a beefy enough (16G) ARM64 server and is willing to give\nus access to it, we are happy to redo the benchmarks on a real machine. One\nmajor concern is that while a virtual CPU is 1-to-1 with a real CPU, it is not\nclear to us how CPU caches are shared across virtual CPUs. Also, note that by no\nmeans is this benchmark suite representative enough to average the results. Read\nthe numbers individually per benchmark.\nThe following graph shows the speedups on AArch64 of PyPy (hg id 2417f925ce94) compared to\nCPython (2.7.15), as well as the speedups on a x86_64 Linux laptop\ncomparing the most recent release, PyPy 7.1.1, to CPython 2.7.16.\n\n\n\nIn the majority of benchmarks, the speedups achieved on AArch64 match those\nachieved on the x86_64 laptop. Over CPython, PyPy on AArch64 achieves speedups\nbetween 0.6x to 44.9x. These speedups are comparable to x86_64, where the\nnumbers are between 0.6x and 58.9x.\nThe next graph compares between the speedups achieved on AArch64 to the speedups\nachieved on x86_64, i.e., how great the speedup is on AArch64 vs. the same\nbenchmark on x86_64. This comparison should give a rough idea about the\nquality of the generated code for the new platform.\n\n\n\nNote that we see a large variance: There are generally three groups of\nbenchmarks - those that run at more or less the same speed, those that\nrun at 2x the speed, and those that run at 0.5x the speed of x86_64.\nThe variance and disparity are likely related to a variety of issues, mostly due\nto differences in architecture. What is however interesting is that, compared\nto measurements performed on older ARM boards, the branch predictor on the\nGraviton A1 machine appears to have improved. As a result, the speedups achieved\nby PyPy over CPython are smaller than on older ARM boards: sufficiently branchy\ncode, like CPython itself, simply runs a lot faster. Hence, the advantage\nof the non-branchy code generated by PyPy's just-in-time compiler is smaller.\nOne takeaway here is that many possible improvements for PyPy have yet to be\nimplemented. This is true for both of the above platforms, but probably more so\nfor AArch64, which comes with a large number of CPU registers. The PyPy backend\nwas written with x86 (the 32-bit variant) in mind, which has a really low number\nof registers. We think that we can improve in the area of emitting more modern\nmachine code, which may have a higher impact on AArch64 than on x86_64. There is\nalso a number of missing features in the AArch64 backend. These features are\ncurrently implemented as expensive function calls instead of inlined native\ninstructions, something we intend to improve.\nBest,\nMaciej Fijalkowski, Armin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/07/pypy-jit-for-aarch64-7161523403247118006.html"
    },
    {
      "title": "PyPy 7.1.1 Bug Fix Release",
      "text": "The PyPy team is proud to release a bug-fix release version 7.1.1 of PyPy, which\nincludes two different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.\nPyPy3.6-beta: the second official release of PyPy to support 3.6\nfeatures.\n\n\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nThis bugfix fixes bugs related to large lists, dictionaries, and sets, some corner cases with unicode, and PEP 3118 memory views of ctype structures. It also fixes a few issues related to the ARM 32-bit backend. For the complete list see the changelog.\n\nYou can download the v7.1.1 releases here:\n\n\n\nhttps://pypy.org/download.html\n\n\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating.\n\nThe PyPy3.6 release is rapidly maturing, but is still considered beta-quality.\n\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/04/pypy-711-bug-fix-release-6539023630991217367.html"
    },
    {
      "title": "An RPython JIT for LPegs",
      "text": "The following is a guest post by Stefan Troost, he describes the work he did in his bachelor thesis:\n\nIn this project we have used the RPython infrastructure to generate an RPython\nJIT for a\nless-typical use-case: string pattern matching. The work in this project is\nbased on Parsing Expression Grammars and\nLPeg, an implementation of PEGs\ndesigned to be used in Lua. In this post I will showcase some of the work that\nwent into this project, explain PEGs in general and LPeg in particular, and\nshow some benchmarking results.\nParsing Expression Grammars\nParsing Expression Grammas (PEGs) are a type of formal grammar similar to\ncontext-free grammars, with the main difference being that they are unambiguous.\nThis is achieved by redefining the ambiguous choice operator of CFGs (usually\nnoted as |) as an ordered choice operator. In practice this means that if a\nrule in a PEG presents a choice, a PEG parser should prioritize the leftmost\nchoice. Practical uses include parsing and pattern-searching. In comparison to\nregular expressions PEGs stand out as being able to be parsed in linear time,\nbeing strictly more powerful than REs, as well as being arguably more readable.\nLPeg\nLPeg is an implementation of PEGs written in C to be used in the Lua\nprogramming language. A crucial detail of this implementation is that it parses\nhigh level function calls, translating them to bytecode, and interpreting that\nbytecode. Therefore, we are able to improve that implementation by replacing\nLPegs C-interpreter with an RPython JIT. I use a modified version of LPeg to\nparse PEGs and pass the generated Intermediate Representation, the LPeg\nbytecode, to my VM.\nThe LPeg Library\nThe LPeg Interpreter executes bytecodes created by parsing a string of commands\nusing the LPeg library. Our JIT supports a subset of the LPeg library, with\nsome of the more advanced or obscure features being left out. Note that this\nsubset is still powerful enough to do things like parse JSON.\n\n\n\nOperator\nDescription\n\n\n\n\nlpeg.P(string)\nMatches string literally\n\n\nlpeg.P(n)\nMatches exactly n characters\n\n\nlpeg.P(-n)\nMatches at most n characters\n\n\nlpeg.S(string)\nMatches any character in string (Set)\n\n\nlpeg.R(\u201cxy\u201d)\nMatches any character between x and y (Range)\n\n\npatternn\nMatches at least n repetitions of pattern\n\n\npattern-n\nMatches at most n repetitions of pattern\n\n\npattern1 * pattern2\nMatches pattern1 followed by pattern2\n\n\npattern1 + pattern2\nMatches pattern1 or pattern2 (ordered choice)\n\n\npattern1 - pattern2\nMatches pattern1 if pattern2 does not match\n\n\n-pattern\nEquivalent to (\"\" - pattern)\n\n\n\nAs a simple example, the pattern lpeg.P\"ab\"+lpeg.P\"cd\" would match either the\nstring ab or the string cd.\nTo extract semantic information from a pattern, captures are needed. These are\nthe following operations supported for capture creation.\n\n\n\nOperation\nWhat it produces\n\n\n\n\nlpeg.C(pattern)\nthe match for patten plus all captures made by pattern\n\n\nlpeg.Cp()\nthe current position (matches the empty string)\n\n\n\n(tables taken from the LPeg documentation)\nThese patterns are translated into bytecode by LPeg, at which point we are able\nto pass them into our own VM.\nThe VM\nThe state of the VM at any point is defined by the following variables:\n\nPC: program counter indicating the current instruction\nfail: an indicator that some match failed and the VM must backtrack\nindex: counter indicating the current character of the input string\nstackentries: stack of return addresses and choice points\ncaptures: stack of capture objects\n\nThe execution of bytecode manipulates the values of these variables in order to\nproduce some output. How that works and what that output looks like will be\nexplained now.\nThe Bytecode\nFor simplicity\u2019s sake I will not go over every individual bytecode, but instead\nchoose some that exemplify the core concepts of the bytecode set.\ngeneric character matching bytecodes\n\n\nany: Checks if there\u2019s any characters left in the inputstring. If it succeeds\nit advances the index and PC by 1, if not the bytecode fails.\n\n\nchar c: Checks if there is another bytecode in the input and if that\ncharacter is equal to c. Otherwise the bytecode fails.\n\n\nset c1-c2: Checks if there is another bytecode in the input and if that\ncharacter is between (including) c1 and c2. Otherwise the bytecode fails.\n\n\nThese bytecodes are the easiest to understand with very little impact on the\nVM. What it means for a bytecode to fail will be explained when\nwe get to control flow bytecodes.\nTo get back to the example, the first half of the pattern lpeg.P\"ab\" could be\ncompiled to the following bytecodes:\nchar a\nchar b\n\ncontrol flow bytecodes\n\n\njmp n: Sets PC to n, effectively jumping to the n\u2019th bytecode. Has no defined\nfailure case.\n\n\ntestchar c n: This is a lookahead bytecode. If the current character is equal\nto c it advances the PC but not the index. Otherwise it jumps to n.\n\n\ncall n: Puts a return address (the current PC + 1) on the stackentries stack\nand sets the PC to n. Has no defined failure case.\n\n\nret: Opposite of call. Removes the top value of the stackentries stack (if\nthe string of bytecodes is valid this will always be a return address) and\nsets the PC to the removed value. Has no defined failure case.\n\n\nchoice n: Puts a choice point on the stackentries stack. Has no defined\nfailure case.\n\n\ncommit n: Removes the top value of the stackentries stack (if the string of\nbytecodes is valid this will always be a choice point) and jumps to n. Has no\ndefined failure case.\n\n\nUsing testchar we can implement the full pattern lpeg.P\"ab\"+lpeg.P\"cd\" with\nbytecode as follows:\ntestchar a -> L1\nany\nchar b\nend\nany\nL1: char c\nchar d\nend\n\nThe any bytecode is needed because testchar does not consume a character\nfrom the input.\nFailure Handling, Backtracking and Choice Points\nA choice point consist of the VM\u2019s current index and capturestack as well as a\nPC. This is not the VM\u2019s PC at the time of creating the\nchoicepoint, but rather the PC where we should continue trying to find\nmatches when a failure occurs later.\nNow that we have talked about choice points, we can talk about how the VM\nbehaves in the fail state. If the VM is in the fail state, it removed entries\nfrom the stackentries stack until it finds a choice point. Then it backtracks\nby restoring the VM to the state defined by the choice point. If no choice\npoint is found this way, no match was found in the string and the VM halts.\nUsing choice points we could implement the example lpeg.P\"ab\" + lpeg.P\"cd\" in\nbytecodes in a different way (LPEG uses the simpler way shown above, but for\nmore complex patterns it can\u2019t use the lookahead solution using testchar):\nchoice L1\nchar a\nchar b\ncommit\nend\nL1: char c\nchar d\nend\n\nCaptures\nSome patterns require the VM to produce more output than just \u201cthe pattern\nmatched\u201d or \u201cthe pattern did not match\u201d. Imagine searching a document for an\nIPv4 address and all your program responded was \u201cI found one\u201d. In order to\nrecieve additional information about our inputstring, captures are used.\nThe capture object\nIn my VM, two types of capture objects are supported, one of them being the\nposition capture. It consists of a single index referencing the point in the\ninputstring where the object was created.\nThe other type of capture object is called simplecapture. It consists of an\nindex and a size value, which are used to reference a substring of the\ninputstring. In addition, simplecaptures have a variable status indicating they\nare either open or full. If a simplecapture object is open, that means that its\nsize is not yet determined, since the pattern we are capturing is of variable\nlength.\nCapture objects are created using the following bytecodes:\n\n\nFullcapture Position: Pushes a positioncapture object with the current index\nvalue to the capture stack.\n\n\nFullcapture Simple n: Pushes a simplecapture object with current index value\nand size=n to the capture stack.\n\n\nOpencapture Simple: Pushes an open simplecapture object with current index\nvalue and undetermined size to the capture stack.\n\n\nclosecapture: Sets the top element of the capturestack to full and sets its\nsize value using the difference between the current index and the index of\nthe capture object.\n\n\nThe RPython Implementation\nThese, and many more bytecodes were implemented in an RPython-interpreter.\nBy adding jit hints, we were able to generate an efficient JIT.\nWe will now take a closer look at some implementations of bytecodes.\n...\n        elif instruction.name == \"any\":\n            if index >= len(inputstring):\n                fail = True\n            else:\n                pc += 1\n                index += 1\n\n...\n\nThe code for the any-bytecode is relatively straight-forward. It either\nadvances the pc and index or sets the VM into the fail state,\ndepending on whether the end of the inputstring has been reached or not.\n...\n        if instruction.name == \"char\":\n            if index >= len(inputstring):\n                fail = True\n            elif instruction.character == inputstring[index]:\n                pc += 1\n                index += 1\n            else:\n                fail = True\n...\n\nThe char-bytecode also looks as one would expect. If the VM\u2019s string index is\nout of range or the character comparison fails, the VM is put into the\nfail state, otherwise the pc and index are advanced by 1. As you can see, the\ncharacter we\u2019re comparing the current inputstring to is stored in the\ninstruction object (note that this code-example has been simplified for\nclarity, since the actual implementation includes a jit-optimization that\nallows the VM to execute multiple successive char-bytecodes at once).\n...\n        elif instruction.name == \"jmp\":\n            pc = instruction.goto\n...\n\nThe jmp-bytecode comes with a goto value which is a pc that we want\nexecution to continue at.\n...\n        elif instruction.name == \"choice\":\n            pc += 1\n            choice_points = choice_points.push_choice_point(\n                instruction.goto, index, captures)\n...\n\nAs we can see here, the choice-bytecode puts a choice point onto the stack that\nmay be backtracked to if the VM is in the fail-state. This choice point\nconsists of a pc to jump to which is determined by the bytecode.\nBut it also includes the current index and captures values at the time the choice\npoint was created. An ongoing topic of jit optimization is which data structure\nis best suited to store choice points and return addresses. Besides naive\nimplementations of stacks and single-linked lists, more case-specific\nstructures are also being tested for performance.\nBenchmarking Result\nIn order to find out how much it helps to JIT LPeg patterns we ran a small\nnumber of benchmarks. We used an otherwise idle Intel Core i5-2430M CPU with\n3072 KiB of cache and 8 GiB of RAM, running with 2.40GHz. The machine was\nrunning Ubuntu 14.04 LTS, Lua 5.2.3 and we used GNU grep 2.16 as a point of\ncomparison for one of the benchmarks. The benchmarks were run 100 times in\na new process each. We measured the full runtime of the called process,\nincluding starting the process.\nNow we will take a look at some plots generated by measuring the runtime of\ndifferent iterations of my JIT compared to lua and using bootstrapping to\ngenerate a sampling distribution of mean values. The plots contain a few different\nvariants of pypeg, only the one called \"fullops\" is important for this blog post, however.\n\n\n\nThis is the plot for a search pattern that searches a text file for valid URLs.\nAs we can see, if the input file is as small as 100 kb, the benefits of JIT\noptimizations do not outweigh the time required to generate the\nmachine code. As a result, all of our attempts perform significantly slower\nthan LPeg.\n\n\n\nThis is the plot for the same search pattern on a larger input file. As we can\nsee, for input files as small as 500 kb our VM already outperforms LPeg\u2019s. An\nongoing goal of continued development is to get this lower boundary as small as\npossible.\n\n\n\nThe benefits of a JIT compared to an Interpreter become more and more relevant\nfor larger input files. Searching a file as large as 5 MB makes this fairly\nobvious and is exactly the behavior we expect.\n\n\n\nThis time we are looking at a different more complicated pattern, one that parses JSON used on a\n50 kb input file. As expected, LPeg outperforms us, however, something\nunexpected happens as we increase the filesize.\n\n\n\nSince LPeg has a defined maximum depth of 400 for the choicepoints and\nreturnaddresses Stack, LPeg by default refuses to parse files as small as\n100kb. This raises the question if LPeg was intended to be used for parsing.\nUntil a way to increase LPeg\u2019s maximum stack depth is found, no comparisons to\nLPeg can be performed at this scale. This has been a low priority in the past\nbut may be addressed in the future.\nTo conclude, we see that at sufficiently high filesizes, our JIT outperforms\nthe native LPeg-interpreter. This lower boundary is currently as low as 100kb\nin filesize.\nConclusion\nWriting a JIT for PEG\u2019s has proven itself to be a challenge worth pursuing, as\nthe expected benefits of a JIT compared to an Interpreter have been achieved.\nFuture goals include getting LPeg to be able to use parsing patterns on larger\nfiles, further increasing the performance of our JIT and comparing it to other\nwell-known programs serving a similar purpose, like grep.\nThe prototype implementation that I described in this post can be found\non Github\n(it's a bit of a hack in some places, though).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/04/an-rpython-jit-for-lpegs-4779548053359386284.html"
    },
    {
      "title": "PyPy v7.1 released; now uses utf-8 internally for unicode strings",
      "text": "The PyPy team is proud to release version 7.1.0 of PyPy, which includes\ntwo different interpreters:\n\n\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7\nPyPy3.6-beta: this is the second official release of PyPy to support 3.6\nfeatures, although it is still considered beta quality.\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nThis release, coming fast on the heels of 7.0 in February, finally merges the\ninternal refactoring of unicode representation as UTF-8. Removing the\nconversions from strings to unicode internally lead to a nice speed bump. We merged the utf-8 changes to the py3.5 branch (Python3.5.3) but will concentrate on 3.6 going forward.\n\n\nWe also improved the ability to use the buffer protocol with ctype structures\nand arrays.\n\n\nThe CFFI backend has been updated to version 1.12.2. We recommend using CFFI\nrather than c-extensions to interact with C, and cppyy for interacting with\nC++ code.\n\u00a0You can download the v7.1 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\nThis PyPy release supports:\n\u00a0\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux\n\u00a0ARM32 although we do not supply downloadable binaries at this time\ns390x running Linux\n\n\n\nWhat else is new?\nPyPy 7.0 was released in February, 2019.\nThere are many incremental improvements to RPython and PyPy, for more information see the changelog.\n\nPlease update, and continue to help us make PyPy better.\n\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/03/pypy-v71-released-now-uses-utf-8-451324088028792912.html"
    },
    {
      "title": "PyPy v7.0.0: triple release of 2.7, 3.5 and 3.6-alpha",
      "text": "The PyPy team is proud to release the version 7.0.0 of PyPy, which includes\nthree different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7\nPyPy3.5, which supports Python 3.5\nPyPy3.6-alpha: this is the first official release of PyPy to support 3.6\nfeatures, although it is still considered alpha quality.\n\n\nAll the interpreters are based on much the same codebase, thus the triple\nrelease.\nUntil we can work with downstream providers to distribute builds with PyPy, we\nhave made packages for some common packages available as wheels.\nThe GC hooks , which can be used to gain more insights into its\nperformance, has been improved and it is now possible to manually manage the\nGC by using a combination of gc.disable and gc.collect_step. See the\nGC blog post.\nWe updated the cffi module included in PyPy to version 1.12, and the\ncppyy backend to 1.4. Please use these to wrap your C and C++ code,\nrespectively, for a JIT friendly experience.\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating.\nThe PyPy3.6 release and the Windows PyPy3.5 release are still not production\nquality so your mileage may vary. There are open issues with incomplete\ncompatibility and c-extension support.\nThe utf8 branch that changes internal representation of unicode to utf8 did not\nmake it into the release, so there is still more goodness coming.\nYou can download the v7.0 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython's JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.5 and 3.6. It's fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThe PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\nUnfortunately at the moment of writing our ARM buildbots are out of service,\nso for now we are not releasing any binary for the ARM architecture.\n\n\nWhat else is new?\nPyPy 6.0 was released in April, 2018.\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\nPlease update, and continue to help us make PyPy better.\n\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/02/pypy-v700-triple-release-of-27-35-and-606875333356156076.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report 2019",
      "text": "Hello everyone!\nWe are happy to report a successful and well attended sprint that is wrapping up\nin D\u00fcsseldorf, Germany. In the last week we had eighteen people sprinting\nat the Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf on various topics.\nTotally serious work going on here constantly.\nA big\nchunk of the sprint was dedicated to various discussions, since we did not\nmanage to gather the core developers in one room in quite a while.\nDiscussion topics included:\n\nFunding and general sustainability of open source.\nCatching up with CPython 3.7/3.8 \u2013 we are planning to release 3.6 some time\nin the next few months and we will continue working on 3.7/3.8.\nWhat to do with VMprof\nHow can we support Cython inside PyPy in a way that will be understood\nby the JIT, hence fast.\nThe future of supporting the numeric stack on pypy \u2013 we have made significant\nprogress in the past few years and most of the numeric stack works out of the box,\nbut deployment and performance remain problems. Improving on those problems\nremains a very important focus for PyPy as a project.\nUsing the presence of a CPython developer (\u0141ukasz Langa) and a Graal Python developer\n(Tim Felgentreff) we discussed ways to collaborate in order to improve Python\necosystem across implementations.\nPierre-Yves David and Georges Racinet from octobus gave us an exciting demo\non Heptapod, which adds mercurial support to gitlab.\nMaciej and Armin gave demos of their current (non-PyPy-related) project VRSketch.\n\n\n\nVisiting the Landschaftspark Duisburg Nord on the break day\n\nSome highlights of the coding tasks worked on:\n\nAarch64 (ARM64) JIT backend work has been started, we are able to run the first\ntest! Tobias Oberstein from Crossbar GmbH and Rodolph Perfetta from ARM joined the\nsprint to help kickstart the project.\nThe long running math-improvements branch that was started by Stian Andreassen got merged\nafter bugfixes done by Alexander Schremmer. It should improve operations on large integers.\nThe arcane art of necromancy was used to revive long dormant regalloc branch started\nand nearly finished by Carl Friedrich Bolz-Tereick. The branch got merged and gives\nsome modest speedups across the board.\nAndrew Lawrence worked on MSI installer for PyPy on windows.\n\u0141ukasz worked on improving failing tests on the PyPy 3.6 branch. He knows very obscure\ndetails of CPython (e.g. how pickling works), hence we managed to progress very quickly.\nMatti Picus set up a new benchmarking server for PyPy 3 branches.\nThe Utf8 branch, which changes the internal representation of unicode might be finally\nmerged at some point very soon. We discussed and improved upon the last few\nblockers. It gives significant speedups in a lot of cases handling strings.\nZlib was missing couple methods, which were added by Ronan Lamy and Julian Berman.\nManuel Jacob fixed RevDB failures.\nAntonio Cuni and Matti Picus worked on 7.0 release which should happen in a few days.\n\nNow we are all quite exhausted, and are looking forward to catching up on sleep.\nBest regards,\nMaciej Fija\u0142kowski, Carl Friedrich Bolz-Tereick and the whole PyPy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/02/dusseldorf-sprint-report-2019-6107623654916313905.html"
    },
    {
      "title": "PyPy for low-latency systems",
      "text": "PyPy for low-latency systems\nRecently I have merged the gc-disable branch, introducing a couple of features\nwhich are useful when you need to respond to certain events with the lowest\npossible latency.  This work has been kindly sponsored by Gambit Research\n(which, by the way, is a very cool and geeky place where to work, in case you\nare interested).  Note also that this is a very specialized use case, so these\nfeatures might not be useful for the average PyPy user, unless you have the\nsame problems as described here.\n\nThe PyPy VM manages memory using a generational, moving Garbage Collector.\nPeriodically, the GC scans the whole heap to find unreachable objects and\nfrees the corresponding memory.  Although at a first look this strategy might\nsound expensive, in practice the total cost of memory management is far less\nthan e.g. on CPython, which is based on reference counting.  While maybe\ncounter-intuitive, the main advantage of a non-refcount strategy is\nthat allocation is very fast (especially compared to malloc-based allocators),\nand deallocation of objects which die young is basically for free. More\ninformation about the PyPy GC is available here.\n\nAs we said, the total cost of memory managment is less on PyPy than on\nCPython, and it's one of the reasons why PyPy is so fast.  However, one big\ndisadvantage is that while on CPython the cost of memory management is spread\nall over the execution of the program, on PyPy it is concentrated into GC\nruns, causing observable pauses which interrupt the execution of the user\nprogram.\nTo avoid excessively long pauses, the PyPy GC has been using an incremental\nstrategy since 2013. The GC runs as a series of \"steps\", letting the user\nprogram to progress between each step.\n\nThe following chart shows the behavior of a real-world, long-running process:\n\n\n\n\nThe orange line shows the total memory used by the program, which\nincreases linearly while the program progresses. Every ~5 minutes, the GC\nkicks in and the memory usage drops from ~5.2GB to ~2.8GB (this ratio is controlled\nby the PYPY_GC_MAJOR_COLLECT env variable).\nThe purple line shows aggregated data about the GC timing: the whole\ncollection takes ~1400 individual steps over the course of ~1 minute: each\npoint represent the maximum time a single step took during the past 10\nseconds. Most steps take ~10-20 ms, although we see a horrible peak of ~100 ms\ntowards the end. We have not investigated yet what it is caused by, but we\nsuspect it is related to the deallocation of raw objects.\n\nThese multi-millesecond pauses are a problem for systems where it is important\nto respond to certain events with a latency which is both low and consistent.\nIf the GC kicks in at the wrong time, it might causes unacceptable pauses during\nthe collection cycle.\n\nLet's look again at our real-world example. This is a system which\ncontinuously monitors an external stream; when a certain event occurs, we want\nto take an action. The following chart shows the maximum time it takes to\ncomplete one of such actions, aggregated every minute:\n\n\n\n\nYou can clearly see that the baseline response time is around ~20-30\nms. However, we can also see periodic spikes around ~50-100 ms, with peaks up\nto ~350-450 ms! After a bit of investigation, we concluded that most (although\nnot all) of the spikes were caused by the GC kicking in at the wrong time.\n\nThe work I did in the gc-disable branch aims to fix this problem by\nintroducing two new features to the gc module:\n\n\ngc.disable(), which previously only inhibited the execution of\nfinalizers without actually touching the GC, now disables the GC major\ncollections. After a call to it, you will see the memory usage grow\nindefinitely.\ngc.collect_step() is a new function which you can use to manually\nexecute a single incremental GC collection step.\n\n\nIt is worth to specify that gc.disable() disables only the major\ncollections, while minor collections still runs.  Moreover, thanks to the\nJIT's virtuals, many objects with a short and predictable lifetime are not\nallocated at all. The end result is that most objects with short lifetime are\nstill collected as usual, so the impact of gc.disable() on memory growth\nis not as bad as it could sound.\n\nCombining these two functions, it is possible to take control of the GC to\nmake sure it runs only when it is acceptable to do so.  For an example of\nusage, you can look at the implementation of a custom GC inside pypytools.\nThe peculiarity is that it also defines a \"with nogc():\" context manager\nwhich you can use to mark performance-critical sections where the GC is not\nallowed to run.\n\nThe following chart compares the behavior of the default PyPy GC and the new\ncustom GC, after a careful placing of nogc() sections:\n\n\n\n\nThe yellow line is the same as before, while the purple line shows the new\nsystem: almost all spikes have gone, and the baseline performance is about 10%\nbetter. There is still one spike towards the end, but after some investigation\nwe concluded that it was not caused by the GC.\n\nNote that this does not mean that the whole program became magically\nfaster: we simply moved the GC pauses in some other place which is not\nshown in the graph: in this specific use case this technique was useful\nbecause it allowed us to shift the GC work in places where pauses are more\nacceptable.\n\nAll in all, a pretty big success, I think.  These functionalities are already\navailable in the nightly builds of PyPy, and will be included in the next\nrelease: take this as a New Year present :)\n\nAntonio Cuni and the PyPy team",
      "tags": "gc,sponsors",
      "url": "https://www.pypy.org/posts/2019/01/pypy-for-low-latency-systems-613165393301401965.html"
    },
    {
      "title": "PyPy Winter Sprint Feb 4-9 in D\u00fcsseldorf",
      "text": "PyPy Sprint February 4th-9th 2019 in D\u00fcsseldorf\n\n\nThe next PyPy sprint will be held in the Computer Science department of Heinrich-Heine Universit\u00e4t D\u00fcsseldorf from the 4th to the 9st of February 2019 (nine years after the last sprint there). This is a fully public sprint, everyone is welcome to join us.\n\nTopics and goals\n\n\n\nimprove Python 3.6 support\ndiscuss benchmarking situation\nprogress on utf-8 branches\ncpyext performance and completeness\npackaging: are we ready to upload to PyPI?\n\nissue 2617\u00a0 - we expose too many functions from lib-pypy.so\nmanylinux2010 - will it solve our build issues?\nformulate an ABI name and upgrade policy\n\n\n\nmemoryview(ctypes.Structure) does not create the correct format string\ndiscussing the state and future of PyPy and the wider Python ecosystem\n\n\n\n\nLocation\n\nThe sprint will take place in seminar room 25.12.02.55 of the computer science department.\u00a0 It is in the building 25.12 of the university campus, second floor. Travel instructions\n\n\nExact times\n\nWork days: starting February 4th (10:00), ending February 9th (~afternoon). The break day will probably be Thursday.\n\nRegistration\n\n\nPlease register by Mercurial::\nhttps://bitbucket.org/pypy/extradoc/\n\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/ddorf2019/people.txt\n\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\n\n\n\nLooking forward to seeing everyone there!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/12/pypy-winter-sprint-feb-4-9-in-dusseldorf-7199110498451574074.html"
    },
    {
      "title": "Funding for 64-bit Armv8-a support in PyPy",
      "text": "Hello everyone\n\nAt PyPy we are trying to support a relatively wide range of platforms. We have PyPy working on OS X, Windows and various flavors of linux (and unofficially various flavors of BSD) on the software side, with hardware side having x86, x86_64, PPC, 32-bit Arm (v7) and even zarch. This is harder than for other projects, since PyPy emits assembler on the fly from the just in time compiler and it requires significant amount of work to port it to a new platform.\n\nWe are pleased to inform that Arm Limited, together with Crossbar.io GmbH, are sponsoring the development of 64-bit Armv8-a architecture support through Baroque Software OU, which would allow PyPy to run on a new variety of low-power, high-density servers with that architecture. We believe this will be beneficial for the funders, for the PyPy project as well as to the wider community.\n\nThe work will commence soon and will be done some time early next year with expected speedups either comparable to x86 speedups or, if our current experience with ARM holds, more significant than x86 speedups.\n\nBest,\nMaciej Fijalkowski and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/11/hello-everyone-at-pypy-we-are-trying-to-5336557946798583063.html"
    },
    {
      "title": "Guest Post: Implementing a Calculator REPL in RPython",
      "text": "This is a tutorial style post that walks through using the RPython translation\ntoolchain to create a REPL that executes basic math expressions. \n\nWe will do that by scanning the user's input into tokens, compiling those \ntokens into bytecode and running that bytecode in our own virtual machine. Don't\nworry if that sounds horribly complicated, we are going to explain it step by\nstep. \n\nThis post is a bit of a diversion while on my journey to create a compliant \nlox implementation\nusing the RPython translation toolchain. The \nmajority of this work is a direct RPython translation of the low level C \nguide from Bob Nystrom (@munificentbob) in the\nexcellent book craftinginterpreters.com\nspecifically the chapters 14 \u2013 17.\n\nThe road ahead\n\nAs this post is rather long I'll break it into a few major sections. In each section we will\nhave something that translates with RPython, and at the end it all comes together. \n\n\nREPL\n\nVirtual Machine\n\nScanning the source\n\nCompiling Expressions\n\nEnd to end\n\n\nA REPL\n\nSo if you're a Python programmer you might be thinking this is pretty trivial right?\n\nI mean if we ignore input errors, injection attacks etc couldn't we just do something\nlike this:\n\n\"\"\"\nA pure python REPL that can parse simple math expressions\n\"\"\"\nwhile True:\n    print(eval(raw_input(\"> \")))\n\n\nWell it does appear to do the trick:\n\n$ python2 section-1-repl/main.py\n> 3 + 4 * ((1.0/(2 * 3 * 4)) + (1.0/(4 * 5 * 6)) - (1.0/(6 * 7 * 8)))\n3.1880952381\n\n\nSo can we just ask RPython to translate this into a binary that runs magically\nfaster?\n\nLet's see what happens. We need to add two functions for RPython to\nget its bearings (entry_point and target) and call the file targetXXX:\n\ntargetrepl1.py\n\ndef repl():\n    while True:\n        print eval(raw_input('> '))\n\n\ndef entry_point(argv):\n    repl()\n    return 0\n\n\ndef target(driver, *args):\n    return entry_point, None\n\n\nWhich at translation time gives us this admonishment that accurately tells us\nwe are trying to call a Python built-in raw_input that is unfortunately not \nvalid RPython.\n\n$ rpython ./section-1-repl/targetrepl1.py\n...SNIP...\n[translation:ERROR] AnnotatorError: \n\nobject with a __call__ is not RPython: <built-in function raw_input>\nProcessing block:\n block@18 is a <class 'rpython.flowspace.flowcontext.SpamBlock'> \n in (target1:2)repl \n containing the following operations: \n       v0 = simple_call((builtin_function raw_input), ('> ')) \n       v1 = simple_call((builtin_function eval), v0) \n       v2 = str(v1) \n       v3 = simple_call((function rpython_print_item), v2) \n       v4 = simple_call((function rpython_print_newline)) \n\n\nOk so we can't use raw_input or eval but that doesn't faze us. Let's get \nthe input from a stdin stream and just print it out (no evaluation).\n\ntargetrepl2.py\n\nfrom rpython.rlib import rfile\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef repl(stdin):\n    while True:\n        print \"> \",\n        line = stdin.readline(LINE_BUFFER_LENGTH)\n        print line\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    try:\n        repl(stdin)\n    except:\n        return 0\n\n\ndef target(driver, *args):\n    return entry_point, None\n\n\nTranslate targetrepl2.py \u2013 we can add an optimization level if we\nare so inclined:\n\n$ rpython --opt=2 section-1-repl/targetrepl2.py\n...SNIP...\n[Timer] Timings:\n[Timer] annotate                       ---  1.2 s\n[Timer] rtype_lltype                   ---  0.9 s\n[Timer] backendopt_lltype              ---  0.6 s\n[Timer] stackcheckinsertion_lltype     ---  0.0 s\n[Timer] database_c                     --- 15.0 s\n[Timer] source_c                       ---  1.6 s\n[Timer] compile_c                      ---  1.9 s\n[Timer] =========================================\n[Timer] Total:                         --- 21.2 s\n\n\nNo errors!? Let's try it out:\n\n$ ./target2-c \n1 + 2\n>  1 + 2\n\nC\n\n\nAhh our first success \u2013 let's quickly deal with the flushing fail by using the \nstdout stream directly as well. Let's print out the input in quotes:\n\nfrom rpython.rlib import rfile\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef repl(stdin, stdout):\n    while True:\n        stdout.write(\"> \")\n        line = stdin.readline(LINE_BUFFER_LENGTH)\n        print '\"%s\"' % line.strip()\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    try:\n        repl(stdin, stdout)\n    except:\n        pass\n    return 0\n\n\ndef target(driver, *args):\n    return entry_point, None\n\n\nTranslation works, and the test run too:\n\n$ ./target3-c \n> hello this seems better\n\"hello this seems better\"\n> C\n\n\nSo we are in a good place with taking user input and printing output... What about\nthe whole math evaluation thing we were promised? For that we are can probably leave\nour RPython REPL behind for a while and connect it up at the end.\n\nA virtual machine\n\nA virtual machine is the execution engine of our basic math interpreter. It will be very simple,\nonly able to do simple tasks like addition. I won't go into any depth to describe why we want\na virtual machine, but it is worth noting that many languages including Java and Python make \nthis decision to compile to an intermediate bytecode representation and then execute that with\na virtual machine. Alternatives are compiling directly to native machine code like (earlier versions of) the V8\nJavaScript engine, or at the other end of the spectrum executing an abstract syntax tree \u2013 \nwhich is what the Truffle approach to building VMs is based on. \n\nWe are going to keep things very simple. We will have a stack where we can push and pop values,\nwe will only support floats, and our VM will only implement a few very basic operations.\n\nOpCodes\n\nIn fact our entire instruction set is:\n\nOP_CONSTANT\nOP_RETURN\nOP_NEGATE\nOP_ADD\nOP_SUBTRACT\nOP_MULTIPLY\nOP_DIVIDE\n\n\nSince we are targeting RPython we can't use the nice enum module from the Python standard\nlibrary, so instead we just define a simple class with class attributes.\n\nWe should start to get organized, so we will create a new file \nopcodes.py and add this:\n\nclass OpCode:\n    OP_CONSTANT = 0\n    OP_RETURN = 1\n    OP_NEGATE = 2\n    OP_ADD = 3\n    OP_SUBTRACT = 4\n    OP_MULTIPLY = 5\n    OP_DIVIDE = 6\n\n\nChunks\n\nTo start with we need to get some infrastructure in place before we write the VM engine.\n\nFollowing craftinginterpreters.com\nwe start with a Chunk object which will represent our bytecode. In RPython we have access \nto Python-esq lists so our code object will just be a list of OpCode values \u2013 which are \njust integers. A list of ints, couldn't get much simpler.\n\nsection-2-vm/chunk.py\n\nclass Chunk:\n    code = None\n\n    def __init__(self):\n        self.code = []\n\n    def write_chunk(self, byte):\n        self.code.append(byte)\n\n    def disassemble(self, name):\n        print \"== %s ==\\n\" % name\n        i = 0\n        while i < len(self.code):\n            i = disassemble_instruction(self, i)\n\n\nFrom here on I'll only present minimal snippets of code instead of the whole lot, but \nI'll link to the repository with the complete example code. For example the \nvarious debugging including disassemble_instruction isn't particularly interesting\nto include verbatim. See the github repo for full details\n\nWe need to check that we can create a chunk and disassemble it. The quickest way to do this\nis to use Python during development and debugging then every so often try to translate it.\n\nGetting the disassemble part through the RPython translator was a hurdle for me as I\nquickly found that many str methods such as format are not supported, and only very basic\n% based formatting is supported. I ended up creating helper functions for string manipulation\nsuch as:\n\ndef leftpad_string(string, width, char=\" \"):\n    l = len(string)\n    if l > width:\n        return string\n    return char * (width - l) + string\n\n\nLet's write a new entry_point that creates and disassembles a chunk of bytecode. We can\nset the target output name to vm1 at the same time:\n\ntargetvm1.py\n\ndef entry_point(argv):\n    bytecode = Chunk()\n    bytecode.write_chunk(OpCode.OP_ADD)\n    bytecode.write_chunk(OpCode.OP_RETURN)\n    bytecode.disassemble(\"hello world\")\n    return 0\n\ndef target(driver, *args):\n    driver.exe_name = \"vm1\"\n    return entry_point, None\n\n\nRunning this isn't going to be terribly interesting, but it is always nice to\nknow that it is doing what you expect:\n\n$ ./vm1 \n== hello world ==\n\n0000 OP_ADD       \n0001 OP_RETURN    \n\n\nChunks of data\n\nRef: https://www.craftinginterpreters.com/chunks-of-bytecode.html#constants\n\nSo our bytecode is missing a very crucial element \u2013 the values to operate on!\n\nAs with the bytecode we can store these constant values as part of the chunk\ndirectly in a list. Each chunk will therefore have a constant data component,\nand a code component. \n\nEdit the chunk.py file and add the new instance attribute constants as an\nempty list, and a new method add_constant.\n\n    def add_constant(self, value):\n        self.constants.append(value)\n        return len(self.constants) - 1\n\n\nNow to use this new capability we can modify our example chunk\nto write in some constants before the OP_ADD:\n\n    bytecode = Chunk()\n    constant = bytecode.add_constant(1.0)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n\n    constant = bytecode.add_constant(2.0)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n\n    bytecode.write_chunk(OpCode.OP_ADD)\n    bytecode.write_chunk(OpCode.OP_RETURN)\n\n    bytecode.disassemble(\"adding constants\")\n\n\nWhich still translates with RPython and when run gives us the following disassembled\nbytecode:\n\n== adding constants ==\n\n0000 OP_CONSTANT  (00)        '1'\n0002 OP_CONSTANT  (01)        '2'\n0004 OP_ADD       \n0005 OP_RETURN\n\n\nWe won't go down the route of serializing the bytecode to disk, but this bytecode chunk\n(including the constant data) could be saved and executed on our VM later \u2013 like a Java\n.class file. Instead we will pass the bytecode directly to our VM after we've created\nit during the compilation process. \n\nEmulation\n\nSo those four instructions of bytecode combined with the constant value mapping\n00 -> 1.0 and 01 -> 2.0 describes individual steps for our virtual machine\nto execute. One major point in favor of defining our own bytecode is we can \ndesign it to be really simple to execute \u2013 this makes the VM really easy to implement.\n\nAs I mentioned earlier this virtual machine will have a stack, so let's begin with that.\nNow the stack is going to be a busy little beast \u2013 as our VM takes instructions like \nOP_ADD it will pop off the top two values from the stack, and push the result of adding \nthem together back onto the stack. Although dynamically resizing Python lists \nare marvelous, they can be a little slow. RPython can take advantage of a constant sized\nlist which doesn't make our code much more complicated.\n\nTo do this we will define a constant sized list and track the stack_top directly. Note\nhow we can give the RPython translator hints by adding assertions about the state that\nthe stack_top will be in.\n\nclass VM(object):\n    STACK_MAX_SIZE = 256\n    stack = None\n    stack_top = 0\n\n    def __init__(self):\n        self._reset_stack()\n\n    def _reset_stack(self):\n        self.stack = [0] * self.STACK_MAX_SIZE\n        self.stack_top = 0\n\n    def _stack_push(self, value):\n        assert self.stack_top < self.STACK_MAX_SIZE\n        self.stack[self.stack_top] = value\n        self.stack_top += 1\n\n    def _stack_pop(self):\n        assert self.stack_top >= 0\n        self.stack_top -= 1\n        return self.stack[self.stack_top]\n\n    def _print_stack(self):\n        print \"         \",\n        if self.stack_top <= 0:\n            print \"[]\",\n        else:\n            for i in range(self.stack_top):\n                print \"[ %s ]\" % self.stack[i],\n        print\n\n\nNow we get to the main event, the hot loop, the VM engine. Hope I haven't built it up to\nmuch, it is actually really simple! We loop until the instructions tell us to stop \n(OP_RETURN), and dispatch to other simple methods based on the instruction.\n\n    def _run(self):\n        while True:\n            instruction = self._read_byte()\n\n            if instruction == OpCode.OP_RETURN:\n                print \"%s\" % self._stack_pop()\n                return InterpretResultCode.INTERPRET_OK\n            elif instruction == OpCode.OP_CONSTANT:\n                constant = self._read_constant()\n                self._stack_push(constant)\n            elif instruction == OpCode.OP_ADD:\n                self._binary_op(self._stack_add)    \n\n\nNow the _read_byte method will have to keep track of which instruction we are up \nto. So add an instruction pointer (ip) to the VM with an initial value of 0.\nThen _read_byte is simply getting the next bytecode (int) from the chunk's code:\n\n    def _read_byte(self):\n        instruction = self.chunk.code[self.ip]\n        self.ip += 1\n        return instruction\n\n\n\n\nIf the instruction is OP_CONSTANT we take the constant's address from the next byte\nof the chunk's code, retrieve that constant value and add it to the VM's stack.\n\n    def _read_constant(self):\n        constant_index = self._read_byte()\n        return self.chunk.constants[constant_index]\n\n\nFinally our first arithmetic operation OP_ADD, what it has to achieve doesn't \nrequire much explanation: pop two values from the stack, add them together, push \nthe result. But since a few operations all have the same template we introduce a\nlayer of indirection \u2013 or abstraction \u2013 by introducing a reusable _binary_op \nhelper method.\n\n    @specialize.arg(1)\n    def _binary_op(self, operator):\n        op2 = self._stack_pop()\n        op1 = self._stack_pop()\n        result = operator(op1, op2)\n        self._stack_push(result)\n\n    @staticmethod\n    def _stack_add(op1, op2):\n        return op1 + op2\n\n\n\n\nNote we tell RPython to specialize _binary_op on the first argument. This causes\nRPython to make a copy of _binary_op for every value of the first argument passed,\nwhich means that each copy contains a call to a particular operator, which can then be\ninlined.\n\nTo be able to run our bytecode the only thing left to do is to pass in the chunk \nand call _run():\n\n    def interpret_chunk(self, chunk):\n        if self.debug_trace:\n            print \"== VM TRACE ==\"\n        self.chunk = chunk\n        self.ip = 0\n        try:\n            result = self._run()\n            return result\n        except:\n            return InterpretResultCode.INTERPRET_RUNTIME_ERROR\n\n\ntargetvm3.py connects the pieces:\n\ndef entry_point(argv):\n    bytecode = Chunk()\n    constant = bytecode.add_constant(1)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n    constant = bytecode.add_constant(2)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n    bytecode.write_chunk(OpCode.OP_ADD)\n    bytecode.write_chunk(OpCode.OP_RETURN)\n\n    vm = VM()\n    vm.interpret_chunk(bytecode)\n\n    return 0\n\n\nI've added some trace debugging so we can see what the VM and stack is doing.\n\nThe whole thing translates with RPython, and when run gives us:\n\n./vm3\n== VM TRACE ==\n          []\n0000 OP_CONSTANT  (00)        '1'\n          [ 1 ]\n0002 OP_CONSTANT  (01)        '2'\n          [ 1 ] [ 2 ]\n0004 OP_ADD       \n          [ 3 ]\n0005 OP_RETURN    \n3\n\n\nYes we just computed the result of 1+2. Pat yourself on the back. \n\nAt this point it is probably valid to check that the translated executable is actually\nfaster than running our program directly in Python. For this trivial example under \nPython2/pypy this targetvm3.py file runs in the 20ms \u2013 90ms region, and the \ncompiled vm3 runs in <5ms. Something useful must be happening during the translation.\n\nI won't go through the code adding support for our other instructions as they are\nvery similar and straightforward. Our VM is ready to execute our chunks of bytecode,\nbut we haven't yet worked out how to take the entered expression and turn that into\nthis simple bytecode. This is broken into two steps, scanning and compiling.\n\nScanning the source\n\nAll the source for this section can be found in \nsection-3-scanning.\n\nThe job of the scanner is to take the raw expression string and transform it into\na sequence of tokens. This scanning step will strip out whitespace and comments, \ncatch errors with invalid token and tokenize the string. For example the input \n\"( 1 + 2 ) would get tokenized into LEFT_PAREN, NUMBER(1), PLUS, NUMBER(2), RIGHT_PAREN.\n\nAs with our OpCodes we will just define a simple Python class to define an int\nfor each type of token:\n\nclass TokenTypes:\n    ERROR = 0\n    EOF = 1\n    LEFT_PAREN = 2\n    RIGHT_PAREN = 3\n    MINUS = 4\n    PLUS = 5\n    SLASH = 6\n    STAR = 7\n    NUMBER = 8\n\n\nA token has to keep some other information as well \u2013 keeping track of the location and \nlength of the token will be helpful for error reporting. The NUMBER token clearly needs \nsome data about the value it is representing: we could include a copy of the source lexeme \n(e.g. the string 2.0), or parse the value and store that, or \u2013 what we will do in this \nblog \u2013 use the location and length information as pointers into the original source \nstring. Every token type (except perhaps ERROR) will use this simple data structure: \n\nclass Token(object):\n\n    def __init__(self, start, length, token_type):\n        self.start = start\n        self.length = length\n        self.type = token_type\n\n\nOur soon to be created scanner will create these Token objects which refer back to \naddresses in some source. If the scanner sees the source \"( 1 + 2.0 )\" it would emit\nthe following tokens:\n\nToken(0, 1, TokenTypes.LEFT_PAREN)\nToken(2, 1, TokenTypes.NUMBER)\nToken(4, 1, TokenTypes.PLUS)\nToken(6, 3, TokenTypes.NUMBER)\nToken(10, 1, TokenTypes.RIGHT_PAREN)\n\n\nScanner\n\nLet's walk through the scanner implementation method\nby method. The scanner will take the source and pass through it once, creating tokens\nas it goes.\n\nclass Scanner(object):\n\n    def __init__(self, source):\n        self.source = source\n        self.start = 0\n        self.current = 0\n\n\nThe start and current variables are character indices in the source string that point to \nthe current substring being considered as a token. \n\nFor example in the string \"(51.05+2)\" while we are tokenizing the number 51.05\nwe will have start pointing at the 5, and advance current character by character\nuntil the character is no longer part of a number. Midway through scanning the number \nthe start and current values might point to 1 and 4 respectively:\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\"(\"\n\"5\"\n\"1\"\n\".\"\n\"0\"\n\"5\"\n\"+\"\n\"2\"\n\")\"\n\n\n\n\u00a0\n\n\n\u00a0\n\n\n\n\n\n\n\n\nFrom current=4 the scanner peeks ahead and sees that the next character (5) is\na digit, so will continue to advance.\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\"(\"\n\"5\"\n\"1\"\n\".\"\n\"0\"\n\"5\"\n\"+\"\n\"2\"\n\")\"\n\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\n\n\n\nWhen the scanner peeks ahead and sees the \"+\" it will create the number\ntoken and emit it. The method that carry's out this tokenizing is _number:\n\n    def _number(self):\n        while self._peek().isdigit():\n            self.advance()\n\n        # Look for decimal point\n        if self._peek() == '.' and self._peek_next().isdigit():\n            self.advance()\n            while self._peek().isdigit():\n                self.advance()\n\n        return self._make_token(TokenTypes.NUMBER)\n\n\nIt relies on a few helpers to look ahead at the upcoming characters:\n\n    def _peek(self):\n        if self._is_at_end():\n            return '\\0'\n        return self.source[self.current]\n\n    def _peek_next(self):\n        if self._is_at_end():\n            return '\\0'\n        return self.source[self.current+1]\n\n    def _is_at_end(self):\n        return len(self.source) == self.current\n\n\nIf the character at current is still part of the number we want to call advance\nto move on by one character.\n\n    def advance(self):\n        self.current += 1\n        return self.source[self.current - 1]\n\n\nOnce the isdigit() check fails in _number() we call _make_token() to emit the\ntoken with the NUMBER type.\n\n    def _make_token(self, token_type):\n        return Token(\n            start=self.start,\n            length=(self.current - self.start),\n            token_type=token_type\n        )\n\n\nNote again that the token is linked to an index address in the source, rather than \nincluding the string value.\n\nOur scanner is pull based, a token will be requested via scan_token. First we skip \npast whitespace and depending on the characters emit the correct token:\n\n    def scan_token(self):\n        # skip any whitespace\n        while True:\n            char = self._peek()\n            if char in ' \\r\\t\\n':\n                self.advance()\n            break\n\n        self.start = self.current\n\n        if self._is_at_end():\n            return self._make_token(TokenTypes.EOF)\n\n        char = self.advance()\n\n        if char.isdigit():\n            return self._number()\n\n        if char == '(':\n            return self._make_token(TokenTypes.LEFT_PAREN)\n        if char == ')':\n            return self._make_token(TokenTypes.RIGHT_PAREN)\n        if char == '-':\n            return self._make_token(TokenTypes.MINUS)\n        if char == '+':\n            return self._make_token(TokenTypes.PLUS)\n        if char == '/':\n            return self._make_token(TokenTypes.SLASH)\n        if char == '*':\n            return self._make_token(TokenTypes.STAR)\n\n        return ErrorToken(\"Unexpected character\", self.current)\n\n\n\n\nIf this was a real programming language we were scanning, this would be the point where we \nadd support for different types of literals and any language identifiers/reserved words.\n\nAt some point we will need to parse the literal value for our numbers, but we leave that\njob for some later component, for now we'll just add a get_token_string helper. To make\nsure that RPython is happy to index arbitrary slices of source we add range assertions:\n\n    def get_token_string(self, token):\n        if isinstance(token, ErrorToken):\n            return token.message\n        else:\n            end_loc = token.start + token.length\n            assert end_loc < len(self.source)\n            assert end_loc > 0\n            return self.source[token.start:end_loc]\n\n\nA simple entry point can be used to test our scanner with a hard coded \nsource string:\n\ntargetscanner1.py\n\nfrom scanner import Scanner, TokenTypes, TokenTypeToName\n\n\ndef entry_point(argv):\n\n    source = \"(   1   + 2.0 )\"\n\n    scanner = Scanner(source)\n    t = scanner.scan_token()\n    while t.type != TokenTypes.EOF and t.type != TokenTypes.ERROR:\n        print TokenTypeToName[t.type],\n        if t.type == TokenTypes.NUMBER:\n            print \"(%s)\" % scanner.get_token_string(t),\n        print\n        t = scanner.scan_token()\n    return 0\n\n\nRPython didn't complain, and lo it works:\n\n$ ./scanner1 \nLEFT_PAREN\nNUMBER (1)\nPLUS\nNUMBER (2.0)\nRIGHT_PAREN\n\n\nLet's connect our REPL to the scanner.\n\ntargetscanner2.py\n\nfrom rpython.rlib import rfile\nfrom scanner import Scanner, TokenTypes, TokenTypeToName\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef repl(stdin, stdout):\n    while True:\n        stdout.write(\"> \")\n        source = stdin.readline(LINE_BUFFER_LENGTH)\n\n        scanner = Scanner(source)\n        t = scanner.scan_token()\n        while t.type != TokenTypes.EOF and t.type != TokenTypes.ERROR:\n            print TokenTypeToName[t.type],\n            if t.type == TokenTypes.NUMBER:\n                print \"(%s)\" % scanner.get_token_string(t),\n            print\n            t = scanner.scan_token()\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    try:\n        repl(stdin, stdout)\n    except:\n        pass\n    return 0\n\n\nWith our REPL hooked up we can now scan tokens from arbitrary input:\n\n$ ./scanner2\n> (3 *4) - -3\nLEFT_PAREN\nNUMBER (3)\nSTAR\nNUMBER (4)\nRIGHT_PAREN\nMINUS\nMINUS\nNUMBER (3)\n> C\n\n\nCompiling expressions\n\nReferences\n\n\nhttps://www.craftinginterpreters.com/compiling-expressions.html\n\nhttps://effbot.org/zone/simple-top-down-parsing.htm\n\n\nThe final piece is to turn this sequence of tokens into our low level \nbytecode instructions for the virtual machine to execute. Buckle up, \nwe are about to write us a compiler.\n\nOur compiler will take a single pass over the tokens using \nVaughan Pratt\u2019s \nparsing technique, and output a chunk of bytecode \u2013 if we do it\nright it will be compatible with our existing virtual machine.\n\nRemember the bytecode we defined above is really simple \u2013 by relying \non our stack we can transform a nested expression into a sequence of\nour bytecode operations.\n\nTo make this more concrete let's go through by hand translating an\nexpression into bytecode.\n\nOur source expression:\n\n(3 + 2) - (7 * 2)\n\n\nIf we were to make an abstract syntax tree we'd get something \nlike this:\n\n\n\nNow if we start at the first sub expression (3+2) we can clearly\nnote from the first open bracket that we must see a close bracket,\nand that the expression inside that bracket must be valid on its \nown. Not only that but regardless of the inside we know that the whole\nexpression still has to be valid. Let's focus on this first bracketed\nexpression, let our attention recurse into it so to speak.\n\nThis gives us a much easier problem \u2013 we just want to get our virtual\nmachine to compute 3 + 2. In this bytecode dialect we would load the \ntwo constants, and then add them with OP_ADD like so:  \n\nOP_CONSTANT  (00) '3.000000'\nOP_CONSTANT  (01) '2.000000'\nOP_ADD\n\n\nThe effect of our vm executing these three instructions is that sitting\npretty at the top of the stack is the result of the addition. Winning.\n\nJumping back out from our bracketed expression, our next token is MINUS,\nat this point we have a fair idea that it must be used in an infix position. \nIn fact whatever token followed the bracketed expression it must be a \nvalid infix operator, if not the expression is over or had a syntax error. \n\nAssuming the best from our user (naive), we handle MINUS the same way\nwe handled the first PLUS. We've already got the first operand on the\nstack, now we compile the right operand and then write out the bytecode\nfor OP_SUBTRACT.\n\nThe right operand is another simple three instructions:\n\nOP_CONSTANT  (02) '7.000000'\nOP_CONSTANT  (03) '2.000000'\nOP_MULTIPLY\n\n\nThen we finish our top level binary expression and write a OP_RETURN to\nreturn the value at the top of the stack as the execution's result. Our\nfinal hand compiled program is:\n\nOP_CONSTANT  (00) '3.000000'\nOP_CONSTANT  (01) '2.000000'\nOP_ADD\nOP_CONSTANT  (02) '7.000000'\nOP_CONSTANT  (03) '2.000000'\nOP_MULTIPLY\nOP_SUBTRACT\nOP_RETURN\n\n\nOk that wasn't so hard was it? Let's try make our code do that.\n\nWe define a parser object which will keep track of where we are, and\nwhether things have all gone horribly wrong:\n\nclass Parser(object):\n    def __init__(self):\n        self.had_error = False\n        self.panic_mode = False\n        self.current = None\n        self.previous = None\n\n\nThe compiler will also be a class, we'll need one of our Scanner instances\nto pull tokens from, and since the output is a bytecode Chunk let's go ahead\nand make one of those in our compiler initializer:\n\nclass Compiler(object):\n\n    def __init__(self, source):\n        self.parser = Parser()\n        self.scanner = Scanner(source)\n        self.chunk = Chunk()\n\n\nSince we have this (empty) chunk of bytecode we will make a helper method\nto add individual bytes. Every instruction will pass from our compiler into\nan executable program through this simple .\n\n    def emit_byte(self, byte):\n        self.current_chunk().write_chunk(byte)\n\n\nTo quote from Bob Nystrom on the Pratt parsing technique:\n\n\n  the implementation is a deceptively-simple handful of deeply intertwined code\n\n\nI don't actually think I can do justice to this section. Instead I suggest \nreading his treatment in \nPratt Parsers: Expression Parsing Made Easy\nwhich explains the magic behind the parsing component. Our only major difference is \ninstead of creating an AST we are going to directly emit bytecode for our VM.\n\nNow that I've absolved myself from taking responsibility in explaining this somewhat\ntricky concept, I'll discuss some of the code from \ncompiler.py, and walk through what happens \nfor a particular rule.\n\nI'll jump straight to the juicy bit the table of parse rules. We define a ParseRule\nfor each token, and each rule comprises:\n\n\nan optional handler for when the token is as a prefix (e.g. the minus in (-2)),\n\nan optional handler for whet the token is used infix (e.g. the slash in 2/47)\n\na precedence value (a number that determines what is of higher precedence)\n\n\nrules = [\n    ParseRule(None,              None,            Precedence.NONE),   # ERROR\n    ParseRule(None,              None,            Precedence.NONE),   # EOF\n    ParseRule(Compiler.grouping, None,            Precedence.CALL),   # LEFT_PAREN\n    ParseRule(None,              None,            Precedence.NONE),   # RIGHT_PAREN\n    ParseRule(Compiler.unary,    Compiler.binary, Precedence.TERM),   # MINUS\n    ParseRule(None,              Compiler.binary, Precedence.TERM),   # PLUS\n    ParseRule(None,              Compiler.binary, Precedence.FACTOR), # SLASH\n    ParseRule(None,              Compiler.binary, Precedence.FACTOR), # STAR\n    ParseRule(Compiler.number,   None,            Precedence.NONE),   # NUMBER\n]\n\n\nThese rules really are the magic of our compiler. When we get to a particular\ntoken such as MINUS we see if it is an infix operator and if so we've gone and\ngot its first operand ready. At all times we rely on the relative precedence; consuming \neverything with higher precedence than the operator we are currently evaluating.\n\nIn the expression:\n\n2 + 3 * 4\n\n\nThe * has higher precedence than the +, so 3 * 4 will be parsed together\nas the second operand to the first infix operator (the +) which follows\nthe BEDMAS \norder of operations I was taught at high school.\n\nTo encode these precedence values we make another Python object moonlighting\nas an enum:\n\nclass Precedence(object):\n    NONE = 0\n    DEFAULT = 1\n    TERM = 2        # + -\n    FACTOR = 3      # * /\n    UNARY = 4       # ! - +\n    CALL = 5        # ()\n    PRIMARY = 6\n\n\nWhat happens in our compiler when turning -2.0 into bytecode? Assume we've just \npulled the token MINUS from the scanner. Every expression has to start with some\ntype of prefix \u2013 whether that is:\n\n\na bracket group (, \n\na number 2, \n\nor a prefix unary operator -. \n\n\nKnowing that, our compiler assumes there is a prefix handler in the rule table \u2013 in\nthis case it points us at the unary handler.\n\n    def parse_precedence(self, precedence):\n        # parses any expression of a given precedence level or higher\n        self.advance()\n        prefix_rule = self._get_rule(self.parser.previous.type).prefix\n        prefix_rule(self)\n\n\n\n\nunary is called:\n\n    def unary(self):\n        op_type = self.parser.previous.type\n        # Compile the operand\n        self.parse_precedence(Precedence.UNARY)\n        # Emit the operator instruction\n        if op_type == TokenTypes.MINUS:\n            self.emit_byte(OpCode.OP_NEGATE)\n\n\nHere \u2013 before writing the OP_NEGATE opcode we recurse back into parse_precedence\nto ensure that whatever follows the MINUS token is compiled \u2013 provided it has \nhigher precedence than unary \u2013 e.g. a bracketed group. \nCrucially at run time this recursive call will ensure that the result is left \non top of our stack. Armed with this knowledge, the unary method just\nhas to emit a single byte with the OP_NEGATE opcode.\n\nTest compilation\n\nNow we can test our compiler by outputting disassembled bytecode\nof our user entered expressions. Create a new entry_point \ntargetcompiler:\n\nfrom rpython.rlib import rfile\nfrom compiler import Compiler\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n\n    try:\n        while True:\n            stdout.write(\"> \")\n            source = stdin.readline(LINE_BUFFER_LENGTH)\n            compiler = Compiler(source, debugging=True)\n            compiler.compile()\n    except:\n        pass\n    return 0\n\n\nTranslate it and test it out:\n\n$ ./compiler1 \n> (2/4 + 1/2)\n== code ==\n\n0000 OP_CONSTANT  (00) '2.000000'\n0002 OP_CONSTANT  (01) '4.000000'\n0004 OP_DIVIDE    \n0005 OP_CONSTANT  (02) '1.000000'\n0007 OP_CONSTANT  (00) '2.000000'\n0009 OP_DIVIDE    \n0010 OP_ADD       \n0011 OP_RETURN\n\n\nNow if you've made it this far you'll be eager to finally connect everything\ntogether by executing this bytecode with the virtual machine.\n\nEnd to end\n\nAll the pieces slot together rather easily at this point, create a new \nfile targetcalc.py and define our \nentry point:\n\nfrom rpython.rlib import rfile\nfrom compiler import Compiler\nfrom vm import VM\n\nLINE_BUFFER_LENGTH = 4096\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    vm = VM()\n    try:\n        while True:\n            stdout.write(\"> \")\n            source = stdin.readline(LINE_BUFFER_LENGTH)\n            if source:\n                compiler = Compiler(source, debugging=False)\n                compiler.compile()\n                vm.interpret_chunk(compiler.chunk)\n    except:\n        pass\n    return 0\n\n\ndef target(driver, *args):\n    driver.exe_name = \"calc\"\n    return entry_point, None\n\n\n\n\nLet's try catch it out with a double negative:\n\n$ ./calc \n> 2--3\n== VM TRACE ==\n          []\n0000 OP_CONSTANT  (00) '2.000000'\n          [ 2.000000 ]\n0002 OP_CONSTANT  (01) '3.000000'\n          [ 2.000000 ] [ 3.000000 ]\n0004 OP_NEGATE    \n          [ 2.000000 ] [ -3.000000 ]\n0005 OP_SUBTRACT  \n          [ 5.000000 ]\n0006 OP_RETURN    \n5.000000\n\n\nOk well let's evaluate the first 50 terms of the \nNilakantha Series:\n\n$ ./calc\n> 3 + 4 * ((1/(2 * 3 * 4)) + (1/(4 * 5 * 6)) - (1/(6 * 7 * 8)) + (1/(8 * 9 * 10)) - (1/(10 * 11 * 12)) + (1/(12 * 13 * 14)) - (1/(14 * 15 * 16)) + (1/(16 * 17 * 18)) - (1/(18 * 19 * 20)) + (1/(20 * 21 * 22)) - (1/(22 * 23 * 24)) + (1/(24 * 25 * 26)) - (1/(26 * 27 * 28)) + (1/(28 * 29 * 30)) - (1/(30 * 31 * 32)) + (1/(32 * 33 * 34)) - (1/(34 * 35 * 36)) + (1/(36 * 37 * 38)) - (1/(38 * 39 * 40)) + (1/(40 * 41 * 42)) - (1/(42 * 43 * 44)) + (1/(44 * 45 * 46)) - (1/(46 * 47 * 48)) + (1/(48 * 49 * 50)) - (1/(50 * 51 * 52)) + (1/(52 * 53 * 54)) - (1/(54 * 55 * 56)) + (1/(56 * 57 * 58)) - (1/(58 * 59 * 60)) + (1/(60 * 61 * 62)) - (1/(62 * 63 * 64)) + (1/(64 * 65 * 66)) - (1/(66 * 67 * 68)) + (1/(68 * 69 * 70)) - (1/(70 * 71 * 72)) + (1/(72 * 73 * 74)) - (1/(74 * 75 * 76)) + (1/(76 * 77 * 78)) - (1/(78 * 79 * 80)) + (1/(80 * 81 * 82)) - (1/(82 * 83 * 84)) + (1/(84 * 85 * 86)) - (1/(86 * 87 * 88)) + (1/(88 * 89 * 90)) - (1/(90 * 91 * 92)) + (1/(92 * 93 * 94)) - (1/(94 * 95 * 96)) + (1/(96 * 97 * 98)) - (1/(98 * 99 * 100)) + (1/(100 * 101 * 102)))\n\n== VM TRACE ==\n          []\n0000 OP_CONSTANT  (00) '3.000000'\n          [ 3.000000 ]\n0002 OP_CONSTANT  (01) '4.000000'\n...SNIP...\n0598 OP_CONSTANT  (101) '102.000000'\n          [ 3.000000 ] [ 4.000000 ] [ 0.047935 ] [ 1.000000 ] [ 10100.000000 ] [ 102.000000 ]\n0600 OP_MULTIPLY  \n          [ 3.000000 ] [ 4.000000 ] [ 0.047935 ] [ 1.000000 ] [ 1030200.000000 ]\n0601 OP_DIVIDE    \n          [ 3.000000 ] [ 4.000000 ] [ 0.047935 ] [ 0.000001 ]\n0602 OP_ADD       \n          [ 3.000000 ] [ 4.000000 ] [ 0.047936 ]\n0603 OP_MULTIPLY  \n          [ 3.000000 ] [ 0.191743 ]\n0604 OP_ADD       \n          [ 3.191743 ]\n0605 OP_RETURN    \n3.191743\n\n\nWe just executed 605 virtual machine instructions to compute pi to 1dp!\n\nThis brings us to the end of this tutorial. To recap we've walked through the whole \ncompilation process: from the user providing an expression string on the REPL, scanning\nthe source string into tokens, parsing the tokens while accounting for relative \nprecedence via a Pratt parser, generating bytecode, and finally executing the bytecode \non our own VM. RPython translated what we wrote into C and compiled it, meaning\nour resulting calc REPL is really fast.\n\n\n  \u201cThe world is a thing of utter inordinate complexity and richness and strangeness that is absolutely awesome.\u201d\n  \n  \u2015 Douglas Adams \n\n\nMany thanks to Bob Nystrom for writing the book that inspired this post, and thanks to \nCarl Friedrich and Matt Halverson for reviewing.\n\n\u2015 Brian (@thorneynzb)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/11/guest-post-implementing-calculator-repl-6271483514675006846.html"
    },
    {
      "title": "Inside cpyext: Why emulating CPython C API is so Hard",
      "text": "cpyext is PyPy's subsystem which provides a compatibility\nlayer to compile and run CPython C extensions inside PyPy.  Often people ask\nwhy a particular C extension doesn't work or is very slow on PyPy.\nUsually it is hard to answer without going into technical details. The goal of\nthis blog post is to explain some of these technical details, so that we can\nsimply link here instead of explaining again and again :).\nFrom a 10.000 foot view, cpyext is PyPy's version of \"Python.h\". Every time\nyou compile an extension which uses that header file, you are using cpyext.\nThis includes extension explicitly written in C (such as numpy) and\nextensions which are generated from other compilers/preprocessors\n(e.g. Cython).\nAt the time of writing, the current status is that most C extensions \"just\nwork\". Generally speaking, you can simply pip install them,\nprovided they use the public, official C API instead of poking at private\nimplementation details.  However, the performance of cpyext is generally\npoor. A Python program which makes heavy use of cpyext extensions\nis likely to be slower on PyPy than on CPython.\nNote: in this blog post we are talking about Python 2.7 because it is still\nthe default version of PyPy: however most of the implementation of cpyext is\nshared with PyPy3, so everything applies to that as well.\n\n\nC API Overview\nIn CPython, which is written in C, Python objects are represented as PyObject*,\ni.e. (mostly) opaque pointers to some common \"base struct\".\nCPython uses a very simple memory management scheme: when you create an\nobject, you allocate a block of memory of the appropriate size on the heap.\nDepending on the details, you might end up calling different allocators, but\nfor the sake of simplicity, you can think that this ends up being a call to\nmalloc(). The resulting block of memory is initialized and casted to to\nPyObject*: this address never changes during the object lifetime, and the\nC code can freely pass it around, store it inside containers, retrieve it\nlater, etc.\nMemory is managed using reference counting. When you create a new reference to\nan object, or you discard a reference you own, you have to increment or\ndecrement the reference counter accordingly. When the reference counter goes to\n0, it means that the object is no longer used and can safely be\ndestroyed. Again, we can simplify and say that this results in a call to\nfree(), which finally releases the memory which was allocated by malloc().\nGenerally speaking, the only way to operate on a PyObject* is to call the\nappropriate API functions. For example, to convert a given PyObject* to a C\ninteger, you can use PyInt_AsLong(); to add two objects together, you can\ncall PyNumber_Add().\nInternally, PyPy uses a similar approach. All Python objects are subclasses of\nthe RPython W_Root class, and they are operated by calling methods on the\nspace singleton, which represents the interpreter.\nAt first, it looks very easy to write a compatibility layer: just make\nPyObject* an alias for W_Root, and write simple RPython functions\n(which will be translated to C by the RPython compiler) which call the\nspace accordingly:\ndef PyInt_AsLong(space, o):\n    return space.int_w(o)\n\ndef PyNumber_Add(space, o1, o2):\n    return space.add(o1, o2)\n\nActually, the code above is not too far from the real\nimplementation. However, there are tons of gory details which make it much\nharder than it looks, and much slower unless you pay a lot of attention\nto performance.\n\n\nThe PyPy GC\nTo understand some of cpyext challenges, you need to have at least a rough\nidea of how the PyPy GC works.\nContrarily to the popular belief, the \"Garbage Collector\" is not only about\ncollecting garbage: instead, it is generally responsible for all memory\nmanagement, including allocation and deallocation.\nWhereas CPython uses a combination of malloc/free/refcounting to manage\nmemory, the PyPy GC uses a completely different approach. It is designed\nassuming that a dynamic language like Python behaves the following way:\n\n\nYou create, either directly or indirectly, lots of objects.\nMost of these objects are temporary and very short-lived. Think e.g. of\ndoing a + b + c: you need to allocate an object to hold the temporary\nresult of a + b, then it dies very quickly because you no longer need it\nwhen you do the final + c part.\nOnly small fraction of the objects survive and stay around for a while.\n\n\nSo, the strategy is: make allocation as fast as possible; make deallocation of\nshort-lived objects as fast as possible; find a way to handle the remaining\nsmall set of objects which actually survive long enough to be important.\nThis is done using a Generational GC: the basic idea is the following:\n\n\nWe have a nursery, where we allocate \"young objects\" very quickly.\nWhen the nursery is full, we start what we call a \"minor collection\".\nWe do a quick scan to determine the small set of objects which survived so\nfar\nWe move these objects out of the nursery, and we place them in the\narea of memory which contains the \"old objects\". Since the address of the\nobjects changes, we fix all the references to them accordingly.\n\n\n\n\nnow the nursery contains only objects which \"died young\". We can\ndiscard all of them very quickly, reset the nursery, and use the same area\nof memory to allocate new objects from now.\n\n\nIn practice, this scheme works very well and it is one of the reasons why PyPy\nis much faster than CPython.  However, careful readers have surely noticed\nthat this is a problem for cpyext. On one hand, we have PyPy objects which\ncan potentially move and change their underlying memory address; on the other\nhand, we need a way to represent them as fixed-address PyObject* when we\npass them to C extensions.  We surely need a way to handle that.\n\n\nPyObject* in PyPy\nAnother challenge is that sometimes, PyObject* structs are not completely\nopaque: there are parts of the public API which expose to the user specific\nfields of some concrete C struct. For example the definition of PyTypeObject\nwhich exposes many of the tp_* slots to the user.\nSince the low-level layout of PyPy W_Root objects is completely different\nthan the one used by CPython, we cannot simply pass RPython objects to C; we\nneed a way to handle the difference.\nSo, we have two issues so far: objects can move, and incompatible\nlow-level layouts. cpyext solves both by decoupling the RPython and the C\nrepresentations. We have two \"views\" of the same entity, depending on whether\nwe are in the PyPy world (the movable W_Root subclass) or in the C world\n(the non-movable PyObject*).\nPyObject* are created lazily, only when they are actually needed. The\nvast majority of PyPy objects are never passed to any C extension, so we don't\npay any penalty in that case. However, the first time we pass a W_Root to\nC, we allocate and initialize its PyObject* counterpart.\nThe same idea applies also to objects which are created in C, e.g. by calling\nPyObject_New(). At first, only the PyObject* exists and it is\nexclusively managed by reference counting. As soon as we pass it to the PyPy\nworld (e.g. as a return value of a function call), we create its W_Root\ncounterpart, which is managed by the GC as usual.\nHere we start to see why calling cpyext modules is more costly in PyPy than in\nCPython. We need to pay some penalty for all the conversions between\nW_Root and PyObject*.\nMoreover, the first time we pass a W_Root to C we also need to allocate\nthe memory for the PyObject* using a slowish \"CPython-style\" memory\nallocator. In practice, for all the objects which are passed to C we pay more\nor less the same costs as CPython, thus effectively \"undoing\" the speedup\nguaranteed by PyPy's Generational GC under normal circumstances.\n\n\nMaintaining the link between W_Root and PyObject*\nWe now need a way to convert between W_Root and PyObject* and\nvice-versa; also, we need to to ensure that the lifetime of the two entities\nare in sync. In particular:\n\n\nas long as the W_Root is kept alive by the GC, we want the\nPyObject* to live even if its refcount drops to 0;\nas long as the PyObject* has a refcount greater than 0, we want to\nmake sure that the GC does not collect the W_Root.\n\n\nThe PyObject* \u21e8 W_Root link is maintained by the special field\nob_pypy_link which is added to all PyObject*. On a 64 bit machine this\nmeans that all PyObject* have 8 bytes of overhead, but then the\nconversion is very quick, just reading the field.\nFor the other direction, we generally don't want to do the same: the\nassumption is that the vast majority of W_Root objects will never be\npassed to C, and adding an overhead of 8 bytes to all of them is a\nwaste. Instead, in the general case the link is maintained by using a\ndictionary, where W_Root are the keys and PyObject* the values.\nHowever, for a few selected W_Root subclasses we do maintain a\ndirect link using the special _cpy_ref field to improve performance. In\nparticular, we use it for W_TypeObject (which is big anyway, so a 8 bytes\noverhead is negligible) and W_NoneObject. None is passed around very\noften, so we want to ensure that the conversion to PyObject* is very\nfast. Moreover it's a singleton, so the 8 bytes overhead is negligible as\nwell.\nThis means that in theory, passing an arbitrary Python object to C is\npotentially costly, because it involves doing a dictionary lookup.  We assume\nthat this cost will eventually show up in the profiler: however, at the time\nof writing there are other parts of cpyext which are even more costly (as we\nwill show later), so the cost of the dict lookup is never evident in the\nprofiler.\n\n\nCrossing the border between RPython and C\nThere are two other things we need to care about whenever we cross the border\nbetween RPython and C, and vice-versa: exception handling and the GIL.\nIn the C API, exceptions are raised by calling PyErr_SetString() (or one of\nmany other functions which have a similar effect), which basically works by\ncreating an exception value and storing it in some global variable. The\nfunction then signals that an exception has occurred by returning an error value,\nusually NULL.\nOn the other hand, in the PyPy interpreter, exceptions are propagated by raising the\nRPython-level OperationError exception, which wraps the actual app-level\nexception values. To harmonize the two worlds, whenever we return from C to\nRPython, we need to check whether a C API exception was raised and if so turn it\ninto an OperationError.\nWe won't dig into details of how the GIL is handled in cpyext.\nFor the purpose of this post, it is enough to know that whenever we enter\nC land, we store the current thread id into a global variable which is\naccessible also from C; conversely, whenever we go back from RPython to C, we\nrestore this value to 0.\nSimilarly, we need to do the inverse operations whenever you need to cross the\nborder between C and RPython, e.g. by calling a Python callback from C code.\nAll this complexity is automatically handled by the RPython function\ngeneric_cpy_call. If you look at the code you see that it takes care of 4\nthings:\n\n\nHandling the GIL as explained above.\nHandling exceptions, if they are raised.\nConverting arguments from W_Root to PyObject*.\nConverting the return value from PyObject* to W_Root.\n\n\nSo, we can see that calling C from RPython introduce some overhead.\nCan we measure it?\nAssuming that the conversion between W_Root and PyObject* has a\nreasonable cost (as explained by the previous section), the overhead\nintroduced by a single border-cross is still acceptable, especially if the\ncallee is doing some non-negligible amount of work.\nHowever this is not always the case. There are basically three problems that\nmake (or used to make) cpyext super slow:\n\n\nPaying the border-crossing cost for trivial operations which are called\nvery often, such as Py_INCREF.\nCrossing the border back and forth many times, even if it's not strictly\nneeded.\nPaying an excessive cost for argument and return value conversions.\n\n\nThe next sections explain in more detail each of these problems.\n\n\nAvoiding unnecessary roundtrips\nPrior to the 2017 Cape Town Sprint, cpyext was horribly slow, and we were\nwell aware of it: the main reason was that we never really paid too much\nattention to performance. As explained in the blog post, emulating all the\nCPython quirks is basically a nightmare, so better to concentrate on\ncorrectness first.\nHowever, we didn't really know why it was so slow. We had theories and\nassumptions, usually pointing at the cost of conversions between W_Root\nand PyObject*, but we never actually measured it.\nSo, we decided to write a set of cpyext microbenchmarks to measure the\nperformance of various operations.  The result was somewhat surprising: the\ntheory suggests that when you do a cpyext C call, you should pay the\nborder-crossing costs only once, but what the profiler told us was that we\nwere paying the cost of generic_cpy_call several times more than what we expected.\nAfter a bit of investigation, we discovered this was ultimately caused by our\n\"correctness-first\" approach. For simplicity of development and testing, when\nwe started cpyext we wrote everything in RPython: thus, every single API call\nmade from C (like the omnipresent PyArg_ParseTuple(), PyInt_AsLong(), etc.)\nhad to cross back the C-to-RPython border. This was especially daunting for\nvery simple and frequent operations like Py_INCREF and Py_DECREF,\nwhich CPython implements as a single assembly instruction!\nAnother source of slow down was the implementation of PyTypeObject slots.\nAt the C level, these are function pointers which the interpreter calls to do\ncertain operations, e.g. tp_new to allocate a new instance of that type.\nAs usual, we have some magic to implement slots in RPython; in particular,\n_make_wrapper does the opposite of generic_cpy_call: it takes a\nRPython function and wraps it into a C function which can be safely called\nfrom C, handling the GIL, exceptions and argument conversions automatically.\nThis was very handy during the development of cpyext, but it might result in\nsome bad nonsense; consider what happens when you call the following C\nfunction:\nstatic PyObject* foo(PyObject* self, PyObject* args)\n{\n    PyObject* result = PyInt_FromLong(1234);\n    return result;\n}\n\n\nyou are in RPython and do a cpyext call to foo: RPython-to-C;\nfoo calls PyInt_FromLong(1234), which is implemented in RPython:\nC-to-RPython;\nthe implementation of PyInt_FromLong indirectly calls\nPyIntType.tp_new, which is a C function pointer: RPython-to-C;\nhowever, tp_new is just a wrapper around an RPython function, created\nby _make_wrapper: C-to-RPython;\nfinally, we create our RPython W_IntObject(1234); at some point\nduring the RPython-to-C crossing, its PyObject* equivalent is\ncreated;\nafter many layers of wrappers, we are again in foo: after we do\nreturn result, during the C-to-RPython step we convert it from\nPyObject* to W_IntObject(1234).\n\nPhew! After we realized this, it was not so surprising that cpyext was very\nslow :). And this was a simplified example, since we are not passing a\nPyObject* to the API call. When we do, we need to convert it back and\nforth at every step.  Actually, I am not even sure that what I described was\nthe exact sequence of steps which used to happen, but you get the general\nidea.\nThe solution is simple: rewrite as much as we can in C instead of RPython,\nto avoid unnecessary roundtrips. This was the topic of most of the Cape Town\nsprint and resulted in the cpyext-avoid-roundtrip branch, which was\neventually merged.\nOf course, it is not possible to move everything to C: there are still\noperations which need to be implemented in RPython. For example, think of\nPyList_Append: the logic to append an item to a list is complex and\ninvolves list strategies, so we cannot replicate it in C.  However, we\ndiscovered that a large subset of the C API can benefit from this.\nMoreover, the C API is huge. While we invented this new way of writing\ncpyext code, we still need to\nconvert many of the functions to the new paradigm.  Sometimes the rewrite is\nnot automatic\nor straighforward. cpyext is a delicate piece of software, so it happens often\nthat we make a mistake and end up staring at a segfault in gdb.\nHowever, the most important takeaway is that the performance improvements we got\nfrom this optimization are impressive, as we will detail later.\n\n\nConversion costs\nThe other potential big source of slowdown is the conversion of arguments\nbetween W_Root and PyObject*.\nAs explained earlier, the first time you pass a W_Root to C, you need to\nallocate its PyObject* counterpart. Suppose you have a foo function\ndefined in C, which takes a single int argument:\nfor i in range(N):\n    foo(i)\n\nTo run this code, you need to create a different PyObject* for each value\nof i: if implemented naively, it means calling N times malloc()\nand free(), which kills performance.\nCPython has the very same problem, which is solved by using a free list to\nallocate ints. So, what we did was to simply steal the code from CPython\nand do the exact same thing. This was also done in the\ncpyext-avoid-roundtrip branch, and the benchmarks show that it worked\nperfectly.\nEvery type which is converted often to PyObject* must have a very fast\nallocator. At the moment of writing, PyPy uses free lists only for ints and\ntuples: one of the next steps on our TODO list is certainly to use this\ntechnique with more types, like float.\nConversely, we also need to optimize the converstion from PyObject* to\nW_Root: this happens when an object is originally allocated in C and\nreturned to Python. Consider for example the following code:\nimport numpy as np\nmyarray = np.random.random(N)\nfor i in range(len(arr)):\n    myarray[i]\n\nAt every iteration, we get an item out of the array: the return type is a an\ninstance of numpy.float64 (a numpy scalar), i.e. a PyObject'*: this is\nsomething which is implemented by numpy entirely in C, so completely\nopaque to cpyext. We don't have any control on how it is allocated,\nmanaged, etc., and we can assume that allocation costs are the same as on\nCPython.\nAs soon as we return these PyObject* to Python, we need to allocate\ntheir W_Root equivalent. If you do it in a small loop like in the example\nabove, you end up allocating all these W_Root inside the nursery, which is\na good thing since allocation is super fast (see the section above about the\nPyPy GC).\nHowever, we also need to keep track of the W_Root to PyObject* link.\nCurrently, we do this by putting all of them in a dictionary, but it is very\ninefficient, especially because most of these objects die young and thus it\nis wasted work to do that for them.  Currently, this is one of the biggest\nunresolved problem in cpyext, and it is what causes the two microbenchmarks\nallocate_int and allocate_tuple to be very slow.\nWe are well aware of the problem, and we have a plan for how to fix it. The\nexplanation is too technical for the scope of this blog post as it requires a\ndeep knowledge of the GC internals to be understood, but the details are\nhere.\n\n\nC API quirks\nFinally, there is another source of slowdown which is beyond our control. Some\nparts of the CPython C API are badly designed and expose some of the\nimplementation details of CPython.\nThe major example is reference counting. The Py_INCREF / Py_DECREF API\nis designed in such a way which forces other implementation to emulate\nrefcounting even in presence of other GC management schemes, as explained\nabove.\nAnother example is borrowed references. There are API functions which do\nnot incref an object before returning it, e.g. PyList_GetItem().  This is\ndone for performance reasons because we can avoid a whole incref/decref pair,\nif the caller needs to handle the returned item only temporarily: the item is\nkept alive because it is in the list anyway.\nFor PyPy, this is a challenge: thanks to list strategies, lists are often\nrepresented in a compact way. For example, a list containing only integers is\nstored as a C array of long.  How to implement PyList_GetItem? We\ncannot simply create a PyObject* on the fly, because the caller will never\ndecref it and it will result in a memory leak.\nThe current solution is very inefficient. The first time we do a\nPyList_GetItem, we convert the whole list to a list of\nPyObject*. This is bad in two ways: the first is that we potentially pay a\nlot of unneeded conversion cost in case we will never access the other items\nof the list. The second is that by doing that we lose all the performance\nbenefit granted by the original list strategy, making it slower for the\nrest of the pure-python code which will manipulate the list later.\nPyList_GetItem is an example of a bad API because it assumes that the list\nis implemented as an array of PyObject*: after all, in order to return a\nborrowed reference, we need a reference to borrow, don't we?\nFortunately, (some) CPython developers are aware of these problems, and there\nis an ongoing project to design a better C API which aims to fix exactly\nthis kind of problem.\nNonetheless, in the meantime we still need to implement the current\nhalf-broken APIs. There is no easy solution for that, and it is likely that\nwe will always need to pay some performance penalty in order to implement them\ncorrectly.\nHowever, what we could potentially do is to provide alternative functions\nwhich do the same job but are more PyPy friendly: for example, we could think\nof implementing PyList_GetItemNonBorrowed or something like that: then, C\nextensions could choose to use it (possibly hidden inside some macro and\n#ifdef) if they want to be fast on PyPy.\n\n\nCurrent performance\nDuring the whole blog post we claimed cpyext is slow. How\nslow it is, exactly?\nWe decided to concentrate on microbenchmarks for now. It should be evident\nby now there are simply too many issues which can slow down a cpyext\nprogram, and microbenchmarks help us to concentrate on one (or few) at a\ntime.\nThe microbenchmarks measure very simple things, like calling functions and\nmethods with the various calling conventions (no arguments, one arguments,\nmultiple arguments); passing various types as arguments (to measure conversion\ncosts); allocating objects from C, and so on.\nHere are the results from the old PyPy 5.8 relative and normalized to CPython\n2.7, the lower the better:\n\n\n\n\n\n\n\n\n\n\n\nPyPy was horribly slow everywhere, ranging from 2.5x to 10x slower. It is\nparticularly interesting to compare simple.noargs, which measures the cost\nof calling an empty function with no arguments, and simple.onearg(i),\nwhich measures the cost calling an empty function passing an integer argument:\nthe latter is ~2x slower than the former, indicating that the conversion cost\nof integers is huge.\nPyPy 5.8 was the last release before the famous Cape Town sprint, when we\nstarted to look at cpyext performance seriously. Here are the performance data for\nPyPy 6.0, the latest release at the time of writing:\n\n\n\n\nThe results are amazing! PyPy is now massively faster than before, and for\nmost benchmarks it is even faster than CPython: yes, you read it correctly:\nPyPy is faster than CPython at doing CPython's job, even considering all the\nextra work it has to do to emulate the C API.  This happens thanks to the JIT,\nwhich produces speedups high enough to counterbalance the slowdown caused by\ncpyext.\nThere are two microbenchmarks which are still slower though: allocate_int\nand allocate_tuple, for the reasons explained in the section about\nConversion costs.\n\n\nNext steps\nDespite the spectacular results we got so far, cpyext is still slow enough to\nkill performance in most real-world code which uses C extensions extensively\n(e.g., the omnipresent numpy).\nOur current approach is something along these lines:\n\n\nrun a real-world small benchmark which exercises cpyext\nmeasure and find the major bottleneck\nwrite a corresponding microbenchmark\noptimize it\nrepeat\n\n\nOn one hand, this is a daunting task because the C API is huge and we need to\ntackle functions one by one.  On the other hand, not all the functions are\nequally important, and is is enough to optimize a relatively small subset to\nimprove many different use cases.\nWhere a year ago we announced we have a working answer to run c-extension in\nPyPy, we now have a clear picture of what are the performance bottlenecks, and\nwe have developed some technical solutions to fix them. It is \"only\" a matter\nof tackling them, one by one.  It is worth noting that most of the work was\ndone during two sprints, for a total 2-3 person-months of work.\nWe think this work is important for the Python ecosystem. PyPy has established\na baseline for performance in pure python code, providing an answer for the\n\"Python is slow\" detractors. The techniques used to make cpyext performant\nwill let PyPy become an alternative for people who mix C extensions with\nPython, which, it turns out, is just about everyone, in particular those using\nthe various scientific libraries. Today, many developers are forced to seek\nperformance by converting code from Python to a lower language. We feel there\nis no reason to do this, but in order to prove it we must be able to run both\ntheir python and their C extensions performantly, then we can begin to educate\nthem how to write JIT-friendly code in the first place.\nWe envision a future in which you can run arbitrary Python programs on PyPy,\nwith the JIT speeding up the pure Python parts and the C parts running as fast\nas today: the best of both worlds!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/09/inside-cpyext-why-emulating-cpython-c-8083064623681286567.html"
    },
    {
      "title": "The First 15 Years of PyPy \u2014 a Personal Retrospective",
      "text": "A few weeks ago I (=Carl Friedrich Bolz-Tereick) gave a keynote at ICOOOLPS in\nAmsterdam with the above title. I was very happy to have been given that\nopportunity, since a number of our papers have been published at ICOOOLPS,\nincluding the very first one I published when I'd just started my PhD. I decided\nto turn the talk manuscript into a (longish) blog post, to make it available to a wider audience.\nNote that this blog post describes my personal recollections and research, it is\nthus necessarily incomplete and coloured by my own experiences.\nPyPy has turned 15 years old this year, so I decided that that's a good reason\nto dig into and talk about the history of the project so far. I'm going to do\nthat using the lens of how performance developed over time, which is from\nsomething like 2000x slower than CPython, to roughly 7x faster. In this post\nI am going to present the history of the project, and also talk about some\nlessons that we learned.\nThe post does not make too many assumptions about any prior knowledge of what\nPyPy is, so if this is your first interaction with it, welcome! I have tried to\nsprinkle links to earlier blog posts and papers into the writing, in case you\nwant to dive deeper into some of the topics.\nAs a disclaimer, in this post I am going to mostly focus on ideas, and not\nexplain who had or implemented them. A huge amount of people contributed to the\ndesign, the implementation, the funding and the organization of PyPy over the\nyears, and it would be impossible to do them all justice.\n\nContents\n\n2003: Starting the Project\n2003: Implementing the Interpreter\nEarly organizational ideas\n2004-2007: EU-Funding\n2005: Bootstrapping PyPy\nRPython's Modularity Problems\n2006: The Meta-JIT\nThe First JIT Generator\nPromote\nVirtuals\nJIT Status 2007\n2007: RSqueak and other languages\n2008-2009: Four More JIT Generators\n2009: Meta-Tracing\nWhy did we Abandon Partial Evaluation?\n2009-2011: The PyJIT Eurostars Project\nTracing JIT improvements\n2010: speed.pypy.org\nContinuous Integration\n2010: Implementing Python Objects with Maps\n2011: Container Storage Strategies\nDeep Changes in the Runtime are Necessary\nJIT Status 2011\n2012-2017: Engineering and Incremental Progress\nCPyExt\nPython 3\nIncentives of OSS compared to Academia\nMeta-Tracing really works!\nAcknowledgements\n\n\n\n2003: Starting the Project\nOn the technical level PyPy is a Python interpreter written in Python, which is\nwhere the name comes from. It also has an automatically generated JIT compiler,\nbut I'm going to introduce that gradually over the rest of the blog post, so\nlet's not worry about it too much yet. On the social level PyPy is an\ninteresting mixture of a open source project, that sometimes had research done\nin it.\nThe project got started in late 2002 and early 2003. To set the stage, at that\npoint Python was a significantly less popular language than it is today. Python\n2.2 was the version at the time, Python didn't even have a bool type yet.\nIn fall 2002 the PyPy project was started by a number of Python programmers on a\nmailing list who said\nsomething like (I am exaggerating somewhat) \"Python is the greatest most\nwonderful most perfect language ever, we should use it for absolutely\neverything. Well, what aren't we using it for? The Python virtual machine itself\nis written in C, that's bad. Let's start a project to fix that.\"\nOriginally that project was called \"minimal python\", or \"ptn\", later gradually\nrenamed to PyPy. Here's the mailing list post to announce the project more\nformally:\nMinimal Python Discussion, Coding and Sprint\n--------------------------------------------\n\nWe announce a mailinglist dedicated to developing\na \"Minimal Python\" version.  Minimal means that\nwe want to have a very small C-core and as much\nas possible (re)implemented in python itself.  This\nincludes (parts of) the VM-Code.\nWhy would that kind of project be useful? Originally it wasn't necessarily meant\nto be useful as a real implementation at all, it was more meant as a kind of\nexecutable explanation of how Python works, free of the low level details of\nCPython. But pretty soon there were then also plans for how the virtual machine\n(VM) could be bootstrapped to be runnable without an existing Python\nimplementation, but I'll get to that further down.\n\n\n\n\n2003: Implementing the Interpreter\nIn early 2003 a group of Python people met in Hildesheim (Germany) for the first\nof many week long development sprints, organized by Holger Krekel. During that\nweek a group of people showed up and started working on the core interpreter.\nIn May 2003 a second sprint was organized by Laura Creighton and Jacob Hal\u00e9n in\nGothenburg (Sweden). And already at that sprint enough of the Python bytecodes\nand data structures were implemented to make it possible to run a program that\ncomputed how much money everybody had to pay for the food bills of the week. And\neverybody who's tried that for a large group of people knows that that\u2019s an\namazingly complex mathematical problem.\nIn the next two years, the project continued as a open source project with\nvarious contributors working on it in their free time, and meeting for the\noccasional sprint. In that time, the rest of the core interpreter and the core\ndata types were implemented.\nThere's not going to be any other code in this post, but to give a bit of a\nflavor of what the Python interpreter at that time looked like, here's the\nimplementation of the DUP_TOP bytecode after these first sprints. As you can\nsee, it's in Python, obviously, and it has high level constructs such as method\ncalls to do the stack manipulations:\ndef DUP_TOP(f):\n    w_1 = f.valuestack.top()\n    f.valuestack.push(w_1)\nHere's the early code for integer addition:\ndef int_int_add(space, w_int1, w_int2):\n    x = w_int1.intval\n    y = w_int2.intval\n    try:\n        z = x + y\n    except OverflowError:\n        raise FailedToImplement(space.w_OverflowError,\n                                space.wrap(\"integer addition\"))\n    return W_IntObject(space, z)\n(the current implementations look slightly but not fundamentally different.)\n\n\n\n\nEarly organizational ideas\nSome of the early organizational ideas of the project were as follows. Since the\nproject was started on a sprint and people really liked that style of working\nPyPy continued to be developed on various subsequent sprints.\nFrom early on there was a very heavy emphasis on testing. All the parts of the\ninterpreter that were implemented had a very careful set of unit tests to make\nsure that they worked correctly. From early on, there was a continuous\nintegration infrastructure, which grew over time (nowadays it is very natural\nfor people to have automated tests, and the concept of green/red builds: but\nembracing this workflow in the early 2000s was not really mainstream yet, and\nit is probably one of the reasons behind PyPy's success).\nAt the sprints there was also an emphasis on doing pair programming to make\nsure that everybody understood the codebase\nequally. There was also a heavy emphasis on writing good code and on regularly\ndoing refactorings to make sure that the codebase remained nice, clean and\nunderstandable. Those ideas followed from the early thoughts that PyPy would be\na sort of readable explanation of the language.\nThere was also a pretty fundamental design decision made at the time. That was\nthat the project should stay out of language design completely. Instead it would\nfollow CPython's lead and behave exactly like that implementation in all cases.\nThe project therefore committed to being almost quirk-to-quirk compatible and to\nimplement even the more obscure (and partially unnecessary) corner cases of\nCPython.\nAll of these principles continue pretty much still today (There are a few places\nwhere we had to deviate from being completely compatible, they are documented\nhere).\n\n\n\n\n2004-2007: EU-Funding\nWhile all this coding was going on it became clear pretty soon that the goals\nthat various participants had for the project would be very hard to achieve with\njust open source volunteers working on the project in their spare time.\nParticularly also the sprints became expensive given that those were just\nvolunteers doing this as a kind of weird hobby. Therefore a couple of people of\nthe project got together to apply for an EU grant in the framework programme 6\nto solve these money problems. In mid-2004 that application proved to be\nsuccessful.\u00a0And so the project got a grant of a 1.3 million Euro for\ntwo years to be able to employ some of the core developers and to make it\npossible for them work on the project full time. The EU grant went to seven\nsmall-to-medium companies and Uni D\u00fcsseldorf. The budget also contained money to\nfund sprints, both for the employed core devs as well as other open source\ncontributors.\n\nThe EU project started in December 2004 and that was a fairly heavy change in\npace for the project. Suddenly a lot of people were working full time on it, and\nthe pace and the pressure picked up quite a lot. Originally it had been a\nleisurely project people worked on for fun. But afterwards people discovered\nthat doing this kind of work full time becomes slightly less fun, particularly\nalso if you have to fulfill the ambitious technical goals that the EU proposal\ncontained. And the proposal indeed contained a bit everything to increase its\nchance of acceptance, such as aspect oriented programming, semantic web, logic\nprogramming, constraint programming, and so on. Unfortunately it\nturned out that those things then have to be implemented, which can be called\nthe first thing we learned: if you promise something to the EU, you'll have to\nactually go do it (After the funding ended, a lot of these features were\nactually removed from the project again, at a cleanup sprint).\n\n\n\n\n2005: Bootstrapping PyPy\nSo what were the actually useful things done as part of the EU project?\nOne of the most important goals that the EU project was meant to solve was the\nquestion of how to turn PyPy into an actually useful VM for Python. The\nbootstrapping plans were taken quite directly from Squeak, which is a Smalltalk\nVM written in a subset of Smalltalk called Slang, which can then be bootstrapped\nto C code. The plan for PyPy was to do something similar, to define a restricted\nsubset of Python called RPython, restricted in such a way that it should be\npossible to statically compile RPython programs to C code. Then the Python\ninterpreter should only use that subset, of course.\nThe main difference from the Squeak approach is that Slang, the subset of Squeak\nused there, is actually quite a low level language. In a way, you could almost\ndescribe it as C with Smalltalk syntax. RPython was really meant to be a\nmuch higher level language, much closer to Python, with full support for single\ninheritance classes, and most of Python's built-in data structures.\n\n\n(BTW, you don\u2019t have to understand any of the illustrations in this blog post,\nthey are taken from talks and project reports we did over the years so they are\nof archaeological interest only and I don\u2019t understand most of them myself.)\nFrom 2005 on, work on the RPython type inference engine and C backend started in\nearnest, which was sort of co-developed with the RPython language definition and\nthe PyPy Python interpreter. This is also roughly the time that I joined the\nproject as a volunteer.\nAnd at the second sprint I went to, in July 2005, two and a half years after the\nproject got started, we managed to bootstrap the PyPy interpreter to C for the\nfirst time. When we ran the compiled program, it of course immediately\nsegfaulted. The reason for that was that the C backend had turned characters\ninto signed chars in C, while the rest of the infrastructure assumed that they\nwere unsigned chars. After we fixed that, the second attempt worked and we\nmanaged to run an incredibly complex program, something like 6 * 7. That\nfirst bootstrapped version was really really slow, a couple of hundred times\nslower than CPython.\n\n\nThe bootstrapping process of RPython has a number of nice benefits, a big one\nbeing that a number of the properties of the generated virtual machine don't\nhave to expressed in the interpreter. The biggest example of this is garbage\ncollection. RPython is a garbage collected language, and the interpreter does\nnot have to care much about GC in most cases. When the C source code is\ngenerated, a GC is automatically inserted. This is a source of great\nflexibility. Over time we experimented with a number of different GC\napproaches, from reference counting to Boehm to our current incremental\ngenerational collector. As an aside, for a long time we were also working on\nother backends to the RPython language and hoped to be able to target Java and\n.NET as well. Eventually we abandoned this strand of work, however.\n\n\n\n\nRPython's Modularity Problems\nNow we come to the first thing I would say we learned in the project, which is\nthat the quality of tools we thought of as internal things still matters a lot.\nOne of the biggest technical mistakes we've made in the project was that we\ndesigned RPython without any kind of story for modularity. There is no concept\nof modules in the language or any other way to break up programs into smaller\ncomponents. We always thought that it would be ok for RPython to be a little bit\ncrappy. It was meant to be this sort of internal language with not too many\nexternal users. And of course that turned out to be completely wrong later.\nThat lack of modularity led to various problems that persist until today. The\nbiggest one is that there is no separate compilation for RPython programs at\nall! You always need to compile all the parts of your VM together, which leads\nto infamously bad compilation times.\nAlso by not considering the modularity question we were never forced to fix\nsome internal structuring issues of the RPython compiler itself.\nVarious layers of the compiler keep very badly defined and porous interfaces between\nthem. This was made possible by being able to work with all the program information in one heap,\nmaking the compiler less approachable and maintainable than it maybe could be.\nOf course this mistake just got more and more costly to fix over time,\nand so it means that so far nobody has actually done it.\nNot thinking more carefully about RPython's design, particularly its\nmodularity story, is in my opinion the biggest technical mistake the project\nmade.\n\n\n\n\n2006: The Meta-JIT\nAfter successfully bootstrapping the VM we did some fairly straightforward\noptimizations on the interpreter and the C backend and managed to reduce the\nslowdown versus CPython to something like 2-5 times slower. That's great! But of\ncourse not actually useful in practice. So where do we go from here?\nOne of the not so secret goals of Armin Rigo, one of the PyPy founders, was to\nuse PyPy together with some advanced partial evaluation magic sauce to\nsomehow automatically generate a JIT compiler from the interpreter. The goal was\nsomething like, \"you write your interpreter in RPython, add a few annotations\nand then we give you a JIT for free for the language that that interpreter\nimplements.\"\nWhere did the wish for that approach come from, why not just write a JIT for\nPython manually in the first place? Armin had actually done just that before he\nco-founded PyPy, in a project called Psyco. Psyco was an extension module for\nCPython that contained a method-based JIT compiler for Python code.\u00a0And Psyco\nproved to be an amazingly frustrating compiler to write. There were two main\nreasons for that. The first reason was that Python is actually quite a complex\nlanguage underneath its apparent simplicity. The second reason for the\nfrustration was that Python was and is very much an alive language, that gains\nnew features in the language core in every version. So every time a new Python\nversion came out, Armin had to do fundamental changes and rewrites to Psyco, and\nhe was getting pretty frustrated with it. So he hoped that that effort could be\ndiminished by not writing the JIT for PyPy by hand at all. Instead, the goal was\nto generate a method-based JIT from the interpreter automatically. By taking the\ninterpreter, and applying a kind of advanced transformation to it, that would\nturn it into a method-based JIT. And all that would still be translated into a\nC-based VM, of course.\n\nSlide from Psyco presentation at EuroPython 2002\n\n\n\n\nThe First JIT Generator\nFrom early 2006 on until the end of the EU project a lot of work went into\nwriting such a JIT generator. The idea was to base it on runtime partial\nevaluation. Partial evaluation is an old idea in computer science. It's supposed\nto be a way to automatically turn interpreters for a language into a compiler\nfor that same language. Since PyPy was trying to generate a JIT compiler, which\nis in any case necessary to get good performance for a dynamic language like\nPython, the partial evaluation was going to happen at runtime.\nThere are various ways to look at partial evaluation, but if you've never heard\nof it before, a simple way to view it is that it will compile a Python function\nby gluing\u00a0together the implementations of the bytecodes of that function and\noptimizing the result.\nThe main new ideas of PyPy's partial-evaluation based JIT generator as opposed\nto earlier partial-evaluation approaches are the ideas of \"promote\" and the idea\nof \"virtuals\". Both of these techniques had already been present (in a slightly\nless general form) in Psyco, and the goal was to keep using them in PyPy. Both\nof these techniques also still remain in use today in PyPy. I'm\ngoing on a slight technical diversion now, to give a high level explanation of\nwhat those ideas are for.\n\n\n\n\n\nPromote\nOne important ingredient of any JIT compiler is the ability to do runtime\nfeedback. Runtime feedback is most commonly used to know something about which\nconcrete types are used by a program in practice. Promote is basically a way to\neasily introduce runtime feedback into the JIT produced by the JIT generator.\nIt's an annotation the implementer of a language can use to express their wish\nthat specialization should happen at this point. This mechanism can be used to\nexpress all kinds of runtime feedback, moving values from the interpreter\ninto the compiler, whether they be types or other things.\n\n\n\n\nVirtuals\nVirtuals are a very aggressive form of partial escape analysis. A dynamic\nlanguage often puts a lot of pressure on the garbage collector, since most\nprimitive types (like integers, floats and strings) are boxed in the heap, and\nnew boxes are allocated all the time.\nWith the help of virtuals a very significant portion of all allocations in the\ngenerated machine code can be completely removed. Even if they can't be removed,\noften the allocation can be delayed or moved into an error path, or even\ninto a deoptimization path, and thus disappear from the generated machine code\ncompletely.\nThis optimization really is the super-power of PyPy's optimizer, since it\ndoesn't work only for primitive boxes but for any kind of object allocated on\nthe heap with a predictable lifetime.\nAs an aside, while this kind of partial escape analysis is sort of new for\nobject-oriented languages, it has actually existed in Prolog-based partial\nevaluation systems since the 80s, because it's just extremely natural there.\n\n\n\n\nJIT Status 2007\nSo, back to our history. We're now in 2007, at the end of the EU project (you\ncan find the EU-reports we wrote during the projects here). The EU project\nsuccessfully finished, we survived the final review with the EU. So, what's the\n2007 status of the JIT generator? It works kind of, it can be applied to PyPy. It\nproduces a VM with a JIT that will turn Python code into machine code at runtime\nand run it. However, that machine code is not particularly fast. Also, it tends\nto generate many megabytes of machine code even for small Python programs. While\nit's always faster than PyPy without JIT, it's only sometimes faster than\nCPython, and most of the time Psyco still beats it. On the one hand, this is\nstill an amazing achievement! It's arguably the biggest application of partial\nevaluation at this point in time! On the other hand, it was still quite\ndisappointing in practice, particularly since some of us had believed at the\ntime that it should have been possible to reach and then surpass the speed of\nPsyco with this approach.\n\n\n\n\n2007: RSqueak and other languages\nAfter the EU project ended we did all kinds of things. Like sleep for a month\nfor example, and have the cleanup sprint that I already mentioned. We also had a\nslightly unusual sprint in Bern, with members of the Software Composition\nGroup of Oscar Nierstrasz. As I wrote above, PyPy had been heavily influenced\nby Squeak Smalltalk, and that group is a heavy user of Squeak, so we wanted to\nsee how to collaborate with them. At the beginning of the sprint, we decided\ntogether that the goal of that week should be to try to write a Squeak virtual\nmachine in RPython, and at the end of the week we'd gotten surprisingly far with\nthat goal. Basically most of the bytecodes and the Smalltalk object system\nworked, we had written an image loader and could run some benchmarks (during the\nsprint we also regularly updated a blog, the success of which led us to start\nthe PyPy blog).\n\n\nThe development of the Squeak interpreter was very interesting for the project,\nbecause it was the first real step that moved RPython from being an\nimplementation detail of PyPy to be a more interesting project in its own right.\nBasically a language to write interpreters in, with the eventual promise to get\na JIT for that language almost for free. That Squeak implementation is now\ncalled RSqueak (\"Research Squeak\").\nI'll not go into more details about any of the other language implementations in\nRPython in this post, but over the years we've had a large variety of language\nof them done by various people and groups, most of them as research vehicles,\nbut also some as real language implementations. Some very cool research results\ncame out of these efforts, here's a slightly outdated list of some of them.\nThe use of RPython for other languages complicated the PyPy narrative a lot, and\nin a way we never managed to recover the simplicity of the original project\ndescription \"PyPy is Python in Python\". Because now it's something like \"we have\nthis somewhat strange language, a subset of Python, that's called RPython, and\nit's good to write interpreters in. And if you do that, we'll give you a JIT for\nalmost free. And also, we used that language to write a Python implementation,\ncalled PyPy.\". It just doesn't roll off the tongue as nicely.\n\n\n\n\n2008-2009: Four More JIT Generators\nBack to the JIT. After writing the first JIT generator as part of the EU\nproject, with somewhat mixed results, we actually wrote several more JIT\ngenerator prototypes with different architectures to try to solve some of the\nproblems of the first approach. To give an impression of these prototypes,\nhere\u2019s a list of them.\n\nThe second JIT generator we started working on in 2008 behaved exactly like\nthe first one, but had a meta-interpreter based architecture, to make it more\nflexible and easier to experiment with. The meta-interpreter was called\nthe \"rainbow interpreter\", and in general the JIT is an area where we went\nsomewhat overboard with borderline silly terminology, with notable\noccurrences of \"timeshifter\", \"blackhole interpreter\" etc.\nThe third JIT generator was an experiment based on the second one which\nchanged\ncompilation strategy. While the previous two had compiled many control flow\npaths of the currently compiled function eagerly, that third JIT was sort of\nmaximally lazy and stopped compilation at every control flow split to avoid\nguessing which path would actually be useful later when executing the code.\nThis was an attempt to reduce the problem of the first JIT generating way too\nmuch machine code. Only later, when execution went down one of the not yet\ncompiled paths would it continue compiling more code. This gives an effect\nsimilar to that of lazy basic block versioning.\nThe fourth JIT generator was a pretty strange prototype, a runtime partial\nevaluator for Prolog, to experiment with various specialization trade-offs. It\nhad an approach that we gave a not at all humble name, called \"perfect\nspecialization\".\nThe fifth JIT generator is the one that we are still using today. Instead of\ngenerating a method-based JIT compiler from our interpreter we switched to\ngenerating a tracing JIT compiler. Tracing JIT compilers were sort of the\nlatest fashion at the time, at least for a little while.\n\n\n\n\n\n2009: Meta-Tracing\nSo, how did that tracing JIT generator work? A tracing JIT generates code by\nobserving and logging the execution of the running program. This yields a\nstraight-line trace of operations, which are then optimized and compiled into\nmachine code.\u00a0Of course most tracing systems mostly focus on tracing loops.\nAs we discovered, it's actually quite simple to apply a tracing JIT to a generic\ninterpreter, by not tracing the execution of the user program directly, but by\ninstead tracing the execution of the interpreter while it is running the user\nprogram (here's the paper we wrote about this approach).\nSo that's what we implemented. Of course we kept the two successful parts of the\nfirst JIT, promote and virtuals (both links go to the papers about these\nfeatures in the meta-tracing context).\n\n\n\n\n\nWhy did we Abandon Partial Evaluation?\nSo one question I get sometimes asked when telling this story is, why did\nwe think that tracing would work better than partial evaluation (PE)? One of the\nhardest parts of compilers in general and partial evaluation based systems in\nparticular is the decision when and how much to inline, how much to specialize,\nas well as the decision when to split control flow paths. In the PE based JIT\ngenerator we never managed to control that question. Either the JIT would\ninline too much, leading to useless compilation of all kinds of unlikely error\ncases. Or it wouldn't inline enough, preventing necessary optimizations.\nMeta tracing solves this problem with a hammer, it doesn't make particularly\ncomplex inlining decisions at all. It instead decides what to inline by\nprecisely following what a real execution through the program is doing. Its\ninlining decisions are therefore very understandable and predictable, and it\nbasically only has one heuristic based on whether the called function contains a\nloop or not: If the called function contains a loop, we'll never inline it, if\nit doesn't we always try to inline it. That predictability is actually what was\nthe most helpful, since it makes it possible for interpreter authors to\nunderstand why the JIT did what it did and to actually influence its inlining\ndecisions by changing the annotations in the interpreter source. It turns out\nthat simple is better than complex.\n\n\n\n\n2009-2011: The PyJIT Eurostars Project\nWhile we were writing all these JIT prototypes, PyPy had sort of reverted back\nto being a volunteer-driven open source project (although some of us, like\nAntonio Cuni and I, had started working for universities and other project\nmembers had other sources of funding). But again, while we did the work it\nbecame clear that to get an actually working fast PyPy with generated JIT we\nwould need actual funding again for the project. So we applied to the EU again,\nthis time for a much smaller project with less money, in the Eurostars\nframework. We got a grant for three participants, merlinux, OpenEnd and Uni\nD\u00fcsseldorf, on the order of a bit more than half a million euro. That money was\nspecifically for JIT development and JIT testing infrastructure.\n\n\n\n\n\nTracing JIT improvements\nWhen writing the grant we had sat together at a sprint and discussed extensively\nand decided that we would not switch JIT generation approaches any more. We all\nliked the tracing approach well enough and thought it was promising. So instead\nwe agreed to try in earnest to make the tracing JIT really practical. So in the\nEurostars project we started with implementing sort of fairly standard JIT\ncompiler optimizations for the meta-tracing JIT, such as:\n\nconstant folding\ndead code elimination\nloop invariant code motion (using LuaJIT's approach)\nbetter heap optimizations\nfaster deoptimization (which is actually a bit of a mess in the\nmeta-approach)\nand dealing more efficiently with Python frames objects and the\nfeatures of Python's debugging facilities\n\n\n\n\n\n2010: speed.pypy.org\nIn 2010, to make sure that we wouldn't accidentally introduce speed regressions\nwhile working on the JIT, we implemented infrastructure to build PyPy and run\nour benchmarks nightly. Then, the https://speed.pypy.org website was implemented\nby Miquel Torres, a volunteer. The website shows the changes in benchmark\nperformance compared to the previous n days. It didn't sound too important at\nfirst, but this was (and is) a fantastic tool, and an amazing motivator over the\nnext years, to keep continually improving performance.\n\n\n\n\n\nContinuous Integration\nThis actually leads me to something else that I'd say we learned, which is that\ncontinuous integration is really awesome, and completely transformative to have\nfor a project. This is not a particularly surprising insight nowadays in the\nopen source community, it's easy to set up continuous integration on Github\nusing Travis or some other CI service. But I still see a lot of research\nprojects that don't have tests, that don't use CI, so I wanted to mention it\nanyway. As I mentioned earlier in the post, PyPy has a quite serious testing\nculture, with unit tests written for new code, regression tests for all bugs,\nand integration tests using the CPython test suite. Those tests are run\nnightly on a number of architectures and operating systems.\nHaving all this kind of careful testing is of course necessary, since PyPy is\nreally trying to be a Python implementation that people actually use, not just\nwrite papers about. But having all this infrastructure also had other benefits,\nfor example it allows us to trust newcomers to the project very quickly.\nBasically after your first patch gets accepted, you immediately get commit\nrights to the PyPy repository. If you screw up, the tests (or the code reviews)\nare probably going to catch it, and that reduction to the barrier to\ncontributing is just super great.\nThis concludes my advertisement for testing in this post.\n\n\n\n\n2010: Implementing Python Objects with Maps\nSo, what else did we do in the Eurostars project, apart from adding traditional\ncompiler optimizations to the tracing JIT and setting up CI infrastructure?\nAnother strand of work, that went on sort of concurrently to the JIT generator\nimprovements, were deep rewrites in the Python runtime, and the Python data\nstructures. I am going to write about two exemplary ones here, maps and storage strategies.\nThe first such rewrite is fairly standard. Python instances are similar to\nJavascript objects, in that you can add arbitrary attributes to them at runtime.\nOriginally Python instances were backed by a dictionary in PyPy, but of course\nin practice most instances of the same class have the same set of attribute\nnames. Therefore we went and implemented Self style maps, which are often\ncalled hidden classes in the JS world to represent instances instead. This\nhas two big benefits, it allows you to generate much better machine code for\ninstance attribute access and makes instances use a lot less memory.\n\n\n\n\n\n2011: Container Storage Strategies\nAnother important change in the PyPy runtime was rewriting the Python container\ndata structures, such as lists, dictionaries and sets. A fairly straightforward\nobservation about how those are used is that in a significant percentage of\ncases they contain type-homogeneous data. As an example it's quite common to\nhave lists of only integers, or lists of only strings. So we changed the list,\ndict and set implementations to use something we called storage strategies. With\nstorage strategies these data structures use a more efficient representations if\nthey contain only primitives of the same type, such as ints, floats, strings.\nThis makes it possible to store the values without boxing them in the underlying\ndata structure. Therefore read and write access are much faster for such type\nhomogeneous containers. Of course when later another data type gets added to\nsuch a list, the existing elements need to all be boxed at that point, which is\nexpensive. But we did a study and found out that that happens quite rarely in\npractice. A lot of that work was done by Lukas Diekmann.\n\n\n\n\n\nDeep Changes in the Runtime are Necessary\nThese two are just two examples for a number of fairly fundamental changes in\nthe PyPy runtime and PyPy data structures, probably the two most important ones,\nbut we did many others. That leads me to another thing we learned. If you want\nto generate good code for a complex dynamic language such as Python, it's\nactually not enough at all to have a good code generator and good compiler\noptimizations. That's not going to help you, if your runtime data-structures\naren't in a shape where it's possible to generate efficient machine code to\naccess them.\nMaybe this is well known in the VM and research community. However it's the main\nmistake that in my opinion every other Python JIT effort has made in the last 10\nyears, where most projects said something along the lines of \"we're not\nchanging the existing CPython data structures at all, we'll just let LLVM\ninline enough C code of the runtime and then it will optimize all the overhead\naway\". That never works very well.\n\n\n\n\nJIT Status 2011\nSo, here we are at the end of the Eurostars project, what's the status of the JIT? Well, it\nseems this meta-tracing stuff really works! We finally started actually\nbelieving in it, when we reached the point in 2010 where self-hosting PyPy was\nactually faster than bootstrapping the VM on CPython. Speeding up the\nbootstrapping process is something that Psyco never managed at all, so we\nconsidered this a quite important achievement. At the end of\nEurostars, we were about 4x faster than CPython on our set of benchmarks.\n\n\n\n\n2012-2017: Engineering and Incremental Progress\n2012 the Eurostars project was finished and PyPy reverted yet another time back\nto be an open source project. From then on, we've had a more diverse set of\nsources of funding: we received some crowd funding via the Software Freedom\nConservancy and contracts of various sizes from companies to implement various\nspecific features, often handled by Baroque Software. Over the next couple of\nyears\nwe revamped various parts of the VM. We improved the GC in major ways. We\noptimized the implementation of the JIT compiler to improve warmup times. We\nimplemented backends for various CPU architectures (including PowerPC and\ns390x). We tried to reduce the number of performance cliffs and make the JIT\nuseful in a broader set of cases.\nAnother strand of work was to push quite significantly to be more\ncompatible with CPython, particularly the Python 3 line as well as extension\nmodule support. Other compatibility improvements we did was making sure that\nvirtualenv works with PyPy, better support for distutils and setuptools and\nsimilar improvements. The continually improving performance as well better\ncompatibility with the ecosystem tools led to the first few users of PyPy in\nindustry.\n\n\n\n\nCPyExt\nAnother very important strand of work that took a lot of effort in recent years\nwas CPyExt. One of the main blockers of PyPy adoption had always been the fact\nthat a lot of people need specific C-extension modules at least in some parts of\ntheir program, and telling them to reimplement everything in Python is just not\na practical solution. Therefore we worked on CPyExt, an emulation layer  to make\nit possible to run CPython C-extension modules in PyPy. Doing that was a very\npainful process, since the CPython extension API leaks a lot of CPython\nimplementation details, so we had to painstakingly emulate all of these details\nto make it possible to run extensions. That this works at all remains completely\namazing to me! But nowadays CPyExt is even getting quite good, a lot of the big\nnumerical libraries such as Numpy and Pandas are now supported (for a while\nwe had worked hard on a reimplementation of Numpy called NumPyPy, but\neventually realized that it would never be complete and useful enough).\nHowever, calling CPyExt modules from PyPy can still be very slow,\nwhich makes it impractical for some applications\nthat's why we are working on it.\nNot thinking about C-extension module emulation earlier in the project history\nwas a pretty bad strategic mistake. It had been clear for a long time that\ngetting people to just stop using all their C-extension modules was never going\nto work, despite our efforts to give them alternatives, such as cffi. So we\nshould have thought of a story for all the existing C-extension modules earlier\nin the project. Not starting CPyExt earlier was mostly a failure of our\nimagination (and maybe a too high pain threshold): We didn't believe this kind\nof emulation was going to be practical, until somebody went and tried it.\n\n\n\n\nPython 3\nAnother main\nfocus of the last couple of years has been to catch up with the CPython 3 line.\nOriginally we had ignored Python 3 for a little bit too long, and were trailing\nseveral versions behind. In 2016 and 2017 we had a grant from the Mozilla open\nsource support program of $200'000 to be able to catch up with Python 3.5. This\nwork is now basically done, and we are starting to target CPython 3.6 and will\nhave to look into 3.7 in the near future.\n\n\n\n\nIncentives of OSS compared to Academia\nSo, what can be learned from those more recent years? One thing we can observe\nis that a lot of the engineering work we did in that time is not really science\nas such. A lot of the VM techniques we implemented are kind of well known, and\ncatching up with new Python features is also not particularly deep researchy\nwork. Of course this kind of work is obviously super necessary if you want\npeople to use your VM, but it would be very hard to try to get research funding\nfor it. PyPy managed quite well over its history to balance phases of more\nresearch oriented work, and more product oriented ones. But getting this balance\nsomewhat right is not easy, and definitely also involves a lot of luck. And, as\nhas been discussed a lot, it's actually very hard to find funding for open\nsource work, both within and outside of academia.\n\n\nMeta-Tracing really works!\nLet me end with what, in my opinion, is the main positive technical result of PyPy the\nproject. Which is that the whole idea of using a meta-tracing JIT can really\nwork! Currently PyPy is about 7 times faster than CPython on a broad set of\nbenchmarks. Also, one of the very early motivations for using a meta-jitting\napproach in PyPy, which was to not have to adapt the JIT to new versions of\nCPython proved to work: indeed we didn't have to change anything in the JIT\ninfrastructure to support Python 3.\nRPython has also worked and improved performance for a number of other\nlanguages. Some of these interpreters had wildly different architectures.\nAST-based interpreters, bytecode based, CPU emulators, really inefficient\nhigh-level ones that allocate continuation objects all the time, and so on. This\nshows that RPython also gives you a lot of freedom in deciding how you want to\nstructure the interpreter and that it can be applied to languages of quite\ndifferent paradigms.\nI'll end with a list of the people that have contributed code to PyPy over its\nhistory, more than 350 of them. I'd like to thank all of them and the various\nroles they played. To the next 15 years!\n\n\n\n\n\n\nAcknowledgements\nA lot of people helped me with this blog post. Tim Felgentreff made me give the\nkeynote, which lead me to start collecting the material. Samuele Pedroni\ngave essential early input when I just started planning the talk, and also gave\nfeedback on the blog post. Maciej Fija\u0142kowski gave me feedback on the post, in\nparticular important insight about the more recent years of the project. Armin\nRigo discussed the talk slides with me, and provided details about the early\nexpectations about the first JIT's hoped-for performance. Antonio Cuni gave\nsubstantial feedback and many very helpful suggestions for the blog post.\nMichael Hudson-Doyle also fixed a number of mistakes in the post and rightfully\ncomplained about the lack of mention of the GC. Christian Tismer provided\naccess to his copy of early Python-de mailing list posts. Matti Picus pointed\nout a number of things I had forgotten and fixed a huge number of typos and\nawkward English, including my absolute inability to put commas correctly.\nAll remaining errors are of course my own.\n\n\nupdate: fixed confusing wording in the maps section.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/09/the-first-15-years-of-pypy-3412615975376972020.html"
    },
    {
      "title": "Repeating a Matrix Multiplication Benchmark",
      "text": "I watched the Hennessy & Patterson's Turing award lecture recently:\n\n\n\nIn it, there's a slide comparing the performance of various matrix\nmultiplication implementations, using Python (presumably CPython) as a baseline\nand comparing that against various C implementations (I couldn't find the\nlinked paper yet):\n\n\n\nI expected the baseline speedup of switching from CPython to C to be\nhigher and I also wanted to know what performance PyPy gets, so I did my own\nbenchmarks. This is a problem that Python is completely unsuited for, so it\nshould give very exaggerated results.\nThe usual disclaimers apply: All benchmarks are lies, benchmarking of\nsynthetic workloads even more so. My implementation is really naive (though I\ndid optimize it a little bit to help CPython), don't use any\nof this code\nfor anything real. The benchmarks ran on my rather old Intel i5-3230M laptop\nunder Ubuntu 17.10.\nWith that said, my results were as follows:\n\n\n\nImplementation\ntime\nspeedup over CPython\nspeedup over PyPy\n\n\n\n\nCPython\n512.588 \u00b1 2.362 s\n1 \u00d7\n\n\n\nPyPy\n8.167 \u00b1 0.007 s\n62.761 \u00b1  0.295 \u00d7\n1 \u00d7\n\n\n'naive' C\n2.164 \u00b1 0.025 s\n236.817 \u00b1  2.918 \u00d7\n3.773 \u00b1 0.044 \u00d7\n\n\nNumPy\n0.171 \u00b1 0.002 s\n2992.286 \u00b1 42.308 \u00d7\n47.678 \u00b1 0.634 \u00d7\n\nThis is running 1500x1500 matrix multiplications with (the same) random matrices. Every\nimplementation is run 50 times in a fresh process. The results are averaged,\nthe errors are bootstrapped 99% confidence intervals.\nSo indeed the speedup that I got of switching from CPython to C is quite a bit higher than\n47x! PyPy is much better than CPython, but of course can't really compete\nagainst GCC. And then the real professionals (numpy/OpenBLAS) are in a whole\n'nother league. The speedup of the AVX numbers in the slide above is even\nhigher than my NumPy numbers, which I assume is the result of my old CPU with\ntwo cores, vs. the 18 core CPU with AVX support.\nLesson confirmed: leave matrix multiplication to people who\nactually know what they are doing.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/06/repeating-matrix-multiplication-8641748742577945875.html"
    },
    {
      "title": "How to ignore the annoying Cython warnings in PyPy 6.0",
      "text": "If you install any Cython-based module in PyPy 6.0.0, it is very likely that you get a warning like this:\n>>>> import numpy\n/data/extra/pypy/6.0.0/site-packages/numpy/random/__init__.py:99: UserWarning: __builtin__.type size changed, may indicate binary incompatibility. Expected 888, got 408\n  from .mtrand import *\n\n\nThe TL;DR version is: the warning is a false alarm, and you can hide it by doing:\n$ pypy -m pip install pypy-fix-cython-warning\n\n\nThe package does not contain any module, only a\u00a0.pth\u00a0file which installs a warning filter at startup.\n\nTechnical details\n\nThis happens because whenever Cython compiles a pyx file, it generates C code which does a sanity check on the C size of\u00a0PyType_Type. PyPy versions up to 5.10 are buggy and report the incorrect size, so Cython includes a workaround to compare it with the incorrect value, when on PyPy.\n\nPyPy 6 fixed the bug and now\u00a0PyType_Type\u00a0reports the correct size; however, Cython still tries to compare it with the old, buggy value, so it (wrongly) emits the warning.\n\nCython 0.28.2 includes a fix for it, so that C files generated by it no longer emit the warning. However, most packages are distributed with pre-cythonized C files. For example,\u00a0numpy-1.14.2.zip\u00a0include C files which were generated by Cython 0.26.1: if you compile it you still get the warning, even if you locally installed a newer version of Cython.\n\nThere is not much that we can do on the PyPy side, apart for waiting for all the Cython-based packages to do a new release which include C files generated by a newer Cython.\u00a0 In the mean time, installing this module will silence the\u00a0warning.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/04/how-to-ignore-annoying-cython-warnings-1007636731207810779.html"
    },
    {
      "title": "PyPy2.7 and PyPy3.5 v6.0 dual release",
      "text": "The PyPy team is proud to release both PyPy2.7 v6.0 (an interpreter supporting\nPython 2.7 syntax), and a PyPy3.5 v6.0 (an interpreter supporting Python\n3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.\nThis release is a feature release following our previous 5.10 incremental\nrelease in late December 2017. Our C-API compatibility layer cpyext is\nnow much faster (see the blog post) as well as more complete. We have made\nmany other improvements in speed and CPython compatibility. Since the changes\naffect the included python development header files, all c-extension modules must\nbe recompiled for this version.\nUntil we can work with downstream providers to distribute builds with PyPy, we\nhave made packages for some common packages available as wheels. You may\ncompile yourself using pip install --no-build-isolation <package>, the\nno-build-isolation is currently needed for pip v10.\nFirst-time python users are often stumped by silly typos and omissions when\ngetting started writing code. We have improved our parser to emit more friendly\nsyntax errors,  making PyPy not only faster but more friendly.\nThe GC now has hooks to gain more insights into its performance\nThe default Matplotlib TkAgg backend now works with PyPy, as do pygame and pygobject.\nWe updated the cffi module included in PyPy to version 1.11.5, and the\ncppyy backend to 0.6.0. Please use these to wrap your C and C++ code,\nrespectively, for a JIT friendly experience.\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating.\nThe Windows PyPy3.5 release is still considered beta-quality. There are open\nissues with unicode handling especially around system calls and c-extensions.\nThe utf8 branch that changes internal representation of unicode to utf8 did not\nmake it into the release, so there is still more goodness coming. We also\nbegan working on a Python3.6 implementation, help is welcome.\nYou can download the v6.0 releases here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThe PyPy release supports:\n\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\n\n\nWhat else is new?\n\nPyPy 5.10 was released in Dec, 2017.\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2018/04/pypy27-and-pypy35-v60-dual-release-7416552143474607997.html"
    },
    {
      "title": "Improving SyntaxError in PyPy",
      "text": "For the last year, my halftime job has been to teach non-CS uni students\nto program in Python. While doing that, I have been trying to see what common\nstumbling blocks exist for novice programmers. There are many\nthings that could be said here, but a common theme that emerges is\nhard-to-understand error messages. One source of such error messages,\nparticularly when starting out, is SyntaxErrors.\nPyPy's parser (mostly following the architecture of CPython) uses a\nregular-expression-based tokenizer with some cleverness to deal with\nindentation, and a simple LR(1) parser. Both of these components obviously\nproduce errors for invalid syntax, but the messages are not very helpful. Often,\nthe message is just \"invalid syntax\", without any hint of what exactly is wrong.\nIn the last couple of weeks I have invested a little bit of effort to make them a\ntiny bit better. They will be part of the upcoming PyPy 6.0 release. Here are\nsome examples of what changed.\n\nMissing Characters\nThe first class of errors occurs when a token is missing, often there is only one\nvalid token that the parser expects. This happens most commonly by leaving out\nthe ':' after control flow statements (which is the syntax error I personally\nstill make at least a few times a day). In such situations, the parser will now\ntell you which character it expected:\n\n>>>> # before\n>>>> if 1\n  File \"<stdin>\", line 1\n    if 1\n       \nSyntaxError: invalid syntax\n>>>>\n\n>>>> # after\n>>>> if 1\n  File \"<stdin>\", line 1\n    if 1\n       \nSyntaxError: invalid syntax (expected ':')\n>>>>\n\nAnother example of this feature:\n\n>>>> # before\n>>>> def f:\n  File \"<stdin>\", line 1\n    def f:\n        \nSyntaxError: invalid syntax\n>>>>\n\n>>>> # after\n>>>> def f:\n  File \"<stdin>\", line 1\n    def f:\n         \nSyntaxError: invalid syntax (expected '(')\n>>>>\n\n\n\nParentheses\nAnother source of errors are unmatched parentheses. Here, PyPy has always had\nslightly better error messages than CPython:\n\n>>> # CPython\n>>> )\n  File \"<stdin>\", line 1\n    )\n    \nSyntaxError: invalid syntax\n>>>\n\n>>>> # PyPy\n>>> )\n  File \"<stdin>\", line 1\n    )\n    \nSyntaxError: unmatched ')'\n>>>>\n\nThe same is true for parentheses that are never closed (the call to eval is\nneeded to get the error, otherwise the repl will just wait for more input):\n\n>>> # CPython\n>>> eval('(')\n  File \"<string>\", line 1\n    (\n    \nSyntaxError: unexpected EOF while parsing\n>>>\n\n>>>> # PyPy\n>>>> eval('(')\n  File \"<string>\", line 1\n    (\n    \nSyntaxError: parenthesis is never closed\n>>>>\n\nWhat I have now improved is the case of parentheses that are matched wrongly:\n\n>>>> # before\n>>>> (1,\n.... 2,\n.... ]\n  File \"<stdin>\", line 3\n    ]\n    \nSyntaxError: invalid syntax\n>>>>\n\n>>>> # after\n>>>> (1,\n.... 2,\n.... ]\n  File \"<stdin>\", line 3\n    ]\n    \nSyntaxError: closing parenthesis ']' does not match opening parenthesis '(' on line 1\n>>>>\n\n\n\nConclusion\nObviously these are just some very simple cases, and there is still a lot of\nroom for improvement (one huge problem is that only a single SyntaxError is\never shown per parse attempt, but fixing that is rather hard).\nIf you have a favorite unhelpful SyntaxError message you love to hate, please\ntell us in the comments and we might try to improve it. Other kinds of\nnon-informative error messages are also always welcome!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/04/improving-syntaxerror-in-pypy-5733639208090522433.html"
    },
    {
      "title": "Leysin Winter Sprint 2018: review",
      "text": "Like every year, the PyPy developers and a couple of newcomers\n      gathered in Leysin, Switzerland, to share their thoughts and\n      contribute to the development of PyPy.\n    As always, we had interesting discussions about how we could\n      improve PyPy, to make it the first choice for even more\n      developers. We also made some progress with current issues, like\n      compatibility with Python 3.6 and improving the performance of\n      CPython extension modules, where we fixed a lot of bugs and gained\n      new insights about where and how we could tweak PyPy.\n    \n     We were very happy about the number of new people who joined us\n      for the first time, and hope they enjoyed it as much as everyone\n      else. \n    \n    Topics\n    We worked on the following topics (and more!):\n    \n      Introductions for newcomers\n      Python 3.5 and 3.6 improvements\n      CPyExt performance improvements and GC implementation\n      \n      JIT: guard-compatible implementation\n      \n      Pygame performance improvements\n      Unicode/UTF8 implementation\n      \n      CFFI tutorial/overview rewrite\n      \n      py3 test runners refactoring\n      RevDB improvements\n      \n    \n    The weather was really fine for most of the week, with only\n    occasional snow and fog. We started our days with a short (and\n    sometimes not so short) planning session and enjoyed our dinners in\n    the great restaurants in the area. Some of us even started earlier\n    and continued till late night. It was a relaxed, but also very\n    productive atmosphere. On our break day on Wednesday, we enjoyed the\n    great conditions and went skiing and hiking.\n    Attendees\n    \n      Arianna\n      Jean-Daniel\n      \n      Stefan Beyer\n      Floris Bruynooghe\n      \n      Antonio Cuni\n      Ren\u00e9 Dudfield\n      Manuel Jacob\n      Ronan Lamy\n      Remi Meier\n      Matti Picus\n      \n      Armin Rigo\n      Alexander Schremmer\n      \n    \n    Leysin is easily reachable by Geneva Airport, so feel free to join\n    us next time!\n    \n    \n    Cheers,\n      Stefan",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/03/leysin-winter-sprint-2018-review-3988364248531980164.html"
    },
    {
      "title": "PyPy 5.10.1 bugfix release for python 3.5",
      "text": "We have released a bug fix PyPy3.5-v5.10.1\ndue to the following issues:\n\n\n\nFix time.sleep(float('nan')) which would hang on Windows\nFix missing errno constants on Windows\nFix issue 2718 for the REPL on Linux\nFix an overflow in converting int secs to nanosecs (issue 2717 )\nUsing kwarg 'flag' to os.setxattr had no effect\nFix the winreg module for unicode entries in the registry on Windows\n\n\n\nNote that many of these fixes are for our new beta version of PyPy3.5 on Windows. There may be more unicode problems in the Windows beta version,\nespecially concerning directory- and file-names with non-ASCII\ncharacters.\n\nOn macOS, we recommend you wait for the\nHomebrew package to prevent issues with third-party packages. For other supported platforms our downloads are available now.\nThanks to those who reported the issues.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\nThis PyPy 3.5 release supports:\n\n\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, macOS 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2018/01/pypy-5101-bugfix-release-for-python-35-8485250762789380657.html"
    },
    {
      "title": "Leysin Winter sprint: 17-24 March 2018",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the thirteenth\ntime.  This is a fully public sprint: newcomers and topics other than\nthose proposed below are welcome.\n\n(Note: this sprint is independent from the suggested April-May sprint in\nPoland.)\n\nGoals and topics of the sprint\n\nThe list of topics is open, but here is our current list:\n\n\n\n\n\n\n\n\n\n cffi tutorial/overview rewrite\n py3 test runners are too complicated\n make win32 builds green\n make packaging more like cpython/portable builds\n get CI builders for PyPy into mainstream projects (Numpy, Scipy, lxml, uwsgi)\n get more of scientific stack working (tensorflow?)\n cpyext performance improvements\n General 3.5 and 3.6 improvements\n JIT topics: guard-compatible, and the subsequent research project to save and reuse traces across processes\n finish unicode-utf8\n update www.pypy.org, speed.pypy.org (web devs needed)\n\n\nAs usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off (for ski or anything else).\n\nExact times\n\nWork days: starting March 18th (~noon), ending March 24th (~noon).\n\nPlease see announcement.txt for more information.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/01/leysin-winter-sprint-17-24-march-2018-7141092581585849418.html"
    },
    {
      "title": "PyPy2.7 and PyPy3.5 v5.10 dual release",
      "text": "The PyPy team is proud to release both PyPy2.7 v5.10 (an interpreter supporting\nPython 2.7 syntax), and a final PyPy3.5 v5.10 (an interpreter for Python\n3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.\nThis release is an incremental release with very few new features, the main\nfeature being the final PyPy3.5 release that works on linux and OS X with beta\nwindows support. It also includes fixes for vmprof cooperation with greenlets.\nCompared to 5.9, the 5.10 release contains mostly bugfixes and small improvements.\nWe have in the pipeline big new features coming for PyPy 6.0 that did not make\nthe release cut and should be available within the next couple months.\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nAs always, we strongly recommend updating.\nThere are quite a few important changes that are in the pipeline that did not\nmake it into the 5.10 release. Most important are speed improvements to cpyext\n(which will make numpy and pandas a bit faster) and utf8 branch that changes\ninternal representation of unicode to utf8, which should help especially the\nPython 3.5 version of PyPy.\nThis release concludes the Mozilla Open Source grant for having a compatible\nPyPy 3.5 release and we're very grateful for that.  Of course, we will continue\nto improve PyPy 3.5 and probably move to 3.6 during the course of 2018.\nYou can download the v5.10 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject.\nWe would also like to thank our contributors and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on pypy, or general help\nwith making RPython's JIT even better.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7 and CPython 3.5. It's fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThe PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\nChangelog\n\nimprove ssl handling on windows for pypy3 (makes pip work)\nimprove unicode handling in various error reporters\nfix vmprof cooperation with greenlets\nfix some things in cpyext\ntest and document the cmp(nan, nan) == 0 behaviour\ndon't crash when calling sleep with inf or nan\nfix bugs in _io module\ninspect.isbuiltin() now returns True for functions implemented in C\nallow the sequences future-import, docstring, future-import for CPython bug compatibility\nIssue #2699: non-ascii messages in warnings\nposix.lockf\nfixes for FreeBSD platform\nadd .debug files, so builds contain debugging info, instead of being stripped\nimprovements to cppyy\nissue #2677 copy pure c PyBuffer_{From,To}Contiguous from cpython\nissue #2682, split firstword on any whitespace in sqlite3\nctypes: allow ptr[0] = foo when ptr is a pointer to struct\nmatplotlib will work with tkagg backend once matplotlib pr #9356 is merged\nimprovements to utf32 surrogate handling\ncffi version bump to 1.11.2\n\nMaciej Fijalkowski, Matti Picus and the whole PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/12/pypy27-and-pypy35-v510-dual-release-3223396318213306071.html"
    },
    {
      "title": "How to make your code 80 times faster",
      "text": "I often hear people who are happy because PyPy makes their code 2 times faster\nor so. Here is a short personal story which shows PyPy can go well beyond\nthat.\n\nDISCLAIMER: this is not a silver bullet or a general recipe: it worked in\nthis particular case, it might not work so well in other cases. But I think it\nis still an interesting technique. Moreover, the various steps and\nimplementations are showed in the same order as I tried them during the\ndevelopment, so it is a real-life example of how to proceed when optimizing\nfor PyPy.\n\nSome months ago I played a bit with evolutionary algorithms: the ambitious\nplan was to automatically evolve a logic which could control a (simulated)\nquadcopter, i.e. a PID controller (spoiler: it doesn't fly).\n\nThe idea is to have an initial population of random creatures: at each\ngeneration, the ones with the best fitness survive and reproduce with small,\nrandom variations.\n\nHowever, for the scope of this post, the actual task at hand is not so\nimportant, so let's jump straight to the code. To drive the quadcopter, a\nCreature has a run_step method which runs at each delta_t (full\ncode):\nclass Creature(object):\n    INPUTS = 2  # z_setpoint, current z position\n    OUTPUTS = 1 # PWM for all 4 motors\n    STATE_VARS = 1\n    ...\n\n    def run_step(self, inputs):\n        # state: [state_vars ... inputs]\n        # out_values: [state_vars, ... outputs]\n        self.state[self.STATE_VARS:] = inputs\n        out_values = np.dot(self.matrix, self.state) + self.constant\n        self.state[:self.STATE_VARS] = out_values[:self.STATE_VARS]\n        outputs = out_values[self.STATE_VARS:]\n        return outputs\n\n\ninputs is a numpy array containing the desired setpoint and the current\nposition on the Z axis;\noutputs is a numpy array containing the thrust to give to the motors. To\nstart easy, all the 4 motors are constrained to have the same thrust, so\nthat the quadcopter only travels up and down the Z axis;\nself.state contains arbitrary values of unknown size which are passed from\none step to the next;\nself.matrix and self.constant contains the actual logic. By putting\nthe \"right\" values there, in theory we could get a perfectly tuned PID\ncontroller. These are randomly mutated between generations.\n\nrun_step is called at 100Hz (in the virtual time frame of the simulation). At each\ngeneration, we test 500 creatures for a total of 12 virtual seconds each. So,\nwe have a total of 600,000 executions of run_step at each generation.\n\nAt first, I simply tried to run this code on CPython; here is the result:\n$ python -m ev.main\nGeneration   1: ... [population = 500]  [12.06 secs]\nGeneration   2: ... [population = 500]  [6.13 secs]\nGeneration   3: ... [population = 500]  [6.11 secs]\nGeneration   4: ... [population = 500]  [6.09 secs]\nGeneration   5: ... [population = 500]  [6.18 secs]\nGeneration   6: ... [population = 500]  [6.26 secs]\n\nWhich means ~6.15 seconds/generation, excluding the first.\n\nThen I tried with PyPy 5.9:\n$ pypy -m ev.main\nGeneration   1: ... [population = 500]  [63.90 secs]\nGeneration   2: ... [population = 500]  [33.92 secs]\nGeneration   3: ... [population = 500]  [34.21 secs]\nGeneration   4: ... [population = 500]  [33.75 secs]\n\nOuch! We are ~5.5x slower than CPython. This was kind of expected: numpy is\nbased on cpyext, which is infamously slow.  (Actually, we are working on\nthat and on the cpyext-avoid-roundtrip branch we are already faster than\nCPython, but this will be the subject of another blog post.)\n\nSo, let's try to avoid cpyext. The first obvious step is to use numpypy\ninstead of numpy (actually, there is a hack to use just the micronumpy\npart). Let's see if the speed improves:\n$ pypy -m ev.main   # using numpypy\nGeneration   1: ... [population = 500]  [5.60 secs]\nGeneration   2: ... [population = 500]  [2.90 secs]\nGeneration   3: ... [population = 500]  [2.78 secs]\nGeneration   4: ... [population = 500]  [2.69 secs]\nGeneration   5: ... [population = 500]  [2.72 secs]\nGeneration   6: ... [population = 500]  [2.73 secs]\n\nSo, ~2.7 seconds on average: this is 12x faster than PyPy+numpy, and more than\n2x faster than the original CPython. At this point, most people would be happy\nand go tweeting how PyPy is great.\n\nIn general, when talking of CPython vs PyPy, I am rarely satified of a 2x\nspeedup: I know that PyPy can do much better than this, especially if you\nwrite code which is specifically optimized for the JIT. For a real-life\nexample, have a look at capnpy benchmarks, in which the PyPy version is\n~15x faster than the heavily optimized CPython+Cython version (both have been\nwritten by me, and I tried hard to write the fastest code for both\nimplementations).\n\nSo, let's try to do better. As usual, the first thing to do is to profile and\nsee where we spend most of the time. Here is the vmprof profile. We spend a\nlot of time inside the internals of numpypy, and allocating tons of temporary\narrays to store the results of the various operations.\n\nAlso, let's look at the jit traces and search for the function run:\nthis is loop in which we spend most of the time, and it is composed of 1796\noperations.  The operations emitted for the line np.dot(...) +\nself.constant are listed between lines 1217 and 1456. Here is the excerpt\nwhich calls np.dot(...); most of the ops are cheap, but at line 1232 we\nsee a call to the RPython function descr_dot; by looking at the\nimplementation we see that it creates a new W_NDimArray to store the\nresult, which means it has to do a malloc():\n\n\n\nThe implementation of the + self.constant part is also interesting:\ncontrary the former, the call to W_NDimArray.descr_add has been inlined by\nthe JIT, so we have a better picture of what's happening; in particular, we\ncan see the call to __0_alloc_with_del____ which allocates the\nW_NDimArray for the result, and the raw_malloc which allocates the\nactual array. Then we have a long list of 149 simple operations which set the\nfields of the resulting array, construct an iterator, and finally do a\ncall_assembler: this is the actual logic to do the addition, which was\nJITtted indipendently; call_assembler is one of the operations to do\nJIT-to-JIT calls:\n\n\n\nAll of this is very suboptimal: in this particular case, we know that the\nshape of self.matrix is always (3, 2): so, we are doing an incredible\namount of work, including calling\u00a0malloc() twice for the temporary arrays, just to\ncall two functions which ultimately do a total of 6 multiplications\nand 6 additions.  Note also that this is not a fault of the JIT: CPython+numpy\nhas to do the same amount of work, just hidden inside C calls.\n\nOne possible solution to this nonsense is a well known compiler optimization:\nloop unrolling.  From the compiler point of view, unrolling the loop is always\nrisky because if the matrix is too big you might end up emitting a huge blob\nof code, possibly uselss if the shape of the matrices change frequently: this\nis the main reason why the PyPy JIT does not even try to do it in this case.\n\nHowever, we know that the matrix is small, and always of the same\nshape. So, let's unroll the loop manually:\nclass SpecializedCreature(Creature):\n\n    def __init__(self, *args, **kwargs):\n        Creature.__init__(self, *args, **kwargs)\n        # store the data in a plain Python list\n        self.data = list(self.matrix.ravel()) + list(self.constant)\n        self.data_state = [0.0]\n        assert self.matrix.shape == (2, 3)\n        assert len(self.data) == 8\n\n    def run_step(self, inputs):\n        # state: [state_vars ... inputs]\n        # out_values: [state_vars, ... outputs]\n        k0, k1, k2, q0, q1, q2, c0, c1 = self.data\n        s0 = self.data_state[0]\n        z_sp, z = inputs\n        #\n        # compute the output\n        out0 = s0*k0 + z_sp*k1 + z*k2 + c0\n        out1 = s0*q0 + z_sp*q1 + z*q2 + c1\n        #\n        self.data_state[0] = out0\n        outputs = [out1]\n        return outputs\n\nIn the actual code there is also a sanity check which asserts that the\ncomputed output is the very same as the one returned by Creature.run_step.\n\nSo, let's try to see how it performs. First, with CPython:\n$ python -m ev.main\nGeneration   1: ... [population = 500]  [7.61 secs]\nGeneration   2: ... [population = 500]  [3.96 secs]\nGeneration   3: ... [population = 500]  [3.79 secs]\nGeneration   4: ... [population = 500]  [3.74 secs]\nGeneration   5: ... [population = 500]  [3.84 secs]\nGeneration   6: ... [population = 500]  [3.69 secs]\n\nThis looks good: 60% faster than the original CPython+numpy\nimplementation. Let's try on PyPy:\nGeneration   1: ... [population = 500]  [0.39 secs]\nGeneration   2: ... [population = 500]  [0.10 secs]\nGeneration   3: ... [population = 500]  [0.11 secs]\nGeneration   4: ... [population = 500]  [0.09 secs]\nGeneration   5: ... [population = 500]  [0.08 secs]\nGeneration   6: ... [population = 500]  [0.12 secs]\nGeneration   7: ... [population = 500]  [0.09 secs]\nGeneration   8: ... [population = 500]  [0.08 secs]\nGeneration   9: ... [population = 500]  [0.08 secs]\nGeneration  10: ... [population = 500]  [0.08 secs]\nGeneration  11: ... [population = 500]  [0.08 secs]\nGeneration  12: ... [population = 500]  [0.07 secs]\nGeneration  13: ... [population = 500]  [0.07 secs]\nGeneration  14: ... [population = 500]  [0.08 secs]\nGeneration  15: ... [population = 500]  [0.07 secs]\n\nYes, it's not an error. After a couple of generations, it stabilizes at around\n~0.07-0.08 seconds per generation. This is around 80 (eighty) times faster\nthan the original CPython+numpy implementation, and around 35-40x faster than\nthe naive PyPy+numpypy one.\n\nLet's look at the trace again: it no longer contains expensive calls, and\ncertainly no more temporary malloc() s. The core of the logic is between\nlines 386-416, where we can see that it does fast C-level multiplications and\nadditions: float_mul and float_add are translated straight into\nmulsd and addsd x86 instructions.\n\nAs I said before, this is a very particular example, and the techniques\ndescribed here do not always apply: it is not realistic to expect an 80x\nspeedup on arbitrary code, unfortunately. However, it clearly shows the potential of PyPy when\nit comes to high-speed computing. And most importantly, it's not a toy\nbenchmark which was designed specifically to have good performance on PyPy:\nit's a real world example, albeit small.\n\nYou might be also interested in the talk I gave at last EuroPython, in which I\ntalk about a similar topic: \"The Joy of PyPy JIT: abstractions for free\"\n(abstract, slides and video).\n\n\n\nHow to reproduce the results\n$ git clone https://github.com/antocuni/evolvingcopter\n$ cd evolvingcopter\n$ {python,pypy} -m ev.main --no-specialized --no-numpypy\n$ {python,pypy} -m ev.main --no-specialized\n$ {python,pypy} -m ev.main",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/10/how-to-make-your-code-80-times-faster-1424098117108093942.html"
    },
    {
      "title": "(Cape of) Good Hope for PyPy",
      "text": "Hello from the other side of the world (for most of you)!\n\nWith the excuse of coming to PyCon ZA during the last two weeks Armin,\nRonan, Antonio and sometimes Maciek had a very nice and productive sprint in\nCape Town, as pictures show :). We would like to say a big thank you to\nKiwi.com, which sponsored part of the travel costs via its awesome Sourcelift\nprogram to help Open Source projects.\n\n\n\nArmin, Anto and Ronan at Cape Point\n\n\nArmin, Ronan and Anto spent most of the time hacking at cpyext, our CPython\nC-API compatibility layer: during the last years, the focus was to make it\nworking and compatible with CPython, in order to run existing libraries such\nas numpy and pandas. However, we never paid too much attention to performance,\nso the net result is that with the latest released version of PyPy, C\nextensions generally work but their speed ranges from \"slow\" to \"horribly\nslow\".\n\nFor example, these very simple microbenchmarks measure the speed of\ncalling (empty) C functions, i.e. the time you spend to \"cross the border\"\nbetween RPython and C.  (Note: this includes the time spent doing the loop in regular Python code.) These are the results on CPython, on PyPy 5.8, and on\nour newest in-progress version:\n\n$ python bench.py     # CPython\nnoargs      : 0.41 secs\nonearg(None): 0.44 secs\nonearg(i)   : 0.44 secs\nvarargs     : 0.58 secs\n\n\n\n$ pypy-5.8 bench.py   # PyPy 5.8\nnoargs      : 1.01 secs\nonearg(None): 1.31 secs\nonearg(i)   : 2.57 secs\nvarargs     : 2.79 secs\n\n\n\n$ pypy bench.py       # cpyext-refactor-methodobject branch\nnoargs      : 0.17 secs\nonearg(None): 0.21 secs\nonearg(i)   : 0.22 secs\nvarargs     : 0.47 secs\n\n\n\n\n\nSo yes: before the sprint, we were ~2-6x slower than CPython. Now, we are\nfaster than it!\nTo reach this result, we did various improvements, such as:\n\n\n\nteach the JIT how to look (a bit) inside the cpyext module;\nwrite specialized code for calling METH_NOARGS, METH_O and\nMETH_VARARGS functions; previously, we always used a very general and\nslow logic;\nimplement freelists to allocate the cpyext versions of int and\ntuple objects, as CPython does;\nthe cpyext-avoid-roundtrip branch: crossing the RPython/C border is\nslowish, but the real problem was (and still is for many cases) we often\ncross it many times for no good reason. So, depending on the actual API\ncall, you might end up in the C land, which calls back into the RPython\nland, which goes to C, etc. etc. (ad libitum).\n\n\nThe branch tries to fix such nonsense: so far, we fixed only some cases, which\nare enough to speed up the benchmarks shown above.  But most importantly, we\nnow have a clear path and an actual plan to improve cpyext more and\nmore. Ideally, we would like to reach a point in which cpyext-intensive\nprograms run at worst at the same speed of CPython.\n\nThe other big topic of the sprint was Armin and Maciej doing a lot of work on the\nunicode-utf8 branch: the goal of the branch is to always use UTF-8 as the\ninternal representation of unicode strings. The advantages are various:\n\n\n\ndecoding a UTF-8 stream is super fast, as you just need to check that the\nstream is valid;\nencoding to UTF-8 is almost a no-op;\nUTF-8 is always more compact representation than the currently\nused UCS-4. It's also almost always more compact than CPython 3.5 latin1/UCS2/UCS4 combo;\nsmaller representation means everything becomes quite a bit faster due to lower cache pressure.\n\n\nBefore you ask: yes, this branch contains special logic to ensure that random\naccess of single unicode chars is still O(1), as it is on both CPython and the\ncurrent PyPy.\nWe also plan to improve the speed of decoding even more by using modern processor features, like SSE and AVX. Preliminary results show that decoding can be done 100x faster than the current setup.\n\n\nIn summary, this was a long and profitable sprint, in which we achieved lots\nof interesting results. However, what we liked even more was the privilege of\ndoing commits from awesome places such as the top of Table Mountain:\n\n\n\nOur sprint venue today #pypy pic.twitter.com/o38IfTYmAV\n\u2014 Ronan Lamy (@ronanlamy) 4 ottobre 2017\n\n\n\n\n\nThe panorama we looked at instead of staring at cpyext code",
      "tags": "cpyext,sprint,unicode",
      "url": "https://www.pypy.org/posts/2017/10/cape-of-good-hope-for-pypy-hello-from-3656631725712879033.html"
    },
    {
      "title": "PyPy v5.9 Released, Now Supports Pandas, NumPy",
      "text": "The PyPy team is proud to release both PyPy3.5 v5.9 (a beta-quality interpreter for Python\n3.5 syntax) and PyPy2.7 v5.9 (an interpreter supporting\nPython 2.7 syntax). \n\n\n\nNumPy and Pandas now work on PyPy2.7 (together with Cython 0.27.1). Many other modules\nbased on C-API extensions work on PyPy as well.\n\n\n\nCython 0.27.1 (released very recently) supports more projects with PyPy, both\non PyPy2.7 and PyPy3.5 beta. Note version 0.27.1 is now the minimum\nversion that supports this version of PyPy, due to some interactions with\nupdated C-API interface code.\n\n\n\n\nWe optimized the JSON parser for recurring string keys, which should decrease\nmemory use by up to 50% and increase parsing speed by up to 15% for large JSON files\nwith many repeating dictionary keys (which is quite common).\n\n\n\nCFFI, which is part of the PyPy release, has been updated to 1.11.1,\nimproving an already great package for interfacing with C. CFFI now supports\ncomplex arguments in API mode, as well as char16_t and char32_t and has\nimproved support for callbacks.\n\n\n\nIssues in the C-API compatibility layer that appeared as excessive memory\nuse were cleared up and other incompatibilities were resolved. The C-API\ncompatibility layer does slow down code which crosses the python-c interface\noften. Some fixes are in the pipelines for some of the performance issues, and we still recommend\nusing pure python on PyPy or interfacing via CFFI.\u00a0 \n\n\nPlease let us know if your use case is slow, we have ideas how to make things\nfaster but need real-world examples (not micro-benchmarks) of problematic code.\n\n\nWork sponsored by a Mozilla grant continues on PyPy3.5; we continue on the path to the goal of a complete python 3.5 implementation. Of course the bug fixes and performance enhancements\nmentioned above are part of both PyPy2.7 and PyPy3.5 beta.\n\n\nAs always, this release fixed many other issues and bugs raised by the\ngrowing community of PyPy users. We strongly recommend updating.\n\n\nYou can download the v5.9 releases here (note that we provide PyPy3.5 binaries for only Linux 64bit for now):\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors and contributors, and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 (stdlib version 2.7.13), and CPython 3.5 (stdlib version 3.5.3). It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux \n\n\n\n\nWhat else is new?\n\nPyPy 5.8 was released in June, 2017.\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/10/pypy-v59-released-now-supports-pandas-2261195727261691228.html"
    },
    {
      "title": "Let's remove the Global Interpreter Lock",
      "text": "Hello everyone\nThe Python community has been discussing removing the Global Interpreter Lock for\na long time.\nThere have been various attempts at removing it:\nJython or IronPython successfully removed it with the help of the underlying\nplatform, and some have yet to bear fruit, like gilectomy. Since our February sprint in Leysin,\nwe have experimented with the topic of GIL removal in the PyPy project.\nWe believe that the work done in IronPython or Jython can be reproduced with\nonly a bit more effort in PyPy. Compared to that, removing the GIL in CPython is a much\nharder topic, since it also requires tackling the problem of multi-threaded reference\ncounting. See the section below for further details.\nAs we announced at EuroPython, what we have so far is a GIL-less PyPy\nwhich can run very simple multi-threaded, nicely parallelized, programs.\nAt the moment, more complicated programs probably segfault. The\nremaining 90% (and another 90%) of work is with putting locks in strategic\nplaces so PyPy does not segfault during concurrent accesses to\ndata structures.\nSince such work would complicate the PyPy code base and our day-to-day work,\nwe would like to judge the interest of the community and the commercial\npartners to make it happen (we are not looking for individual\ndonations at this point).  We estimate a total cost of $50k,\nout of which we already have backing for about 1/3 (with a possible 1/3\nextra from the STM money, see below).  This would give us a good\nshot at delivering a good proof-of-concept working PyPy with no GIL. If we can get a $100k\ncontract, we will deliver a fully working PyPy interpreter with no GIL as a release,\npossibly separate from the default PyPy release.\nPeople asked several questions, so I'll try to answer the technical parts\nhere.\nWhat would the plan entail?\nWe've already done the work on the Garbage Collector to allow doing multi-\nthreaded programs in RPython.  \"All\" that is left is adding locks on mutable\ndata structures everywhere in the PyPy codebase. Since it would significantly complicate\nour workflow, we require real interest in that topic, backed up by\ncommercial contracts in order to justify the added maintenance burden.\nWhy did the STM effort not work out?\nSTM was a research project that proved that the idea is possible. However,\nthe amount of user effort that is required to make programs run in a\nparallelizable way is significant, and we never managed to develop tools\nthat would help in doing so.  At the moment we're not sure if more work\nspent on tooling would improve the situation or if the whole idea is really doomed.\nThe approach also ended up adding significant overhead on single threaded programs,\nso in the end it is very easy to make your programs slower.  (We have some money\nleft in the donation pot for STM which we are not using; according to the rules, we\ncould declare the STM attempt failed and channel that money towards the present\nGIL removal proposal.)\nWouldn't subinterpreters be a better idea?\nPython is a very mutable language - there are tons of mutable state and\nbasic objects (classes, functions,...) that are compile-time in other\nlanguage but runtime and fully mutable in Python.  In the end, sharing\nthings between subinterpreters would be restricted to basic immutable\ndata structures, which defeats the point. Subinterpreters suffers from the same problems as\nmultiprocessing with no additional benefits.\nWe believe that reducing mutability to implement subinterpreters is not viable without seriously impacting the\nsemantics of the language (a conclusion which applies to many other\napproaches too).\nWhy is it easier to do in PyPy than CPython?\nRemoving the GIL in CPython has two problems:\n\nhow do we guard access to mutable  data structures with locks and\nwhat to do with reference counting that needs to be guarded.\n\nPyPy only has the former problem; the latter doesn't exist,\ndue to a different garbage collector approach.  Of course the first problem\nis a mess too, but at least we are already half-way there. Compared to Jython\nor IronPython, PyPy lacks some data structures that are provided by JVM or .NET,\nwhich we would need to implement, hence the problem is a little harder\nthan on an existing multithreaded platform. However, there is good research\nand we know how that problem can be solved.\nBest regards,\nMaciej Fijalkowski",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/08/lets-remove-global-interpreter-lock-748023554216649595.html"
    },
    {
      "title": "Binary wheels for PyPy",
      "text": "Hi,\n\nthis is a short blog post, just to announce the existence of this Github repository, which contains binary PyPy wheels for some selected packages. The availability of binary wheels means that you can install the packages much more quickly, without having to wait for compilation.\n\n\nAt the moment of writing, these packages are available:\n\n\nnumpy\nscipy\npandas\npsutil\nnetifaces\n\n\nFor now, we provide only wheels built on Ubuntu, compiled for PyPy 5.8.\nIn particular, it is worth noting that they are not\u00a0manylinux1 wheels, which means they could not work on other Linux distributions. For more information, see the explanation in the README of the above repo.\n\nMoreover, the existence of the wheels does not guarantee that they work correctly 100% of the time. they still depend on cpyext, our C-API emulation layer, which is still work-in-progress, although it has become better and better during the last months. Again, the wheels are there only to save compilation time.\n\nTo install a package from the wheel repository, you can invoke pip like this:\n\n$ pip install --extra-index https://antocuni.github.io/pypy-wheels/ubuntu numpy\n\n\n\nHappy installing!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/07/binary-wheels-for-pypy-8718353804433344916.html"
    },
    {
      "title": "PyPy v5.8 released",
      "text": "The PyPy team is proud to release both PyPy2.7 v5.8 (an interpreter supporting\nPython 2.7 syntax), and a beta-quality PyPy3.5 v5.8 (an interpreter for Python\n3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.  Note that PyPy3.5 supports Linux 64bit only for now.\n\nThis new PyPy2.7 release includes the upstream stdlib version 2.7.13, and\nPyPy3.5 includes the upstream stdlib version 3.5.3.\n\nWe fixed critical bugs in the shadowstack rootfinder garbage collector\nstrategy that crashed multithreaded programs and very rarely showed up\neven in single threaded programs.\n\nWe added native PyPy support to profile frames in the vmprof statistical\nprofiler.\n\nThe struct module functions pack* and unpack* are now much faster,\nespecially on raw buffers and bytearrays. Microbenchmarks show a 2x to 10x\nspeedup. Thanks to Gambit Research for sponsoring this work.\n\nThis release adds (but disables by default) link-time optimization and\nprofile guided optimization of the base interpreter, which may make\nunjitted code run faster. To use these, translate with appropriate\noptions.  Be aware of issues with gcc toolchains, though.\n\nPlease let us know if your use case is slow, we have ideas how to make things\nfaster but need real-world examples (not micro-benchmarks) of problematic code.\n\nWork sponsored by a Mozilla grant continues on PyPy3.5; numerous fixes from\nCPython were ported to PyPy and PEP 489 was fully implemented. Of course the\nbug fixes and performance enhancements mentioned above are part of both PyPy\n2.7 and PyPy 3.5.\n\nCFFI, which is part of the PyPy release, has been updated to an unreleased 1.10.1,\nimproving an already great package for interfacing with C.\n\nAnyone using NumPy 1.13.0, must upgrade PyPy to this release since we implemented some previously missing C-API functionality. Many other c-extension modules now work with PyPy, let us know if yours does not.\n\nAs always, this release fixed many issues and bugs raised by the\ngrowing community of PyPy users. We strongly recommend updating.\n\nYou can download the v5.8 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors and contributors, and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux \n\n\n\n\n\nWhat else is new?\n\nPyPy 5.7 was released in March, 2017.\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release,sponsors",
      "url": "https://www.pypy.org/posts/2017/06/pypy-v58-released-739876359584854017.html"
    },
    {
      "title": "PyPy 5.7.1 bugfix released",
      "text": "We have released a bugfix PyPy2.7-v5.7.1 and PyPy3.5-v5.7.1 beta (Linux 64bit),\ndue to the following issues:\n\n\n\ncorrectly handle an edge case in dict.pop (issue 2508)\nfix a regression to correctly handle multiple inheritance in a C-API type\nwhere the second base is an app-level class with a __new__ function\nfix a regression to fill a C-API type\u2019s tp_getattr slot from a\n__getattr__ method (issue 2523)\n\n\n\nThanks to those who reported issues and helped test out the fixes\n\nYou can download the v5.7.1 release here:\n\n\nhttps://pypy.org/download.html\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/04/pypy-571-bugfix-released-8519267986159880133.html"
    },
    {
      "title": "Native profiling in VMProf",
      "text": "We are happy to announce a new release for the PyPI package vmprof.\nIt is now able to capture native stack frames on Linux and Mac OS X to show you bottle necks in compiled code (such as CFFI modules, Cython or C Python extensions). It supports PyPy, CPython versions 2.7, 3.4, 3.5 and 3.6. Special thanks to Jetbrains for funding the native profiling support.\n\n\n\n\n\n\nWhat is vmprof?\n\nIf you have already worked with vmprof you can skip the next two section. If not, here is a short introduction:\n\nThe goal of vmprof package is to give you more insight into your program. It is a statistical profiler. Another prominent profiler you might already have worked with is cProfile. It is bundled with the Python standard library.\n\nvmprof's distinct feature (from most other profilers) is that it does not significantly slow down your program execution. The employed strategy is statistical, rather than deterministic. Not every function call is intercepted, but it samples stack traces and memory usage at a configured sample rate (usually around 100hz). You can imagine that this creates a lot less contention than doing work before and after each function call.\n\nAs mentioned earlier cProfile gives you a complete profile, but it needs to intercept every function call (it is a deterministic profiler). Usually this means that you have to capture and record every function call, but this takes an significant amount time.\n\n The overhead vmprof consumes is roughly 3-4% of your total program runtime or even less if you reduce the sampling frequency. Indeed it lets you sample and inspect much larger programs. If you failed to profile a large application with cProfile, please give vmprof a shot.\n\nvmprof.com or PyCharm\n\n\n\nThere are two major alternatives to the command-line tools shipped with vmprof:\n\nA web service on vmprof.com\nPyCharm Professional Edition \n\n\nWhile the command line tool is only good for quick inspections, vmprof.com\n and PyCharm compliment each other providing deeper insight into your \nprogram. With PyCharm you can view the per-line profiling results inside\n the editor. With the vmprof.com you get a\u00a0handy visualization of the profiling results as a flame chart and memory usage graph.\n\n\n\n\n\nSince the PyPy Team runs and maintains the service on vmprof.com (which is by the way free and open-source), I\u2019ll explain some more details here. On vmprof.com you can inspect the generated profile interactively instead of looking at console output. What is sent to vmprof.com? You can find details here.\n\nFlamegraph: Accumulates and displays the most frequent codepaths. It allows you to quickly and accurately identify hot spots in your code. The flame graph below is a very short run of richards.py (Thus it shows a lot of time spent in PyPy's JIT compiler).\n\n\n\n\n\nList all functions (optionally sorted): the equivalent of the vmprof command line output in the web.\n\n\n\n\n\u00a0Memory curve: A line plot that shows how how many MBytes have been consumed over the lifetime of your program (see more info in the section below).\n\n\n\nNative programs\n\nThe new feature introduced in vmprof 0.4.x allows you to look beyond the Python level. As you might know, Python maintains a stack of frames to save the execution. Up to now the vmprof profiles only contained that level of information. But what if you program jumps to native code (such as calling gzip compression on a large file)? Up to now you would not see that information.\n\nMany packages make use of the CPython C API (which we discurage, please lookup cffi for a better way to call C). Have you ever had the issue that you know that your performance problems reach down to, but you could not profile it properly? Now you can!\n\n Let's inspect a very simple Python program to find out why a program is significantly slower on Linux than on Mac:\n\nimport numpy as np\nn = 1000\na = np.random.random((n, n))\nb = np.random.random((n, n))\nc = np.dot(np.abs(a), b)\n\n\nTake two NxN random matrix objects and create a dot product. The first argument to the dot product provides the absolute value of the random matrix.\n\n\nRunPythonNumPyOSn=... Took \n [1]CPython 3.5.2NumPy 1.12.1Mac OS X, 10.12.3n=5000~9 sec\n [2]CPython 3.6.0NumPy 1.12.1Linux 64, Kernel 4.9.14n=1000~26 sec\n\n\nNote that the Linux machine operates on a 5 times smaller matrix, still it takes much longer. What is wrong? Is Linux slow? CPython 3.6.0? Well no, lets inspect and [1] and [2] (shown below in that order).\n\n\n\n\n\n[2] runs on Linux, spends nearly all of the time in PyArray_MatrixProduct2, if you compare to [1] on Mac OS X, you'll see that a lot of time is spent in generating the random numbers and the rest in cblas_matrixproduct.\n\nBlas has a very efficient implementation so you can achieve the same on Linux if you install a blas implementation (such as openblas).\n\nUsually you can spot potential program source locations that take a lot of time and might be the first starting point to resolve performance issues.\n\nBeyond Python programs \n\nIt is not unthinkable that the strategy can be reused for native programs. Indeed this can already be done by creating a small cffi wrapper around an entry point of a compiled C program. It would even work for programs compiled from other languages (e.g. C++ or Fortran). The resulting function names are the full symbol name embedded into either the executable symboltable or extracted from the dwarf debugging information. Most of those will be compiler specific and contain some cryptic information.\n\nMemory profiling\nWe thankfully received a code contribution from the company Blue Yonder. They have built a memory profiler (for Linux and Mac OS X) on top of vmprof.com that displays the memory consumption for the runtime of your process.\n\nYou can run it the following way:\n\n$ python -m vmprof --mem --web script.py\n\nBy adding --mem, vmprof will capture memory information and display it in the dedicated view on vmprof.com. You can tha view by by clicking the 'Memory' switch in the flamegraph view.\n\nThere is more\n\nSome more minor highlights contained in 0.4.x:\n\nVMProf support for Windows 64 bit (No native profiling)\nVMProf can read profiles generated by another host system\nVMProf is now bundled in several binary wheel for fast and easy installation (Mac OS X, Linux 32/64 for CPython 2.7, 3.4, 3.5, 3.6)\n\nFuture plans - Profile Streaming\n\nvmprof has not reached the end of development. There are many features we could implement. But there is one feature that could be a great asset to many Python developers.\n\nContinuous delivery of your statistical profile, or in short, profile streaming. One of the great strengths of vmprof is that is consumes very little overhead. It is not a crazy idea to run this in production.\n\nIt would require a smart way to stream the profile in the background to vmprof.com and new visualizations to look at much more data your Python service produces.\n\nIf that sounds like a solid vmprof improvement, don't hesitate to get in touch with us (e.g. IRC #pypy, mailing list pypy-dev, or comment below)\n\nYou can help! \n\nThere are some immediate things other people could help with. Either by donating time or money (yes we have occasional contributors which is great)!\n\nWe gladly received code contribution for the memory profiler. But it was not enough time to finish the migration completely. Sadly it is a bit brittle right now.\nWe would like to spend more time on other visualizations. This should include to give a much better user experience on vmprof.com (like a tutorial that explains the visualization that we already have).\u00a0\nBuild Windows 32/64 bit wheels (for all CPython versions we currently support)\n\nWe are also happy to accept google summer of code projects on vmprof for new visualizations and other improvements. If you qualify and are interested, don't hesitate to ask!\n\nRichard Plangger (plan_rich) and the PyPy Team\n\n[1] Mac OS X https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac\n[2] Linux64 https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/04/native-profiling-in-vmprof-6949065546884243105.html"
    },
    {
      "title": "PyPy2.7 and PyPy3.5 v5.7 - two in one release",
      "text": "The PyPy team is proud to release both PyPy2.7 v5.7 (an interpreter supporting\nPython v2.7 syntax), and a beta-quality PyPy3.5 v5.7 (an interpreter for Python\nv3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.  Note that PyPy3.5 only supports Linux 64bit for now.\n\nThis new PyPy2.7 release includes the upstream stdlib version 2.7.13, and PyPy3.5 (our first in the 3.5 series) includes the upstream stdlib version 3.5.3.\n\nWe continue to make incremental improvements to our C-API compatibility layer (cpyext). PyPy2 can now import and run many C-extension packages, among the most notable are Numpy, Cython, and Pandas. Performance may be slower than CPython, especially for frequently-called short C functions. Please let us know if your use case is slow, we have ideas how to make things faster but need real-world examples (not micro-benchmarks) of problematic code.\n\nWork proceeds at a good pace on the PyPy3.5 version due to a grant from the Mozilla Foundation, hence our first 3.5.3 beta release. Thanks Mozilla !!! While we do not pass all tests yet, asyncio works and as these benchmarks show it already gives a nice speed bump. We also backported the f\"\" formatting from 3.6 (as an exception; otherwise \u201cPyPy3.5\u201d supports the Python 3.5 language).\n\nCFFI has been updated to 1.10, improving an already great package for interfacing with C.\n\nWe now use shadowstack as our default gcrootfinder even on Linux. The alternative, asmgcc, will be deprecated at some future point. While about 3% slower, shadowstack is much more easily maintained and debuggable. Also, the performance of shadowstack has been improved in general: this should close the speed gap between other platforms and Linux.\n\nAs always, this release fixed many issues and bugs raised by the growing community of PyPy users. We strongly recommend updating.\n\nYou can download the v5.7 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\n\u00a0\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\n\n\u00a0\n\nWhat else is new?\n\n(since the releases of PyPy 2.7 and 3.3 at the end of 2016)\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/03/pypy27-and-pypy35-v57-two-in-one-release-4736633226245374150.html"
    },
    {
      "title": "Leysin Winter Sprint Summary",
      "text": "Today\n is the last day of our yearly sprint event in Leysin. We had lots of \nideas on how to enhance the current state of PyPy, we went skiing and \nhad interesting discussions around virtual machines, the Python \necosystem, and other real world problems.\n\u00a0\n\n\nWhy don't you join us next time?\n\n\nA usual PyPy sprints day goes through the following stages:\n\n\n\n\n\u00a0Planning Session: Tasks from previous days that have seen progress or \nare completed are noted in a shared document. Everyone adds new tasks \nand then assigns themselves to one or more tasks (usually in pairs). As \nsoon as everybody is happy with their task and has a partner to work \nwith, the planning session is concluded and the work can start.\nDiscussions: A sprint is a good occasion to discuss difficult \nand important topics in person. We usually sit down in a separate area \nin the sprint room and discuss until a) nobody wants to discuss anymore \nor b) we found a solution to the problem. The good thing is that usally \nthe outcome is b).\nLunch: For lunch we prepare sandwiches and other finger food.\nContinue working until dinner, which we eat at a random restaurant in Leysin.\nGoto 1 the next day, if sprint has not ended.\n\n\n\nSprints\n are open to everybody and help newcomers to get started with PyPy (we usually\n pair you with a developer familiar with PyPy). They are perfect to \ndiscuss and find solutions to problems we currently face. If you are \neager to join next year, please don't hesitate to register next year \naround January.\n\n\u00a0\n\n\nSprint Summary\u00a0 \u00a0\nSprint goals included to work on the following topics: \n\n\nWork towards releasing PyPy 3.5 (it will be released soon)\nCPython Extension (CPyExt) modules on PyPy\nHave fun in winter sports (a side goal)\n\n\n\n\nHighlights\n\n\n\n\n\n\n\nWe have spent lots of time debugging and fixing memory issues on CPyExt.\n In particular, we fixed a serious memory leak where taking a memoryview\n would prevent numpy arrays from ever being freed. More work is still required to ensure that our GC always releases arrays in a timely \nmanner.\nFruitful discussions and progress about how to flesh out some details about the unicode representation in PyPy. Our current goal is to use utf-8 as the unicode representation internally and have fast vectorized operations (indexing, check if valid, ...).\nPyPy will participate in GSoC 2017 and we will try to allocate more resources to that than last year.\nProfile and think about some details how to reduce the starting size of the interpreter. The starting point would be to look at the parser and reduce the amount of strings to keep alive.\nFound a topic for a student's master thesis: correctly freeing cpyext reference cycles.\nRun lots of Python3 code on top of PyPy3 and resolve issues we found along the way.\nInitial work on making RPython thread-safe without a GIL.\n\n\n\n\nList of attendees\n\n\n- Stefan Beyer\n\n- Antonio Cuni\n\n- Maciej Fijalkowski\n\n- Manuel Jacob\n\n- Ronan Lamy\n\n- Remi Meier\n\n- Richard Plangger\n\n- Armin Rigo\n\n- Robert Zaremba\n\n\u00a0\n\n\u00a0 \n\n\n\n\n\n\n\n\nWe\n would like to thank our donors for the continued support of the PyPy \nproject and we looking forward to next years sprint in Leysin.\n\n\n\n\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/03/leysin-winter-sprint-summary-4587213628578490701.html"
    },
    {
      "title": "Async HTTP benchmarks on PyPy3",
      "text": "Hello everyone,\n\n\n\nSince Mozilla announced funding, we've been working quite hard on delivering you a working Python 3.5.\n\n\u00a0\n\nWe are almost ready to release an alpha version of PyPy 3.5. Our goal is to release it shortly after the sprint. Many modules have already been ported and\u00a0 it can probably run many Python 3 programs already. We are happy to receive any feedback after the next release.\u00a0 \n\n\n\nTo show that the heart (asyncio) of Python 3 is already working we have prepared some benchmarks. They are done by Pawe\u0142 Piotr Przeradowski @squeaky_pl for a HTTP workload on serveral asynchronous IO libraries, namely the relatively new asyncio and curio libraries and the battle-tested tornado, gevent and Twisted libraries. To see the benchmarks check out https://github.com/squeaky-pl/zenchmarks and the instructions for reproducing can be found inside README.md in the repository. Raw results can be obtained from https://github.com/squeaky-pl/zenchmarks/blob/master/results.csv.\n\n\n\nThe\n purpose of the presented benchmarks is showing that the upcoming PyPy release \nis already working with unmodified code that runs on CPython 3.5. PyPy \nalso manages to make them run significantly faster.\n\n\n\nThe\n benchmarks consist of HTTP servers implemented on the top of the mentioned \nlibraries. All the servers are single-threaded relying on underlying \nevent loops to provide concurrency. Access logging was disabled to \nexclude terminal I/O from the results. The view code consists of a \nlookup in a dictionary mapping ASCII letters to verses from the famous \nZen of Python. If a verse is found the view returns it, otherwise a 404 \nNot Found response is served. The 400 Bad Request and 500 Internal \nServer Error cases are also handled.\n\n\n\nThe workload was generated with the wrk HTTP benchmarking tool. It is run with one thread opening up to 100 \nconcurrent connections for 2 seconds and repeated 1010 times to get \nconsecutive measures. There is a Lua script provided\n that instructs wrk to continuously send 24 different requests that hit \ndifferent execution paths (200, 404, 400) in the view code. Also it is \nworth noting that wrk will only count 200 responses as successful so the actual request per second throughput is higher.\n\n\n\nFor your convenience all the used libraries versions are vendored into the benchmark repository. There is also a precompiled portable version of wrk provided\n that should run on any reasonably recent (10 year old or newer) Linux \nx86_64 distribution. The benchmark was performed on a public cloud scaleway x86_64 server launched in a Paris data center. The server was running \nUbuntu 16.04.01 LTS and reported Intel(R) Xeon(R) CPU D-1531 @ 2.20GHz \nCPU. CPython 3.5.2 (shipped by default in Ubuntu) was benchmarked \nagainst a pypy-c-jit-90326-88ef793308eb-linux64 snapshot of the 3.5 compatibility branch of PyPy.\n\n\n\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nWe want to thank Mozilla for supporting our work!\n\n\n\nCheers,\n\nfijal, squeaky_pl and the PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/03/async-http-benchmarks-on-pypy3-1092124994927894138.html"
    },
    {
      "title": "Leysin Winter Sprint: 25/26th Feb. - 4th March 2017",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the twelveth time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\n\nGoals and topics of the sprint\nThe list of topics is very open.\n\nThe main topic is Python 3.5 support in PyPy, as most py3.5\ncontributors should be present.  It is also a good topic if you have\nno or limited experience with PyPy contribution: we can easily find\nsomething semi-independent that is not done in py3.5 so far, and\ndo pair-programming with you.\nAny other topic is fine too: JIT compiler optimizations, CFFI,\nthe RevDB reverse debugger, improving to speed of your program on\nPyPy, etc.\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off (for ski or anything else).\n\n\n\nExact times\nWork days: starting 26th Feb (~noon), ending March 4th (~noon).\nI have pre-booked the week from Saturday Feb 25th to Saturday March 4th.\nIf it is possible for you to arrive Sunday before mid-afternoon, then\nyou should get a booking from Sunday only.  The break day should be\naround Wednesday.\nIt is fine to stay a few more days on either side, or conversely to book\nfor a part of that time only.\n\n\nLocation & Accomodation\n\nLeysin, Switzerland, \"same place as before\".\n\n\n\nLet me refresh your\nmemory: both the sprint venue and the lodging will be in a\npair of chalets built specifically for bed & breakfast:\nhttps://www.ermina.ch/.  The place has a good ADSL Internet connection\nwith wireless installed.  You can also arrange your own lodging\nelsewhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue).\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.\nThe options of rooms are a bit more limited than on previous years\nbecause the place for bed-and-breakfast is shrinking; but we should\nstill have enough room for us.  The price is around 60 CHF, breakfast\nincluded, in shared rooms (3 or 4 people).  If there are people that\nwould prefer a double or single room, please contact me and we'll see\nwhat choices you have.  There are also a choice of hotels in Leysin.\nPlease register by Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/leysin-winter-2017/\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around, and at least one EU-format power strip.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/01/leysin-winter-sprint-2526th-feb-4th-3831779797804484935.html"
    },
    {
      "title": "PyPy2.7 v5.6 released - stdlib 2.7.12 support, C-API improvements, and more",
      "text": "We have released PyPy2.7 v5.6 [0], about two months after PyPy2.7 v5.4. This new PyPy2.7 release includes the upstream stdlib version 2.7.12.\n\nWe continue to make incremental improvements to our C-API compatibility layer (cpyext). We pass all but 12 of the over-6000 tests in the upstream NumPy test suite, and have begun examining what it would take to support Pandas and PyQt. \n\nWork proceeds at a good pace on the PyPy3.5 version due to a grant from the Mozilla Foundation, and some of those changes have been backported to PyPy2.7 where relevant.\n\nThe PowerPC and s390x backend have been enhanced with the capability to use SIMD instructions for micronumpy loops.\n\nWe changed timeit to now report average +/- standard deviation, which is better than the misleading minimum value reported in CPython.\n\nWe now support building PyPy with OpenSSL 1.1 in our built-in _ssl module, as well as maintaining support for previous versions.\n\nCFFI has been updated to 1.9, improving an already great package for interfacing with C.\n\nAs always, this release fixed many issues and bugs raised by the growing community of PyPy users. We strongly recommend updating. You can download the PyPy2.7 v5.6 release here:\n\n\nhttps://pypy.org/download.html\n\nDownstream packagers have been hard at work. The Debian package is already available, and the portable PyPy versions are also ready, for those who wish to run PyPy on other Linux distributions like RHEL/Centos 5.\n\nWe would like to thank our donors for the continued support of the PyPy project.\n\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\n\n\nWhat else is new?\n\n(since the release of PyPy 5.4 in August, 2016)\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team\n\n[0] We skipped 5.5 since we share a code base with PyPy3, and PyPy3.3-v.5.5-alpha was released last month",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/11/pypy27-v56-released-stdlib-2712-support-5671090852400583673.html"
    },
    {
      "title": "Vectorization extended. PowerPC and s390x",
      "text": "We are happy to announce that JIT support in both the PowerPC backend and the\ns390x backend have been enhanced. Both can now vectorize loops via SIMD\ninstructions. Special thanks to IBM for funding this work.\n\nIf you are not familiar with this topic you can read more details\u00a0here.\n\n\nThere are many more enhancements under the hood. Most notably, all pure operations are now delayed until the latest possible point. In some cases indices have been calculated more than once or they needed an additional register, because the old value is still used. Additionally it is now possible to load quadword-aligned memory in both PPC and s390x (x86 currently cannot do that).\n\nNumPy & CPyExt\nThe community and core developers have been moving CPyExt towards a complete, but emulated, layer for CPython C extensions. This is great, because the one restriction preventing the wider deployment of PyPy in several scenarios will hopefully be removed. However, we advocate not to use CPyExt, but rather to not write C code at all (let PyPy speed up your Python code) or use cffi.\n\nThe work done here to support vectorization helps micronumpy (NumPyPy) to speed up operations for PPC and s390x. So why is PyPy supporting both NumPyPy and NumPy, do we actually need both? Yes, there are places where gcc can beat the JIT, and places where the tight integration between NumPyPy and PyPy is more performant. We do have plans to integrate both, hijacking the C-extension method calls to use NumPyPy where we know NumPyPy can be faster.\n\nJust to give you an idea why this is a benefit:\n\nNumPy arrays can carry custom dtypes and apply user defined python functions on the arrays. How could one optimize this kind of scenario? In a traditional setup, you cannot. But as soon as NumPyPy is turned on, you can suddenly JIT compile this code and vectorize it.\n\nAnother example is element access that occurs frequently, or any other calls that cross between Python and the C level frequently.\n\nBenchmarks\nLet's have a look at some benchmarks reusing\u00a0mikefc's numpy benchmark suite\u00a0(find the forked version here).\u00a0I only ran a subset of microbenchmarks, showing that the core functionality is\nfunctioning properly. Additionally it has been rewritten to use\u00a0perf\u00a0instead of the timeit stdlib module.\n\n\nSetup\nx86 runs on a Intel i7-2600 clocked at 3.40GHz using 4 cores. PowerPC runs on the Power 8 clocked at 3.425GHz providing 160 cores. Last but not least the mainframe machine clocked up to 4 GHz, but fully virtualized (as it is common for such machines). Note that PowerPC is a non private remote machine. It is used by many users and it is crowded with processes. It is hard to extract a stable benchmark there.\n\nx86 ran on Fedora 24 (kernel version of 4.8.4), PPC ran on Fedora 21 (kernel version 3.17.4) and s390x ran on Redhat Linux 7.2 (kernel version 3.10.0). Respectivley, numpy on cpython had openblas available on x86, no blas implementation were present on s390x and PPC provided blas and lapack.\n\nAs you can see all machines run very different configurations. It does not make sense to compare across platforms, but rather implementations on the same platform.\n\n\n\n\n\n\n\nBlue shows CPython 2.7.10+ available on that platform using the latest NumPy (1.11). Micro NumPy is used for PyPy. PyPy+ indicates that the vectorization optimization is turned on.\nAll bar charts show the median value of all runs (5 samples, 100 loops, 10 inner loops, for the operations on vectors (not matrices) the loops are set to 1000). PyPy additionally gets 3 extra executions to warmup the JIT.\n\nThe comparison is really comparing speed of machine code. It compares the PyPy's JIT output vs GCC's output. It has little to do with the speed of the interpreter.\n\nBoth new SIMD backends speedup the numeric kernels. Some times it is near to the speed of CPython, some times it is faster. The maximum parallelism very much depends on the extension emitted by the compiler. All three SIMD backends have the same vector register size (which is 128 bit). This means that all three behave similar but ppc and s390x gain more because they can load 128bit of memory from quadword aligned memory.\n\n\nFuture directions\nPython is achieving rapid adoption in data science. This is currently a trend emerging in Europe, and Python is already heavily used for data science in the USA many other places around the world.\n\n\nPyPy can make a valuable contribution for data scientists, helping them to rapidly write scientific programs in Python and run them at near native speed. If you happen to be in that situation, we are eager to hear you feedback or resolve your issues and also work together to improve the performance of your,\ncode. Just get in touch!\n\n\nRichard Plangger (plan_rich) and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/11/vectorization-extended-powerpc-and-s390x-4042433015460084057.html"
    },
    {
      "title": "PyPy3 5.5.0 released",
      "text": "We're pleased to announce the release of PyPy3 v5.5.0. Coming four months after PyPy3.3 v5.2, it improves compatibility with Python 3.3 (3.3.5). We strongly recommend updating from previous PyPy3 versions.\n\nWe would like to thank all of the people who donated to the py3k proposal for supporting the work that went into this release.\n\nYou can download the PyPy3.3 v5.5.0 release here:\u00a0https://pypy.org/download.html\n\nImproved Python 3.3.5 support.\n\nos.get_terminal_size(), time.monotonic(), str.casefold()\u00a0\nfaulthandler module\nThere are still some missing features such as a PEP 393-like space efficient string representation and including performance regressions (e.g. issue #2305). The focus for this release has been updating to 3.3 compatibility. Windows is also not yet supported.\n\nensurepip is also included (it's only included in CPython 3 >= 3.4).\nBuffer interface improvements (numpy on top of cpyext)\nSeveral JIT improvements (force-virtual-state, residual calls)\nSearch path for libpypy-c.so has changed (helps with cffi embedding on linux distributions)\nImprove the error message when the user forgot the \"self\" argument of a method\nMany more small improvements, please head over to our documentation for more information\n\n\nTowards Python 3.5\n\n\nWe have started to work on Python 3.5, which is a version used by many software projects. It seems to get wide adoption. We are happy to be part of the\u00a0Mozilla Open Source Support (MOSS) initiative.\n\n\n\nNevertheless we want to give our users the chance to use PyPy in their Python 3 projects, thus we have prepared this release.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\u00a0CPython 2.7.10 and 3.3.5. It's fast due to its integrated tracing JIT\u00a0compiler.\n We also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems except Windows\u00a0\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux\u00a0\nbig- and little-endian variants of PPC64 running Linux\u00a0\ns390x running Linux\n\nPlease try it out and let us know what you think. We welcome feedback, we know\nyou are using PyPy, please tell us about it!\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/10/pypy3-550-released-8069558680221199646.html"
    },
    {
      "title": "RevDB released, v5.4.1",
      "text": "Hi all,\n\n\nThe first beta version of RevDB is out!  Remember that RevDB is a reverse debugger for Python.  The idea is that it is a debugger that can run forward and backward in time, letting you more easily understand your subtle bug in your big Python program.\n\n\nRevDB should work on almost any Python program.  Even if you are normally only using CPython, trying to reproduce the bug with RevDB is similar to trying to run the program on a regular PyPy---usually it just works, even if not quite always.\n\n\nNews from the alpha version in the previous blog post include notably support for:\n\nThreads.\nCPyExt, the compatibility layer of PyPy that can run CPython C extension modules.\n\nas well as many other improvements.\n\n\nYou need to build it yourself for now.  It is tested on 64-bit Linux.  32-bit Linux, OS/X, and other POSIX platforms should all either work out of the box or be just a few fixes away (contributions welcome).  Win32 support is a lot more involved but not impossible.\n\n\nSee https://bitbucket.org/pypy/revdb/ for more information!\n\nArmin",
      "tags": "releaserevdb",
      "url": "https://www.pypy.org/posts/2016/09/revdb-released-v541-6719768292347391304.html"
    },
    {
      "title": "PyPy 5.4.1 bugfix released",
      "text": "We have released a bugfix for PyPy2.7-v5.4.0, released last week, due to the following issues:\n\n\n\nUpdate list of contributors in documentation and LICENSE file, this was unfortunately left out of 5.4.0. My apologies to the new contributors\nAllow tests run with -A to find libm.so even if it is a script not a dynamically loadable file\nBump sys.setrecursionlimit() when translating PyPy, for translating with CPython\nTweak a float comparison with 0 in backendopt.inline to avoid rounding errors\nFix for an issue for translating the sandbox\nFix for and issue where unicode.decode('utf8', 'custom_replace') messed up the last byte of a unicode string sometimes\nUpdate built-in cffi to version 1.8.1\nExplicitly detect that we found as-yet-unsupported OpenSSL 1.1, and crash translation with a message asking for help porting it\nFix a regression where a PyBytesObject was forced (converted to a RPython object) when not required, reported as issue #2395\n\n\nThanks to those who reported the issues.\n\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/09/pypy-541-bugfix-released-3217566297258542810.html"
    },
    {
      "title": "PyPy2 v5.4 released - incremental improvements and enhancements",
      "text": "We have released PyPy2.7 v5.4, a little under two months after PyPy2.7 v5.3.\nThis new PyPy2.7 release includes incremental improvements to our C-API\ncompatibility layer (cpyext), enabling us to pass over 99% of the upstream\nnumpy test suite.\n\nWe updated built-in cffi support to version 1.8,\nwhich now supports the \u201climited API\u201d mode for c-extensions on\nCPython >=3.2.\n\n\nWe improved tooling for the PyPy JIT, and expanded VMProf\nsupport to OpenBSD and Dragon Fly BSD\n\n\nAs always, this release fixed many issues and bugs raised by the\ngrowing community of PyPy users. We strongly recommend updating.\n\n\nYou can download the PyPy2 v5.4 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for their continued support of the PyPy\nproject. We would also like to thank our contributors and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, testing and adapting popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7 performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux\nbig- and little-endian variants of PPC64 running Linux\ns390x running Linux\n\n\n\nWhat is New?\n\n(since the release of PyPy 5.3 in June, 2016)\nThere are many incremental improvements to RPython and PyPy, the complete listing is here. Mozilla generously sponsored work toward python 3.5 compatibility, and we are beginning to see some cross-over improvements of RPython and PyPy2.7 as a result.\n\nPlease update, and continue to help us make PyPy better.\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/08/pypy2-v54-released-incremental-3611318295736669599.html"
    },
    {
      "title": "PyPy Tooling Upgrade: JitViewer and VMProf",
      "text": "We are happy to announce a major JitViewer (JV) update.\nJV allows you to inspect RPython's internal compiler representation (the language in which PyPy is implemented) including the generated machine code of your program. It can graphically show you details of the JIT compiled code and helps you pinpoint issues in your program.\n\nVMProf is a statistical CPU profiler for python imposing very little overhead at runtime.\n\nBoth VMProf and JitViewer share a common goal: Present useful information for your python program.\nThe combination of both can reveal more information than either alone.\nThat is the reason why they are now both packaged together.\nWe also updated vmprof.com\u00a0with various bug fixes and changes including an all new interface to JV.\n\nThis work was done with the goal of improving tooling and libraries around the Python/PyPy/RPython ecosystem.\nSome of the tools we have developed:\n\n\nCFFI - Foreign Function Interface that avoids CPyExt (CFFI docs)\nRevDB - A reverse debugger for python (RevDB blog post)\n\n\nand of course the tools we discuss here:\n\n\nVMProf - A statistical CPU profiler (VMProf docs)\nJitViewer - Visualization of the log file produced by RPython (JitLog docs)\n\n\n\nA \"brand new\" JitViewer\n\nJitViewer has two pieces: you create a log file when running your program, and then use a graphic tool to view what happened.\n\nThe old logging format was a hard-to-maintain, plain-text-logging facility. Frequent changes often broke internal tools.\nAdditionally, the logging output of a long running program required a lot of disk space.\n\nOur new binary format encodes data densely, makes use of some compression (gzip), and tries to remove repetition where possible.\nIt also supports versioning for future proofing and can be extended easily.\n\nAnd *drumroll* you no longer need to install a tool to view the log yourself\nanymore! The whole system moved to vmprof.com and you can use it any time.\n\nSounds great. But what can you do with it? Here are two examples for a PyPy user:\n\nPyPy crashed? Did you discover a bug?\n\nFor some hard to find bugs it is often necessary to look at the compiled code. The old\nprocedure often required you to upload a plain text file which was hard to parse and to look through.\n\nA better way to share a crash report is to install the ``vmprof`` module from PyPi and execute either of the two commands:\n\n# this program does not crash, but has some weird behaviour\n$ pypy -m jitlog --web <your program args>\n...\nPyPy Jitlog: https://vmprof.com/#/<hash>/traces\n# this program segfaults\n$ pypy -m jitlog -o /tmp/log <your program args>\n...\n<Segfault>\n$ pypy -m jitlog --upload /tmp/log\nPyPy Jitlog: https://vmprof.com/#/<hash>/traces\n\n\nProviding the link in the bug report allows PyPy developers to browse and identify potential issues.\n\n\nSpeed issues\n\nVMProf is a great tool to find hot spots that consume a lot of time in your program. As soon as you have identified code that runs slowly, you can switch to jitlog and maybe pinpoint certain aspects that do not behave as expected. You will find an overview, and are able to browse the generated code. If you cannot make sense of all that, you can just share the link with us and we can have a look too.\n\nFuture direction\n\nWe hope that the new release will help both PyPy developers and PyPy users resolve potential issues and easily point them out.\n\nHere are a few ideas what might come in the next few releases:\n\n\n\n\u00a0Combination of CPU profiles and the JITLOG (sadly did not make it into the current release).\nExtend vmprof.com to be able to query vmprof/jitlog. An example query for vmprof: 'methods.callsites() > 5' andfor the jitlog would be 'traces.contains('call_assembler').hasbridge('*my_func_name*')'.\nExtend the jitlog to capture the information of the optimization stage.\n\n\n\nRichard Plangger (plan_rich) and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/08/pypy-tooling-upgrade-jitviewer-and-5107430577468391432.html"
    },
    {
      "title": "PyPy gets funding from Mozilla for Python 3.5 support",
      "text": "\"Python 2.x versus Python 3.x\": this is by now an old question.  In the eyes of some people Python 2 is here to stay, and in the eyes of others Python has long been 3 only.\n\nPyPy's own position is that PyPy will support Python 2.7 forever---the RPython language in which PyPy is written is a subset of  2.7, and we have no plan to upgrade that.  But at the same time, we want to support 3.x.  This is particularly true now: a relatively recent development is that Python 3.5 seems to attract more and more people.  The \"switch\" to Python 3.x might be starting to happen.\n\nCorrespondingly, PyPy has been searching for a while for a way to support a larger-scale development effort.  The goal is to support not just any old version of Python 3.x, but Python 3.5, as this seems to be the version that people are switching to.  PyPy is close to supporting all of Python 3.3 now; but the list of what is new in Python 3.4 and 3.5 is far, far longer than anyone imagines.  The long-term goal is also to get a version of \"PyPy3\" that is as good as \"PyPy2\" is, including its performance and its cpyext layer (CPython C API interoperability), for example.\n\nSo, the end result: Mozilla recently decided to award $200,000 to Baroque Software to work on PyPy as part of its Mozilla Open Source Support (MOSS) initiative.  This money will be used to implement the Python 3.5 features in PyPy. Within the next year, we plan to use the money to pay four core PyPy developers half-time to work on the missing features and on some of the big performance and cpyext issues. This should speed up the progress of catching up with Python 3.x significantly. We are extremely thankful to Mozilla for supporting us in this way, and will keep you updated on the progress via this blog.",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2016/08/pypy-gets-funding-from-mozilla-for-5569307998787871200.html"
    },
    {
      "title": "Reverse debugging for Python",
      "text": "RevPDB\nA \"reverse debugger\" is a debugger where you can go forward and\nbackward in time.  It is an uncommon feature, at least in the open\nsource world, but I have no idea why.  I have used undodb-gdb and\nrr, which are reverse debuggers for C code, and I can only say that\nthey saved me many, many days of poking around blindly in gdb.\nThe PyPy team is pleased to give you \"RevPDB\", a reverse-debugger\nsimilar to rr but for Python.\nAn example is worth a thousand words.  Let's say your big Python\nprogram has a bug that shows up inconsistently.  You have nailed it\ndown to something like:\n\nstart x.py, which does stuff (maybe involving processing files,\nanswering some web requests that you simulate from another terminal,\netc.);\nsometimes, after a few minutes, your program's state becomes\ninconsistent and you get a failing assert or another exception.\n\nThis is the case where RevPDB is useful.\nRevPDB is available only on 64-bit Linux and OS/X right now, but should\nnot be too hard to port to other OSes.  It is very much alpha-level!\n(It is a debugger full of bugs.  Sorry about that.)  I believe it is\nstill useful---it helped me in one real use case already.\n\n\nHow to get RevPDB\nThe following demo was done with an alpha version for 64-bit Linux,\ncompiled for Arch Linux.  I won't provide the binary; it should be\neasy enough to retranslate (much faster than a regular PyPy because it\ncontains neither a JIT nor a custom GC).  Grab the PyPy sources from\nMercurial, and then:\n\nhg update reverse-debugger\n# or \"hg update ff376ccacb36\" for exactly this demo\ncd pypy/goal\n../../rpython/bin/rpython -O2 --revdb targetpypystandalone.py  \\\n                  --withoutmod-cpyext --withoutmod-micronumpy\n\nand possibly rename the final pypy-c to pypy-revdb to avoid\nconfusion.\nOther platforms than 64-bit Linux and OS/X need some fixes before they work.\n\n\nDemo\nFor this demo, we're going to use this x.py as the \"big program\":\n\nimport os\n\nclass Foo(object):\n    value = 5\n\nlst1 = [Foo() for i in range(100)]\nlst1[50].value += 1\nfor x in lst1:\n    x.value += 1\n\nfor x in lst1:\n    if x.value != 6:\n        print 'oops!'\n        os._exit(1)\n\nOf course, it is clear what occurs in this small example: the check\nfails on item 50.  For this demo, the check has been written with\nos._exit(1), because this exits immediately the program.  If it\nwas written with an assert, then its failure would execute things\nin the traceback module afterwards, to print the traceback; it\nwould be a minor mess just to find the exact point of the failing\nassert.  (This and other issues are supposed to be fixed in the\nfuture, but for now it is alpha-level.)\nAnyway, with a regular assert and a regular post-mortem pdb,\nwe could observe that x.value is indeed 7 instead of 6 when the\nassert fails.  Imagine that the program is much bigger: how would we\nfind the exact chain of events that caused this value 7 to show up on\nthis particular Foo object?  This is what RevPDB is for.\nFirst, we need for now to disable Address Space Layout Randomization\n(ASLR), otherwise replaying will not work.  This is done once with the\nfollowing command line, which changes the state until the next\nreboot:\n\necho 0 | sudo tee /proc/sys/kernel/randomize_va_space\n\nUPDATE: the above is no longer necessary from revision ff376ccacb36.\nRun x.py with RevPDB's version of PyPy instead of the regular\ninterpreter (CPython or PyPy):\n\nPYPYRDB=log.rdb ./pypy-revdb x.py\n\nThis pypy-revdb executable is like a slow PyPy executable, running\n(for now) without a JIT.  This produces a file log.rdb which\ncontains a complete log of this execution.  (If the bug we are\ntracking occurs rarely, we need to re-run it several times until we\nget the failure.  But once we got the failure, then we're done with\nthis step.)\nStart:\n\nrpython/translator/revdb/revdb.py log.rdb\n\nWe get a pdb-style debugger.  This revdb.py is a normal Python\nprogram, which you run with an unmodified Python; internally, it looks\ninside the log for the path to pypy-revdb and run it as needed (as\none forking subprocess, in a special mode).\nInitially, we are at the start of the program---not at the end, like\nwe'd get in a regular debugger:\n\nFile \"<builtin>/app_main.py\", line 787 in setup_bootstrap_path:\n(1)$\n\nThe list of commands is available with help.\nGo to the end with continue (or c):\n\n(1)$ continue\nFile \"/tmp/x.py\", line 14 in <module>:\n...\n  lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n          print 'oops!'\n>         os._exit(1)\n(19727)$\n\nWe are now at the beginning of the last executed line.  The number\n19727 is the \"time\", measured in number of lines executed.  We can go\nbackward with the bstep command (backward step, or bs), line\nby line, and forward again with the step command.  There are also\ncommands bnext, bcontinue and bfinish and their forward\nequivalents.  There is also \"go TIME\" to jump directly to the specified\ntime.  (Right now the debugger only stops at \"line start\"\nevents, not at function entry or exit, which makes some cases a bit\nsurprising: for example, a step from the return statement of\nfunction foo() will jump directly to the caller's caller, if the\ncaller's current line was return foo() + 2, because no \"line\nstart\" event occurs in the caller after foo() returns to it.)\nWe can print Python expressions and statements using the p\ncommand:\n\n(19727)$ p x\n$0 = <__main__.Foo object at 0xfffffffffffeab3e>\n(19727)$ p x.value\n$1 = 7\n(19727)$ p x.value + 1\n8\n\nThe \"$NUM =\" prefix is only shown when we print an object that\nreally exists in the debugged program; that's why the last line does\nnot contain it.  Once a $NUM has been printed, then we can use\nit in further expressions---even at a different point time.  It\nbecomes an anchor that always refers to the same object:\n\n(19727)$ bstep\n\nFile \"/tmp/x.py\", line 13 in <module>:\n...\n\n  lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n>         print 'oops!'\n          os._exit(1)\n(19726)$ p $0.value\n$1 = 7\n\nIn this case, we want to know when this value 7 was put in this\nattribute.  This is the job of a watchpoint:\n\n(19726)$ watch $0.value\nWatchpoint 1 added\nupdating watchpoint value: $0.value => 7\n\nThis watchpoint means that $0.value will be evaluated at each line.\nWhen the repr() of this expression changes, the watchpoint activates\nand execution stops:\n\n(19726)$ bcontinue\n[searching 19629..19726]\n[searching 19338..19629]\n\nupdating watchpoint value: $0.value => 6\nReverse-hit watchpoint 1: $0.value\nFile \"/tmp/x.py\", line 9 in <module>:\n  import os\n\n  class Foo(object):\n      value = 5\n\n  lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n>     x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n          print 'oops!'\n          os._exit(1)\n(19524)$\n\nNote that using the $NUM syntax is essential in watchpoints.  You\ncan't say \"watch x.value\", because the variable x will go out\nof scope very soon when we move forward or backward in time.  In fact\nthe watchpoint expression is always evaluated inside an environment\nthat contains the builtins but not the current locals and globals.\nBut it also contains all the $NUM, which can be used to refer to\nknown objects.  It is thus common to watch $0.attribute if $0\nis an object, or to watch len($1) if $1 is some list.  The\nwatch expression can also be a simple boolean: for example, \"watch\n$2 in $3\" where $3 is some dict and $2 is some object that\nyou find now in the dict; you would use this to find out the time when\n$2 was put inside $3, or removed from it.\nUse \"info watchpoints\" and \"delete <watchpointnum>\" to manage\nwatchpoints.\nThere are also regular breakpoints, which you set with \"b\nFUNCNAME\".  It breaks whenever there is a call to a function that\nhappens to have the given name.  (It might be annoying to use for a\nfunction like __init__() which has many homonyms.  There is no\nsupport for breaking on a fully-qualified name or at a given line\nnumber for now.)\nIn our demo, we stop at the line x.value += 1, which is where the\nvalue was changed from 6 to 7.  Use bcontinue again to stop at the\nline lst1[50].value += 1, which is where the value was changed from\n5 to 6.  Now we know how this value attribute ends up being 7.\n\n(19524)$ bcontinue\n[searching 19427..19524]\n[searching 19136..19427]\n\nupdating watchpoint value: $0.value => 5\nReverse-hit watchpoint 1: $0.value\nFile \"/tmp/x.py\", line 7 in <module>:\n  import os\n\n  class Foo(object):\n      value = 5\n\n  lst1 = [Foo() for i in range(100)]\n> lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n...\n(19422)$\n\nTry to use bcontinue yet another time.  It will stop now just before\n$0 is created.  At that point in time, $0 refers to\nan object that does not exist yet, so the watchpoint now evaluates to\nan error message (but it continues to work as before, with that error\nmessage as the string it currently evaluates to).\n\n(19422)$ bcontinue\n[searching 19325..19422]\n\nupdating watchpoint value: $0.value => RuntimeError:\n               '$0' refers to an object created later in time\nReverse-hit watchpoint 1: $0.value\nFile \"/tmp/x.py\", line 6 in <module>:\n  import os\n\n  class Foo(object):\n      value = 5\n\n> lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n...\n(19371)$\n\nIn big programs, the workflow is similar, just more complex.  Usually\nit works this way: we find interesting points in time with some\ncombination of watchpoints and some direct commands to move around.\nWe write down on a piece of (real or virtual) paper these points in\nhistory, including most importantly their time, so that we can\nconstruct an ordered understanding of what is going on.\nThe current revdb can be annoying and sometimes even crash; but\nthe history you reconstruct can be kept.  All the times and\nexpressions printed are still valid when you restart revdb.  The\nonly thing \"lost\" is the $NUM objects, which you need to print\nagain.  (Maybe instead of $0, $1, ...  we should use $<big\nnumber>, where the big number identifies uniquely the object by its\ncreation time.  These numbers would continue to be valid even after\nrevdb is restarted.  They are more annoying to use than just\n$0 though.)\nScreencast: Here's a (slightly typo-y) screencast of cfbolz using the reverse debugger:\n\n\n\nCurrent issues\nGeneral issues:\n\nIf you are using revdb on a log that took more than a few\nminutes to record, then it can be painfully slow.  This is because\nrevdb needs to replay again big parts of the log for some\noperations.\nThe pypy-revdb is currently missing the following modules:\nthread (implementing multithreading is possible, but not done\nyet);\ncpyext (the CPython C API compatibility layer);\nmicronumpy (minor issue only);\n_continuation (for greenlets).\n\n\nDoes not contain a JIT, and does not use our fast garbage\ncollectors.  You can expect pypy-revdb to be maybe 3 times\nslower than CPython.\nOnly works on Linux and OS/X.  There is no fundamental reason for\nthis restriction, but it is some work to fix.\nReplaying a program uses a lot more memory; maybe 15x as much than\nduring the recording.  This is because it creates many forks.  If\nyou have a program that consumes 10% of your RAM or more, you will\nneed to reduce MAX_SUBPROCESSES in process.py.\n\nReplaying also comes with a bunch of user interface issues:\n\nAttempted to do I/O or access raw memory: we get this whenever\ntrying to print some expression that cannot be evaluated with\nonly the GC memory---or which can, but then the __repr__()\nmethod of the result cannot.  We need to reset the state with\nbstep + step before we can print anything else.  However,\nif only the __repr__() crashes, you still see the $NUM =\nprefix, and you can use that $NUM afterwards.\nid() is globally unique, returning a reproducible 64-bit number,\nso sometimes using id(x) is a workaround for when using x\ndoesn't work because of Attempted to do I/O issues (e.g.  p\n[id(x) for x in somelist]).\nas explained in the demo, next/bnext/finish/bfinish might jump\naround a bit non-predictably.\nsimilarly, breaks on watchpoints can stop at apparently unexpected\nplaces (when going backward, try to do \"step\" once).  The issue is\nthat it can only stop at the beginning of every line.  In the\nextreme example, if a line is foo(somelist.pop(getindex())),\nthen somelist is modified in the middle.  Immediately before\nthis modification occurs, we are in getindex(), and\nimmediately afterwards we are in foo().  The watchpoint will\nstop the program at the end of getindex() if running backward,\nand at the start of foo() if running forward, but never\nactually on the line doing the change.\nwatchpoint expressions must not have any side-effect at all.  If\nthey do, the replaying will get out of sync and revdb.py will\ncomplain about that.  Regular p expressions and statements can\nhave side-effects; these effects are discarded as soon as you move\nin time again.\nsometimes even \"p import foo\" will fail with Attempted to do\nI/O.  Use instead \"p import sys; foo = sys.modules['foo']\".\nuse help to see all commands.  backtrace can be useful.\nThere is no up command; you have to move in time instead,\ne.g. using bfinish to go back to the point where the current\nfunction was called.\n\n\n\nHow RevPDB is done\nIf I had to pick the main advantage of PyPy over CPython, it is that\nwe have got with the RPython translation toolchain a real place for\nexperimentation.  Every now and then, we build inside RPython some\nfeature that gives us an optionally tweaked version of the PyPy\ninterpreter---tweaked in a way that would be hard to do with CPython,\nbecause it would require systematic changes everywhere.  The most\nobvious and successful examples are the GC and the JIT.  But there\nhave been many other experiments along the same lines, from the\nso-called stackless transformation in the early days, to the STM\nversion of PyPy.\nRevPDB works in a similar way.  It is a version of PyPy in which some\noperations are systematically replaced with other operations.\nTo keep the log file at a reasonable size, we duplicate the content of\nall GC objects during replaying---by repeating the same actions on\nthem, without writing anything in the log file.  So that means that in\nthe pypy-revdb binary, the operations that do arithmetic or\nread/write GC-managed memory are not modified.  Most operations are\nlike that.  However, the other operations, the ones that involve\neither non-GC memory or calls to external C functions, are tweaked.\nEach of these operations is replaced with code that works in two\nmodes, based on a global flag:\n\nin \"recording\" mode, we log the result of the operation (but not the\narguments);\nin \"replaying\" mode, we don't really do the operation at all, but\ninstead just fetch the result from the log.\n\nHopefully, all remaining unmodified operations (arithmetic and GC\nload/store) are completely deterministic.  So during replaying, every\ninteger or non-GC pointer variable will have exactly the same value as\nit had during recording.  Interestingly, it means that if the\nrecording process had a big array in non-GC memory, then in the\nreplaying process, the array is not allocated at all; it is just\nrepresented by the same address, but there is nothing there.  When we\nrecord \"read item 123 from the array\", we record the result of the\nread (but not the \"123\").  When we replay, we're seeing again the same\n\"read item 123 from the array\" operation.  At that point, we don't\nread anything; we just return the result from the log.  Similarly,\nwhen recording a \"write\" to the array, we record nothing (this write\noperation has no result); so that when replaying, we redo nothing.\nNote how that differs from anything managed by GC memory: GC objects\n(including GC arrays) are really allocated, writes really occur, and\nreads are redone.  We don't touch the log in this case.\n\n\nOther reverse debuggers for Python\nThere are already some Python experiments about reverse debugging.\nThis is also known as \"omniscient debugging\".  However, I claim that\nthe result they get to is not very useful (for the purpose presented\nhere).  How they work is typically by recording changes to some\nobjects, like lists and dictionaries, in addition to recording the\nhistory of where your program passed through.  However, the problem of\nPython is that lists and dictionaries are not the end of the story.\nThere are many, many, many types of objects written in C which are\nmutable---in fact, the immutable ones are the exception.  You can try\nto systematically record all changes, but it is a huge task and easy\nto forget a detail.\nIn other words it is a typical use case for tweaking the RPython\ntranslation toolchain, rather than tweaking the CPython (or PyPy)\ninterpreter directly.  The result that we get here with RevPDB is more\nsimilar to rr anyway, in that only a relatively small number of\nexternal events are recorded---not every single change to every single\nlist and dictionary.\nSome links:\n\nepdb: https://github.com/native-human/epdb\npode: https://github.com/rodsenra/pode\n\nFor C:\n\nrr: https://rr-project.org/\nundodb-gdb: https://undo.io/\n\n\n\nFuture work\nAs mentioned above, it is alpha-level, and only works on Linux and OS/X.\nSo the plans for the immediate future are to fix the various\nissues described above, and port to more operating systems.  The core of the system\nis in the C file and headers in rpython/translator/revdb/src-revdb.\nFor interested people, there is also the Duhton interpreter and its\nreverse-debugger branch, which is where I prototyped the RPython\nconcept before moving to PyPy.  The basics should work for any\ninterpreter written in RPython, but they require some specific code to\ninterface with the language; in the case of PyPy, it is in\npypy/interpreter/reverse_debugging.py.\nIn parallel, there are various user interface improvements that people\ncould be interested in, like a more \"pdb++\" experience.  (And the script\nat rpython/translator/revdb/revdb.py should be moved out into some\nmore \"official\" place, and the reverse-debugger branch should be\nmerged back to default.)\nI would certainly welcome any help!\n-+- Armin",
      "tags": "revdb",
      "url": "https://www.pypy.org/posts/2016/07/reverse-debugging-for-python-8854823774141612670.html"
    },
    {
      "title": "PyPy2 v5.3 released - major C-extension support improvements",
      "text": "We have released PyPy2.7 v5.3, about six weeks after PyPy 5.1 and a week after\nPyPy3.3 v5.2 alpha 1, the first PyPy release targeting 3.3\ncompatibility. This new PyPy2.7 release includes major improvements for the\nC-API compatibility layer. In addition to complete support\nfor lxml, we now pass most (more than 95%) of the upstream numpy test suite. We can build and run scipy and matplotlib as well. Most of the failures have to do with (ab) use of the C-API, for instance writing to a read-only pointer obtained from PyString_AsString().\n\nNote that the C-API compatibility layer is significantly slower than CPython, as explained in the blog post about the new strategy for reflection of C objects into the PyPy interpreter.\n\nWe updated cffi to version 1.7 (incremental changes which provide a nicer developer experience, documented here). We would encourage developers to move their C-extension modules to cffi, but are willing to help you work through issues with existing code; come to #pypy on IRC and let us know how we can help you help us do better.\n\nYou can download the PyPy2 v5.3 release here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for their continued support of the PyPy\nproject. We would also like to thank our contributors and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7 performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux\nbig- and little-endian variants of PPC64 running Linux\ns390x running Linux\n\n\n\nOther Highlights\n\n(since the release of PyPy 5.1 in April, 2016)\n\n\nNew features:\n\n\nMerge a major expansion of the C-API support in cpyext, also expand cpyext tests to allow running them after translation as well as untranslated\n\n\nInstead of \u201cGIL not held when a CPython C extension module\ncalls PyXxx\u201d, we now silently acquire/release the GIL.  Helps with\nC extension modules that call some PyXxx() functions without\nholding the GIL (arguably, they are theoretically buggy).\n\n\nSupport command line -v to trace import statements\n\n\nRevive traceviewer, a tool to use pygame to view traces\n\n\n\n\n\n\nNumpy via our internal _numpypy module:\n\nImplement ufunc.outer\nMove PyPy-specific numpypy headers to a subdirectory (also changed the repo\naccordingly)\n\n\u00a0\n\n\nPerformance improvements:\n\nUse bitstrings to compress lists of descriptors that are attached to an\nEffectInfo\nRemove most of the _ovf, _zer and _val operations from RPython.  Kills\nquite some code internally, and allows the JIT to do better\noptimizations: for example, app-level code like x / 2 or x % 2\ncan now be turned into x >> 1 or x & 1, even if x is possibly\nnegative.\nRework the way registers are moved/spilled in before_call()\n\n\n\n\nInternal refactorings:\n\nRefactor code to better support Python3-compatible syntax\nReduce the size of generated C sources during translation by\neliminating many many unused struct declarations (Issue #2281)\nReduce the size of generated code by using the same function objects in\nall generated subclasses\nShare cpyext Py* function wrappers according to the signature, shrinking the\ntranslated libpypy.so by about 10% (without the JIT)\n\n\n\nPlease update, and continue to help us make PyPy better.\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/06/pypy2-v53-released-major-c-extension-7708576047190172431.html"
    },
    {
      "title": "PyPy3.3 v5.2 alpha 1 released",
      "text": "We're pleased to announce the first alpha release of PyPy3.3 v5.2. This is the\nfirst release of PyPy which targets Python 3.3 (3.3.5) compatibility.We would like to thank all of the people who donated to the py3k proposal\nfor supporting the work that went into this and future releases.You can download the PyPy3.3 v5.2 alpha 1 release here:https://pypy.org/download.html#python-3-3-5-compatible-pypy3-3-v5-2HighlightsPython 3.3.5 support!Being an early alpha release, there are some missing features such as a\nPEP 393-like space efficient string representation and known issues\nincluding performance issues (e.g. issue #2305). The focus for this\nrelease has been updating to 3.3 compatibility. Windows is also not yet\nsupported.\n\nensurepip is also included (it's only included in CPython 3 >= 3.4).\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.10 and one day 3.3.5. It's fast due to its integrated tracing JIT\ncompiler.We also welcome developers of other dynamic languages to see what RPython\ncan do for them.This release supports:x86 machines on most common operating systems except Windows\n(Linux 32/64, Mac OS X 64, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\nPlease try it out and let us know what you think. We welcome feedback, we know\nyou are using PyPy, please tell us about it!We'd especially like to thank these people for their contributions to this\nrelease:Manuel Jacob, Ronan Lamy, Mark Young, Amaury Forgeot d'Arc, Philip Jenvey,\nMartin Matusiak, Vasily Kuznetsov, Matti Picus, Armin Rigo and many others.CheersThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/05/pypy33-v52-alpha-1-released-1725927506363370346.html"
    },
    {
      "title": "PyPy 5.1.1 bugfix released",
      "text": "We have released a bugfix for PyPy 5.1, due to a regression in installing third-party packages depending on numpy (using our numpy fork available at https://bitbucket.org/pypy/numpy ).Thanks to those who reported the issue. We also fixed a regression in translating PyPy which increased the memory required to translate. Improvement will be noticed by downstream packagers and those who translate rather thandownload pre-built binaries.\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.We also welcome developers of other dynamic languages to see what RPython can do for them.This release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\nPlease update, and continue to help us make PyPy better.CheersThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/05/pypy-511-bugfix-released-7586640750680293200.html"
    },
    {
      "title": "PyPy 5.1 released",
      "text": "We have released PyPy 5.1, about a month after PyPy 5.0.\n\nThis release includes more improvement to warmup time and memory requirements, extending the work done on PyPy 5.0. We have seen an additional reduction of about 20% in memory requirements, and up to 30% warmup time improvement, more detail in the blog post.\n\nWe also now have full support for the IBM s390x. Since this support is in RPython, any dynamic language written using RPython, like PyPy, will automagically be supported on that architecture.\n\nWe updated cffi to 1.6 (cffi 1.6 itself will be released shortly), and continue to improve support for the wider python ecosystem using the PyPy interpreter.\n\nYou can download the PyPy 5.1 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\nOther Highlights\n\n(since the release of PyPy 5.0 in March, 2016\n\n\n\nNew features:\n\nA new jit backend for the IBM s390x, which was a large effort over the past few months.\nAdd better support for PyUnicodeObject in the C-API compatibility layer\nSupport GNU/kFreeBSD Debian ports in vmprof\nAdd __pypy__._promote\nMake attrgetter a single type for CPython compatibility\n\n\n\n\nBug Fixes\n\nCatch exceptions raised in an exit function\nFix a corner case in the JIT\nFix edge cases in the cpyext refcounting-compatible semantics (more work on cpyext compatibility is coming in the cpyext-ext branch, but isn\u2019t ready yet)\nTry harder to not emit NEON instructions on ARM processors without NEON support\nImprove the rpython posix module system interaction function calls\nDetect a missing class function implementation instead of calling a random function\nCheck that PyTupleObjects do not contain any NULLs at the point of conversion to W_TupleObjects\nIn ctypes, fix _anonymous_ fields of instances\nFix JIT issue with unpack() on a Trace which contains half-written operations\nFix sandbox startup (a regression in 5.0)\nFix possible segfault for classes with mangled mro or __metaclass__\nFix isinstance(deque(), Hashable) on the pure python deque\nFix an issue with forkpty()\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\n\n\nNumpy:\n\nImplemented numpy.where for a single argument\nIndexing by a numpy scalar now returns a scalar\nFix transpose(arg) when arg is a sequence\nRefactor include file handling, now all numpy ndarray, ufunc, and umath functions exported from libpypy.so are declared in pypy_numpy.h, which is included only when building our fork of numpy\nAdd broadcast\n\n\n\n\nPerformance improvements:\n\nImprove str.endswith([tuple]) and str.startswith([tuple]) to allow JITting\nMerge another round of improvements to the warmup performance\nCleanup history rewriting in pyjitpl\nRemove the forced minor collection that occurs when rewriting the assembler at the start of the JIT backend\nPort the resource module to cffi\n\n\u00a0\n\n\nInternal refactorings:\n\nUse a simpler logger to speed up translation\nDrop vestiges of Python 2.5 support in testing\nUpdate rpython functions with ones needed for py3k\n\n\n\n\n\n\n\n\n\n\nPlease update, and continue to help us make PyPy better.\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/04/pypy-51-released-4979856639628970409.html"
    },
    {
      "title": "PyPy Enterprise Edition",
      "text": "With the latest additions, PyPy's JIT now supports the Z architecture on Linux. The newest architecture revision (also known as s390x, or colloquially referred to as \"big iron\") is the 64-bit extension for IBM mainframes. Currently only Linux 64 bit is supported (not z/OS nor TPF).\nThis is the fourth assembler backend supported by PyPy in addition to x86 (32 and 64), ARM (32-bit only) and PPC64 (both little- and big-endian). It might seem that we kind of get a hang of new architectures. Thanks to IBM for funding this work!\n\n\nHistory \nWhen I went to university one lecture covered the prediction of Thomas Watson in 1943. His famous quote \"I think there is a world market for maybe five computers ...\", turned out not to be true. \n\nHowever, even 70 years later, mainframes are used more often than you think. They back critical tasks requiring a high level of stability/security and offer high hardware and computational utilization rates by virtualization.\n\nWith the new PyPy JIT backend we are happy to present a fast Python virtual machine for mainframes and contribute more free software running on s390x.\n\n\nMeta tracing\nEven though the JIT backend has been tested on PyPy, it is not restricted to\u00a0 the Python programming language. Do you have a great idea for a DSL, or another language that should run on mainframes? Go ahead and just implement your interpreter using RPython.\n\n\nHow do I get a copy?\nPyPy can be built using the usual instructions found here. As soon as the next PyPy version has been released we will provide binaries. Until then you can just grab a nightly here.We are currently busy to get the next version of PyPy ready, so an official release will be rolled out soon.\n\n\nComparing s390x to x86\nThe goal of this comparison is not to scientifically evaluate the benefits/disadvantages on s390x, but rather to see that PyPy's architecture delivers the same benefits as it does on other platforms. Similar to the comparison done for PPC I ran the benchmarks using the same setup. The first column is the speedup of the PyPy JIT VM compared to the speedup of a pure PyPy interpreter 1). Note that the s390x's OS was virtualized.\n\n\u00a0 Label \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x86\u00a0\u00a0\u00a0\u00a0 s390x\u00a0\u00a0\u00a0\u00a0\u00a0 s390x (run 2)\n\n\u00a0 ai\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13.7 \u00a0\u00a0\u00a0\u00a0 12.4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 11.9\u00a0 bm_chameleon\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.5 \u00a0\u00a0\u00a0\u00a0\u00a0 6.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6.8\u00a0 bm_dulwich_log\u00a0\u00a0\u00a0\u00a0\u00a0 5.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.1\u00a0 bm_krakatau\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.5 \u00a0\u00a0\u00a0\u00a0\u00a0 2.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.0\u00a0 bm_mako\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.4 \u00a0\u00a0\u00a0\u00a0\u00a0 5.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.9\u00a0 bm_mdp\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.0 \u00a0\u00a0\u00a0\u00a0\u00a0 3.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.8\u00a0 chaos\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 56.9 \u00a0\u00a0\u00a0\u00a0 52.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 53.4\u00a0 crypto_pyaes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 62.5 \u00a0\u00a0\u00a0\u00a0 64.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 64.2\u00a0 deltablue\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.3 \u00a0\u00a0\u00a0\u00a0\u00a0 3.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.6\u00a0 django\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 28.8 \u00a0\u00a0\u00a0\u00a0 22.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 21.7\u00a0 eparse\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.3 \u00a0\u00a0\u00a0\u00a0\u00a0 2.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.6\u00a0 fannkuch\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.1 \u00a0\u00a0\u00a0\u00a0\u00a0 9.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10.1\u00a0 float\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13.8 \u00a0\u00a0\u00a0\u00a0 12.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13.8\u00a0 genshi_text\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 16.4 \u00a0\u00a0\u00a0\u00a0 10.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10.9\u00a0 genshi_xml\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2 \u00a0\u00a0\u00a0\u00a0\u00a0 7.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2\u00a0 go\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6.7 \u00a0\u00a0\u00a0\u00a0\u00a0 6.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 11.2\u00a0 hexiom2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 24.3 \u00a0\u00a0\u00a0\u00a0 23.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 23.5\u00a0 html5lib\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.4 \u00a0\u00a0\u00a0\u00a0\u00a0 5.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.7\u00a0 json_bench\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 28.8 \u00a0\u00a0\u00a0\u00a0 27.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 28.1\u00a0 meteor-contest\u00a0\u00a0\u00a0\u00a0\u00a0 5.1 \u00a0\u00a0\u00a0\u00a0\u00a0 4.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.4\u00a0 nbody_modified\u00a0\u00a0\u00a0\u00a0 20.6 \u00a0\u00a0\u00a0\u00a0 19.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 19.4\u00a0 pidigits\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.0 \u00a0\u00a0\u00a0\u00a0 -1.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -1.0\u00a0 pyflate-fast\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.0 \u00a0\u00a0\u00a0\u00a0\u00a0 8.7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.5\u00a0 pypy_interp\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.3 \u00a0 \u00a0\u00a0\u00a0 4.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.4\u00a0 raytrace-simple\u00a0\u00a0\u00a0 69.0 \u00a0\u00a0\u00a0 100.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 93.4\u00a0 richards\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 94.1 \u00a0\u00a0\u00a0\u00a0 96.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 84.3\u00a0 rietveld\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.2 \u00a0\u00a0\u00a0\u00a0\u00a0 2.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.7\u00a0 slowspitfire\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.8 \u00a0\u00a0\u00a0\u00a0\u00a0 3.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.2\u00a0 spambayes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.0 \u00a0\u00a0\u00a0\u00a0\u00a0 4.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.8\u00a0 spectral-norm\u00a0\u00a0\u00a0\u00a0\u00a0 41.9 \u00a0\u00a0\u00a0\u00a0 39.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 42.6\u00a0 spitfire\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.8 \u00a0\u00a0\u00a0\u00a0\u00a0 3.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.3\u00a0 spitfire_cstringio\u00a0 7.6 \u00a0\u00a0\u00a0\u00a0\u00a0 7.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2\u00a0 sympy_expand\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.9 \u00a0\u00a0\u00a0\u00a0\u00a0 1.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.8\u00a0 sympy_integrate\u00a0\u00a0\u00a0\u00a0 4.3 \u00a0\u00a0\u00a0\u00a0\u00a0 3.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.0\u00a0 sympy_str\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.5 \u00a0\u00a0\u00a0\u00a0\u00a0 1.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.3\u00a0 sympy_sum\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6.2 \u00a0\u00a0\u00a0\u00a0\u00a0 5.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.9\u00a0 telco\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 61.2 \u00a0\u00a0\u00a0\u00a0 48.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 54.8\u00a0 twisted_iteration\u00a0 55.5 \u00a0\u00a0\u00a0\u00a0 41.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 43.8\u00a0 twisted_names\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2 \u00a0\u00a0\u00a0\u00a0\u00a0 9.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.7\u00a0 twisted_pb\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 12.1 \u00a0\u00a0\u00a0\u00a0 10.4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10.2\u00a0 twisted_tcp\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.9 \u00a0\u00a0\u00a0\u00a0\u00a0 4.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.2\n\u00a0 Geometric mean:\u00a0\u00a0\u00a0 9.31\u00a0\u00a0\u00a0\u00a0\u00a0 9.10\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.43\n\nAs you can see the benefits are comparable on both platforms.\nOf course this is scientifically not good enough, but it shows a tendency. s390x can achieve the same results as you can get on x86. \n\nAre you running your business application on a mainframe? We would love to get some feedback. Join us in IRC tell us if PyPy made your application faster! \n\nplan_rich & the PyPy Team\n\n1) PyPy revision for the benchmarks: 4b386bcfee54",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/04/pypy-enterprise-edition-3688275697656890948.html"
    },
    {
      "title": "Warmup improvements: more efficient trace representation",
      "text": "Hello everyone.\nI'm pleased to inform that we've finished another round of\nimprovements to the warmup performance of PyPy. Before I go\ninto details, I'll recap the achievements that we've done since we've started\nworking on the warmup performance. I picked a random PyPy from November 2014\n(which is definitely before we started the warmup work) and compared it with\na recent one, after 5.0. The exact revisions are respectively ffce4c795283\nand cfbb442ae368. First let's compare pure warmup benchmarks that\ncan be found in our benchmarking suite. Out of those,\npypy-graph-alloc-removal numbers should be taken with a grain of salt,\nsince other work could have influenced the results.\nThe rest of the benchmarks mentioned is bottlenecked purely by warmup times.\nYou can see how much your program spends in warmup running\nPYPYLOG=jit-summary:- pypy your-program.py under \"tracing\" and \"backend\"\nfields (in the first three lines). An example looks like that:\n\n[e00c145a41] {jit-summary\nTracing:        71      0.053645 <- time spent tracing & optimizing\nBackend:        71      0.028659 <- time spent compiling to assembler\nTOTAL:                  0.252217 <- total run time of the program\n\nThe results of the benchmarks\n\n\n\n\n\n\n\n\n\n\nbenchmark\ntime - old\ntime - new\nspeedup\nJIT time - old\nJIT time - new\n\nfunction_call\n1.86\n1.42\n1.3x\n1.12s\n0.57s\n\nfunction_call2\n5.17s\n2.73s\n1.9x\n4.2s\n1.6s\n\nbridges\n2.77s\n2.07s\n1.3x\n1.5s\n0.8s\n\npypy-graph-alloc-removal\n2.06s\n1.65s\n1.25x\n1.25s\n0.79s\n\n\n\nAs we can see, the overall warmup benchmarks got up to 90% faster with\nJIT time dropping by up to 2.5x. We have more optimizations in the pipeline,\nwith an idea how to transfer some of the JIT gains into more of a total program\nruntime by jitting earlier and more eagerly.\n\nDetails of the last round of optimizations\nNow the nitty gritty details - what did we actually do? I covered a lot of\nwarmup improvements in the past blog posts so I'm going to focus on\nthe last change, the jit-leaner-frontend branch. This last change is simple, instead of using\npointers to store the \"operations\" objects created during tracing, we use a compact list of\n16-bit integers (with 16bit pointers in between). On 64bit machine the memory wins are\ntremendous - the new representation is 4x more efficient to use 16bit pointers than full 64bit pointers.\nAdditionally, the smaller representation has much better cache behavior and much less\npointer chasing in memory. It also has a better defined lifespan, so we don't need to\nbother tracking them by the GC, which also saves quite a bit of time.\nThe change sounds simple, but the details in the underlaying data mean that\neverything in the JIT had to be changed which took quite a bit of effort :-)\nGoing into the future on the JIT front, we have an exciting set of optimizations,\nranging from faster loops through faster warmup to using better code generation\ntechniques and broadening the kind of program that PyPy speeds up. Stay tuned\nfor the updates.\nWe would like to thank our commercial partners for making all of this possible.\nThe work has been performed by baroquesoftware and would not be possible\nwithout support from people using PyPy in production. If your company uses\nPyPy and want it to do more or does not use PyPy but has performance problems\nwith the Python installation, feel free to get in touch with me, trust me using\nPyPy ends up being a lot cheaper than rewriting everything in go :-)\nBest regards,\nMaciej Fijalkowski",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/04/warmup-improvements-more-efficient-7082900097299909512.html"
    },
    {
      "title": "PyPy 5.0.1 bugfix released",
      "text": "PyPy 5.0.1\n\nWe have released a bugfix for PyPy 5.0, after reports that the newly released\nlxml 3.6.0, which now supports PyPy 5.0 +, can crash on large files.\nThanks to those who reported the crash. Please update, downloads are available\nat\n\npypy.org/download.html\n\n\nThe changes between PyPy 5.0 and 5.0.1 are only two bug fixes: one in\ncpyext, which fixes notably (but not only) lxml; and another for a\ncorner case of the JIT.\n\n\nWhat is PyPy?\n\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\nWe also welcome developers of other\ndynamic languages to see what RPython can do for them.\n\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux, and the\nbig- and little-endian variants of PPC64 running Linux.\n\n\nPlease update, and continue to help us make PyPy better.\n\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/03/pypy-501-bugfix-released-2218405735970044084.html"
    },
    {
      "title": "PyPy 5.0 released",
      "text": "PyPy 5.0\nWe have released PyPy 5.0, about three months after PyPy 4.0.1. We encourage all users of PyPy to update to this version.\n\nYou can download the PyPy 5.0 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\u00a0\n\nFaster and Leaner\n\nWe continue to improve the warmup time and memory usage of JIT-related metadata. The exact effects depend vastly on the program you\u2019re running and can range from insignificant to warmup being up to 30% faster and memory dropping by about 30%. \n\n\u00a0\n\nC-API Upgrade\n\nWe also merged a major upgrade to our C-API layer (cpyext), simplifying the interaction between c-level objects and PyPy interpreter level objects. As a result, lxml  (prerelease) with its cython compiled component passes all tests on PyPy. The new cpyext is also much faster. This major refactoring will soon be followed by an expansion of our C-API compatibility.\n\n\u00a0\n\nProfiling with vmprof supported on more platforms\n\n\nvmprof has been a go-to profiler for PyPy on linux for a few releases and we\u2019re happy to announce that thanks to the cooperation with jetbrains, vmprof now works on Linux, OS X and Windows on both PyPy and CPython.\n\n\n\u00a0\n\nCFFI\nWhile not applicable only to PyPy, cffi is arguably our most significant contribution to the python ecosystem. PyPy 5.0 ships with cffi-1.5.2 which now allows embedding PyPy (or CPython) in a C program.\n\n\n\u00a0\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd), newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux, and 64 bit PowerPC hardware, specifically Linux running the big- and little-endian variants of ppc64.\n\n\n\u00a0\n\nOther Highlights (since 4.0.1 released in November 2015)\n\nNew features:\nSupport embedding PyPy in a C-program via cffi and static callbacks in cffi.\nThis deprecates the old method of embedding PyPy\nRefactor vmprof to work cross-operating-system, deprecate using buggy\nlibunwind on Linux platforms. Vmprof even works on Windows now.\nSupport more of the C-API type slots, like tp_getattro, and fix C-API\nmacros, functions, and structs such as _PyLong_FromByteArray(),\nPyString_GET_SIZE, f_locals in PyFrameObject, Py_NAN, co_filename in\nPyCodeObject\nUse a more stable approach for allocating PyObjects in cpyext. (see\nblog post). Once the PyObject corresponding to a PyPy object is created,\nit stays around at the same location until the death of the PyPy object.\nDone with a little bit of custom GC support.  It allows us to kill the\nnotion of \u201cborrowing\u201d inside cpyext, reduces 4 dictionaries down to 1, and\nsignificantly simplifies the whole approach (which is why it is a new\nfeature while technically a refactoring) and allows PyPy to support the\npopulart lxml module (as of the next release) with no PyPy specific\npatches needed\nMake the default filesystem encoding ASCII, like CPython\nUse hypothesis in test creation, which is great for randomizing tests\n\n\u00a0\n\n\nBug Fixes\nBackport always using os.urandom for uuid4 from cpython and fix the JIT as well\n(issue #2202)\nMore completely support datetime, optimize timedelta creation\nFix for issue #2185 which caused an inconsistent list of operations to be\ngenerated by the unroller, appeared in a complicated DJango app\nFix an elusive issue with stacklets on shadowstack which showed up when\nforgetting stacklets without resuming them\nFix entrypoint() which now acquires the GIL\nFix direct_ffi_call() so failure does not bail out before setting CALL_MAY_FORCE\nFix (de)pickling long values by simplifying the implementation\nFix RPython rthread so that objects stored as threadlocal do not force minor\nGC collection and are kept alive automatically. This improves perfomance of\nshort-running Python callbacks and prevents resetting such object between\ncalls\nSupport floats as parameters to itertools.isslice()\nCheck for the existence of CODESET, ignoring it should have prevented PyPy\nfrom working on FreeBSD\nFix for corner case (likely shown by Krakatau) for consecutive guards with\ninterdependencies\nFix applevel bare class method comparisons which should fix pretty printing\nin IPython\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\u00a0\n\n\nNumpy:\nUpdates to numpy 1.10.2 (incompatibilities and not-implemented features\nstill exist)\nSupport dtype=((\u2018O\u2019, spec)) union while disallowing record arrays with\nmixed object, non-object values\nRemove all traces of micronumpy from cpyext if \u2013withoutmod-micronumpy option used\nSupport indexing filtering with a boolean ndarray\nSupport partition() as an app-level function, together with a cffi wrapper\nin pypy/numpy, this now provides partial support for partition()\n\n\u00a0\n\n\nPerformance improvements:\nOptimize global lookups\nImprove the memory signature of numbering instances in the JIT. This should\nmassively decrease the amount of memory consumed by the JIT, which is\nsignificant for most programs. Also compress the numberings using variable-\nsize encoding\nOptimize string concatenation\nUse INT_LSHIFT instead of INT_MUL when possible\nImprove struct.unpack by casting directly from the underlying buffer.\nUnpacking floats and doubles is about 15 times faster, and integer types\nabout 50% faster (on 64 bit integers). This was then subsequently\nimproved further in optimizeopt.py.\nOptimize two-tuple lookups in mapdict, which improves warmup of instance\nvariable access somewhat\nReduce all guards from int_floordiv_ovf if one of the arguments is constant\nIdentify permutations of attributes at instance creation, reducing the\nnumber of bridges created\nGreatly improve re.sub() performance\n\n\u00a0\n\n\nInternal refactorings:\nRefactor and improve exception analysis in the annotator\nRemove unnecessary special handling of space.wrap().\nSupport list-resizing setslice operations in RPython\nTweak the trace-too-long heuristic for multiple jit drivers\nRefactor bookkeeping (such a cool word - three double letters) in the\nannotater\nRefactor wrappers for OS functions from rtyper to rlib and simplify them\nSimplify backend loading instructions to only use four variants\nSimplify GIL handling in non-jitted code\nRefactor naming in optimizeopt\nChange GraphAnalyzer to use a more precise way to recognize external\nfunctions and fix null pointer handling, generally clean up external\nfunction handling\nRemove pure variants of getfield_gc_* operations from the JIT by\ndetermining purity while tracing\nRefactor databasing\nSimplify bootstrapping in cpyext\nRefactor rtyper debug code into python.rtyper.debug\nSeperate structmember.h from Python.h Also enhance creating api functions\nto specify which header file they appear in (previously only pypy_decl.h)\nFix tokenizer to enforce universal newlines, needed for Python 3 support\n\n\n\nPlease try it out and let us know what you think. We welcome feedback, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/03/pypy-50-released-5730569530415927220.html"
    },
    {
      "title": "C-API Support update",
      "text": "As you know, PyPy can emulate the CPython C API to some extent. In this post I will describe an important optimization that we merged to improve the performance and stability of the C-API emulation layer.\n\nThe C-API is implemented by passing around PyObject * pointers in the C code.  The problem with providing the same interface with PyPy is that\nobjects don't natively have the same PyObject * structure at all; and\nadditionally their memory address can change.  PyPy handles the\ndifference by maintaining two sets of objects.  More precisely, starting\nfrom a PyPy object, it can allocate on demand a PyObject structure\nand fill it with information that points back to the original PyPy\nobjects; and conversely, starting from a C-level object, it can allocate\na PyPy-level object and fill it with information in the opposite\ndirection.\n\nI have merged a rewrite of the interaction between C-API C-level objects\nand PyPy's interpreter level objects.  This is mostly a simplification\nbased on a small hack in our garbage collector.  This hack makes the\ngarbage collector aware of the reference-counted PyObject\nstructures.  When it considers a pair consisting of a PyPy object and a\nPyObject, it will always free either none or both of them at the\nsame time.  They both stay alive if either there is a regular GC\nreference to the PyPy object, or the reference counter in the\nPyObject is bigger than zero.\n\nThis gives a more stable result.  Previously, a PyPy object might grow a\ncorresponding PyObject, loose it (when its reference counter goes to\nzero), and later have another corresponding PyObject re-created at a\ndifferent address.  Now, once a link is created, it remains alive until\nboth objects die.\n\nThe rewrite significantly simplifies our previous code (which used to be\nbased on at least 4 different dictionaries), and should make using the\nC-API somewhat faster (though it is still slower than using pure\npython or cffi).\n\nA side effect of this work is that now PyPy actually supports the upstream lxml package---which is is one\nof the most popular packages on PyPI.  (Specifically, you need version\n3.5.0 with this pull\nrequest to remove old PyPy-specific hacks that were not really\nworking.  See\ndetails.)  At this point, we no longer recommend using the\ncffi-lxml alternative: although it may still be faster, it might be\nincomplete and old.\n\nWe are actively working on extending our C-API support, and hope to soon\nmerge a branch to support more of the C-API functions (some numpy news\ncoming!).  Please try\nit out and let us know how it works for you.\n\nArmin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/02/c-api-support-update-8582726091670983181.html"
    },
    {
      "title": "Using CFFI for embedding",
      "text": "Introduction\n\nCFFI has been a great success so far to call C libraries in your\nPython programs, in a way that is both simple and that works across\nCPython 2.x and 3.x and PyPy.\n\nThis post assumes that you know what CFFI is and how to use it in\nAPI mode (ffi.cdef(), ffi.set_source(), ffi.compile()).\nA quick overview can be found in this paragraph.\n\nThe major news of CFFI 1.4, released last december, was that you can\nnow declare C functions with extern \"Python\" in the cdef().\nThese magic keywords make the function callable from C (where it is\ndefined automatically), but calling it will call some Python code\n(which you attach with the @ffi.def_extern() decorator).  This is\nuseful because it gives a more straightforward, faster and\nlibffi-independent way to write callbacks.  For more details, see the\ndocumentation.\n\nYou are, in effect, declaring a static family of C functions which\ncall Python code.  The idea is to take pointers to them, and pass them\naround to other C functions, as callbacks.  However, the idea of a set\nof C functions which call Python code opens another path: embedding\nPython code inside non-Python programs.\n\nEmbedding\n\nEmbedding is traditionally done using the CPython C API: from C code,\nyou call Py_Initialize() and then some other functions like\nPyRun_SimpleString().  In the simple cases it is, indeed, simple\nenough; but it can become a complicated story if you throw in\nsupporting application-dependent object types; and a messy story if\nyou add correctly running on multiple threads, for example.\nMoreover, this approach is specific to CPython (2.x or 3.x).  It does\nnot work at all on PyPy, which has its own very different, minimal\nembedding API.\n\nThe new-and-coming thing about CFFI 1.5, meant as replacement of the\nabove solutions, is direct embedding support---with no fixed API at\nall.  The idea is to write some Python script with a cdef() which\ndeclares a number of extern \"Python\" functions.  When running the\nscript, it creates the C source code and compiles it to a\ndynamically-linked library (.so on Linux).  This is the same as in\nthe regular API-mode usage.  What is new is that these extern\n\"Python\" can now also be exported from the .so, in the C\nsense.  You also give a bit of initialization-time Python code\ndirectly in the script, which will be compiled into the .so too.\nThis library can now be used directly from any C program (and it is\nstill importable in Python).  It exposes the C API of your choice,\nwhich you specified with the extern \"Python\" declarations.  You\ncan use it to make whatever custom API makes sense in your particular\ncase.  You can even directly make a \"plug-in\" for any program that\nsupports them, just by exporting the API expected for such plugins.\n\nTrying it out on CPython\n\nThis is still being finalized, but please try it out.  You can\nsee embedding.py directly online for a quick glance.  Or\nsee below the instructions on Linux with CPython 2.7 (CPython 3.x and\nnon-Linux platforms are still a work in progress right now, but this\nshould be quickly fixed):\n\nget the branch static-callback-embedding of CFFI:\n\nhg clone https://foss.heptapod.net/cffi/cffi\nhg up static-callback-embedding\n\n\nmake the _cffi_backend.so:\n\npython setup_base.py build_ext -f -i\n\n\nrun embedding.py in the demo directory:\n\ncd demo\nPYTHONPATH=.. python embedding.py\n\n\nthis produces _embedding_cffi.c.  Run gcc to build it.  On Linux:\n\ngcc -shared -fPIC _embedding_cffi.c -o _embedding_cffi.so  \\\n    -lpython2.7 -I/usr/include/python2.7\n\n\ntry out the demo C program in embedding_test.c:\n\ngcc embedding_test.c _embedding_cffi.so\nPYTHONPATH=.. LD_LIBRARY_PATH=. ./a.out\n\n\n\nNote that if you get ImportError: cffi extension module\n'_embedding_cffi' has unknown version 0x2701, it means that the\n_cffi_backend module loaded is a pre-installed one instead of the\nmore recent one in \"..\".  Be sure to use PYTHONPATH=.. for now.  (Some installations manage to be confused enough to load the system-wide cffi even if another version is in the PYTHONPATH.  I think a virtualenv can be used to work around this issue.)\n\nTry it out on PyPy\n\nVery similar steps can be followed on PyPy, but it requires the\ncffi-static-callback-embedding branch of PyPy, which you must\nfirst translate from sources.  The difference is then that you need to\nadapt the first gcc command line: replace -lpython2.7 with\n-lpypy-c and to fix the -I path (and possibly add a -L\npath).\n\nMore details\n\nHow it works, more precisely, is by automatically initializing CPython/PyPy\nthe first time any of the extern \"Python\"\nfunctions is called from the C program.  This is done using locks in case of multi-threading,\nso several threads can concurrently do this \"first call\".  This should work even if two\ndifferent threads call the first time a function from two different\nembedded CFFI extensions that happen to be linked with the same program.  Explicit initialization is\nnever needed.\n\nThe custom initialization-time Python code you put in\nffi.embedding_init_code() is executed at that time.  If this code\nstarts to be big, you can move it to independent modules or packages.\nThen the initialization-time Python code only needs to import them.  In\nthat case, you have to carefully set up sys.path if the modules are\nnot installed in the usual Python way.\nIf the Python code is big and full of dependencies, a better alternative\nwould be to use virtualenv.  How to do that is not fully fleshed out so\nfar.  You can certainly run the whole program with the environment\nvariables set up by the virtualenv's activate script first.  There\nare probably other solutions that involve using gcc's\n-Wl,-rpath=\\$ORIGIN/ or -Wl,-rpath=/fixed/path/ options to load\na specific libpython or libypypy-c library.  If you try it out and it\ndoesn't work the way you would like, please complain :-)\nAnother point: right now this does not support CPython's notion of\nmultiple subinterpreters.  The logic creates a single global Python\ninterpreter, and runs everything in that context.  Maybe a future\nversion would have an explicit API to do that \u2014 or maybe it should be\nthe job of a 3rd-party extension module to provide a Python interface\nover the notion of subinterpreters...\nMore generally, any feedback is appreciated.\nHave fun,\nArmin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/01/using-cffi-for-embedding-8493496761738752124.html"
    },
    {
      "title": "Leysin Winter Sprint (20-27th February 2016)",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the eleventh time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\n\nGoals and topics of the sprint\nThe details depend on who is here and ready to work.  The list of\ntopics is mostly the same as last year (did PyPy became a mature\nproject with only long-term goals?):\n\ncpyext (CPython C API emulation layer): various speed and\ncompleteness topics\ncleaning up the optimization step in the JIT, change the register\nallocation done by the JIT's backend, or more improvements to the\nwarm-up time\nfinish vmprof - a statistical profiler for CPython and PyPy\nPy3k (Python 3.x support), NumPyPy (the numpy module)\nSTM (Software Transaction Memory), notably: try to come up with\nbenchmarks, and measure them carefully in order to test and improve\nthe conflict reporting tools, and more generally to figure out how\npractical it is in large projects to avoid conflicts\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski.\n\n\n\nExact times\nI have booked the week from Saturday 20 to Saturday 27.  It is fine to\nleave either the 27 or the 28, or even stay a few\nmore days on either side.  The plan is to work full days between the 21\nand the 27.  You are of course allowed to show up for a part of that\ntime only, too.\n\n\nLocation & Accomodation\nLeysin, Switzerland, \"same place as before\".  Let me refresh your\nmemory: both the sprint venue and the lodging will be in a\npair of chalets built specifically for bed & breakfast:\nhttps://www.ermina.ch/.  The place has a good ADSL Internet connection\nwith wireless installed.  You can also arrange your own lodging\nelsewhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue).\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.\nThe options of rooms are a bit more limited than on previous years\nbecause the place for bed-and-breakfast is shrinking: what is\nguaranteed is only one double-bed room and a bigger room with 5-6\nindividual beds (the latter at 50-60 CHF per night, breakfast\nincluded).  If there are more people that would prefer a single room,\nplease contact me and we'll see what choices you have.  There are a\nchoice of hotels, many of them reasonably priced for Switzerland.\nPlease register by Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/leysin-winter-2016\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around, and at least one EU-format power strip.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/01/leysin-winter-sprint-20-27th-february-1737200016169608469.html"
    },
    {
      "title": "PyPy 4.0.1 released please update",
      "text": "PyPy 4.0.1\n\nWe have released PyPy 4.0.1, three weeks after PyPy 4.0.0. We have fixed a few critical bugs in the JIT compiled code, reported by users. We therefore encourage all users of PyPy to update to this version. There are a few minor enhancements in this version as well.\n\nYou can download the PyPy 4.0.1 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n \n\n\n\u00a0\n\nCFFI update\n\nWhile not applicable only to PyPy, cffi is arguably our most significant contribution to the python ecosystem. PyPy 4.0.1 ships with cffi-1.3.1 with the improvements it brings.\n\n\n\u00a0\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd), newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux, and the big- and little-endian variants of ppc64 running Linux.\n\n\n\u00a0\n\nOther Highlights (since 4.0.0 released three weeks ago)\n\n\n\nBug Fixes\nFix a bug when unrolling double loops in JITted code\nFix multiple memory leaks in the ssl module, one of which affected CPython as well (thanks to Alex Gaynor for pointing those out)\nUse pkg-config to find ssl headers on OS-X\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\nNew features\nInternal cleanup of RPython class handling\nSupport stackless and greenlets on PPC machines\nImprove debug logging in subprocesses: use PYPYLOG=jit:log.%d for example to have all subprocesses write the JIT log to a file called \u2018log.%d\u2019, with \u2018%d\u2019 replaced with the subprocess\u2019 PID.\nSupport PyOS_double_to_string in our cpyext capi compatibility layer\n\n\nNumpy\nImprove support for __array_interface__\nPropagate most NAN mantissas through float16-float32-float64 conversions\n\n\nPerformance improvements and refactorings\nImprovements in slicing byte arrays\nImprovements in enumerate()\nSilence some warnings while translating\n\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers \nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/11/pypy-401-released-please-update-2652340737298251005.html"
    },
    {
      "title": "PyPy 4.0.0 Released - A Jit with SIMD Vectorization and More",
      "text": "PyPy 4.0.0\nWe\u2019re pleased and proud to unleash PyPy 4.0.0, a major update of the PyPy python 2.7.10 compatible interpreter with a Just In Time compiler. We have improved warmup time and memory overhead used for tracing, added vectorization for numpy and general loops where possible on x86 hardware (disabled by default), refactored rough edges in rpython, and increased functionality of numpy.\nYou can download the PyPy 4.0.0 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors (7 new ones since PyPy 2.6.0) and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on PyPy, or general help with making RPython\u2019s JIT even better.\n\n\nNew Version Numbering\n\n\nSince the past release, PyPy 2.6.1, we decided to update the PyPy 2.x.x versioning directly to PyPy 4.x.x, to avoid confusion with CPython 2.7 and 3.5. Note that this version of PyPy uses the stdlib and implements the syntax of CPython 2.7.10.\n\n\nVectorization\n\n\nRichard Plangger began work in March and continued over a Google Summer of Code to add a vectorization step to the trace optimizer. The step recognizes common constructs and emits SIMD code where possible, much as any modern compiler does. This vectorization happens while tracing running code,  so it is actually easier at run-time to determine the availability of possible vectorization than it is for ahead-of-time compilers.\nAvailability of SIMD hardware is detected at run time, without needing to precompile various code paths into the executable.\nThe first version of the vectorization has been merged in this release, since it is so new it is off by default. To enable the vectorization in built-in JIT drivers (like numpy ufuncs), add \u2013jit vec=1, to enable all implemented vectorization add \u2013jit vec_all=1\nBenchmarks and a summary of this work appear here\n\n\nInternal Refactoring: Warmup Time Improvement and Reduced Memory Usage\n\n\nMaciej Fijalkowski and Armin Rigo refactored internals of Rpython that now allow PyPy to more efficiently use guards in jitted code. They also rewrote unrolling, leading to a warmup time improvement of 20% or so. The reduction in guards also means a reduction in the use of memory, also a savings of around 20%.\n\n\n\nNumpy\n\nOur implementation of numpy continues to improve. ndarray and the numeric dtypes are very close to feature-complete; record, string and unicode dtypes are mostly supported.  We have reimplemented numpy linalg, random and fft as cffi-1.0 modules that call out to the same underlying libraries that upstream numpy uses. Please try it out, especially using the new vectorization (via \u2013jit vec=1 on the command line) and let us know what is missing for your code.\n\n\n\nCFFI\n\nWhile not applicable only to PyPy, cffi is arguably our most significant contribution to the python ecosystem. Armin Rigo continued improving it, and PyPy reaps the benefits of cffi-1.3: improved manangement of object lifetimes, __stdcall on Win32, ffi.memmove(), and percolate const, restrict keywords from cdef to C code.\n\n\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd), as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\nWe also introduce support for the 64 bit PowerPC hardware, specifically Linux running the big- and little-endian variants of ppc64.\n\n\nOther Highlights (since 2.6.1 release two months ago)\n\nBug Fixes\nApplied OPENBSD downstream fixes\nFix a crash on non-linux when running more than 20 threads\nIn cffi, ffi.new_handle() is more cpython compliant\nAccept unicode in functions inside the _curses cffi backend exactly like cpython\nFix a segfault in itertools.islice()\nUse gcrootfinder=shadowstack by default, asmgcc on linux only\nFix ndarray.copy() for upstream compatability when copying non-contiguous arrays\nFix assumption that lltype.UniChar is unsigned\nFix a subtle bug with stacklets on shadowstack\nImprove support for the cpython capi in cpyext (our capi compatibility layer). Fixing these issues inspired some thought about cpyext in general, stay tuned for more improvements\nWhen loading dynamic libraries, in case of a certain loading error, retry loading the library assuming it is actually a linker script, like on Arch and Gentoo\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\nNew features:\nAdd an optimization pass to vectorize loops using x86 SIMD intrinsics.\nSupport __stdcall on Windows in CFFI\nImprove debug logging when using PYPYLOG=???\nDeal with platforms with no RAND_egd() in OpenSSL\n\n\nNumpy:\nAdd support for ndarray.ctypes\nFast path for mixing numpy scalars and floats\nAdd support for creating Fortran-ordered ndarrays\nFix casting failures in linalg (by extending ufunc casting)\nRecognize and disallow (for now) pickling of ndarrays with objects embedded in them\n\n\nPerformance improvements and refactorings:\nReuse hashed keys across dictionaries and sets\nRefactor JIT interals to improve warmup time by 20% or so at the cost of a minor regression in JIT speed\nRecognize patterns of common sequences in the JIT backends and optimize them\nMake the garbage collecter more incremental over external_malloc() calls\nShare guard resume data where possible which reduces memory usage\nFast path for zip(list, list)\nReduce the number of checks in the JIT for lst[a:]\nMove the non-optimizable part of callbacks outside the JIT\nFactor in field immutability when invalidating heap information\nUnroll itertools.izip_longest() with two sequences\nMinor optimizations after analyzing output from vmprof and trace logs\nRemove many class attributes in rpython classes\nHandle getfield_gc_pure* and getfield_gc_* uniformly in heap.py\nImprove simple trace function performance by lazily calling fast2locals and locals2fast only if truly necessary \n\n\n\n\n\n\n\n\nPlease try it out and let us know what you think. We welcome feedback, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/10/pypy-400-released-jit-with-simd-8282134928733384063.html"
    },
    {
      "title": "Automatic SIMD vectorization support in PyPy",
      "text": "Hi everyone,\n\nit took some time to catch up with the JIT refacrtorings merged in this summer. But, (drums) we are happy to announce that:\n\n\nThe next release of PyPy,\u00a0 \"PyPy 4.0.0\", will ship the new auto vectorizer\nThe goal of this project was to increase the speed of numerical applications in both the NumPyPy library and for arbitrary Python programs. In PyPy we have focused a lot on improvements in the 'typical python workload',  which usually involves object and string manipulations, mostly for web development. We're hoping with this work that we'll continue improving the other very important Python use case - numerics.\n\n\nWhat it can do! \nIt targets numerics only. It \nwill not execute object manipulations faster, but it is capable of \nenhancing common vector and matrix operations.\nGood news is that it is not specifically targeted for the NumPy library and the PyPy \nvirtual machine. Any interpreter (written in RPython) is able make use \nof the vectorization. For more information about that take a look here, or consult the documentation. For the time being it is not turn on by default, so be sure to enable it by specifying --jit vec=1\u00a0before running your program.\n\nIf your language (written in RPython) contains many array/matrix operations, you can easily integrate the optimization by adding the parameter 'vec=1' to the JitDriver.\n\n\nNumPyPy Improvements\n\nLet's take a look at the core functions of the NumPyPy library (*). \nThe following tests tests show the speedup of the core functions commonly used in Python code interfacing with NumPy, on CPython with NumPy, on the PyPy 2.6.1 relased several weeks ago, and on PyPy 15.11 to be released soon. Timeit was used to test the time needed to run the operation in the plot title on various vector (lower case) and square matrix (upper case) sizes displayed on the X axis. The Y axis shows the speedup compared to CPython 2.7.10. This means that higher is better.\u00a0\n\n\n\n\n\n\n\n\nIn comparison to PyPy 2.6.1, the speedup greatly improved. The hardware support really strips down the runtime of the vector and matrix operations. There is another operation we would like to highlight: the dot product.\nIt is a very common operation in numerics and PyPy now (given a moderate sized matrix and vector) decreases the time spent in that operation. See for yourself:\n\n\n\n\n\nThese are nice improvements in the NumPyPy library and we got to a competitive level only making use of SSE4.1.\n\n\nFuture work\u00a0\u00a0 \n\nThis is not the end of the road. The GSoC project showed that it is possible to implement this optimization in PyPy. There might be other improvements we can make to carry this further:\n\nCheck alignment at runtime to increase the memory throughput of the CPU\nSupport the AVX vector extension which (at least) doubles the size of the vector register\nHandle each and every corner case in Python traces to enable it\u00a0 globally\nDo not rely only on loading operations to trigger the analysis, there might be cases where combination of floating point values could be done in parallel \n\nCheers,\nThe PyPy Team\n\n(*) The benchmark code can be found here it was run using this configuration: i7-2600 CPU @ 3.40GHz (4 cores).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/10/automatic-simd-vectorization-support-in-639063580401330508.html"
    },
    {
      "title": "PowerPC backend for the JIT",
      "text": "Hi all,\n\nPyPy's JIT now supports the 64-bit PowerPC architecture!  This is the\nthird architecture supported, in addition to x86 (32 and 64) and ARM\n(32-bit only).  More precisely, we support Linux running the big- and the\nlittle-endian variants of ppc64.  Thanks to IBM for funding this work!\n\nThe new JIT backend has been merged into \"default\".  You should be able\nto translate PPC versions\nas usual\ndirectly on the machines.  For\nthe foreseeable future, I will compile and distribute binary versions\ncorresponding to the official releases (for Fedora), but of course I'd\nwelcome it if someone else could step in and do it.  Also, it is unclear\nyet if we will run a buildbot.\n\nTo check that the result performs well, I logged in a ppc64le machine\nand ran the usual benchmark suite of PyPy (minus sqlitesynth: sqlite\nwas not installed on that machine).  I ran it twice at a difference of\n12 hours, as an attempt to reduce risks caused by other users suddenly\nusing the machine.  The machine was overall relatively quiet.  Of\ncourse, this is scientifically not good enough; it is what I could come\nup with given the limited resources.\n\nHere are the results, where the numbers are speed-up factors between the\nnon-jit and the jit version of PyPy.  The first column is x86-64, for\nreference.  The second and third columns are the two ppc64le runs.  All\nare Linux.  A few benchmarks are not reported here because the runner\ndoesn't execute them on non-jit (however, apart from sqlitesynth, they\nall worked).\n\n\n    ai                        13.7342        16.1659     14.9091\n    bm_chameleon               8.5944         8.5858        8.66\n    bm_dulwich_log             5.1256         5.4368      5.5928\n    bm_krakatau                5.5201         2.3915      2.3452\n    bm_mako                    8.4802         6.8937      6.9335\n    bm_mdp                     2.0315         1.7162      1.9131\n    chaos                     56.9705        57.2608     56.2374\n    sphinx\n    crypto_pyaes               62.505         80.149     79.7801\n    deltablue                  3.3403         5.1199      4.7872\n    django                    28.9829         23.206       23.47\n    eparse                     2.3164         2.6281       2.589\n    fannkuch                   9.1242        15.1768     11.3906\n    float                     13.8145        17.2582     17.2451\n    genshi_text               16.4608        13.9398     13.7998\n    genshi_xml                 8.2782         8.0879      9.2315\n    go                         6.7458        11.8226     15.4183\n    hexiom2                   24.3612        34.7991     33.4734\n    html5lib                   5.4515         5.5186       5.365\n    json_bench                28.8774        29.5022     28.8897\n    meteor-contest             5.1518         5.6567      5.7514\n    nbody_modified            20.6138        22.5466     21.3992\n    pidigits                   1.0118          1.022      1.0829\n    pyflate-fast               9.0684        10.0168     10.3119\n    pypy_interp                3.3977         3.9307      3.8798\n    raytrace-simple           69.0114       108.8875    127.1518\n    richards                  94.1863       118.1257    102.1906\n    rietveld                   3.2421         3.0126      3.1592\n    scimark_fft\n    scimark_lu\n    scimark_montecarlo\n    scimark_sor\n    scimark_sparsematmul\n    slowspitfire               2.8539         3.3924      3.5541\n    spambayes                  5.0646         6.3446       6.237\n    spectral-norm             41.9148        42.1831     43.2913\n    spitfire                   3.8788         4.8214       4.701\n    spitfire_cstringio          7.606         9.1809      9.1691\n    sqlitesynth\n    sympy_expand               2.9537         2.0705      1.9299\n    sympy_integrate            4.3805         4.3467      4.7052\n    sympy_str                  1.5431         1.6248      1.5825\n    sympy_sum                  6.2519          6.096      5.6643\n    telco                     61.2416        54.7187     55.1705\n    trans2_annotate\n    trans2_rtype\n    trans2_backendopt\n    trans2_database\n    trans2_source\n    twisted_iteration         55.5019        51.5127     63.0592\n    twisted_names              8.2262         9.0062      10.306\n    twisted_pb                12.1134         13.644     12.1177\n    twisted_tcp                4.9778          1.934      5.4931\n\n    GEOMETRIC MEAN               9.31           9.70       10.01\n\n\nThe last line reports the geometric mean of each column.  We see that\nthe goal was reached: PyPy's JIT actually improves performance by a\nfactor of around 9.7 to 10 times on ppc64le.  By comparison, it \"only\"\nimproves performance by a factor 9.3 on Intel x86-64.  I don't know why,\nbut I'd guess it mostly means that a non-jitted PyPy performs slightly\nbetter on Intel than it does on PowerPC.\n\nWhy is that?  Actually, if we do the same comparison with an ARM\ncolumn too, we also get higher numbers there than on Intel.\nWhen we discovered that a few years ago, we guessed that\non ARM running the whole interpreter in\nPyPy takes up a lot of resources, e.g. of instruction cache, which the\nJIT's assembler doesn't need any more after the process is warmed up.\nAnd caches are much bigger on Intel.  However, PowerPC is much closer\nto Intel, so this argument doesn't work for PowerPC.\nBut there are other more subtle\nvariants of it.  Notably, Intel is doing crazy things about branch\nprediction, which likely helps a big interpreter---both the non-JITted\nPyPy and CPython, and both for the interpreter's main loop itself and\nfor the numerous indirect branches that depend on the types of the\nobjects.  Maybe the PowerPC is as good as Intel, and so this argument\ndoesn't work either.  Another one would be:\non PowerPC I did notice that gcc itself is not\nperfect at optimization.  During development of this backend, I often\nlooked at assembler produced by gcc, and there are a number of small\ninefficiencies there.  All these are factors that slow down the\nnon-JITted version of PyPy, but don't influence the speed of the\nassembler produced just-in-time.\n\nAnyway, this is just guessing.  The fact remains that PyPy can now\nbe used on PowerPC machines.  Have fun!\n\nA bient\u00f4t,\n\nArmin.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/10/powerpc-backend-for-jit-3014100267884692148.html"
    },
    {
      "title": "PyPy memory and warmup improvements (2) - Sharing of Guards",
      "text": "Hello everyone!\nThis is the second part of the series of improvements in warmup time and\nmemory consumption in the PyPy JIT. This post covers recent work on sharing guard\nresume data that was recently merged to trunk. It will be a part\nof the next official PyPy release. To understand what it does, let's\nstart with a loop for a simple example:\n\nclass A(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def call_method(self, z):\n        return self.x + self.y + z\n\ndef f():\n    s = 0\n    for i in range(100000):\n        a = A(i, 1 + i)\n        s += a.call_method(i)\n\nAt the entrance of the loop, we have the following set of operations:\n\nguard(i5 == 4)\nguard(p3 is null)\np27 = p2.co_cellvars\np28 = p2.co_freevars\nguard_class(p17, 4316866008, descr=<Guard0x104295e08>)\np30 = p17.w_seq\nguard_nonnull(p30, descr=<Guard0x104295db0>)\ni31 = p17.index\np32 = p30.strategy\nguard_class(p32, 4317041344, descr=<Guard0x104295d58>)\np34 = p30.lstorage\ni35 = p34..item0\n\nThe above operations gets executed at the entrance, so each time we call f(). They ensure\nall the optimizations done below stay valid. Now, as long as nothing\nout of the ordinary happens, they only ensure that the world around us never changed. However, if e.g. someone puts new\nmethods on class A, any of the above guards might fail. Despite the fact that it's a very unlikely\ncase, PyPy needs to track how to recover from such a situation. Each of those points needs to keep the full\nstate of the optimizations performed, so we can safely deoptimize them and reenter the interpreter.\nThis is vastly wasteful since most of those guards never fail, hence some sharing between guards\nhas been performed.\nWe went a step further - when two guards are next to each other or the\noperations in between them don't have side effects, we can safely redo the operations or to simply\nput, resume in the previous guard. That means every now and again we execute a few\noperations extra, but not storing extra info saves quite a bit of time and memory. This is similar to the approach that LuaJIT takes, which is called sparse snapshots.\n\n\nI've done some measurements on annotating & rtyping translation of pypy, which\nis a pretty memory hungry program that compiles a fair bit. I measured, respectively:\n\ntotal time the translation step took (annotating or rtyping)\ntime it took for tracing (that excludes backend time for the total JIT time) at\nthe end of rtyping.\nmemory the GC feels responsible for after the step. The real amount of memory\nconsumed will always be larger and the coefficient of savings is in 1.5-2x mark\n\nHere is the table:\n\n\n\n\n\n\n\n\n\n\nbranch\ntime annotation\ntime rtyping\nmemory annotation\nmemory rtyping\ntracing time\n\n\n\ndefault\n317s\n454s\n707M\n1349M\n60s\n\nsharing\n302s\n430s\n595M\n1070M\n51s\n\nwin\n4.8%\n5.5%\n19%\n26%\n17%\n\n\n\nObviously pypy translation is an extreme example - the vast majority of the code out there\ndoes not have that many lines of code to be jitted. However, it's at the very least\na good win for us :-)\nWe will continue to improve the warmup performance and keep you posted!\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/10/pypy-memory-and-warmup-improvements-2-4598780879518640015.html"
    },
    {
      "title": "PyPy warmup improvements",
      "text": "Hello everyone!\nI'm very pleased to announce that we've just managed to merge\nthe optresult branch.\nUnder this cryptic name is the biggest JIT refactoring we've done in a couple\nyears, mostly focused on the warmup time and memory impact of PyPy.\nTo understand why we did that, let's look back in time - back when we\ngot the first working JIT prototype in 2009 we were focused exclusively\non achieving peak performance with some consideration towards memory usage, but\nwithout serious consideration towards warmup time. This means we accumulated\nquite a bit of technical debt over time that we're trying, with difficulty,\nto address right now. This branch mostly does not affect the peak performance\n- it should however help you with short-living scripts, like test runs.\nWe identified warmup time to be one of the major pain points for pypy users,\nalong with memory impact and compatibility issues with CPython C extension\nworld. While we can't address all the issues at once, we're trying to address\nthe first two in the work contributing to this blog post. I will write\na separate article on the last item separately.\nTo see how much of a problem warmup is for your program, you can run your\nprogram with PYPYLOG=jit-summary:- environment variable set.\nThis should show you something like this:\n\n(pypy-optresult)fijal@hermann:~/src/botbot-web$ PYPYLOG=jit-summary:- python orm.py 1500\n[d195a2fcecc] {jit-summary\nTracing:            781     2.924965\nBackend:            737     0.722710\nTOTAL:                      35.912011\nops:                1860596\nrecorded ops:       493138\n  calls:            81022\nguards:             131238\nopt ops:            137263\nopt guards:         35166\nforcings:           4196\nabort: trace too long:      22\nabort: compiling:   0\nabort: vable escape:        22\nabort: bad loop:    0\nabort: force quasi-immut:   0\nnvirtuals:          183672\nnvholes:            25797\nnvreused:           116131\nTotal # of loops:   193\nTotal # of bridges: 575\nFreed # of loops:   6\nFreed # of bridges: 75\n[d195a48de18] jit-summary}\n\nThis means that the total (wall clock) time was 35.9s, out of which we spent\n2.9s tracing 781 loops and 0.72s compiling them. The remaining couple were\naborted (trace too long is normal, vable escape means someone called\nsys._getframe() or equivalent). You can do the following things:\n\ncompare the numbers with pypy --jit off and see at which number of\niterations pypy jit kicks in\nplay with the thresholds:\npypy --jit threshold=500,function_threshold=400,trace_eagerness=50 was\nmuch better in this example. What this does is to lower the threshold\nfor tracing loops from default of 1039 to 400, threshold for tracing\nfunctions from the start from 1619 to 500 and threshold for tracing bridges\nfrom 200 to 50. Bridges are \"alternative paths\" that JIT did not take that\nare being additionally traced. We believe in sane defaults, so we'll try\nto improve upon those numbers, but generally speaking there is no one-size\nfits all here.\nif the tracing/backend time stays high, come and complain to us with\nbenchmarks, we'll try to look at them\n\nWarmup, as a number, is notoriously hard to measure. It's a combination of:\n\npypy running interpreter before jitting\npypy needing time to JIT the traces\nadditional memory allocations needed during tracing to accomodate bookkeeping\ndata\nexiting and entering assembler until there is enough coverage of assembler\n\nWe're working hard on making a better assesment at this number, stay tuned :-)\n\nSpeedups\nOverall we measured about 50% speed improvement in the optimizer, which reduces\nthe overall warmup time between 10% and 30%. The very\nobvious warmup benchmark got a speedup from 4.5s to 3.5s, almost\n30% improvement. Obviously the speedups on benchmarks would vastly\ndepend on how much warmup time is there in those benchmarks. We observed\nannotation of pypy to decreasing by about 30% and the overall translation\ntime by about 7%, so your mileage may vary.\nOf course, as usual with the large refactoring of a crucial piece of PyPy,\nthere are expected to be bugs. We are going to wait for the default branch\nto stabilize so you should see warmup improvements in the next release.\nIf you're not afraid to try, nightlies will already have them.\nWe're hoping to continue improving upon warmup time and memory impact in the\nfuture, stay tuned for improvements.\n\n\nTechnical details\nThe branch does \"one\" thing - it changes the underlying model of how operations\nare represented during tracing and optimizations. Let's consider a simple\nloop like:\n\n[i0, i1]\ni2 = int_add(i0, i1)\ni3 = int_add(i2, 1)\ni4 = int_is_true(i3)\nguard_true(i4)\njump(i3, i2)\n\nThe original representation would allocate a Box for each of i0 - i4\nand then store those boxes in instances of ResOperation. The list of such\noperations would then go to the optimizer. Those lists are big - we usually\nremove 90% of them during optimizations, but they can be a couple thousand\nelements. Overall, allocating those big lists takes a toll on warmup time,\nespecially due to the GC pressure. The branch removes the existance of Box\ncompletely, instead using a link to ResOperation itself. So say in the above\nexample, i2 would refer to its producer - i2 = int_add(i0, i1) with\narguments getting special treatment.\nThat alone reduces the GC pressure slightly, but a reduced number\nof instances also lets us store references on them directly instead\nof going through expensive dictionaries, which were used to store optimizing\ninformation about the boxes.\nCheers!\nfijal & arigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/09/pypy-warmup-improvements-8349465374608676233.html"
    },
    {
      "title": "PyPy 2.6.1 released",
      "text": "PyPy 2.6.1\nWe\u2019re pleased to announce PyPy 2.6.1, an update to PyPy 2.6.0 released June 1.\nWe have fixed many issues, updated stdlib to 2.7.10, cffi to version 1.3, extended support for\nthe new vmprof statistical profiler for multiple threads, and increased\nfunctionality of numpy.\nYou can download the PyPy 2.6.1 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject, and our volunteers and contributors.\n\nWe would also like to encourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on pypy, or general help with making\nRPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd),\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\n\nWe also welcome developers of other\ndynamic languages to see what RPython can do for them.\n\n\n\nHighlights\n\nBug Fixes\nRevive non-SSE2 support\nFixes for detaching _io.Buffer*\nOn Windows, close (and flush) all open sockets on exiting\nDrop support for ancient macOS v10.4 and before\nClear up contention in the garbage collector between trace-me-later and pinning\nIssues reported with our previous release were resolved after reports from users on\nour issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at\n#pypy.\n\n\nNew features:\ncffi was updated to version 1.3\nThe python stdlib was updated to 2.7.10 from 2.7.9\nvmprof now supports multiple threads and OS X\nThe translation process builds cffi import libraries for some stdlib\npackages, which should prevent confusion when package.py is not used\nbetter support for gdb debugging\nfreebsd should be able to translate PyPy \u201cout of the box\u201d with no patches\n\n\nNumpy:\nBetter support for record dtypes, including the align keyword\nImplement casting and create output arrays accordingly (still missing some corner cases)\nSupport creation of unicode ndarrays\nBetter support ndarray.flags\nSupport axis argument in more functions\nRefactor array indexing to support ellipses\nAllow the docstrings of built-in numpy objects to be set at run-time\nSupport the buffered nditer creation keyword\n\n\nPerformance improvements:\nDelay recursive calls to make them non-recursive\nSkip loop unrolling if it compiles too much code\nTweak the heapcache\nAdd a list strategy for lists that store both floats and 32-bit integers.\nThe latter are encoded as nonstandard NaNs.  Benchmarks show that the speed\nof such lists is now very close to the speed of purely-int or purely-float\nlists.\nSimplify implementation of ffi.gc() to avoid most weakrefs\nMassively improve the performance of map() with more than\none sequence argument\n\n\n\nPlease try it out and let us know what you think. We welcome\nsuccess stories, experiments,  or benchmarks, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/08/pypy-261-released-3638960649983103796.html"
    },
    {
      "title": "PyPy and ijson - a guest blog post",
      "text": "This gem was posted in the ijson issue tracker after some discussion on #pypy, and Dav1dde kindly allowed us to repost it here:\n\n\"So, I was playing around with parsing huge JSON files (19GiB, testfile is ~520MiB) and wanted to try a sample code with PyPy, turns out, PyPy needed ~1:30-2:00 whereas CPython 2.7 needed ~13 seconds (the pure python implementation on both pythons was equivalent at ~8 minutes). \"Apparantly ctypes is really bad performance-wise, especially on PyPy. So I made a quick CFFI mockup: https://gist.github.com/Dav1dde/c509d472085f9374fc1d\n\nBefore:\nCPython 2.7: \u00a0\u00a0\u00a0 python -m emfas.server size dumps/echoprint-dump-1.json \u00a0\u00a0\u00a0 11.89s user 0.36s system 98% cpu 12.390 total\u00a0\nPYPY: \u00a0\u00a0\u00a0 python -m emfas.server size dumps/echoprint-dump-1.json \u00a0\u00a0\u00a0 117.19s user 2.36s system 99% cpu 1:59.95 total \nAfter (CFFI): CPython 2.7: \u00a0\u00a0\u00a0\u00a0 python jsonsize.py ../dumps/echoprint-dump-1.json\u00a0\u00a0\u00a0\u00a0  8.63s user 0.28s system 99% cpu 8.945 total\u00a0\nPyPy: \u00a0\u00a0\u00a0\u00a0 python jsonsize.py ../dumps/echoprint-dump-1.json \u00a0\u00a0\u00a0\u00a0 4.04s user 0.34s system 99% cpu 4.392 total\n\"\n\n\nDav1dd goes into more detail in the issue itself, but we just want to emphasize a few significant points from this brief interchange:\n\nHis CFFI implementation is faster than the ctypes one even on CPython 2.7.\nPyPy + CFFI is faster than CPython even when using C code to do the heavy parsing.\n\n\u00a0The PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/06/pypy-and-ijson-guest-blog-post-8143007374752482637.html"
    },
    {
      "title": "PyPy 2.6.0 release",
      "text": "PyPy 2.6.0 - Cameo Charm\n\nWe\u2019re pleased to announce PyPy 2.6.0, only two months after PyPy 2.5.1. We are particulary happy to update cffi to version 1.1, which makes the popular ctypes-alternative even easier to use, and to support the new vmprof statistical profiler.\n\n\n\nYou can download the PyPy 2.6.0 release here:\n\n\n\n\nhttps://pypy.org/download.html\n\n\n\n\nWe would like to thank our donors for the continued support of the PyPy project, and for those who donate to our three sub-projects, as well as our volunteers and contributors.\n\n\n\nThanks also to Yury V. Zaytsev and David Wilson who recently started running nightly builds on Windows and MacOSX buildbots.\n\n\n\nWe\u2019ve shown quite a bit of progress, but we\u2019re slowly running out of funds. Please consider donating more, or even better convince your employer to donate, so we can finish those projects! The three sub-projects are:\n\n\n\nPy3k (supporting Python 3.x): We have released a Python 3.2.5 compatible version we call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version\nSTM (software transactional memory): We have released a first working version, and continue to try out new promising paths of achieving a fast multithreaded Python\nNumPy which requires installation of our fork of upstream numpy, available on bitbucket\n\n\n\n\nWe would also like to encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better. Nine new people contributed since the last release, you too could be one of them.\n\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\n\n\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows, OpenBSD, freebsd), as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\n\n\n\nWhile we support 32 bit python on Windows, work on the native Windows 64 bit python is still stalling, we would welcome a volunteer to handle that. We also welcome developers with other operating systems or dynamic languages to see what RPython can do for them.\n\n\n\n\n\nHighlights\n\nPython compatibility:\n\nImprove support for TLS 1.1 and 1.2\nWindows downloads now package a pypyw.exe in addition to pypy.exe\nSupport for the PYTHONOPTIMIZE environment variable (impacting builtin\u2019s __debug__ property)\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy.\n\n\nNew features:\n\nAdd preliminary support for a new lightweight statistical profiler vmprof, which has been designed to accomodate profiling JITted code\n\n\nNumpy:\n\nSupport for object dtype via a garbage collector hook\nSupport for .can_cast and .min_scalar_type as well as beginning a refactoring of the internal casting rules\nBetter support for subtypes, via the __array_interface__, __array_priority__, and __array_wrap__ methods (still a work-in-progress)\nBetter support for ndarray.flags\n\n\nPerformance improvements:\n\nSlight improvement in frame sizes, improving some benchmarks\nInternal refactoring and cleanups leading to improved JIT performance\n\n\nImproved IO performance of zlib and bz2 modules\nWe continue to improve the JIT\u2019s optimizations. Our benchmark suite is now over 7 times faster than cpython\n\n\n\n\n\n\n\nPlease try it out and let us know what you think. We welcome success stories, experiments,  or benchmarks, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/06/pypy-260-release-8983050552628070433.html"
    },
    {
      "title": "CFFI 1.0.1 released",
      "text": "CFFI 1.0.1 final has now been released for CPython!  CFFI is a (CPython and PyPy) module to interact with C code from Python.\nThe main news from CFFI 0.9 is the new way to build extension modules:\nthe \"out-of-line\" mode, where you have a separate build script.  When\nthis script is executed, it produces the extension module.  This comes\nwith associated Setuptools support that fixes the headache of\ndistributing your own CFFI-using packages.  It also massively cuts\ndown the import times.\nAlthough this is a major new version, it should be fully\nbackward-compatible: existing projects should continue to work, in\nwhat is now called the \"in-line mode\".\nThe documentation has been reorganized and split into a few pages.\nFor more information about this new \"out-of-line\" mode, as well as\nmore general information about what CFFI is and how to use it, read the Goals and proceed to\nthe Overview.\nUnlike the 1.0 beta 1 version (ffi.dlopen(), instead of only\nffi.verify().\nPyPy support: PyPy needs integrated support for efficient JITting,\nso you cannot install a different version of CFFI on top of an\nexisting PyPy.  You need to wait for the upcoming PyPy 2.6 to use\nCFFI 1.0---or get a nightly build.\nMy thanks again to the PSF (Python Software Foundation) for their\nfinancial support!\n\nUPDATE:Bug with the first example \"ABI out-of-line\": variadic functions (like printf, ending in a \"...\" argument) crash.  Fixed in CFFI 1.0.2.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/05/cffi-101-released-756545636419794802.html"
    },
    {
      "title": "CFFI 1.0 beta 1",
      "text": "Finally!  CFFI 1.0 is almost ready.  CFFI gives Python developers a convenient way to call external C libraries.  Here \"Python\" == \"CPython or PyPy\", but this post is mostly about the CPython side of CFFI, as the PyPy version is not ready yet.\nOn CPython, you can download the version\n\"1.0.0b1\" either by looking for the cffi-1.0 branch in\nthe repository, or by\nsaying\n\npip install \"cffi>=1.0.dev0\"\n\n(Until 1.0 final is ready,\npip install cffi will still give you version 0.9.2.)\nThe main news: you can now explicitly generate and compile a CPython C\nextension module from a \"build\" script.  Then in the rest of your\nprogram or library, you no longer need to import cffi at all.\nInstead, you simply say:\n\nfrom _my_custom_module import ffi, lib\n\nThen you use ffi and lib just like you did in your\nverify()-based project in CFFI 0.9.2.  (The lib is what used to\nbe the result of verify().)  The details of how you use them\nshould not have changed at all, so that the rest of your program should\nnot need any update.\n\nBenefits\nThis is a big step towards standard practices for making and\ndistributing Python packages with C extension modules:\n\non the one hand, you need an explicit compilation step, triggered\nhere by running the \"build\" script;\non the other hand, what you gain in return is better control over\nwhen and why the C compilation occurs, and more standard ways to write\ndistutils- or setuptools-based setup.py files (see below).\n\nAdditionally, this completely removes one of the main drawbacks of using\nCFFI to interface with large C APIs: the start-up time.  In some cases\nit could be extreme on slow machines (cases of 10-20 seconds on ARM\nboards occur commonly).  Now, the import above is instantaneous.\nIn fact, none of the pure Python cffi package is needed any more at\nruntime (it needs only an internal extension module from CFFI, which\ncan be installed by doing \"pip install cffi-runtime\" [*] if you only need that).\nThe ffi object you get by the import above is of a\ncompletely different class written entirely in C.  The two\nimplementations might get merged in the future; for now they are\nindependent, but give two compatible APIs.  The differences are that\nsome methods like cdef() and verify() and set_source() are\nomitted from the C version, because it is supposed to be a complete FFI\nalready; and other methods like new(), which take as parameter a\nstring describing a C type, are faster now because that string is parsed\nusing a custom small-subset-of-C parser, written in C too.\n\n\nIn practice\nCFFI 1.0 beta 1 was tested on CPython 2.7 and 3.3/3.4, on Linux and to\nsome extent on Windows and OS/X.  Its PyPy version is not ready yet,\nand the only docs available so far are those below.\nThis is beta software, so there might be bugs and details may change.  We are interested in hearing any feedback (irc.freenode.net #pypy) or bug reports.\nTo use the new features, create a source file that is not imported by the rest of\nyour project, in which you place (or move) the code to build the FFI\nobject:\n\n# foo_build.py\nimport cffi\nffi = cffi.FFI()\n\nffi.cdef(\"\"\"\n    int printf(const char *format, ...);\n\"\"\")\n\nffi.set_source(\"_foo\", \"\"\"\n    #include <stdio.h>\n\"\"\")   # and other arguments like libraries=[...]\n\nif __name__ == '__main__':\n    ffi.compile()\n\nThe ffi.set_source() replaces the ffi.verify() of CFFI 0.9.2.\nCalling it attaches the given source code to the ffi object, but this call doesn't\ncompile or return anything by itself.  It may be placed above the ffi.cdef()\nif you prefer.  Its first argument is the name of the C extension module\nthat will be produced.\nActual compilation (including generating the complete C sources) occurs\nlater, in one of two places: either in ffi.compile(), shown above,\nor indirectly from the setup.py, shown next.\nIf you directly execute the file foo_build.py above, it will\ngenerate a local file _foo.c and compile it to _foo.so (or the\nappropriate extension, like _foo.pyd on Windows).  This is the\nextension module that can be used in the rest of your program by saying\n\"from _foo import ffi, lib\".\n\n\nDistutils\nIf you want to distribute your program, you write a setup.py using\neither distutils or setuptools.  Using setuptools is generally\nrecommended nowdays, but using distutils is possible too.  We show it\nfirst:\n\n# setup.py\nfrom distutils.core import setup\nimport foo_build\n\nsetup(\n    name=\"example\",\n    version=\"0.1\",\n    py_modules=[\"example\"],\n    ext_modules=[foo_build.ffi.distutils_extension()],\n)\n\nThis is similar to the CFFI 0.9.2 way.  It only works if cffi was\ninstalled previously, because otherwise foo_build cannot be\nimported.  The difference is that you use ffi.distutils_extension()\ninstead of ffi.verifier.get_extension(), because there is no longer\nany verifier object if you use set_source().\n\n\nSetuptools\nThe modern way is to write setup.py files based on setuptools, which\ncan (among lots of other things) handle dependencies.  It is what you\nnormally get with pip install, too.  Here is how you'd write it:\n\n# setup.py\nfrom setuptools import setup\n\nsetup(\n    name=\"example\",\n    version=\"0.1\",\n    py_modules=[\"example\"],\n    setup_requires=[\"cffi>=1.0.dev0\"],\n    cffi_modules=[\"foo_build:ffi\"],\n    install_requires=[\"cffi-runtime\"],    # see [*] below\n)\n\nNote that \"cffi\" is mentioned on three lines here:\n\nthe first time is in setup_requires, which means that cffi will\nbe locally downloaded and used for the setup.\nthe second mention is a custom cffi_modules argument.  This\nargument is handled by cffi as soon as it is locally downloaded.  It\nshould be a list of \"module:ffi\" strings, where the ffi part\nis the name of the global variable in that module.\nthe third mention is in install_requires.  It means that in\norder to install this example package, \"cffi-runtime\" must also be\ninstalled.  This is (or will be) a PyPI entry that only contains a\ntrimmed down version of CFFI, one that does not include the pure\nPython \"cffi\" package and its dependencies.  None of it is needed at\nruntime.\n\n[*] NOTE: The \"cffi-runtime\" PyPI entry is not ready yet.  For now, use \"cffi>=1.0.dev0\" instead.  Considering PyPy, which has got a built-in \"_cffi_backend\" module, the \"cffi-runtime\" package could never be upgraded there; but it would still be nice if we were able to upgrade the \"cffi\" pure Python package on PyPy.  This might require some extra care in writing the interaction code.  We need to sort it out now...\n\n\nThanks\nSpecial thanks go to the PSF (Python Software Foundation) for their\nfinancial support, without which this work---er... it might likely have occurred anyway, but at an unknown future date :-)\n(For reference, the amount I asked for (and got) is equal to one\nmonth of what a Google Summer of Code student gets, for work that will\ntake a bit longer than one month. At least I personally am running mostly\non such money, and so I want to thank the PSF again for their\ncontribution to CFFI---and while I'm at it, thanks to all other\ncontributors to PyPy---for making this job more than an unpaid hobby on\nthe side :-)\n\nArmin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/05/cffi-10-beta-1-4375652711495636911.html"
    },
    {
      "title": "PyPy-STM 2.5.1 released",
      "text": "PyPy-STM 2.5.1 - Mawhrin-Skel\n\nWe're pleased to announce PyPy-STM 2.5.1, codenamed Mawhrin-Skel.\nThis is the second official release of PyPy-STM.  You can download\nthis release here (64-bit Linux only):\n\nhttps://pypy.org/download.html\nDocumentation:\n\nhttps://pypy.readthedocs.org/en/latest/stm.html\nPyPy is an implementation of the Python programming language which focuses\non performance. So far we've been relentlessly optimizing for the single\ncore/process scenario. PyPy STM brings to the table a version of PyPy\nthat does not have the infamous Global Interpreter Lock, hence can run\nmultiple threads on multiple cores. Additionally it comes with a set\nof primitives that make writing multithreaded applications a lot easier,\nas explained below (see TransactionQueue) and in the documentation.\nInternally, PyPy-STM is based on the Software Transactional Memory\nplug-in called stmgc-c7.  This version comes with a relatively\nreasonable single-core overhead but scales only up to around 4 cores\non some examples; the next version of the plug-in, stmgc-c8, is in\ndevelopment and should address that limitation (as well as reduce the\noverhead).  These versions only support 64-bit Linux; we'd welcome\nsomeone to port the upcoming stmgc-c8 to other (64-bit) platforms.\nThis release passes all regular PyPy tests, except for a few\nspecial cases.  In other words, you should be able to drop in\nPyPy-STM instead of the regular PyPy and your program should still\nwork.  See current status for more information.\nThis work was done by Remi Meier and Armin Rigo.  Thanks to all donors\nfor crowd-funding the STM work so far!  As usual, it took longer\nthan we would have thought.  I really want to thank the people that\nkept making donations anyway.  Your trust is greatly appreciated!\n\n\nWhat's new?\nCompared to the July 2014 release, the main addition is a way to\nget reports about STM conflicts.  This is an essential new feature.\nTo understand why this is so important, consider that if you already\nplayed around with the previous release, chances are that you didn't\nget very far.  It probably felt like a toy: on very small examples it\nwould nicely scale, but on any larger example it would not scale at\nall.  You didn't get any feedback about why, but the underlying reason\nis that, in a typical large example, there are some STM conflicts that\noccur all the time and that won't be immediately found just by\nthinking.  This prevents any parallelization.\nNow PyPy-STM is no longer a black box: you have a way to learn about\nthese conflicts, fix them, and try again.  The tl;dr version is to run:\n\n    PYPYSTM=stmlog ./pypy-stm example.py\n    ./print_stm_log.py stmlog\n\nMore details in the STM user guide.\n\n\n\nPerformance\nThe performance is now more stable than it used to be.  More\nprecisely, the best case is still \"25%-40% single-core slow-down with\nvery good scaling up to 4 threads\", but the average performance seems\nnot too far from that.  There are still dark spots --- notably, the\nJIT is still slower to warm up, though it was improved a lot.  These\nare documented in the current status section.  Apart from\nthat, we should not get more than 2x single-core slow-down in the\nworst case.  Please report such cases as bugs!\n\n\n\nTransactionQueue\nAs explained before, PyPy-STM is more than \"just\" a Python without\nGIL.  It is a Python in which you can do minor tweaks to your\nexisting, non-multithreaded programs and get them to use multiple\ncores.  You identify medium- or large-sized, likely-independent parts\nof the code and to ask PyPy-STM to run these parts in parallel.  An\nexample would be every iteration of some outermost loop over all items\nof a dictionary.  This is done with a new API:\ntransaction.TransactionQueue().  See help(TransactionQueue) or\nread more about it in the STM user guide.\nThis is not a 100% mechanical change: very likely, you need to hunt\nfor and fix \"STM conflicts\" that prevent parallel execution (see\ndocs).  However, at all points your program runs correctly, and you\ncan stop the hunt when you get acceptable performance.  You don't get\ndeadlocks or corrupted state.\n\nThanks for reading!\nArmin, Remi, Fijal",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/03/pypy-stm-251-released-1342113838236225773.html"
    },
    {
      "title": "PyPy 2.5.1 Released",
      "text": "PyPy 2.5.1 - Pineapple Bromeliad\nWe\u2019re pleased to announce PyPy 2.5.1, Pineapple Bromeliad following on the heels of 2.5.0. You can download the PyPy 2.5.1 release here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject, and for those who donate to our three sub-projects, as well as our\nvolunteers and contributors.\nWe\u2019ve shown quite a bit of progress, but we\u2019re slowly running out of funds.\nPlease consider donating more, or even better convince your employer to donate,\nso we can finish those projects! The three sub-projects are:\n\n\n\n\nPy3k (supporting Python 3.x): We have released a Python 3.2.5 compatible version we call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version\n\u00a0\n\n\n\nSTM (software transactional memory): We have released a first working version,\nand continue to try out new promising paths of achieving a fast multithreaded Python\n\n\n\n\nNumPy which requires installation of our fork of upstream numpy,\navailable on bitbucket\n\n\nWe would also like to encourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and Rpython documentation\nimprovements, tweaking popular modules to run on pypy, or general help with making\nRpython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\n\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows, and OpenBSD),\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\n\n\nWhile we support 32 bit python on Windows, work on the native Windows 64\nbit python is still stalling, we would welcome a volunteer\nto handle that.\n\n\n\n\nHighlights\n\nThe past months have seen pypy mature and grow, as rpython becomes the goto\nsolution for writing fast dynamic language interpreters. Our separation of\nRpython from the python interpreter PyPy is now much clearer in the\nPyPy documentation  and we now have seperate RPython documentation.\nTell us what still isn\u2019t clear, or even better help us improve the documentation. \n\n\n\n\nWe merged version 2.7.9 of python\u2019s stdlib. From the python release notice:\nThe entirety of Python 3.4\u2019s ssl module has been backported.\nSee PEP 466 for justification.\nHTTPS certificate validation using the system\u2019s certificate store is now\nenabled by default. See PEP 476 for details.\nSSLv3 has been disabled by default in httplib and its reverse dependencies\ndue to the POODLE attack.\nThe ensurepip module has been backported, which provides the pip\npackage manager in every Python 2.7 installation. See PEP 477.\n\n\n\nThe garbage collector now ignores parts of the stack which did not change\nsince the last collection, another performance boost \n\n\nerrno and LastError are saved around cffi calls so things like pdb will not\noverwrite it \n\n\nWe continue to asymptotically approach a score of 7 times faster than cpython\non our benchmark suite, we now rank 6.98 on latest runs \n\n\nIssues reported with our previous release were resolved after reports from users on\nour issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at\n#pypy.\n\nPlease try it out and let us know what you think. We welcome\nsuccess stories, experiments,  or benchmarks, we know you are using PyPy, please tell us about it!\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/03/pypy-251-released-5657064769385723517.html"
    },
    {
      "title": "Pydgin: Using RPython to Generate Fast Instruction-Set Simulators",
      "text": "Note: This is a guest blog post by Derek Lockhart and Berkin Ilbeyi from\nComputer Systems Laboratory of Cornell University.\nIn this blog post I'd like to describe some recent work on using the RPython\ntranslation toolchain to generate fast instruction set simulators.\nOur open-source framework, Pydgin [a], provides a domain-specific\nlanguage (DSL) embedded in Python for concisely describing instruction set\narchitectures [b] and then uses these descriptions to generate fast,\nJIT-enabled simulators.\nPydgin will be presented at the IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS) and in this post we provide a\npreview of that work.\nIn addition, we discuss some additional progress updates that occurred after\nthe publishing deadline and will not appear in the final paper [1].\nOur area of research expertise is computer architecture, which is perhaps an\nunfamiliar topic for some readers of the PyPy blog.\nBelow we provide some brief background on hardware simulation in the field of\ncomputer architecture, as well as some context as to why instruction set\nsimulators in particular are such an important tool.\n\nSimulators: Designing Hardware with Software\nFor computer architects in both academia and industry, a key step in designing\nnew computational hardware (e.g., CPUs, GPUs, and mobile system-on-chips) is\nsimulation [c] of the target system.\nWhile numerous models for simulation exist, three classes are particularly\nimportant in hardware design.\nFunctional Level models simulate the behavior of the target system.\nThese models are useful for creating a \"golden\" reference which can serve as an\nexecutable specification or alternatively as an emulation platform for software\ndevelopment.\nCycle Level models aim to simulate both the behavior and the approximate\ntiming of a hardware component.\nThese models help computer architects explore design tradeoffs and quickly\ndetermine things like how big caches should be, how many functional units are\nneeded to meet throughput targets, and how the addition of a custom accelerator\nblock may impact total system performance.\nRegister-Transfer Level (RTL) models specify the behavior, timing, and\nresources (e.g., registers, wires, logic gates) of a hardware component.\nRTL models are bit-accurate hardware specifications typically written in a\nhardware description language (HDL) such as Verilog or VHDL.\nOnce verified through extensive simulation, HDL specifications can be passed\ninto synthesis and place-and-route tools to estimate area/energy/timing or to\ncreate FPGA or ASIC prototypes.\nAn instruction set simulator (ISS) is a special kind of\nfunctional-level model that simulates the behavior of a processor or\nsystem-on-chip (SOC).  ISSs serve an important role in hardware design\nbecause they model the instruction set architecture (ISA) interface: the\ncontractual boundary between hardware designers and software developers.\nISSs allow hardware designers to quickly experiment with adding new processor\ninstructions while also allowing software developers to build new compilers,\nlibraries, and applications long before physical silicon is available.\n\n\nInstruction-Set Simulators Must be Fast and Productive\nInstruction-set simulators are more important than ever because the ISA\nboundary has become increasingly fluid.\nWhile Moore's law has continued to deliver larger numbers of transistors\nwhich computer architects can use to build increasingly complex chips, limits\nin Dennard scaling have restricted how these transistors can be used [d].\nIn more simple terms, thermal constraints (and energy constraints in mobile\ndevices) have resulted in a growing interest in pervasive specialization:\nusing custom accelerators to more efficiently perform compute intensive tasks.\nThis is already a reality for designers of mobile SOCs who continually add new\naccelerator blocks and custom processor instructions in order to achieve higher\nperformance with less energy consumption.\nISSs are indispensable tools in this SOC design process for both hardware\narchitects building the silicon and software engineers developing the software\nstack on top of it.\nAn instruction set simulator has two primary responsibilities: 1) accurately\nemulating the external execution behavior of the target, and 2) providing\nobservability by accurately reproducing the target's internal state (e.g.,\nregister values, program counter, status flags) at each time step.\nHowever, other qualities critical to an effective ISS are simulation\nperformance and designer productivity.\nSimulation performance is important because shorter simulation times allow\ndevelopers to more quickly execute and verify large software applications.\nDesigner productivity is important because it allows hardware architects to\neasily experiment with adding new instructions and estimate their impact on\napplication performance.\nTo improve simulation performance, high-performance ISSs use dynamic binary\ntranslation (DBT) as a mechanism to translate frequently visited blocks of\ntarget instructions into optimized sequences of host instructions.\nTo improve designer productivity, many design toolchains automatically generate\nISSs from an architectural description language (ADL): a special\ndomain-specific language for succinctly specifying instruction encodings and\ninstruction semantics of an ISA.\nVery few existing systems have managed to encapsulate the design complexity of\nDBT engines such that high-performance, DBT-accelerated ISSs could be\nautomatically generated from ADLs [e].\nUnfortunately, tools which have done so are either proprietary software or\nleave much to be desired in terms of performance or productivity.\n\n\nWhy RPython?\nOur research group learned of the RPython translation toolchain through our\nexperiences with PyPy, which we had used in conjunction with our Python\nhardware modeling framework to achieve significant improvements in simulation\nperformance [2].\nWe realized that the RPython translation toolchain could potentially be adapted\nto create fast instruction set simulators since the process of interpreting\nexecutables comprised of binary instructions shared many similarities with the\nprocess of interpreting bytecodes in a dynamic-language VM.\nIn addition, we were inspired by PyPy's meta-tracing approach to JIT-optimizing\nVM design which effectively separates the process of specifying a language\ninterpreter from the optimization machinery needed to achieve good performance.\nExisting ADL-driven ISS generators have tended to use domain-specific\nlanguages that require custom parsers or verbose C-based syntax that\ndistracts from the instruction specification.\nCreating an embedded-ADL within Python provides several benefits over these\nexisting approaches including a gentler learning curve for new users, access to\nbetter debugging tools, and easier maintenance and extension by avoiding a\ncustom parser.\nAdditionally, we have found that the ability to directly execute Pydgin\nISA descriptions in a standard Python interpreter such as CPython or PyPy\nsignificantly helps debugging and testing during initial ISA exploration.\nPython's concise, pseudocode-like syntax also manages to map quite closely to\nthe pseudocode specifications provided by many ISA manuals [f].\n\n\nThe Pydgin embedded-ADL\nDefining a new ISA in the Pydgin embedded-ADL requires four primary pieces of\ninformation: the architectural state (e.g. register file, program counter,\ncontrol registers), the bit encodings of each instruction, the instruction\nfields, and the semantic definitions for each instruction. Pydgin aims to make\nthis process as painless as possible by providing helper classes and functions\nwhere possible.\nFor example, below we provide a truncated example of the ARMv5 instruction\nencoding table. Pydgin maintains encodings of all instructions in a centralized\nencodings data structure for easy maintenance and quick lookup. The\nuser-provided instruction names and bit encodings are used to automatically\ngenerate decoders for the simulator. Unlike many ADLs, Pydgin does not require\nthat the user explicitly specify instruction types or mask bits for field\nmatching because the Pydgin decoder generator can automatically infer decoder\nfields from the encoding table.\n\nencodings = [\n  ['adc',      'xxxx00x0101xxxxxxxxxxxxxxxxxxxxx'],\n  ['add',      'xxxx00x0100xxxxxxxxxxxxxxxxxxxxx'],\n  ['and',      'xxxx00x0000xxxxxxxxxxxxxxxxxxxxx'],\n  ['b',        'xxxx1010xxxxxxxxxxxxxxxxxxxxxxxx'],\n  ['bl',       'xxxx1011xxxxxxxxxxxxxxxxxxxxxxxx'],\n  ['bic',      'xxxx00x1110xxxxxxxxxxxxxxxxxxxxx'],\n  ['bkpt',     '111000010010xxxxxxxxxxxx0111xxxx'],\n  ['blx1',     '1111101xxxxxxxxxxxxxxxxxxxxxxxxx'],\n  ['blx2',     'xxxx00010010xxxxxxxxxxxx0011xxxx'],\n  # ...\n  ['teq',      'xxxx00x10011xxxxxxxxxxxxxxxxxxxx'],\n  ['tst',      'xxxx00x10001xxxxxxxxxxxxxxxxxxxx'],\n]\n\nA major goal of Pydgin was ensuring instruction semantic definitions map to ISA\nmanual specifications as much as possible. The code below shows one such\ndefinition for the ARMv5 add instruction.\nA user-defined Instruction class (not shown) specifies field names that can\nbe used to conveniently access bit positions within an instruction (e.g.\nrd, rn, S).\nAdditionally, users can choose to define their own helper functions, such as\nthe condition_passed function, to create more concise syntax that better\nmatches the ISA manual.\n\ndef execute_add( s, inst ):\n  if condition_passed( s, inst.cond() ):\n    a,   = s.rf[ inst.rn() ]\n    b, _ = shifter_operand( s, inst )\n    result = a + b\n    s.rf[ inst.rd() ] = trim_32( result )\n\n    if inst.S():\n      if inst.rd() == 15:\n        raise FatalError('Writing SPSR not implemented!')\n      s.N = (result >> 31)&1\n      s.Z = trim_32( result ) == 0\n      s.C = carry_from( result )\n      s.V = overflow_from_add( a, b, result )\n\n    if inst.rd() == 15:\n      return\n\n  s.rf[PC] = s.fetch_pc() + 4\n\nCompared to the ARM ISA Reference manual shown below, the Pydgin instruction\ndefinition is a fairly close match. Pydgin's definitions could certainly be\nmade more concise by using a custom DSL, however, this would lose many of the\ndebugging benefits afforded to a well-supported language such as Python and\nadditionally require using a custom parser that would likely need modification\nfor each new ISA.\n\nif ConditionPassed(cond) then\n   Rd = Rn + shifter_operand\n   if S == 1 and Rd == R15 then\n     if CurrentModeHasSPSR() then CPSR = SPSR\n   else UNPREDICTABLE else if S == 1 then\n     N Flag = Rd[31]\n     Z Flag = if Rd == 0 then 1 else 0\n     C Flag = CarryFrom(Rn + shifter_operand)\n     V Flag = OverflowFrom(Rn + shifter_operand)\n\nCreating an ISS that can run real applications is a rather complex task, even\nfor a bare metal simulator with no operating system such as Pydgin.\nEach system call in the C library must be properly implemented, and\nbootstrapping code must be provided to set up the program stack and\narchitectural state.\nThis is a very tedious and error prone process which Pydgin tries to\nencapsulate so that it remains as transparent to the end user as possible.\nIn future versions of Pydgin we hope to make bootstrapping more painless and\nsupport a wider variety of C libraries.\n\n\n\n\nPydgin Performance\nIn order to achieve good simulation performance from Pydgin ISSs, significant\nwork went into adding appropriate JIT annotations to the Pydgin library\ncomponents.\nThese optimization hints, which allow the JIT generated by the RPython\ntranslation toolchain to produce more efficient code, have been specifically\nselected for the unique properties of ISSs.\nFor the sake of brevity, we do not talk about the exact optimizations here but\na detailed discussion can be found in the ISPASS paper [1].\nIn the paper we evaluate two ISSs, one for a simplified MIPS ISA and another\nfor the ARMv5 ISA, whereas below we only discuss results for the ARMv5 ISS.\nThe performance of Pydgin-generated ARMv5 ISSs were compared against\nseveral reference ISSs: the gem5 ARM atomic simulator (gem5),\ninterpretive and JIT-enabled versions of SimIt-ARM (simit-nojit and\nsimit-jit), and QEMU.\nAtomic models from the gem5 simulator were chosen for comparison due their wide\nusage amongst computer architects [g].\nSimIt-ARM was selected because it is currently the highest performance\nADL-generated DBT-ISS publicly available.\nQEMU has long been held as the gold-standard for DBT simulators due to its\nextremely high performance, however, QEMU is generally intended for usage as an\nemulator rather than a simulator [c] and therefore achieves its excellent\nperformance at the cost of observability.\nUnlike QEMU, all other simulators in our study faithfully track architectural\nstate at an instruction level rather than block level.\nPydgin ISSs were generated with and without JITs using the RPython translation\ntoolchain in order to help quantify the performance benefit of the meta-tracing\nJIT.\nThe figure below shows the performance of each ISS executing applications from\nthe SPEC CINT2006 benchmark suite [h].\nBenchmarks were run to completion on the high-performance DBT-ISSs\n(simit-jit, pydgin-jit, and QEMU), but were terminated after only\n10 billion simulated instructions for the non-JITed interpretive ISSs\n(these would require many hours, in some cases days, to run to completion).\nSimulation performance is measured in MIPS [i] and plotted on a log\nscale due to the wide variance in performance.\nThe WHMEAN group summarizes each ISS's performance across all benchmarks\nusing the weighted harmonic mean.\n\n\n\nA few points to take away from these results:\n\nISSs without JITs (gem5, simit-nojit, and pydgin-nojit) demonstrate\nrelatively consistent performance across applications, whereas ISSs with JITs\n(simit-jit, pydgin-jit, and QEMU) demonstrate much greater\nperformance variability from application-to-application.\nThe gem5 atomic model demonstrates particularly miserable performance, only\n2-3 MIPS!\nQEMU lives up to its reputation as a gold-standard for simulator performance,\nleading the pack on nearly every benchmark and reaching speeds of 240-1120\nMIPS.\npydgin-jit is able to outperform simit-jit on four of the\napplications, including considerable performance improvements of 1.44\u20131.52\u00d7\nfor the applications 456.hmmer, 462.libquantum, and 471.omnetpp\n(managing to even outperform QEMU on 471.omnetpp).\nsimit-jit is able to obtain much more consistent performance (230-459\nMIPS across all applications) than pydgin-jit (9.6-659 MIPS).  This is\ndue to simit-jit's page-based approach to JIT optimization compared to\npydgin-jit's tracing-based approach.\n464.h264ref displays particularly bad pathological behavior in Pydgin\u2019s\ntracing JIT and is the only application to perform worse on pydgin-jit\nthan pydgin-nojit (9.6 MIPS vs. 21 MIPS).\n\nThe pathological behavior demonstrated by 464.h264ref was of particular\nconcern because it caused pydgin-jit to perform even worse than having no\nJIT at all. RPython JIT logs indicated that the reason for this performance\ndegradation was a large number of tracing aborts due to JIT traces growing too\nlong. However, time limitations before the publication deadline prevented us\nfrom investigating this issue thoroughly.\nSince the deadline we've applied some minor bug fixes and made some small\nimprovements in the memory representation.\nMore importantly, we've addressed the performance degradation in 464.h264ref\nby increasing trace lengths for the JIT.\nBelow we show how the performance of 464.h264ref changes as the\ntrace_limit parameter exposed by the RPython JIT is varied from the default\nsize of 6000 operations.\n\n\n\n\nBy quadrupling the trace limit we achieve an 11x performance improvement in\n464.h264ref.\nThe larger trace limit allows the JIT to optimize long code paths that were\npreviously triggering trace aborts, greatly helping amortize the costs of\ntracing.\nNote that arbitrarily increasing this limit can potentially hurt performance if\nlonger traces are not able to detect optimizable code sequences.\nAfter performing similar experiments across the applications in the SPEC\nCINT2006 benchmark suite, we settled on a trace limit of 400,000 operations.\nIn the figure below we show how the updated Pydgin ISS (pydgin-400K) improves\nperformance across all benchmarks and fixes the performance degradation\npreviously seen in 464.h264ref. Note that the non-JITted simulators have been\nremoved for clarity, and simulation performance is now plotted on a\nlinear scale to more clearly distinguish the performance gap between\neach ISS.\n\n\n\nWith these improvements, we are now able to beat simit-jit on all but two\nbenchmarks. In future work we hope to further close the gap with QEMU as well.\n\n\nConclusions and Future Work\nPydgin demonstrates that the impressive work put into the RPython translation\ntoolchain, designed to simplify the process of building fast dynamic-language\nVMs, can also be leveraged to build fast instruction set simulators.\nOur prototype ARMv5 ISS shows that Pydgin can generate ISSs with performance\ncompetitive to SimIt-ARM while also providing a more productive development\nexperience: RPython allowed us to develop Pydgin with only four person-months\nof work.\nAnother significant benefit of the Pydgin approach is that any performance\nimprovements applied to the RPython translation toolchain immediately benefit\nPydgin ISSs after a simple software download and retranslation.\nThis allows Pydgin to track the continual advances in JIT technology introduced\nby the PyPy development team.\nPydgin is very much a work in progress. There are many features we would like\nto add, including:\n\nmore concise syntax for accessing arbitrary instruction bits\nsupport for other C libraries such as glibc, uClibc, and musl\n(we currently only support binaries compiled with newlib)\nsupport for self-modifying code\nfeatures for more productive debugging of target applications\nISS descriptions for other ISAs such as RISC-V, ARMv8, and x86\nautomatic generation of compilers and toolchains from Pydgin descriptions\n\nIn addition, we think there are opportunities for even greater performance\nimprovements with more advanced techniques such as:\n\nautomatic generation of optimized instruction decoders\noptimizations for floating-point intensive applications\nmultiple tracing-JITs for parallel simulation of multicore SOCs\na parallel JIT compilation engine as proposed by Bo\u0308hm et al. [3]\n\nWe hope that Pydgin can be of use to others, so if you try it out please let us\nknow what you think. Feel free to contact us if you find any of the above\ndevelopment projects interesting, or simply fork the project on GitHub and hack\naway!\n-- Derek Lockhart and Berkin Ilbeyi\n\n\nAcknowledgements\n We would like to sincerely thank Carl Friedrich Bolz and Maciej Fijalkowski for their feedback on the Pydgin publication and their guidance on improving the JIT performance of our simulators. We would also like to thank for the whole PyPy team for their incredible work on the PyPy and the RPython translation toolchain. Finally, thank you to our research advisor, Prof. Christopher Batten, and the sponsors of this work which include the National Science Foundation, the Defense Advanced Research Projects Agency, and Intel Corporation.\n\n\nFootnotes\n\n\n\n[a]Pydgin loosely stands for [Py]thon [D]SL for [G]enerating\n[In]struction set simulators and is pronounced the same as \u201cpigeon\u201d. The\nname is inspired by the word \u201cpidgin\u201d which is a grammatically simplified\nform of language and captures the intent of the Pydgin embedded-ADL.\nhttps://github.com/cornell-brg/pydgin\n\n\n\n\n\n[b]Popular instruction set architectures (ISAs) include MIPs, ARM,\nx86, and more recently RISC-V\n\n\n\n\n\n[c](1, 2) For a good discussion of simulators vs. emulators, please see the\nfollowing post on StackOverflow:\nhttps://stackoverflow.com/questions/1584617/simulator-or-emulator-what-is-the-difference\n\n\n\n\n\n[d]https://en.wikipedia.org/wiki/Dark_silicon\n\n\n\n\n\n[e]Please see the Pydgin paper for a more detailed discussion of prior work.\n\n\n\n\n\n[f]For more examples of Pydgin ISA specifications, please see the ISPASS\npaper [1] or the Pydgin source code on GitHub.\nPydgin instruction definitions for a simple MIPS-inspired ISA can be\nfound here:\n\nhttps://github.com/cornell-brg/pydgin/blob/master/parc/isa.py\n\nPydgin instruction definitions for a simplified ARMv5 ISA can be found\nhere:\n\nhttps://github.com/cornell-brg/pydgin/blob/master/arm/isa.py\n\n\n\n\n\n\n\n[g]gem5 is a cycle-level simulation framework that contains both\nfunctional-level (atomic) and cycle-level processor models. Although\nprimarily used for detailed, cycle-approximate processor simulation,\ngem5's atomic model is a popular tool for many ISS tasks.\n\nhttps://www.m5sim.org/SimpleCPU\n\n\n\n\n\n\n\n[h]All performance measurements were taken on an unloaded server-class\nmachine.\n\n\n\n\n\n[i]Millions of instructions per second.\n\n\n\n\nReferences\n\n\n\n[1](1, 2, 3) Derek Lockhart, Berkin Ilbeyi, and Christopher Batten. \"Pydgin:\nGenerating Fast Instruction Set Simulators from Simple Architecture\nDescriptions with Meta-Tracing JIT Compilers.\" IEEE Int'l Symp. on\nPerformance Analysis of Systems and Software (ISPASS), Mar. 2015.\n\nhttps://csl.cornell.edu/~cbatten/pdfs/lockhart-pydgin-ispass2015.pdf\nhttps://github.com/cornell-brg/pydgin\n\n\n\n\n\n\n\n[2]Derek Lockhart, Gary Zibrat, and Christopher Batten. \"PyMTL: A Unified\nFramework for Vertically Integrated Computer Architecture Research.\" 47th\nACM/IEEE Int'l Symp. on Microarchitecture (MICRO-47), Dec. 2014.\n\nhttps://csl.cornell.edu/~cbatten/pdfs/lockhart-pymtl-micro2014.pdf\nhttps://github.com/cornell-brg/pymtl\n\n\n\n\n\n\n\n[3]I. Bo\u0308hm, B. Franke, and N. Topham. Generalized Just-In-Time Trace\nCompilation Using a Parallel Task Farm in a Dynamic Binary Translator.\nACM SIGPLAN Conference on Programming Language Design and Implementation\n(PLDI), Jun 2011.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html"
    },
    {
      "title": "Experiments in Pyrlang with RPython",
      "text": "Pyrlang is an Erlang BEAM bytecode interpreter written in RPython.\nIt implements approximately 25% of BEAM instructions. It can support\ninteger calculations (but not bigint), closures, exception handling,\nsome operators to atom, list and tuple, user modules, and multi-process\nin single core. Pyrlang is still in development.\nThere are some differences between BEAM and the VM of PyPy:\n\nBEAM is a register-based VM, whereas the VM in PyPy is stack-based.\nThere is no traditional call-stack in BEAM. The Y register in BEAM is\nsimilar to a call-stack, but the Y register can sometimes store some\nvariables.\nThere are no typical language-level threads and OS-level threads in\nBEAM; only language-level processes, whose behavior is very similar\nto the actor model.\n\nRegarding bytecode dispatch loop, Pyrlang uses a while loop to fetch\ninstructions and operands, call the function corresponding to every\ninstruction, and jump back to the head of the while loop. Due to the\ndifferences between the RPython call-stack and BEAM\u2019s Y register, we\ndecided to implement and manage the Y register by hand. On the other\nhand, PyPy uses RPython\u2019s call stack to implement Python\u2019s call stack.\nAs a result, the function for the dispatch loop in PyPy calls itself\nrecursively. This does not happen in Pyrlang.\nThe Erlang compiler (erlc) usually compiles the bytecode instructions\nfor function invocation into CALL (for normal invocation) and CALL_ONLY\n(for tail recursive invocation). You can use a trampoline semantic to\nimplement it:\n\nCALL instruction: The VM pushes the current instruction pointer (or\ncalled-program counter in PyPy) to the Y register, and jumps to the\ndestination label. When encountering a RETURN instruction, the VM\npops the instruction pointer from the Y register and returns to the\nlocation of the instruction pointer to continue executing the outer\nfunction.\nCALL_ONLY instruction: The VM simply jumps to the destination label,\nwithout any modification of the Y register. As a result, the tail\nrecursive invocation never increases the Y register.\n\nThe current implementation only inserts the JIT hint of can_enter_jit\nfollowing the CALL_ONLY instruction. This means that the JIT only\ntraces the tail-recursive invocation in Erlang code, which has a very\nsimilar semantic to the loop in imperative programming languages like\nPython.\nWe have also written a single scheduler to implement the language level\nprocess in a single core. There is a runable queue in the scheduler. On\neach iteration, the scheduler pops one element (which is a process\nobject with dispatch loop) from the queue, and executes the dispatch\nloop of the process object. In the dispatch loop, however, there is a\ncounter-call \u201creduction\u201d inside the dispatch loop. The reduction\ndecrements during the execution of the loop, and when the reduction\nbecomes 0, the dispatch loop terminates. Then the scheduler pushes that\nelement into the runable queue again, and pops the next element for the\nqueue, and so on.\nWe are planning to implement a multi-process scheduler for multi-core\nCPUs, which will require multiple schedulers and even multiple runable\nqueues for each core, but that will be another story. :-)\n\nMethods\nWe wrote two benchmark programs of Erlang:\n\nFACT: A benchmark to calculate the factorial in a tail-recursive\nstyle, but because we haven\u2019t implemented big int, we do a remainder\ncalculation to the argument for the next iteration, so the number\nnever overflows.\nREVERSE: The benchmark creates a reversed list of numbers, such as\n[20000, 19999, 19998, \u2026], and applies a bubble sort to it.\n\n\n\nResults\n\nThe Value of Reduction\nWe used REVERSE to evaluate the JIT with different values of\nreduction:\n\n\nThe X axis is the value of reduction, and the Y axis is the execution\ntime (by second).\nIt seems that when the value of reduction is small, the reduction\ninfluences the performance significantly, but when reduction becomes\nlarger, it only increases the speed very slightly. In fact, we use 2000\nas the default reduction value (as well as the reduction value in the\nofficial Erlang interpreter).\nSurprisingly, the trace is always generated even when the reduction is\nvery small, such as 0, which means the dispatch loop can only run for a\nvery limited number of iterations, and the language level process\nexecutes fewer instructions than an entire loop in one switch of the\nscheduler). The generated trace is almost the same, regardless of\ndifferent reduction values.\nActually, the RPython JIT only cares what code it meets, but does not\ncare who executes it, thus the JIT always generates the results above.\nThe trace even can be shared among different threads if they execute the\nsame code.\nThe overhead at low reduction value may be due to the scheduler, which\nswitches from different processes too frequently, or from the\ntoo-frequent switching between bytecode interpreter and native code, but\nnot from JIT itself.\nHere is more explanation from Armin Rigo:\n\n\u201cThe JIT works well because you\u2019re using a scheme where some counter\nis decremented (and the soft-thread interrupted when it reaches\nzero) only once in each app-level loop. The soft-thread switch is\ndone by returning to some scheduler, which will resume a different\nsoft-thread by calling it. It means the JIT can still compile each\nof the loops as usual, with the generated machine code containing\nthe decrease-and-check-for-zero operation which, when true, exits\nthe assembler.\"\n\n\nFair Process Switching vs. Unfair Process Switching\nWe are also concerned about the timing for decreasing reduction value.\nIn our initial version of Pyrlang, we decrease reduction value at every\nlocal function invocation, module function invocation, and BIF (built-in\nfunction) invocation, since this is what the official Erlang interpreter\ndoes. However, since the JIT in RPython basically traces the target\nlanguage loop (which is the tail recursive invocation in Pyrlang) it is\ntypically better to keep the loop whole during a switch of the language\nlevel process. We modified Pyrlang, and made the reduction decrement\nonly occur after CALL_ONLY, which is actually the loop boundary of the\ntarget language.\nOf course, this strategy may cause an \u201cunfair\u201d execution among language\nlevel processes. For example, if one process has only a single\nlong-sequence code, it executes until the end of the code. On the other\nhand, if a process has a very short loop, it may be executed by very\nlimited steps then be switched out by the scheduler. However, in the\nreal world, this \u201cunfairness\u201d is usually considered acceptable, and is\nused in many VM implementations including PyPy for improving the overall\nperformance.\nWe compared these two versions of Pyrlang in the FACT benchmark. The\nreduction decrement is quite different because there are some BIF\ninvocations inside the loop. In the old version the process can be\nsuspended at loop boundaries or other function invocation, but in the\nnew version, it can be suspended only at loop boundaries.\nWe show that the strategy is effective, removing around 7% of the\noverhead. We have also compared it in REVERSE, but since there are no\nextra invocations inside the trace, it cannot provide any performance\nimprovement. In the real world, we believe there is usually more than\none extra invocation inside a single loop, so this strategy is effective\nfor most cases.\n\n\nComparison with Default Erlang and HiPE\nWe compared the performance of Pyrlang with the default Erlang\ninterpreter and the HiPE (High Performance Erlang) complier. HiPE is an\nofficial Erlang compiler that can compile Erlang source code to native\ncode. The speed of Erlang programs obviously improves but loses its\ngenerality instead.\nPlease note that Pyrlang is still in development, so in some situations\nit does less work than the default Erlang interpreter, such as not\nchecking integer overflow when dealing with big integer, and not\nchecking and adding locks when accessing message queues in the\nlanguage-level process, so is therefore faster. The final version of\nPyrlang may be slower.\nWe used the two benchmark programs above, and made sure both of them are\nexecuted for more than five seconds to cover the JIT warm-up time for\nRPython. The experiment environment is a OS X 10.10 machine with 3.5GHZ\n6-core Intel Xeon E5 CPU and 14GB 1866 MHz DDR3 ECC memory.\nLet\u2019s look at the result of FACT. The graph shows that Pyrlang runs\n177.41% faster on average than Erlang, and runs at almost the same speed\nas HiPE. However, since we haven\u2019t implemented big integer in Pyrlang,\nthe arithmetical operators do not do any extra overflow checking. It is\nreasonable that the final version for Pyrlang will be slower than the\ncurrent version and HiPE.\n\nAs for REVERSE, the graph shows that Pyrlang runs 45.09% faster than\nErlang, but 63.45% slower than HiPE on average. We think this is\nreasonable because there are only few arithmetical operators in this\nbenchmark so the speeds of these three implementations are closer.\nHowever, we observed that at the scale of 40,000, the speed of Pyrlang\nslowed down significantly (111.35% slower than HiPE) compared with the\nother two scales (56.38% and 22.63% slower than HiPE).\nUntil now we can only hypothesize why Pyrlang slows down at that scale.\nWe guess that the overhead might be from GC. This is because the BEAM\nbytecode provides some GC hints to help the default Erlang compiler to\nperform some GC operations immediately. For example, using GC_BIF\ninstead of a BIF instruction tells the VM that there may be a GC\nopportunity, and tells the VM how many live variables should be around\none instruction. In Pyrlang we do not use these kinds of hints but rely\non RPython\u2019s GC totally. When there are a huge number of objects during\nruntime, (as for REVERSE, it should be the Erlang list object) the speed\ntherefore slows down.\n\nRuochen Huang",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/02/experiments-in-pyrlang-with-rpython-8103387814587972227.html"
    },
    {
      "title": "linalg support in pypy/numpy",
      "text": "Introduction\nPyPy's numpy support has matured enough that it can now support the  lapack/blas libraries through the numpy.linalg module. To install the  version of numpy this blog post refers to, install PyPy version 2.5.0 or  newer, and run this:\n\npypy -m pip install git+https://bitbucket.org/pypy/numpy.git\n\n\nThis update is a major step forward for PyPy's numpy support. Many of  the basic matrix operations depend on linalg, even matplotlib requires  it to display legends (a pypy-friendly version of matplotlib 1.3 is  available  at https://github.com/mattip/matplotlib).\n\nA number of improvements and adaptations, some of which are in the newly-released PyPy 2.5.0, made this possible:\n\nSupport for an extended frompyfunc(), which in the PyPy  version supports much of the ufunc API (signatures, multiple dtypes)  allowing creation of pure-python, jit-friendly ufuncs. An additional  keyword allows choosing between out = func(in) or func(in, out) ufunc signatures. More explanation follows.\nSupport for GenericUfuncs via PyPy's (slow) capi-compatibility  layer. The underlying mechanism actually calls the internal  implementation of frompyfunc().\nA cffi version of _umath_linalg. Since cffi uses dlopen()  to call into shared objects, we added support in the numpy build system  to create non-python shared libraries from source code in the numpy  tree. We also rewrote parts of the c-based _umath_linalg.c.src in python, renamed numpy's umath_linalg capi module to umath_linag_capi, and use it as a shared object through cffi.\n\n\n\nStatus\nWe have not completely implemented all the linalg features. dtype  resolution via casting is missing, especially for complex ndarrays,  which leads to slight numerical errors where numpy uses a more precise  type for intermediate calculations. Other missing features in PyPy's  numpy support may have implications for complete linalg support.\n\nSome OSX users have noticed they need to update pip to version 6.0.8 to overcome a regression in pip, and it is not clear if we support all combinations of blas/lapack implementations on all platforms.\n\nOver  the next few weeks we will be ironing out these issues.\n\n\nPerformance\nA simple benchmark is shown below, but let's state the obvious:  PyPy's JIT and the iterators built into PyPy's ndarray implementation  will in most cases be no faster than CPython's numpy. The JIT can help  where there is a mixture of python and numpy-array code. We do have  plans to implement lazy evaluation and to further optimize PyPy's  support for numeric python, but numpy is quite good at what it does.\n\n\nHowTo for PyPy's extended frompyfunc \nThe magic enabling blas support is a rewrite of the _umath_linalg c-based module as a cffi-python module that creates ufuncs via frompyfunc. We extended the numpy frompyfunc to allow it to function as a replacement for the generic ufunc available in numpy only through the c-api.\n\nWe start with the basic frompyfunc, which wraps a python function into a ufunc:\n\u00a0\ndef times2(in0):\n    return in0 * 2\nufunc = frompyfunc(times2, 1, 1)\n\n\nIn cpython's numpy the dtype of the result is always object, which is  not implemented (yet) in PyPy, so this example will fail. While the  utility of object dtypes can be debated, in the meantime we add a  non-numpy-compatible keyword argument dtypes to frompyfunc. If dtype=['match'] the output dtype will match the dtype of the first input ndarray:\n\nufunc = frompyfunc(times2, 1, 1, dtype=['match'])\nai = arange(24).reshape(3, 4, 2)\nao = ufunc(ai)\nassert  (ao == ai * 2).all()\n\n\nI hear you ask \"why is the dtypes keyword argument a list?\" This is so we can support the Generalized Universal Function API, which allows specifying a number of specialized functions and the input-output dtypes each specialized function accepts.\nNote that the function feeds the values of ai one at a time,  the function operates on scalar values. To support more complicated  ufunc calls, the generalized ufunc API allows defining a signature,  which specifies the layout of the ndarray inputs and outputs. So we extended frompyfunc with a signature keyword as well.\nWe add one further extension to frompyfunc: we allow a Boolean keyword stack_inputs to specify the argument layout of the function itself. If the function is of the form:\n\u00a0\nout0, out1, ... = func(in0, in1,...)\n\n\nthen stack_inputs is False. If it is True the function is of the form:\n\u00a0\nfunc(in0, in1, ... out0, out1, ...)\n\n\nHere is a complete example of using frompyfunc to create a ufunc, based on this link:\n\u00a0\ndef times2(in_array, out_array):\n    in_flat = in_array.flat\n    out_flat = out_array.flat\n    for i in range(in_array.size):\n        out_flat[i] = in_flat[i] * 2\nufunc = frompyfunc([times2, times2], 1, 1,\n                signature='(i)->(i)',\n                dtypes=[dtype(int), dtype(int),\n                        dtype(float), dtype(float),\n                       ],\n                stack_inputs=True,\n                )\nai = arange(10, dtype=int)\nai2 = ufunc(ai)\nassert all(ai2 == ai * 2)\n\n\nUsing this extended syntax, we rewrote the lapack calls into the blas  functions in pure python, no c needed. Benchmarking this approach  actually was much slower than using the upstream umath_linalg  module via cpyext, as can be seen in the following benchmarks. This is  due to the need to copy c-aligned data into Fortran-aligned format. Our __getitem__ and __setitem__ iterators are not as fast as pointer arithmetic in C. So we next tried a hybrid approach: compile and use numpy's umath_linalg python module as a shared object, and call the optimized specific wrapper function from it.\n\n\nBenchmarks\nHere are some benchmarks, running a tight loop of the different versions of linalg.inv(a), where a is a 10x10 double ndarray. The benchmark ran on an i7 processor running ubuntu 14.04 64 bit:\n\n Impl. Time after warmup \n \n CPython 2.7 + numpy 1.10.dev + lapack 8.9 msec/1000 loops \n PyPy 2.5.0  + numpy + lapack via cpyext 8.6 msec/1000 loops \n PyPy 2.5.0  + numpy + lapack via pure python + cffi 19.9 msec/1000 loops \n PyPy 2.5.0  + numpy + lapack via python + c + cffi 9.5 msec/1000 loops\n\n\n\n\n\n\n\n\nWhile no general conclusions may be drawn from a single micro-benchmark, it does indicate that there is some merit in the approach taken. \n\nConclusion\nPyPy's numpy now includes a working linalg module. There are still  some rough corners, but hopefully we have implemented the parts you  need. While the speed of the isolated linalg function is no faster than  CPython and upstream numpy, it should not be significantly slower  either. Your use case may see an improvement if you use a mix of python  and lapack, which is the usual case.\n\nPlease let us know how it goes. We love to hear success stories too.\n\nWe still have challenges at all levels of programming,and are always  looking for people willing to contribute, so stop by on IRC at #pypy.\n\nmattip and the PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/02/linalg-support-in-pypynumpy-1131217944329711855.html"
    },
    {
      "title": "NumPyPy status - January 2015",
      "text": "Hi Everyone\n\nHere is what has been done in January thanks to the funding of NumPyPy,\u00a0I would like to thank all the donors and tell you that you can still donate\u00a0:\n\nI have focused on implementing the object dtype this month, it is now possible to store objects inside ndarrays using the object dtype\nIt is also possible to add an object ndarray to any other ndarray (implementing other operators is trivial)\n\n\nThe next things I plan on working on next are :\n\n\nImplementing the missing operations for object arrays\nImplementing garbage collection support for object arrays (currently, storing an object inside an ndarray doesn't keep the object alive)\nPackaging NumPyPy on PyPI\n\n\nCheers\n\n\nRomain",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/02/numpypy-status-january-2015-5092986229783279944.html"
    },
    {
      "title": "PyPy 2.5.0 released",
      "text": "PyPy 2.5.0 - Pincushion Protea\nWe\u2019re pleased to announce PyPy 2.5, which contains significant performance\nenhancements and bug fixes.\nYou can download the PyPy 2.5.0 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject, and for those who donate to our three sub-projects, as well as our\nvolunteers and contributors (10 new commiters joined PyPy since the last\nrelease).\nWe\u2019ve shown quite a bit of progress, but we\u2019re slowly running out of funds.\nPlease consider donating more, or even better convince your employer to donate,\nso we can finish those projects! The three sub-projects are:\n\n\n\nPy3k (supporting Python 3.x): We have released a Python 3.2.5 compatible version\n\nwe call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version\n\n\n\nSTM (software transactional memory): We have released a first working version,\nand continue to try out new promising paths of achieving a fast multithreaded Python\n\n\nNumPy which requires installation of our fork of upstream numpy,\navailable on bitbucket\n\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows, and OpenBSD),\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\nWhile we support 32 bit python on Windows, work on the native Windows 64\nbit python is still stalling, we would welcome a volunteer\nto handle that.\n\n\nHighlights\n\nThe past months have seen pypy mature and grow, as rpython becomes the goto\nsolution for writing fast dynamic language interpreters. Our separation of\nrpython and the python interpreter PyPy is now much clearer in the\nPyPy documentation  and we now have separate RPython documentation.\nWe have improved warmup time as well as jitted code performance: more than 10%\ncompared to pypy-2.4.0.\nWe no longer zero-out memory allocated in the gc nursery by default, work that\nwas started during a GSoC.\nPassing objects between C and PyPy has been improved. We are now able to pass\nraw pointers to C (without copying) using pinning. This improves I/O;\nbenchmarks that use networking intensively improved by about 50%. File()\noperations still need some refactoring but are already showing a 20%\nimprovement on our benchmarks. Let us know if you see similar improvements.\nOur integrated numpy support gained much of the GenericUfunc api in order to\nsupport the lapack/blas linalg module of numpy. This dovetails with work in the\npypy/numpy repository to support linalg both through the (slower) cpyext capi\ninterface and also via (the faster) pure python cffi interface, using an\nextended frompyfunc() api. We will soon post a seperate blog post specifically\nabout linalg and PyPy.\nDictionaries are now ordered by default, see the blog post\nOur nightly translations use \u2013shared by default, including on OS/X and linux\nWe now more carefully handle errno (and GetLastError, WSAGetLastError) tying\nthe handlers as close as possible to the external function call, in non-jitted\nas well as jitted code.\nIssues reported with our previous release were resolved after reports from users on\nour issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at\n#pypy.\n\nWe have further improvements on the way: rpython file handling,\nfinishing numpy linalg compatibility, numpy object dtypes, a better profiler,\nas well as support for Python stdlib 2.7.9.\nPlease try it out and let us know what you think. We especially welcome\nsuccess stories, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/02/pypy-250-released-247160062953533060.html"
    },
    {
      "title": "Faster, more memory efficient and more ordered dictionaries on PyPy",
      "text": "Hello everyone!\nAs of today, we merged the latest branch that brings better dictionaries to PyPy by default. The work is based on an idea by Raymond Hettinger on python-dev, with prior work done notably in Java.\u00a0 It was done by Maciej Fija\u0142kowski and Armin Rigo, with Laurence Tratt recently prodding us to finish it.\u00a0 (Earlier work going in a similar direction include Alex Gaynor's work on ordered dicts in Topaz, which was also used in the Hippy VM.\u00a0 Each of these pieces of work is itself based on the original dict implementation in RPython, whose origins fade in the Subversion prehistory of PyPy.)\u00a0 Coincidentally, a very similar idea has been implemented in Zend PHP very recently. Zend implementation description.\nThis post covers the basics of design and implementation as well as some basic benchmarks.\n\n\nDictionaries are now ordered!\nOne surprising part is that the new design, besides being more\nmemory efficient, is ordered by design: it preserves the\ninsertion order.\u00a0 This is not forbidden by the Python language, which allows any order.\u00a0 It makes the collections.OrderedDict subclass much faster than before: it is now a thin subclass of dict.\u00a0 Obviously, we recommend that any portable Python program continues to use OrderedDict when ordering is important.\u00a0 Note that a non-portable program might rely on more: for example, a **keywords argument now receives the keywords in the same order as the one in which they were given in the call.\u00a0 (Whether such a thing might be called a language design change or not is a bit borderline.)\u00a0 The point is that Python programs that work on CPython or previous versions of PyPy should continue to work on PyPy.\nThere is one exception, though.\u00a0 The iterators of the OrderedDict subclass are now working just like the ones of the dict builtin: they will raise RuntimeError when iterating if the dictionary was modified.\u00a0 In the CPython design, the class OrderedDict explicitly doesn't worry about that, and instead you get some result that might range from correct to incorrect to crashes (i.e. random Python exceptions).\n\n\nOriginal PyPy dictionary design\nOriginally, PyPy dictionaries, as well as CPython dictionaries\nare implemented as follows (simplified view):\n\nstruct dict {\n   long num_items;\n   dict_entry* items;\u00a0\u00a0 /* pointer to array */\n}\n\nstruct dict_entry {\n   long hash;\n   PyObject* key;\n   PyObject* value;\n}\n\nWhere items is a sparse array, with 1/3 to 1/2 of the items being NULL.\nThe average space occupied by a dictionary is 3 * WORD * 12/7 plus some small constant (the smallest dict has 8 entries, which is\n8 * 3 * WORD + 2 * WORD = 26 WORDs).\n\n\nNew PyPy dictionary design\nThe new PyPy dictionary is split in two arrays:\n\nstruct dict {\n    long num_items;\n    variable_int *sparse_array;\n    dict_entry* compact_array;\n}\n\nstruct dict_entry {\n    long hash;\n    PyObject *key;\n    PyObject *value;\n}\n\nHere, compact_array stores all the items in order of insertion, while sparse_array is a 1/2 to 2/3 full array of integers. The integers themselves are of the smallest size necessary for indexing the compact_array. So if compact_array has less than 256 items, then sparse_array will be made of bytes; if less than 216, it'll be two-byte integers; and so on.\nThis design saves quite a bit of memory. For example, on 64bit systems we can, but almost never, use indexing of more than 4 billion elements; and for small dicts, the extra sparse_array takes very little space.\u00a0 For example a 100 element dict, would be on average for the original design on 64bit: 100 * 12/7 * WORD * 3 =~ 4100 bytes, while on new design it's 100 * 12/7 + 3 * WORD * 100 =~ 2600 bytes, quite a significant saving.\n\n\nGC friendliness\nThe obvious benefit of having more compact dictionaries is an increased cache friendliness. In modern CPUs cache misses are much more costly than doing additional simple work, like having an additional level of (in-cache) indirection. Additionally, there is a GC benefit coming from it. When doing a minor collection, the GC has to visit all the GC fields in old objects that can point to young objects. In the case of large arrays, this can prove problematic since the array grows and with each minor collection we need to visit more and more GC pointers. In order to avoid it, large arrays in PyPy employ a technique called \"card marking\" where the GC only visits \"cards\" or subsets of arrays that were modified between collections. The problem with dictionaries was that by design modifications in a dictionary occur randomly, hence a lot of cards used to get invalidated. In the new design, however, new items are typically appended to the compact_array, hence invalidate much fewer cards --- which improves GC performance.\u00a0 (The new sparse_array is an array of integers, so it does not suffer from the same problems.)\n\n\nDeletion\nDeleting entries from dictionaries is not very common, but important in a few use cases.\u00a0 To preserve order, when we delete an entry, we mark the entry as removed but don't otherwise shuffle the remaining entries.\u00a0 If we repeat this operation often enough, there will be a lot of removed entries in the (originally compact) array.\u00a0 At this point, we need to do a \"packing\" operation, which moves all live entries to the start of the array (and then reindexes the sparse array, as the positions changed).\u00a0 This works well, but there are use cases where previously no reindexing was ever needed, so it makes these cases a bit slower (for example when repeatedly adding and removing keys in equal number).\n\n\nBenchmarks\nThe PyPy speed benchmarks show mostly small effect, see changes. The microbenchmarks that we did show large improvements on large and very large dictionaries (particularly, building dictionaries of at least a couple 100s of items is now twice faster) and break-even on small ones (between 20% slower and 20% faster depending very much on the usage patterns and sizes of dictionaries). The new dictionaries enable various optimization possibilities which we're going to explore in the near future.\nCheers,\nfijal, arigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/01/faster-more-memory-efficient-and-more-4096950404745375390.html"
    },
    {
      "title": "Leysin Winter Sprint (20-28th February 2015)",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the tenth time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\n\nGoals and topics of the sprint\n\nThe details depend on who is here and ready to work.  We might touch\ntopics such as:\n\n\ncleaning up the optimization step in the JIT, change the register\nallocation done by the JIT's backend, or improvements to the\nwarm-up time\n\nSTM (Software Transaction Memory), notably: try to come up with\nbenchmarks, and measure them carefully in order to test and improve\nthe conflict reporting tools, and more generally to figure out how\npractical it is in large projects to avoid conflicts\n\nvmprof - a statistical profiler for CPython and PyPy work, including\nmaking it more user friendly.\n\nPy3k (Python 3.x support), NumPyPy (the numpy module)\n\nadded: cffi 1.0, trying out pygame+cffi on Raspberry Pi devices\n\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski.\n\n\n\n\nExact times\n\nFor a change, and as an attempt to simplify things, I specified the\ndates as 20-28 Februrary 2015, where 20 and 28 are travel days.  We will\nwork full days between the 21 and the 27.  You are of course allowed to\nshow up for a part of that time only, too.\n\nLocation and Accomodation\n\nLeysin, Switzerland, \"same place as before\".  Let me refresh your\nmemory: both the sprint venue and the lodging will be in a very spacious\npair of chalets built specifically for bed & breakfast:\nErmina.  The place has a good ADSL Internet connection\nwith wireless installed.  You can of course arrange your own lodging\nanywhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue), but I definitely recommend\nlodging there too -- you won't find a better view anywhere else (though\nyou probably won't get much worse ones easily, either :-)\n\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.  In the past, the rates were around 60 CHF a\nnight all included in 2-person rooms, with breakfast.  Now, the rooms\navailable are either single-person (or couple), or rooms for 3 persons.\nThe latter choice is recommended and should be under 60 CHF per person.\n\nPlease register by Mercurial, or on the pypy-dev mailing list if you do not yet have check-in rights.\n\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around, and at least one EU-format power strip.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/01/leysin-winter-sprint-20-28th-february-2590212640945547308.html"
    },
    {
      "title": "September donations and thank you to the Python Software Foundation!",
      "text": "Hello everyone!\nWe would like to show you a short update on the PyPy funding.\nWe gathered a total of $15,986 in the month of September and as per\nearlier agreement, the Python Software Foundation donated $10,000\nto PyPy. We would like to thank everyone participating and the PSF in\nparticular for supporting the PyPy project and making our work possible!\nWe've been working hard on the goals outlined in the funding proposals.\n\nPyPy Python 3 support has been in beta for a while and it's already\nbeing used by many people, as seen per the number of reported bugs.\nWe're currently supporting 3.2, planning on moving towards 3.4 in the\nfuture.\nSoftware Transactional Memory has been a successful research project,\nwith first real world results shown during the Warsaw sprint.\nMore detailed update on numpy will be published soon. A little spoiler is\nthat we're planning on addressing matplotlib, scipy and the larger ecosystem\nto some extent. Stay tuned!\n\nAgain, thanks to everyone who donated and happy Thanksgiving to everyone\non that side of the world!\nCheers,\nfijal and the entire PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/11/september-donations-and-thank-you-to-4531550307707104017.html"
    },
    {
      "title": "Tornado without a GIL on PyPy STM",
      "text": "This post is by Konstantin Lopuhin, who tried PyPy STM during the\nWarsaw sprint.\nPython has a GIL, right? Not quite - PyPy STM is a python implementation\nwithout a GIL, so it can scale CPU-bound work to several cores.\nPyPy STM is developed by Armin Rigo and Remi Meier,\nand supported by community donations.\nYou can read more about it in the\ndocs.\nAlthough PyPy STM is still a work in progress, in many cases it can already\nrun CPU-bound code faster than regular PyPy, when using multiple cores.\nHere we will see how to slightly modify Tornado IO loop to use\ntransaction\nmodule.\nThis module is described\nin the docs and is really simple to use - please see an example there.\nAn event loop of Tornado, or any other asynchronous\nweb server, looks like this (with some simplifications):\n\nwhile True:\n    for callback in list(self._callbacks):\n        self._run_callback(callback)\n    event_pairs = self._impl.poll()\n    self._events.update(event_pairs)\n    while self._events:\n        fd, events = self._events.popitem()\n        handler = self._handlers[fd]\n        self._handle_event(fd, handler, events)\n\nWe get IO events, and run handlers for all of them, these handlers can\nalso register new callbacks, which we run too. When using such a framework,\nit is very nice to have a guaranty that all handlers are run serially,\nso you do not have to put any locks. This is an ideal case for the\ntransaction module - it gives us guaranties that things appear\nto be run serially, so in user code we do not need any locks. We just\nneed to change the code above to something like:\n\nwhile True:\n    for callback in list(self._callbacks):\n        transaction.add(                # added\n            self._run_callback, callback)\n    transaction.run()                   # added\n    event_pairs = self._impl.poll()\n    self._events.update(event_pairs)\n    while self._events:\n        fd, events = self._events.popitem()\n        handler = self._handlers[fd]\n        transaction.add(                # added\n            self._handle_event, fd, handler, events)\n    transaction.run()                   # added\n\nThe actual commit is\nhere,\n- we had to extract a little function to run the callback.\n\nPart 1: a simple benchmark: primes\nNow we need a simple benchmark, lets start with\nthis\n- just calculate a list of primes up to the given number, and return it\nas JSON:\n\ndef is_prime(n):\n    for i in xrange(2, n):\n        if n % i == 0:\n            return False\n    return True\n\nclass MainHandler(tornado.web.RequestHandler):\n    def get(self, num):\n        num = int(num)\n        primes = [n for n in xrange(2, num + 1) if is_prime(n)]\n        self.write({'primes': primes})\n\nWe can benchmark it with siege:\n\nsiege -c 50 -t 20s https://localhost:8888/10000\n\nBut this does not scale. The CPU load is at 101-104 %, and we handle 30 %\nless request per second. The reason for the slowdown is STM overhead,\nwhich needs to keep track of all writes and reads in order to detect conflicts.\nAnd the reason for using only one core is, obviously, conflicts!\nFortunately, we can see what this conflicts are, if we run code like this\n(here 4 is the number of cores to use):\n\nPYPYSTM=stm.log ./primes.py 4\n\nThen we can use print_stm_log.py\nto analyse this log. It lists the most expensive conflicts:\n\n14.793s lost in aborts, 0.000s paused (1258x STM_CONTENTION_INEVITABLE)\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/httpserver.py\", line 455, in __init__\n    self._start_time = time.time()\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/httpserver.py\", line 455, in __init__\n    self._start_time = time.time()\n...\n\nThere are only three kinds of conflicts, they are described in\nstm source,\nHere we see that two threads call into external function to get current time,\nand we can not rollback any of them, so one of them must wait till the other\ntransaction finishes.\nFor now we can hack around this by disabling this timing - this is only\nneeded for internal profiling in tornado.\nIf we do it, we get the following results (but see caveats below):\n\n\n\n\n\n\n\n\nImpl.\nreq/s\n\n\n\nPyPy 2.4\n14.4\n\nCPython\u00a02.7\n3.2\n\nPyPy-STM 1\n9.3\n\nPyPy-STM 2\n16.4\n\nPyPy-STM 3\n20.4\n\nPyPy\u00a0STM\u00a04\n24.2\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\nAs we can see, in this benchmark PyPy STM using just two cores\ncan beat regular PyPy!\nThis is not linear scaling, there are still conflicts left, and this\nis a very simple example but still, it works!\nBut its not that simple yet :)\nFirst, these are best-case numbers after long (much longer than for regular\nPyPy) warmup. Second, it can sometimes crash (although removing old pyc files\nfixes it). Third, benchmark meta-parameters are also tuned.\nHere we get relatively good results only when there are a lot of concurrent\nclients - as a results, a lot of requests pile up, the server is not keeping\nwith the load, and transaction module is busy with work running this piled up\nrequests. If we decrease the number of concurrent clients, results get slightly worse.\nAnother thing we can tune is how heavy is each request - again, if we ask\nprimes up to a lower number, then less time is spent doing calculations,\nmore time is spent in tornado, and results get much worse.\nBesides the time.time() conflict described above, there are a lot of others.\nThe bulk of time is lost in these two conflicts:\n\n14.153s lost in aborts, 0.000s paused (270x STM_CONTENTION_INEVITABLE)\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/web.py\", line 1082, in compute_etag\n    hasher = hashlib.sha1()\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/web.py\", line 1082, in compute_etag\n    hasher = hashlib.sha1()\n\n13.484s lost in aborts, 0.000s paused (130x STM_CONTENTION_WRITE_READ)\nFile \"/home/ubuntu/pypy/lib_pypy/transaction.py\", line 164, in _run_thread\n    got_exception)\n\nThe first one is presumably calling into some C function from stdlib, and we get\nthe same conflict as for time.time() above, but is can be fixed on PyPy\nside, as we can be sure that computing sha1 is pure.\nIt is easy to hack around this one too, just removing etag support, but if\nwe do it, performance is much worse, only slightly faster than regular PyPy,\nwith the top conflict being:\n\n83.066s lost in aborts, 0.000s paused (459x STM_CONTENTION_WRITE_WRITE)\nFile \"/home/arigo/hg/pypy/stmgc-c7/lib-python/2.7/_weakrefset.py\", line 70, in __contains__\nFile \"/home/arigo/hg/pypy/stmgc-c7/lib-python/2.7/_weakrefset.py\", line 70, in __contains__\n\nComment by Armin: It is unclear why this happens so far.  We'll investigate...\nThe second conflict (without etag tweaks) originates\nin the transaction module, from this piece of code:\n\nwhile True:\n    self._do_it(self._grab_next_thing_to_do(tloc_pending),\n                got_exception)\n    counter[0] += 1\n\nComment by Armin: This is a conflict in the transaction module itself; ideally,\nit shouldn't have any, but in order to do that we might need a little bit\nof support from RPython or C code.  So this is pending improvement.\nTornado modification used in this blog post is based on 3.2.dev2.\nAs of now, the latest version is 4.0.2, and if we\napply\nthe same changes to this version, then we no longer get any scaling on this benchmark,\nand there are no conflicts that take any substantial time.\nComment by Armin: There are two possible reactions to a conflict.  We can either\nabort one of the two threads, or (depending on the circumstances) just\npause the current thread until the other one commits, after which the\nthread will likely be able to continue.  The tool ``print_stm_log.py``\ndid not report conflicts that cause pauses.  It has been fixed very\nrecently.  Chances are that on this test it would report long pauses and\npoint to locations that cause them.\n\n\nPart 2: a more interesting benchmark: A-star\nAlthough we have seen that PyPy STM is not all moonlight and roses,\nit is interesting to see how it works on a more realistic application.\nastar.py\nis a simple game where several players move on a map\n(represented as a list of lists of integers),\nbuild and destroy walls, and ask server to give them\nshortest paths between two points\nusing A-star search, adopted from ActiveState recipie.\nThe benchmark bench_astar.py\nis simulating players, and tries to put the main load on A-star search,\nbut also does some wall building and destruction. There are no locks\naround map modifications, as normal tornado is executing all callbacks\nserially, and we can keep this guaranty with atomic blocks of PyPy STM.\nThis is also an example of a program that is not trivial\nto scale to multiple cores with separate processes (assuming\nmore interesting shared state and logic).\nThis benchmark is very noisy due to randomness of client interactions\n(also it could be not linear), so just lower and upper bounds for\nnumber of requests are reported\n\n\n\n\n\n\nImpl.\nreq/s\n\n\n\nPyPy 2.4\n5 .. 7\n\nCPython 2.7\n0.5 .. 0.9\n\nPyPy-STM 1\n2 .. 4\n\nPyPy STM 4\n2 .. 6\n\n\n\nClearly this is a very bad benchmark, but still we can see that scaling is worse\nand STM overhead is sometimes higher.\nThe bulk of conflicts come from the transaction module (we have seen it\nabove):\n\n91.655s lost in aborts, 0.000s paused (249x STM_CONTENTION_WRITE_READ)\nFile \"/home/ubuntu/pypy/lib_pypy/transaction.py\", line 164, in _run_thread\n    got_exception)\n\nAlthough it is definitely not ready for production use, you can already try\nto run things, report bugs, and see what is missing in user-facing tools\nand libraries.\nBenchmarks setup:\n\nAmazon c3.xlarge (4 cores) running Ubuntu 14.04\npypy-c-r74011-stm-jit for the primes benchmark (but it has more bugs\nthan more recent versions), and\npypy-c-r74378-74379-stm-jit\nfor astar benchmark (put it inside pypy source checkout at 38c9afbd253c)\nhttps://bitbucket.org/kostialopuhin/tornado-stm-bench at 65144cda7a1f",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/11/tornado-without-gil-on-pypy-stm-7284102716557557428.html"
    },
    {
      "title": "PyPy IO improvements",
      "text": "Hello everyone!\nWe've wrapped up the Warsaw sprint, so I would like to describe some\nbranches which have been recently merged and which improved the I/O and the\nGC: gc_no_cleanup_nursery and gc-incminimark-pinning.\nThe first branch was started by Wenzhu Man for her Google Summer of Code\nand finished by Maciej Fija\u0142kowski and Armin Rigo.\nThe PyPy GC works by allocating new objects in the young object\narea (the nursery), simply by incrementing a pointer. After each minor\ncollection, the nursery has to be cleaned up. For simplicity, the GC used\nto do it by zeroing the whole nursery.\nThis approach has bad effects on the cache, since you zero a large piece of\nmemory at once and do unnecessary work for things that don't require zeroing\nlike large strings. We mitigated the first problem somewhat with incremental\nnursery zeroing, but this branch removes the zeroing completely, thus\nimproving the string handling and recursive code (since jitframes don't\nrequires zeroed memory either). I measured the effect on two examples:\na recursive implementation of  fibonacci and gcbench,\nto measure GC performance.\nThe results for fibonacci and gcbench are below (normalized to cpython\n2.7). Benchmarks were run 50 times each (note that the big standard\ndeviation comes mostly from the warmup at the beginning, true figures\nare smaller):\n\n\n\n\n\n\n\n\n\nbenchmark\nCPython\nPyPy 2.4\nPyPy non-zero\n\nfibonacci\n4.8+-0.15 (1.0x)\n0.59+-0.07 (8.1x)\n0.45+-0.07 (10.6x)\n\ngcbench\n22+-0.36 (1.0x)\n1.34+-0.28 (16.4x)\n1.02+-0.15 (21.6x)\n\n\n\n\nThe second branch was done by Gregor Wegberg for his master thesis and finished\nby Maciej Fija\u0142kowski and Armin Rigo. Because of the way it works, the PyPy GC from\ntime to time moves the objects in memory, meaning that their address can change.\nTherefore, if you want to pass pointers to some external C function (for\nexample, write(2) or read(2)), you need to ensure that the objects they are\npointing to will not be moved by the GC (e.g. when running a different thread).\nPyPy up to 2.4 solves the problem by copying the data into or from a non-movable buffer, which\nis obviously inefficient.\nThe branch introduce the concept of \"pinning\", which allows us to inform the\nGC that it is not allowed to move a certain object for a short period of time.\nThis introduces a bit of extra complexity\nin the garbage collector, but improves the I/O performance quite drastically,\nbecause we no longer need the extra copy to and from the non-movable buffers.\nIn this benchmark, which does I/O in a loop,\nwe either write a number of bytes from a freshly allocated string into\n/dev/null or read a number of bytes from /dev/full. I'm showing the results\nfor PyPy 2.4, PyPy with non-zero-nursery and PyPy with non-zero-nursery and\nobject pinning. Those are wall times for cases using os.read/os.write\nand file.read/file.write, normalized against CPython 2.7.\nBenchmarks were done using PyPy 2.4 and revisions 85646d1d07fb for\nnon-zero-nursery and 3d8fe96dc4d9 for non-zero-nursery and pinning.\nThe benchmarks were run once, since the standard deviation was small.\n\n\n\nThe Y axis is speed, normalized to CPython, the more the better\n\nWhat we can see is that os.read and os.write both improved greatly\nand outperforms CPython now for each combination. file operations are\na little more tricky, and while those branches improved the situation a bit,\nthe improvement is not as drastic as in os versions.  It really should not\nbe the case and it showcases how our file buffering is inferior to CPython.\nWe plan on removing our own buffering and using FILE* in C in the near future,\nso we should outperform CPython on those too (since our allocations are cheaper).\nIf you look carefully in the benchmark, the write function is copied three times.\nThis hack is intended to avoid JIT overspecializing the assembler code, which happens\nbecause the buffering code was written way before the JIT was done. In fact, our buffering\nis hilariously bad, but if stars align correctly it can be JIT-compiled to something\nthat's not half bad. Try removing the hack and seeing how the performance of the last\nbenchmark drops :-) Again, this hack should be absolutely unnecessary once we remove\nour own buffering, stay tuned for more.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/11/pypy-io-improvements-1042070332447047674.html"
    },
    {
      "title": "PyPy3 2.4.0 released",
      "text": "We're pleased to announce the availability of PyPy3 2.4.0!\n\nThis release contains several bugfixes and enhancements. Among the user-facing improvements specific to PyPy3:\nBetter Windows compatibility, e.g. the nt module functions _getfinalpathname\u00a0& _getfileinformation are now supported (the former is required for the popular pathlib library for example)\nVarious fsencode PEP 383 related fixes to the posix module (readlink, uname,\u00a0ttyname and ctermid) and improved locale handling\nSwitched the default binary name on POSIX distributions from 'pypy' to 'pypy3' (which symlinks to to 'pypy3.2')\nFixed a couple different crashes related to parsing Python 3 source code\n\nAnd improvements shared with the recent PyPy 2.4.0 release:\ninternal refactoring in string and GIL handling which led to significant speedups\nimproved handling of multiple objects (like sockets) in long-running  programs. They are collected and released more efficiently, reducing  memory use. In simpler terms - we closed what looked like a memory leak\nWindows builds now link statically to zlib, expat, bzip, and openssl-1.0.1i\nMany issues were resolved since the 2.3.1 release in June\n\nYou can download PyPy3 2.4.0 here https://pypy.org/download.html.\n\nPyPy\u00a0is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and 3.2.5. It's fast (pypy 2.4 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nThis  release supports x86 machines running Linux 32/64, Mac OS X 64,   Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,   with VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease  try it out and let us know what you think. We especially welcome  success stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2014/10/pypy3-240-released-5007750685927360190.html"
    },
    {
      "title": "Couchbase contribution to PyPy",
      "text": "Hello everyone!\nWe always offer to put on the blog info about our sponsors who donate substantial amounts of money. So far most people decided to stay anonymous, so this is the first blog post describing our sponsor and his relationship to PyPy, hopefully not the last. We'll also publish a full blog post about the PSF-matched fundraiser soon. This is a guest post by Brent Woodruff from Couchbase.\n\n\n\n\nCouchbase is a leading NoSQL document database that provides a flexible data model, high performance, scalability, and high availability. Couchbase is a commercially supported open source project. Visit us at https://www.couchbase.com and https://github.com/couchbase.\n\n\nCouchbase Inc. donated $2000.00, and employees of Couchbase personally contributed a disclosed additional $230.00, towards Pypy progress during the September funding drive. These funds will see a match from the Python Software Foundation.\n\nPypy is primarily used by Couchbase employees to perform product analysis and troubleshooting using internally developed tools. Every customer of Couchbase benefits from the use of Pypy; both due to the rapid development provided by Python, and the speed of the resulting tools provided by the Pypy JIT interpreter.\n\n\u201cPyPy is great - it gave us a 4x speedup in our CPU-intensive internal application over CPython\u201d\n-Dave Rigby and Daniel Owen, Couchbase Engineers\n\n\nAdditionally, Couchbase has a preliminary CFFI based Couchbase client available for Pypy users.",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2014/10/couchbase-contribution-to-pypy-2360892117372790069.html"
    },
    {
      "title": "PyPy 2.4.0 released, 9 days left in funding drive",
      "text": "We're pleased to announce the availability of PyPy 2.4.0; faster, fewer bugs, and updated to the python 2.7.8 stdlib.\n\nThis release contains several bugfixes and enhancements. Among the user-facing improvements:\n\ninternal refactoring in string and GIL handling which led to significant speedups\nimproved handling of multiple objects (like sockets) in long-running  programs. They are collected and released more efficiently, reducing  memory use. In simpler terms - we closed what looked like a memory leak\nWindows builds now link statically to zlib, expat, bzip, and openssl-1.0.1i\nMany issues were resolved since the 2.3.1 release in June \n\n\nYou can download PyPy 2.4.0 here https://pypy.org/download.html.\n\nWe would like to also point out that in September, the Python Software Foundation will match funds for any donations up to $10k, so head over to our website and help this mostly-volunteer effort out.\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and 3.2.5. It's fast (pypy 2.4 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler. \n\nThis  release supports x86 machines running Linux 32/64, Mac OS X 64,   Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,   with VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease  try it out and let us know what you think. We especially welcome  success stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2014/09/pypy-240-released-9-days-left-in-7722154416024407111.html"
    },
    {
      "title": "PyPy 2.4-beta just in time for PSF's funding drive",
      "text": "We're pleased to announce the availability of PyPy 2.4-beta1; faster, fewer bugs, and updated to the python 2.7.8 stdlib.\n\nThis release contains several bugfixes and enhancements. Among the user-facing improvements:\n\ninternal refactoring in string and GIL handling which led to significant speedups\nimproved handling of multiple objects (like sockets) in long-running programs. They are collected and released more efficiently, reducing memory use. In simpler terms - we closed what looked like a memory leak\nWindows builds now link statically to zlib, expat, bzip, and openssl-1.0.1i\nMany issues were resolved since the 2.3.1 release in June \n\n\nYou can download the PyPy 2.4-beta1 release here https://pypy.org/download.html.\n\nWe would like to also point out that in\nSeptember, the Python Software Foundation will match funds for\nany donations up to $10k, so head over to our website and help this mostly-volunteer effort out.\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and 3.2.5. It's fast (pypy 2.4 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nThis\n release supports x86 machines running Linux 32/64, Mac OS X 64,  \nWindows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,  \nwith VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease\n try it out and let us know what you think. We especially welcome \nsuccess stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team\n\nNews Flash from the beta release cycle:\n\nNote that the beta release mistakenly identifies itself in sys.pypy_version_info as releaselevel=='final', please do not mistake this for a final version\nThe beta can hit a \"Illegal instruction\" exception in jitted code on ARMv6 processors like the RaspberryPi. This will be fixed for the release.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/09/pypy-24-beta-just-in-time-for-psfs-5956090195665204063.html"
    },
    {
      "title": "Python Software Foundation Matching Donations this Month",
      "text": "We're extremely excited to announce that for the month of September, any amount\nyou donate to PyPy will be match (up to $10,000) by the Python Software\nFoundation.This includes any of our ongoing fundraisers: NumPyPy, STM, Python3, or our\ngeneral fundraising.Here are some of the things your previous donations have helped accomplish:Getting PyPy3 completed (currently 3.2, with 3.3 work underway)\nNew research and production engineering on STM for PyPy\nLots of progress on NumPy for PyPy\nSignificant performance improvements\nYou can see a preview of what's coming in our next 2.4 release in the draft\nrelease notes.Thank you to all the individuals and companies which have donated so far.So please, donate today: https://pypy.org/(Please be aware that the donation progress bars are not live updating, so\ndon't be afraid if your donation doesn't show up immediately).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/09/python-software-foundation-matching-2230529993193139046.html"
    },
    {
      "title": "A Field Test of Software Transactional Memory Using the RSqueak Smalltalk VM",
      "text": "Extending the Smalltalk RSqueakVM with STM\nby Conrad Calmez, Hubert Hesse, Patrick Rein and Malte Swart supervised by Tim Felgentreff and Tobias Pape\n\nIntroduction\nAfter pypy-stm we can announce that through the RSqueakVM (which used to be called SPyVM) a second VM implementation supports software transactional memory. RSqueakVM is a Smalltalk implementation based on the RPython toolchain. We have added STM support based on the STM tools from RPython (rstm). The benchmarks indicate that linear scale up is possible, however in some situations the STM overhead limits speedup.\nThe work was done as a master's project at the Software Architechture Group of Professor Robert Hirschfeld at at the Hasso Plattner Institut at the University of Potsdam. We - four students - worked about one and a half days per week for four months on the topic. The RSqueakVM was originally developped during a sprint at the University of Bern. When we started the project we were new to the topic of building VMs / interpreters.\nWe would like to thank  Armin, Remi and the #pypy IRC channel who supported us over the course of our project. We also like to thank Toni Mattis and Eric Seckler, who have provided us with an initial code base.\n\nIntroduction to RSqueakVM\nAs the original Smalltalk implementation, the RSqueakVM executes a given Squeak Smalltalk image, containing the Smalltalk code and a snapshot of formerly created objects and active execution contexts. These execution contexts are scheduled inside the image (greenlets) and not mapped to OS threads. Thereby the non-STM RSqueakVM runs on only one OS thread.\n\nChanges to RSqueakVM\nThe core adjustments to support STM were inside the VM and transparent from the view of a Smalltalk user. Additionally we added Smalltalk code to influence the behavior of the STM. As the RSqueakVM has run in one OS thread so far, we added the capability to start OS threads. Essentially, we added an additional way to launch a new Smalltalk execution context (thread). But in contrast to the original one this one creates a new native OS thread, not a Smalltalk internal green thread.\n\nSTM (with automatic transaction boundaries) already solves the problem of concurrent access on one value as this is protected by the STM transactions (to be more precise one instruction). But there are cases were the application relies on the fact that a bigger group of changes is executed either completely or not at all (atomic). Without further information transaction borders could be in the middle of such a set of atomic statements. rstm allows to aggregate multiple statements into one higher level transaction. To let the application mark the beginning and the end of these atomic blocks (high-level transactions), we added two more STM specific extensions to Smalltalk.\n\n\nBenchmarks\nRSqueak was executed in a single OS thread so far. rstm enables us to execute the VM using several OS threads. Using OS threads we expected a speed-up in benchmarks which use multiple threads. We measured this speed-up by using two benchmarks: a simple parallel summation where each thread sums up a predefined interval and an implementation of Mandelbrot where each thread computes a range of predefined lines.\n\nTo assess the speed-up, we used one RSqueakVM compiled with rstm enabled, but once running the benchmarks with OS threads and once with Smalltalk green threads. The workload always remained the same and only the number of threads increased. To assess the overhead imposed by the STM transformation we also ran the green threads version on an unmodified RSqueakVM. All VMs were translated with the JIT optimization and all benchmarks were run once before the measurement to warm up the JIT. As the JIT optimization is working it is likely to be adoped by VM creators (the baseline RSqueakVM did that) so that results with this optimization are more relevant in practice than those without it. We measured the execution time by getting the system time in Squeak. The results are:\n\nParallel Sum Ten Million\n\n\n\n\n\n\nBenchmark Parallel Sum 10,000,000\n\n\n Thread Count RSqueak green threads RSqueak/STM green threads RSqueak/STM OS threads Slow down from  RSqueak green threads to RSqueak/STM green threads Speed up from RSqueak/STM green threads to RSQueak/STM OS Threads \n \n   1   168.0 ms   240.0 ms   290.9 ms   0.70   0.83  \n   2   167.0 ms   244.0 ms   246.1 ms   0.68   0.99  \n   4   167.8 ms   240.7 ms   366.7 ms   0.70   0.66  \n   8   168.1 ms   241.1 ms   757.0 ms   0.70   0.32  \n   16   168.5 ms   244.5 ms   1460.0 ms   0.69   0.17  \n \n\n\n\nParallel Sum One Billion\n\n\n\n\n\n\nBenchmark Parallel Sum 1,000,000,000\n\n\n\nThread CountRSqueak green threadsRSqueak/STM green threadsRSqueak/STM OS threadsSlow down from  RSqueak green threads to RSqueak/STM green threadsSpeed up from RSqueak/STM green threads to RSQueak/STM OS Threads\n\n   1   16831.0 ms   24111.0 ms   23346.0 ms   0.70   1.03  \n   2   17059.9 ms   24229.4 ms   16102.1 ms   0.70   1.50  \n   4   16959.9 ms   24365.6 ms   12099.5 ms   0.70   2.01  \n   8   16758.4 ms   24228.1 ms   14076.9 ms   0.69   1.72  \n   16   16748.7 ms   24266.6 ms   55502.9 ms   0.69   0.44  \n\n\n\n\n\nMandelbrot Iterative\n\n\n\n\n\n\nBenchmark Mandelbrot\n\n\n Thread Count RSqueak green threads RSqueak/STM green threads RSqueak/STM OS threads Slow down from  RSqueak green threads to RSqueak/STM green threads Speed up from RSqueak/STM green threads to RSqueak/STM OS Threads \n \n   1   724.0 ms   983.0 ms   1565.5 ms   0.74   0.63  \n   2   780.5 ms   973.5 ms   5555.0 ms   0.80   0.18  \n   4   781.0 ms   982.5 ms   20107.5 ms   0.79   0.05  \n   8   779.5 ms   980.0 ms   113067.0 ms   0.80   0.01\n\n\n\n\n\nDiscussion of benchmark results\nFirst of all, the ParallelSum benchmarks show that the parallelism is actually paying off, at least for sufficiently large embarrassingly parallel problems. Thus RSqueak can also benefit from rstm.\nOn the other hand, our Mandelbrot implementation shows the limits of our current rstm integration. We implemented two versions of the algorithm one using one low-level array and one using two nested collections. In both versions, one job only calculates a distinct range of rows and both lead to a slowdown. The summary of the state of rstm transactions shows that there are a lot of inevitable transactions (transactions which must be completed). One reason might be the interactions between the VM and its low-level extensions, so called plugins. We have to investigate this further.\n\nLimitations\nAlthough the current VM setup is working well enough to support our benchmarks, the VM still has limitations. First of all, as it is based on rstm, it has the current limitation of only running on 64-bit Linux.\nBesides this, we also have two major limitations regarding the VM itself. First, the atomic interface exposed in Smalltalk is currently not working, when the VM is compiled using the just-in-time compiler transformation. Simple examples such as concurrent parallel sum work fine while more complex benchmarks such as chameneos fail. The reasons for this are currently beyond our understanding. Second, Smalltalk supports green threads, which are threads which are managed by the VM and are not mapped to OS threads. We currently support starting new Smalltalk threads as OS threads instead of starting them as green threads. However, existing threads in a Smalltalk image are not migrated to OS threads, but remain running as green threads.\n\nFuture work for STM in RSqueak\nThe work we presented showed interesting problems, we propose the following problem statements for further analysis:\n\nInevitable transactions in benchmarks. This looks like it could limit other applications too so it should be solved.\nCollection implementation aware of STM: The current implementation of collections can cause a lot of STM collisions due to their internal memory structure. We believe it could bear potential for performance improvements,  if we replace these collections in an STM enabled interpreter with implementations with less STM collisions. As already proposed by Remi Meier, bags, sets and lists are of particular interest.\nFinally, we exposed STM through languages features such as the atomic method, which is provided through the VM. Originally, it was possible to model STM transactions barriers implicitly by using clever locks, now its exposed via the atomic keyword. From a language design point of view, the question arises whether this is a good solution and what features an stm-enabled interpreter must provide to the user in general? Of particular interest are for example, access to the transaction length and hints for transaction borders to and their  performance impact.\n\n\n\nDetails for the technically inclined\n\nAdjustments to the interpreter loop were minimal.\nSTM works on bytecode granularity that means, there is a implicit transaction border after every bytecode executed. Possible alternatives: only break transactions after certain  bytecodes, break transactions on one abstraction layer above, e.g. object methods (setter, getter).\nrstm calls were exposed using primtives (a way to expose native code in Smalltalk), this was mainly used for atomic.\nStarting and stopping OS threads is exposed via primitives as well. Threads are started from within the interpreter.\nFor Smalltalk enabled STM code we currently have different image versions. However another way to add, load and replace code to the Smalltalk code base is required to make a switch between STM and non-STM code simple.\n\n\n\nDetails on the project setup\nFrom a non-technical perspective, a problem we encountered was the huge roundtrip times (on our machines up to 600s, 900s with JIT enabled). This led to a tendency of bigger code changes (\"Before we compile, let's also add this\"), lost flow (\"What where we doing before?\") and different compiled interpreters in parallel testing (\"How is this version different from the others?\") As a consequence it was harder to test and correct errors. While this is not as much of a problem for other RPython VMs, RSqueakVM needs to execute the entire image, which makes running it untranslated even slower.\n\nSummary\nThe benchmarks show that speed up is possible, but also that the STM overhead in some situations can eat up the speedup. The  resulting STM-enabled VM still has some limitations: As rstm is  currently only running on 64-bit Linux the RSqueakVM is doing so as  well. Eventhough it is possible for us now to create new threads that  map to OS threads within the VM, the migration of exiting Smalltalk threads keeps being problematic.\nWe showed that an existing VM code base can benefit of STM in terms of scaling up. Further it was relatively easy to enable STM support. This may also be valuable to VM developers considering to get STM support for their VMs.",
      "tags": "Smalltalk,Squeak,stm",
      "url": "https://www.pypy.org/posts/2014/08/a-field-test-of-software-transactional-5659022209916605798.html"
    },
    {
      "title": "PyPy-STM: first \"interesting\" release",
      "text": "Hi all,\n\nPyPy-STM is now reaching a point where we can say it's good enough to be\na GIL-less Python.  (We don't guarantee there are no more bugs, so please\nreport them :-)  The first official STM release:\n\n\npypy-stm-2.3-r2-linux64\n(UPDATE: this is release r2, fixing a systematic segfault at start-up on some systems)\n\n\nThis corresponds roughly to PyPy 2.3 (not 2.3.1).  It requires 64-bit\nLinux.  More precisely, this release is built for Ubuntu 12.04 to 14.04;\nyou can also rebuild it\nfrom source by getting the branch stmgc-c7.  You need\nclang to compile, and you need a patched\nversion of llvm.\n\nThis version's performance can reasonably be compared with a regular\nPyPy, where both include the JIT.  Thanks for following the meandering progress of PyPy-STM over the past three years --- we're finally getting somewhere really interesting!  We cannot thank enough all contributors to the previous PyPy-STM money pot that made this possible.  And, although this blog post is focused on the results from that period of time, I have of course to remind you that we're running a second call for donation for future work, which I will briefly mention again later.\n\nA recap of what we did to get there: around the start of the year we found a new model, a \"redo-log\"-based STM which uses a couple of hardware tricks to not require chasing pointers, giving it (in this context) exceptionally cheap read barriers.  This idea was developed over the following months and (relatively) easily integrated with the JIT compiler.  The most recent improvements on the Garbage Collection side are closing the gap with a regular PyPy (there is still a bit more to do there).  There is some preliminary user documentation.\n\nToday, the result of this is a PyPy-STM that is capable of running pure Python code on multiple threads in parallel, as we will show in the benchmarks that follow.  A quick warning: this is only about pure Python code.  We didn't try so far to optimize the case where most of the time is spent in external libraries, or even manipulating \"raw\" memory like array.array or numpy arrays.  To some extent there is no point because the approach of CPython works well for this case, i.e. releasing the GIL around the long-running operations in C.  Of course it would be nice if such cases worked as well in PyPy-STM --- which they do to some extent; but checking and optimizing that is future work.\n\nAs a starting point for our benchmarks, when running code that\nonly uses one thread, we get a slow-down between 1.2 and 3: at worst,\nthree times as slow; at best only 20% slower than a regular\nPyPy.  This worst case has been brought down --it used to be 10x-- by\nrecent work on \"card marking\", a useful GC technique that is also\npresent in the regular PyPy (and about which I don't find any blog post;\nmaybe we should write one :-)  The main remaining issue is fork(), or\nany function that creates subprocesses: it works, but is very slow.  To\nremind you of this fact, it prints a line to stderr when used.\n\nNow the real main part: when you run multithreaded code, it scales very nicely with two\nthreads, and less-than-linearly but still not badly with three or four\nthreads.  Here is an artificial example:\n\n    total = 0\n    lst1 = [\"foo\"]\n    for i in range(100000000):\n        lst1.append(i)\n        total += lst1.pop()\n\nWe run this code N times, once in each of N threads\n(full\nbenchmark).  Run times, best of three:\n\n\n\nNumber of threads\n    Regular PyPy (head)\n    PyPy-STM\nN = 1\n    real 0.92s \nuser+sys 0.92s\n    real 1.34s \nuser+sys 1.34s\nN = 2\n    real 1.77s \nuser+sys 1.74s\n    real 1.39s \nuser+sys 2.47s\nN = 3\n    real 2.57s \nuser+sys 2.56s\n    real 1.58s \nuser+sys 4.106s\nN = 4\n    real 3.38s \nuser+sys 3.38s\n    real 1.64s \nuser+sys 5.35s\n\n\n(The \"real\" time is the wall clock time.  The \"user+sys\" time is the\nrecorded CPU time, which can be larger than the wall clock time if\nmultiple CPUs run in parallel.  This was run on a 4x2 cores machine.\nFor direct comparison, avoid loops that are so trivial\nthat the JIT can remove all allocations from them: right now\nPyPy-STM does not handle this case well.  It has to force a dummy allocation\nin such loops, which makes minor collections occur much more frequently.)\n\nFour threads is the limit so far: only four threads can be executed in\nparallel.  Similarly, the memory usage is limited to 2.5 GB of GC\nobjects.  These two limitations are not hard to increase, but at least\nincreasing the memory limit requires fighting against more LLVM bugs.\n(Include here snark remarks about LLVM.)\n\nHere are some measurements from more real-world benchmarks.  This time,\nthe amount of work is fixed and we parallelize it on T threads.  The first benchmark is just running translate.py on a trunk PyPy.  The last\nthree benchmarks are here.\n\n\n\nBenchmark\n    PyPy 2.3\n    (PyPy head)\n    PyPy-STM, T=1\n    T=2\n    T=3\n    T=4\ntranslate.py --no-allworkingmodules\n(annotation step)\n    184s\n    (170s)\n    386s (2.10x)\n    n/a\nmultithread-richards\n5000 iterations\n    24.2s\n    (16.8s)\n    52.5s (2.17x)\n    37.4s (1.55x)\n    25.9s (1.07x)\n    32.7s (1.35x)\nmandelbrot\ndivided in 16-18 bands\n    22.9s\n    (18.2s)\n    27.5s (1.20x)\n    14.4s (0.63x)\n    10.3s (0.45x)\n    8.71s (0.38x)\nbtree\n    2.26s\n    (2.00s)\n    2.01s (0.89x)\n    2.22s (0.98x)\n    2.14s (0.95x)\n    2.42s (1.07x)\n\n\nThis shows various cases that can occur:\n\nThe mandelbrot example runs with minimal overhead and very good parallelization.\nIt's dividing the plane to compute in bands, and each of the T threads receives the\nsame number of bands.\n\nRichards, a classical benchmark for PyPy (tweaked to run the iterations\nin multiple threads), is hard to beat on regular PyPy:\nwe suspect that the difference is due to the fact that a lot of\npaths through the loops don't allocate, triggering the issue already\nexplained above.  Moreover, the speed of Richards was again improved\ndramatically recently, in trunk.\n\nThe translation benchmark measures the time translate.py\ntakes to run the first phase only, \"annotation\" (for now it consumes too much memory\nto run translate.py to the end).  Moreover the timing starts only after the large number of\nsubprocesses spawned at the beginning (mostly gcc).  This benchmark is not parallel, but we\ninclude it for reference here.  The slow-down factor of 2.1x is still too much, but\nwe have some idea about the reasons: most likely, again the Garbage Collector, missing the regular PyPy's\nvery fast small-object allocator for old objects.  Also, translate.py\nis an example of application that could, with\nreasonable efforts, be made largely parallel in the future using atomic blocks.\n\nAtomic blocks are also present in the btree benchmark.  I'm not completely sure\nbut it seems that, in this case, the atomic blocks create too many\nconflicts between the threads for actual parallization: the base time is very good,\nbut running more threads does not help at all.\n\n\nAs a summary, PyPy-STM looks already useful to run CPU-bound multithreaded\napplications.  We are certainly still going to fight slow-downs, but it\nseems that there are cases where 2 threads are enough to outperform a regular\nPyPy, by a large margin.  Please try it out on your own small examples!\n\nAnd, at the same time, please don't attempt to retrofit threads inside\nan existing large program just to benefit from PyPy-STM!\nOur goal is not to send everyone down the obscure route of multithreaded\nprogramming and its dark traps.  We are going finally to shift our main\nfocus on the phase 2 of our\nresearch (donations welcome): how to enable a better way of writing multi-core programs.\nThe starting point is to fix and test atomic blocks.  Then we will have to\ndebug common causes of conflicts and fix them or work around them; and\ntry to see how common frameworks like Twisted can be adapted.\n\nLots of work ahead, but lots of work behind too :-)\n\nArmin (thanks Remi as well for the work).",
      "tags": "releasestm",
      "url": "https://www.pypy.org/posts/2014/07/pypy-stm-first-interesting-release-8684276541915333814.html"
    },
    {
      "title": "PyPy3 2.3.1 - Fulcrum",
      "text": "We're pleased to announce the first stable release of PyPy3. PyPy3\ntargets Python 3 (3.2.5) compatibility.We would like to thank all of the people who donated to the py3k proposal\nfor supporting the work that went into this.You can download the PyPy3 2.3.1 release here:https://pypy.org/download.html#pypy3-2-3-1HighlightsThe first stable release of PyPy3: support for Python 3!\nThe stdlib has been updated to Python 3.2.5\nAdditional support for the u'unicode' syntax (PEP 414) from Python 3.3\nUpdates from the default branch, such as incremental GC and various JIT\nimprovements\nResolved some notable JIT performance regressions from PyPy2:\nRe-enabled the previously disabled collection (list/dict/set) strategies\nResolved performance of iteration over range objects\nResolved handling of Python 3's exception __context__ unnecessarily forcing\nframe object overhead\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.6 or 3.2.5. It's fast due to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64, Windows,\nand OpenBSD,\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.While we support 32 bit python on Windows, work on the native Windows 64\nbit python is still stalling, we would welcome a volunteer\nto handle that.How to use PyPy?We suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.Cheers,\nthe PyPy team",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2014/06/pypy3-231-fulcrum-3765964217640322884.html"
    },
    {
      "title": "PyPy 2.3.1 - Terrestrial Arthropod Trap Revisited",
      "text": "We're pleased to announce PyPy 2.3.1, a feature-and-bugfix improvement over our recent 2.3 release last month.\n\nThis release contains several bugfixes and enhancements among the user-facing improvements:\nThe built-in struct module was renamed to _struct, solving issues with IDLE and other modules\nSupport for compilation with gcc-4.9\nA CFFI-based version of the gdbm module is now included in our binary bundle\nMany issues were resolved since the 2.3 release on May 8 \n\nYou can download the PyPy 2.3.1 release here:\n\nhttps://pypy.org/download.html\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (pypy 2.3.1 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nThis release supports x86 machines running Linux 32/64, Mac OS X 64,  Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,  with VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease try it out and let us know what you think. We especially welcome success stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/06/pypy-231-terrestrial-arthropod-trap-5076300474324870908.html"
    },
    {
      "title": "PyPy 2.3 - Terrestrial Arthropod Trap",
      "text": "We\u2019re pleased to announce PyPy 2.3, which targets version 2.7.6 of the Python language. This release updates the stdlib from 2.7.3, jumping directly to 2.7.6.\n\nThis release also contains several bugfixes and performance improvements, many generated by real users finding corner cases.\u00a0CFFI\u00a0has made it easier than ever to use existing C code with both cpython and PyPy, easing the transition for packages like\u00a0cryptography,\u00a0Pillow(Python Imaging Library [Fork]), a basic port of\u00a0pygame-cffi, and others.\n\nPyPy can now be embedded in a hosting application, for instance inside\u00a0uWSGI\n\nYou can download the PyPy 2.3 release here:\n\nhttps://pypy.org/download.html\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (pypy 2.3 and cpython 2.7.x performance comparison; note that cpython's speed has not changed since 2.7.2) due to its integrated tracing JIT compiler.\n\nThis release supports x86 machines running Linux 32/64, Mac OS X 64, Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\u00a0\n\nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here\n\nCheers, The PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/05/pypy-23-terrestrial-arthropod-trap-9057496904945555741.html"
    },
    {
      "title": "NumPy on PyPy - Status Update",
      "text": "Work on NumPy on PyPy continued in March, though at a lighter pace than the previous few months. Progress was made on both compatibility and speed fronts. Several behavioral issues reported to the bug tracker were resolved. The most significant of these was probably the correction of casting to built-in Python types. Previously, int/long conversions of numpy scalars such as inf/nan/1e100 would return bogus results. Now, they raise or return values, as appropriate.\n\nOn the speed front, enhancements to the PyPy JIT were made to support virtualizing the raw_store/raw_load memory operations used in numpy arrays. Further work remains here in virtualizing the alloc_raw_storage when possible. This will allow scalars to have storages but still be virtualized when possible in loops.\n\nAside from continued work on compatibility/speed of existing code, we also hope to begin implementing the C-level components of other numpy modules such as mtrand, nditer, linalg, and so on. Several approaches could be taken to get C-level code in these modules working, ranging from reimplementing in RPython to interfacing with existing code with CFFI, if possible. The appropriate approach depends on many factors and will probably vary from module to module.To try out PyPy + NumPy, grab a nightly PyPy and install our NumPy fork. Feel free to report comments/issues to IRC, our mailing list, or bug tracker. Thanks to the contributors to the NumPy on PyPy\u00a0proposal for supporting this work.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2014/04/numpy-on-pypy-status-update-1103134247318103282.html"
    },
    {
      "title": "STM results and Second Call for Donations",
      "text": "Hi all,\n\nWe now have a preliminary version of PyPy-STM\nwith the JIT, from the new STM documentation\npage.  This PyPy-STM is still not quite useful, failing to top the\nperformance of a regular PyPy by a small margin on most benchmarks, but\nit's definitely getting there :-)  The overheads with the JIT are still\na bit too high.  (I've been tracking an obscure bug since days.\nIt turned out to be a simple buffer overflow.  But if anybody has\na clue about why a hardware watchpoint in gdb, set on one of the garbled\nmemory locations, fails to trigger but the memory ends up being modified\nanyway... and, it turns out, by just a regular pointer write... ideas\nwelcome.)\n\nBut I go off-topic :-)  The main point of this post is to announce the\n2nd Call for Donation about\nSTM.  We achieved most of the goals laid out in the first call.  We\neven largely overachieved them in terms of raw performance, even if\nthere are many cases that are unreasonably slow for now.  So, after the\nsuccessful research, we are launching a second proposal about the\ndevelopment part of the project:\n\nPolish PyPy-STM to get a consistently reasonable speed, 25%-40%\nslower than a regular JITted PyPy when running single-threaded code.  Of\ncourse it is supposed to scale nicely as long as there are no\nuser-visible conflicts.\n\nFocus on developing the Python-facing interface: both internal things\n(e.g. do dictionaries need to be more TM-friendly in general?) as well\nas directly visible things (e.g. some profiler-like interface to explore\ncommon conflicts in a program).\n\nRegular multithreaded code should benefit out of the box, but the\nfinal goal is to explore and tweak some existing non-multithreaded\nframeworks and improve their TM-friendliness.  So existing programs\nusing Twisted or Stackless, for example, should run on multiple cores\nwithout any major change.\n\nSee the full call for more\ndetails!  I'd like to thank Remi Meier for getting involved.  And a big\nthank you to everybody who contributed money on the first call.  It\ntook more time than anticipated, but it's there in good but rough shape.\nNow it needs a lot of polishing :-)\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2014/04/stm-results-and-second-call-for-1767845182888902777.html"
    },
    {
      "title": "pygame_cffi: pygame on PyPy",
      "text": "The Raspberry Pi aims to be a low-cost educational tool that anyone can use to learn about electronics and programming. Python and pygame are included in the Pi's programming toolkit. And since last year, thanks in part to sponsorship from the Raspberry Pi Foundation, PyPy also works on the Pi (read more here).\nWith PyPy working on the Pi, game logic written in Python stands to gain an awesome performance boost. However, the original pygame is a Python C extension. This means it performs poorly on PyPy and negates any speedup in the Python parts of the game code.\nOne solution to making pygame games run faster on PyPy, and eventually on the Raspberry Pi, comes in the form of pygame_cffi. pygame_cffi uses CFFI to wrap the underlying SDL library instead of a C extension. A few months ago, the Raspberry Pi Foundation sponsored a Cape Town Python User Group hackathon to build a proof-of-concept pygame using CFFI. This hackathon was a success and it produced an early working version of pygame_cffi.\nSo for the last 5 weeks Raspberry Pi has been funding work on pygame_cffi. The goal was a complete implementation of the core modules. We also wanted benchmarks to illuminate performance differences between pygame_cffi on PyPy and pygame on CPython. We are happy to report that those goals were met. So without further ado, here's a rundown of what works.\n\nCurrent functionality\n\nSurfaces support all the usual flags for SDL and OpenGL rendering (more about OpenGL below).\nThe graphics-related modules color, display, font and image, and parts of draw and transform are mostly complete.\nEvents! No fastevent module yet, though.\nMouse and keyboard functionality, as provided by the mouse and key modules, is complete.\nSound functionality, as provided by the mixer and music modules, is complete.\nMiscellaneous modules, cursors, rect, sprite and time are also complete.\n\n\nInvention screenshot:\n\n\n\nMutable mamba screenshot:\n\n\n\nWith the above-mentioned functionality in place we could get 10+ of the pygame examples to work, and a number of PyWeek games. At the time of writing, if a game doesn't work it is most likely due to an unimplemented transform or draw function. That will be remedied soon.\n\n\nPerformance\nIn terms of performance, pygame_cffi on PyPy is showing a lot of promise. It beats pygame on CPython by a significant margin in our events processing and collision detection benchmarks, while blit and fill benchmarks perform similarly. The pygame examples we checked also perform better.\n\n\n\n\n\nHowever, there is still work to be done to identify and eliminate bottlenecks. On the Raspberry Pi performance is markedly worse compared to pygame (barring collision detection). The PyWeek games we tested also performed slightly worse. Fortunately there is room for improvement in various places.\n\nInvention & Mutable Mamba (x86)\n\n\n\nStandard pygame examples (Raspberry Pi)\n\n\n\nHere's a summary of some of the benchmarks. Relative speed refers to the frame rate obtained in pygame_cffi on PyPy relative to pygame on CPython.\n\n\n\n\n\n\nBenchmark\nRelative speed (pypy speedup)\n\n\n\nEvents (x86)\n1.41\n\nEvents (Pi)\n0.58\n\nN2 collision detection on 100 sprites (x86)\n4.14\n\nN2 collision detection on 100 sprites (Pi)\n1.01\n\nBlit 100 surfaces (x86)\n1.06\n\nBlit 100 surfaces (Pi)\n0.60\n\nInvention (x86)\n0.95\n\nMutable Mamba (x86)\n0.72\n\nstars example (x86)\n1.95\n\nstars example (Pi)\n0.84\n\n\n\n\nOpenGL\nSome not-so-great news is that PyOpenGL performs poorly on PyPy since PyOpenGL uses ctypes. This translates into a nasty reduction in frame rate for games that use OpenGL surfaces. It might be worthwhile creating a CFFI-powered version of PyOpenGL as well.\n\n\n\nWhere to now?\nWork on pygame_cffi is ongoing. Here are some things that are in the pipeline:\n\nGet pygame_cffi on PyPy to a place where it is consistently faster than pygame on CPython.\nImplement the remaining modules and functions, starting with draw and transform.\nImprove test coverage.\nReduce the time it takes for CFFI to parse the cdef. This makes the initial pygame import slow.\n\nIf you want to contribute you can find pygame_cffi on Github.\nFeel free to find us on #pypy on freenode or post issues on github.\nCheers,\nRizmari Versfeld",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2014/03/pygamecffi-pygame-on-pypy-8679802461301121984.html"
    },
    {
      "title": "STMGC-C7 with PyPy",
      "text": "Hi all,\n\nHere is one of the first full PyPy's\n(edit: it was r69967+, but the general list of versions is currently here)\ncompiled with the new StmGC-c7\nlibrary.  It has no JIT so far, but it runs some small\nsingle-threaded benchmarks by taking around 40% more time than a\ncorresponding non-STM, no-JIT version of PyPy.  It scales --- up to two\nthreads only, which is the hard-coded maximum so far in the c7 code.\nBut the scaling looks perfect in these small benchmarks without\nconflict: starting two threads each running a copy of the benchmark\ntakes almost exactly the same amount of total time, simply using two\ncores.\n\nFeel free to try it!  It is not actually useful so far, because it is\nlimited to two cores and CPython is something like 2.5x faster.  One of\nthe important next steps is to re-enable the JIT.  Based on our current\nunderstanding of the \"40%\" figure, we can probably reduce it with\nenough efforts; but also, the JIT should be able to easily produce\nmachine code that suffers a bit less than the interpreter from these\neffects.  This seems to mean that we're looking at 20%-ish slow-downs\nfor the future PyPy-STM-JIT.\n\nInteresting times :-)\n\nFor reference, this is what you get by downloading the\nPyPy binary linked above: a Linux 64 binary (Ubuntu 12.04) that\nshould behave mostly like a regular PyPy.  (One main missing feature is\nthat destructors are never called.)  It uses two cores, but obviously\nonly if the Python program you run is multithreaded.  The only new\nbuilt-in feature is with __pypy__.thread.atomic: this gives\nyou a way to enforce that a block of code runs \"atomically\", which means\nwithout any operation from any other thread randomly interleaved.\n\nIf you want to translate it yourself, you need a trunk version of clang\nwith three patches applied.  That's the number of bugs that we couldn't\nfind workarounds for, not the total number of bugs we found by (ab)using\nthe address_space feature...\n\nStay tuned for more!\n\nArmin & Remi",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2014/03/hi-all-here-is-one-of-first-full-pypys-8725931424559481728.html"
    },
    {
      "title": "PyPy on uWSGI",
      "text": "Hello everyone\nThere is an interview with Roberto De Ioris (from uWSGI fame) about embedding PyPy in uWSGI that covers recent addition of a PyPy embedding interface using cffi and the experience with using it. Read The full interview\nCheers\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/03/hello-everyone-there-is-interview-with-7561523711224053700.html"
    },
    {
      "title": "NumPy on PyPy - Progress in February",
      "text": "More progress was made on the NumPy front in the past month. On the compatibility front, we now pass ~130 more tests from NumPy's suite since the end of January. Currently, we pass 2336 tests out of 3265 tests run, with many of the failures representing portions of NumPy that we don't plan to implement in the near future (object dtypes, unicode, etc). There are still some failures that do represent issues, such as special indexing cases and failures to respect subclassed ndarrays in return values, which we do plan to resolve. There are also some unimplemented components and ufuncs remaining which we hope to implement, such as nditer and mtrand. Overall, the most common array functionality should be working.\n\nAdditionally, I began to take a look at some of the loops generated by our code. One widely used loop is dot, and we were running about 5x slower than NumPy's C version. I was able to optimize the dot loop and also the general array iterator to get us to ~1.5x NumPy C time on dot operations of various sizes. Further progress in this area could be made by using CFFI to tie into BLAS libraries, when available. Also, work remains in examining traces generated for our other loops and checking for potential optimizations.\n\nTo try out PyPy + NumPy, grab a nightly PyPy and install our NumPy fork. Feel free to report comments/issues to IRC, our mailing list, or bug tracker. Thanks to the contributors to the NumPy on PyPy proposal for supporting this work.\n\nCheers,\nBrian",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2014/03/numpy-status-update-february-1245769841736493525.html"
    },
    {
      "title": "Py3k status update #13",
      "text": "This is the 13th status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.We're just finishing up a cleanup of int/long types. This work helps the py3k\nbranch unify these types into the Python 3 int and restore JIT compilation of\nmachine sized integers.This cleanup also removes multimethods from these types. PyPy has\nhistorically used a clever implementation of multimethod dispatch for declaring\nmethods of the __builtin__ types in RPython.This multimethod scheme provides some convenient features for doing this,\nhowever we've come to the conclusion that it may be more trouble than it's\nworth. A major problem of multimethods is that they generate a large amount of\nstub methods which burden the already lengthy and memory hungry RPython\ntranslation process. Also, their implementation and behavior can be somewhat\ncomplicated/obscure.The alternative to multimethods involves doing the work of the type checking\nand dispatching rules in a more verbose, manual way. It's a little more work in\nthe end but less magical.Recently, Manuel Jacob finished a large cleanup effort of the\nunicode/string/bytearray types that also removed their multimethods. This work\nalso benefits the py3k branch: it'll help with future PEP 393 (or PEP 393\nalternative) work. This effort was partly sponsored by Google's Summer of\nCode: thanks Manuel and Google!Now there's only a couple major pieces left in the multimethod removal (the\nfloat/complex types and special marshaling code) and a few minor pieces that\nshould be relatively easy.In conclusion, there's been some good progress made on py3k and multimethod\nremoval this winter, albeit a bit slower than we would have liked.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2014/02/py3k-status-update-13-4630607029125647100.html"
    },
    {
      "title": "Rewrites of the STM core model -- again",
      "text": "Hi all,\n\nA quick note about the Software Transactional Memory (STM) front.\n\nSince the previous\npost, we believe we progressed a lot by discovering an alternative\ncore model for software transactions.  Why do I say \"believe\"?  It's\nbecause it means again that we have to rewrite from scratch the C\nlibrary handling STM.  This is currently work in progress.  Once this is\ndone, we should be able to adapt the existing pypy-stm to run on top of\nit without much rewriting efforts; in fact it should simplify the\ndifficult issues we ran into for the JIT.  So while this is basically\nyet another restart similar to last\nJune's, the difference is that the work that we have already put in the PyPy\npart (as opposed to the C library) remains.\n\nYou can read about the basic ideas of this new C library here.\nIt is still STM-only, not HTM, but because it doesn't constantly move\nobjects around in memory, it would be easier to adapt an HTM version.\nThere are even potential ideas about a hybrid TM, like using HTM but\nonly to speed up the commits.  It is based on a Linux-only system call, remap_file_pages()\n(poll: who heard about it before? :-).  As previously, the work is done\nby Remi Meier and myself.\n\nCurrently, the C library is incomplete, but early experiments show good\nresults in running duhton,\nthe interpreter for a minimal language created for the purpose of\ntesting STM.  Good results means we brough down the slow-downs from\n60-80% (previous version) to around 15% (current version).  This number\nmeasures the slow-down from the non-STM-enabled to the STM-enabled\nversion, on one CPU core; of course, the idea is that the STM version\nscales up when using more than one core.\n\nThis means that we are looking forward to a result that is much better\nthan originally predicted.  The pypy-stm has chances to run at a\none-thread speed that is only \"n%\" slower than the regular pypy-jit, for\na value of \"n\" that is optimistically 15 --- but more likely some number\naround 25 or 50.  This is seriously better than the original estimate,\nwhich was \"between 2x and 5x\".  It would mean that using pypy-stm is\nquite worthwhile even with just two cores.\n\nMore updates later...\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2014/02/rewrites-of-stm-core-model-again-633249729751034512.html"
    },
    {
      "title": "NumPy Status Update - December/January",
      "text": "Work continued on the NumPy + PyPy front steadily in December and more lightly in January. The continued focus was compatibility, targeting incorrect or unimplemented features that appeared in multiple NumPy test suite failures. We now pass ~2/3 of the NumPy test suite. The biggest improvements were made in these areas:\n\n- Bugs in conversions of arrays/scalars to/from native types\n- Fix cases where we would choose incorrect dtypes when initializing or computing results\n- Improve handling of subclasses of ndarray through computations\n- Support some optional arguments for array methods that are used in the pure-python part of NumPy\n- Support additional attributes in arrays, array.flags, and dtypes\n- Fix some indexing corner cases that arise in NumPy testing\n- Implemented part of\u00a0numpy.fft (cffti and cfftf)\n\nLooking forward, we plan to continue improving the correctness of the existing implemented NumPy functionality, while also beginning to look at performance. The initial focus for performance will be to look at areas where we are significantly worse than CPython+NumPy. Those interested in trying these improvements out will need a PyPy nightly, and an install of the PyPy NumPy fork. Thanks again to the NumPy on PyPy donors for funding this work.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2014/02/numpy-status-update-decemberjanuary-4292961614234099787.html"
    },
    {
      "title": "NumPy Status Update - November",
      "text": "Since the PyPy 2.2 release last month, more progress has been made on the NumPy compatibility front. Initial work has been directed by running the NumPy test suite and targeting failures that appear most frequently, along with fixing the few bugs reported on the bug tracker.\n\nImprovements were made in these areas:\n- Many missing/broken scalar functionalities were added/fixed. The scalar API should match up more closely with arrays now.\n- Some missing dtype functionality was added (newbyteorder, hasobject, descr, etc)\n- Support for optional arguments (axis, order) was added to some ndarray functions\n- Fixed some corner cases for string/record types\n\nMost of these improvements went onto trunk after 2.2 was split, so if you're interested in trying them out or running into problems on 2.2, try the\nnightly.\n\nThanks again to the NumPy on PyPy donors who make this continued progress possible.\n\nCheers,\nBrian",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/12/numpy-status-update-november-364321959153372759.html"
    },
    {
      "title": "PyGame CFFI",
      "text": "One of the RaspberryPi's goals is to be a fun toolkit for school children (and adults!) to learn programming and electronics with. Python and pygame are part of this toolkit. Recently the RaspberryPi Foundation funded parts of the effort of porting of pypy to the Pi -- making Python programs on the Pi faster!\nUnfortunately pygame is written as a Python C extension that wraps SDL which means performance of pygame under pypy remains mediocre. To fix this pygame needs to be rewritten using cffi to wrap SDL instead.\nRaspberryPi sponsored a CTPUG (Cape Town Python User Group) hackathon to put together a proof-of-concept pygame-cffi. The day was quite successful - we got a basic version of the bub'n'bros client working on pygame-cffi (and on PyPy). The results can be found on github with contributions from the five people present at the sprint.\nWhile far from complete, the proof of concept does show that there are no major obstacles to porting pygame to cffi and that cffi is a great way to bind your Python package to C libraries.\nAmazingly, we managed to have machines running all three major platforms (OS X, Linux and Windows) at the hackathon so the code runs on all of them!\nWe would like to thank the Praekelt foundation for providing the venue and The Raspberry Pi foundation for providing food and drinks!\nCheers,\nSimon Cross, Jeremy Thurgood, Neil Muller, David Sharpe and fijal.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/12/pygame-cffi-8991437796535033699.html"
    },
    {
      "title": "PyPy Leysin Winter Sprint (11-19st January 2014)",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the ninth time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\nGoals and topics of the sprint\n\nPy3k: work towards supporting Python 3 in PyPy\nNumPyPy: work towards supporting the numpy module in PyPy\nSTM: work towards supporting Software Transactional Memory\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski.\n\n\nExact times\nFor a change, and as an attempt to simplify things, I specified the\ndates as 11-19 January 2014, where 11 and 19 are travel days.  We will\nwork full days between the 12 and the 18.  You are of course allowed to\nshow up for a part of that time only, too.\nLocation & Accomodation\nLeysin, Switzerland, \"same place as before\".  Let me refresh your\nmemory: both the sprint venue and the lodging will be in a very spacious\npair of chalets built specifically for bed & breakfast:\nhttps://www.ermina.ch/.  The place has a good ADSL Internet connexion\nwith wireless installed.  You can of course arrange your own lodging\nanywhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue), but I definitely recommend\nlodging there too -- you won't find a better view anywhere else (though\nyou probably won't get much worse ones easily, either :-)\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.  The rate so far has been around 60 CHF a\nnight all included in 2-person rooms, with breakfast.  There are larger\nrooms too (less expensive per person) and maybe the possibility to get a\nsingle room if you really want to.\nPlease register by Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/leysin-winter-2014\n\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around -- bring a EU-format power strip if you\nhave one.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/11/pypy-leysin-winter-sprint-11-19st-8860782754173653661.html"
    },
    {
      "title": "PyPy 2.2.1 - Incrementalism.1",
      "text": "We're pleased to announce PyPy 2.2.1, which targets version 2.7.3 of the Python\nlanguage. This is a bugfix release over 2.2.\nYou can download the PyPy 2.2.1 release here:\n\nhttps://pypy.org/download.html\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.2 and cpython 2.7.2 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64, Windows\n32, or ARM (ARMv6 or ARMv7, with VFPv3).\nWork on the native Windows 64 is still stalling, we would welcome a volunteer\nto handle that.\nHighlights\nThis is a bugfix release.  The most important bugs fixed are:\n\nan issue in sockets' reference counting emulation, showing up\nnotably when using the ssl module and calling makefile().\nTkinter support on Windows.\nIf sys.maxunicode==65535 (on Windows and maybe OS/X), the json\ndecoder incorrectly decoded surrogate pairs.\nsome FreeBSD fixes.\n\nNote that CFFI 0.8.1 was released.  Both versions 0.8 and 0.8.1 are\ncompatible with both PyPy 2.2 and 2.2.1.\nCheers,\nArmin Rigo & everybody",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/11/pypy-221-incrementalism1-9197847629771910947.html"
    },
    {
      "title": "CFFI 0.8",
      "text": "Hi all,\n\nCFFI 0.8 for CPython (2.6-3.x) has been released.\n\nQuick download: pip install cffi --upgrade\nDocumentation: https://cffi.readthedocs.org/en/release-0.8/\n\nWhat's new: a number of small fixes; ffi.getwinerror(); integrated support for C99 variable-sized structures; multi-thread safety.\n\n--- Armin\n\nUpdate: CFFI 0.8.1, with fixes on Python 3 on OS/X, and some FreeBSD fixes (thanks Tobias).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/11/cffi-08-6086756821078041950.html"
    },
    {
      "title": "NumPy status update",
      "text": "Here is what has been happening with NumPy in PyPy in October thanks to the people who donated to the NumPyPy proposal:\n\n The biggest change is that we shifted to using an external fork of numpy rather than a minimal numpypy module. The idea is that we will be able to\u00a0reuse\u00a0most of the upstream pure-python numpy\u00a0components, replacing the C modules with appropriate RPython micronumpy pieces at the correct places in the module namespace.\n\n The numpy fork should work just as well as the old numpypy for functionality that existed previously, and also include much new functionality from the pure-python numpy pieces that simply hadn't been imported yet in numpypy. However, this new functionality will not have been \"hand picked\" to only include pieces that work, so you may run into functionality that relies on unimplemented components (which should fail with user-level exceptions).\n\n This setup also allows us to run the entire numpy test suite, which will help in directing future compatibility development. The recent PyPy release includes these changes, so download it and let us know how it works! And if you want to live on the edge, the nightly includes even more numpy progress made in November.\n\n To install the fork, download the latest release, and then install numpy either separately with a virtualenv: pip install git+https://bitbucket.org/pypy/numpy.git; or directly: git clone https://bitbucket.org/pypy/numpy.git; cd numpy; pypy setup.py install.\n\nEDIT: if you install numpy as root, you may need to also import it once as root before it works: sudo pypy -c 'import numpy'\n\n\n Along with this change, progress was made in fixing internal micronumpy bugs and increasing compatibility:\nFixed a bug with strings in record dtypes\nFixed a bug where the multiplication of an ndarray with a Python int or float resulted in loss of the array's dtype\nFixed several segfaults encountered in the numpy test suite (suite should run now without segfaulting)\n\n We also began working on __array_prepare__ and __array_wrap__, which are necessary pieces for a working matplotlib module.\n\n Cheers,\nRomain and Brian",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/11/numpy-status-update-1609808546418002632.html"
    },
    {
      "title": "PyPy 2.2 - Incrementalism",
      "text": "We're pleased to announce PyPy 2.2, which targets version 2.7.3 of the Python language. This release main highlight is the introduction of the incremental garbage collector, sponsored by the Raspberry Pi Foundation.\nThis release also contains several bugfixes and performance improvements.\nYou can download the PyPy 2.2 release here:\nhttps://pypy.org/download.htmlWe would like to thank our donors for the continued support of the PyPy project. We showed quite a bit of progress on all three projects (see below) and we're slowly running out of funds. Please consider donating more so we can finish those projects!  The three projects are:\nPy3k (supporting Python 3.x): the release PyPy3 2.2 is imminent.\nSTM (software transactional memory): a preview will be released very soon, as soon as we fix a few bugs\nNumPy: the work done is included in the PyPy 2.2 release. More details below.\n\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (pypy 2.2 and cpython 2.7.2 performance comparison) due to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64, Windows 32, or ARM (ARMv6 or ARMv7, with VFPv3).\nWork on the native Windows 64 is still stalling, we would welcome a volunteer to handle that.HighlightsOur Garbage Collector is now \"incremental\".  It should avoid almost all pauses due to a major collection taking place.  Previously, it would pause the program (rarely) to walk all live objects, which could take arbitrarily long if your process is using a whole lot of RAM.  Now the same work is done in steps.  This should make PyPy more responsive, e.g. in games.  There are still other pauses, from the GC and the JIT, but they should be on the order of 5 milliseconds each.\nThe JIT counters for hot code were never reset, which meant that a process running for long enough would eventually JIT-compile more and more rarely executed code.  Not only is it useless to compile such code, but as more compiled code means more memory used, this gives the impression of a memory leak.  This has been tentatively fixed by decreasing the counters from time to time.\nNumPy has been split: now PyPy only contains the core module, called _numpypy.  The numpy module itself has been moved to https://bitbucket.org/pypy/numpy and numpypy disappeared. You need to install NumPy separately with a virtualenv: pip install git+https://bitbucket.org/pypy/numpy.git; or directly: git clone https://bitbucket.org/pypy/numpy.git;\u00a0cd numpy;\u00a0pypy setup.py install.\nnon-inlined calls have less overhead\nThings that use sys.set_trace are now JITted (like coverage)\nJSON decoding is now very fast (JSON encoding was already very fast)\nvarious buffer copying methods experience speedups (like list-of-ints to int[] buffer from cffi)\nWe finally wrote (hopefully) all the missing os.xxx() functions, including os.startfile() on Windows and a handful of rare ones on Posix.\nnumpy has a rudimentary C API that cooperates with cpyext\nCheers,\nArmin Rigo and Maciej Fijalkowski",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/11/pypy-22-incrementalism-4723643710897639332.html"
    },
    {
      "title": "Py3k status update #12",
      "text": "This is the 12th status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.Here's an update on the recent progress:Thank you to everyone who has provided initial feedback on the PyPy3 2.1 beta\n1 release. We've gotten a number of bug reports, most of which have been\nfixed.\nAs usual, we're continually keeping up with changes from the default\nbranch. Oftentimes these merges come at a cost (conflicts and or\nreintegration of py3k changes) but occasionally we get goodies for free, such\nas the recent JIT optimizations and incremental garbage collection.\nWe've been focusing on re-optimizing Python 2 int sized (machine sized)\nintegers:\nWe have a couple of known, notable speed regressions in the PyPy3 beta release\nvs regular PyPy. The major one being with Python 2.x int sized (or machine\nsized) integers.Python 3 drops the distinction between int and long types. CPython 3.x\naccomplishes this by removing the old int type entirely and renaming the long\ntype to int. Initially, we've done the same for PyPy3 for the sake of\nsimplicity and getting everything working.However PyPy's JIT is capable of heavily optimizing these machine sized integer\noperations, so this came with a regression in performance in this area.We're now in the process of solving this. Part of this work also involves some\nhouse cleaning on these numeric types which also benefits the default branch.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/11/py3k-status-update-12-5307085693947812769.html"
    },
    {
      "title": "Making coverage.py faster under PyPy",
      "text": "If you've ever tried to run your programs with coverage.py under PyPy,\nyou've probably experienced some incredible slowness. Take this simple\nprogram:def f():\n    return 1\n\n\ndef main():\n    i = 10000000\n    while i:\n        i -= f()\n\nmain()\nRunning time coverage.py run test.py five times, and looking at the best\nrun, here's how PyPy 2.1 stacks up against CPython 2.7.5:\n\n\n\n\n\nPython\nTime\nNormalized to CPython\n\n\n\nCPython 2.7.5\n3.879s\n1.0x\n\nPyPy 2.1\n53.330s\n13.7x slower\n\n\nTotally ridiculous. I got turned onto this problem because on one of my\nprojects CPython takes about 1.5 minutes to run our test suite on the build\nbot, but PyPy takes 8-10 minutes.So I sat down to address it. And the results:\n\n\n\n\n\nPython\nTime\nNormalized to CPython\n\n\n\nCPython 2.7.5\n3.879s\n1.0x\n\nPyPy 2.1\n53.330s\n13.7x slower\n\nPyPy head\n1.433s\n2.7x faster\n\n\nNot bad.Technical detailsSo how'd we do it? Previously, using sys.settrace() (which coverage.py\nuses under the hood) disabled the JIT. Except it didn't just disable the JIT,\nit did it in a particularly insidious way \u2014 the JIT had no idea it was being\ndisabled!Instead, every time PyPy discovered that one of your functions was a hotspot,\nit would start tracing to observe what the program was doing, and right when it\nwas about to finish, coverage would run and cause the JIT to abort. Tracing\nis a slow process, it makes up for it by generating fast machine code at the\nend, but tracing is still incredibly slow. But we never actually got to the\n\"generate fast machine code\" stage. Instead we'd pay all the cost of tracing,\nbut then we'd abort, and reap none of the benefits.To fix this, we adjusted some of the heuristics in the JIT, to better show it\nhow sys.settrace(<tracefunc>) works. Previously the JIT saw it as an opaque\nfunction which gets the frame object, and couldn't tell whether or not it\nmessed with the frame object. Now we let the JIT look inside the\n<tracefunc> function, so it's able to see that coverage.py isn't\nmessing with the frame in any weird ways, it's just reading the line number and\nfile path out of it.I asked several friends in the VM implementation and research field if they\nwere aware of any other research into making VMs stay fast when debugging tools\nlike coverage.py are running. No one I spoke to was aware of any (but I\ndidn't do a particularly exhaustive review of the literature, I just tweeted at\na few people), so I'm pleased to say that PyPy is quite possibly the first VM\nto work on optimizing code in debugging mode! This is possible because of our\nyears spent investing in meta-tracing research.Happy testing,\nAlex",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/10/making-coveragepy-faster-under-pypy-935409618297062344.html"
    },
    {
      "title": "Update on STM",
      "text": "Hi all,\nThe sprint in London was a lot of fun and very fruitful. In the last\nupdate on STM, Armin was working on improving and specializing the\nautomatic barrier placement. There is still a lot to do in that area,\nbut that work is merged now. Specializing and improving barrier placement\nis still to be done for the JIT.\nBut that is not all. Right after the sprint, we were able to squeeze\nthe last obvious bugs in the STM-JIT combination. However, the performance\nwas nowhere near to what we want. So until now, we fixed some of the most\nobvious issues. Many come from RPython erring on the side of caution\nand e.g. making a transaction inevitable even if that is not strictly\nnecessary, thereby limiting parallelism. Another problem came from\nincreasing counters everytime a guard fails, which caused transactions\nto conflict on these counter updates. Since these counters do not have\nto be completely accurate, we update them non-transactionally now with\na chance of small errors.\nThere are still many such performance issues of various complexity left\nto tackle: we are nowhere near done. So stay tuned or contribute :)\n\nPerformance\nNow, since the JIT is all about performance, we want to at least\nshow you some numbers that are indicative of things to come.\nOur set of STM benchmarks is very small unfortunately\n(something you can help us out with), so this is\nnot representative of real-world performance. We tried to\nminimize the effect of JIT warm-up in the benchmark results.\nThe machine these benchmarks were executed on has 4 physical\ncores with Hyper-Threading (8 hardware threads).\nRaytracer from stm-benchmarks:\nRender times in seconds for a 1024x1024 image:\n\n\n\n\n\n\n\nInterpreter\nBase time: 1 thread\n8 threads (speedup)\n\n\n\nPyPy-2.1\n2.47\n2.56 (0.96x)\n\nCPython\n81.1\n73.4 (1.1x)\n\nPyPy-STM\n50.2\n10.8 (4.6x)\n\n\n\nFor comparison, disabling the JIT gives 148s on PyPy-2.1 and 87s on\nPyPy-STM (with 8 threads).\nRichards from PyPy repository on the stmgc-c4\nbranch:\nAverage time per iteration in milliseconds:\n\n\n\n\n\n\n\nInterpreter\nBase time: 1 thread\n8 threads (speedup)\n\n\n\nPyPy-2.1\n15.6\n15.4 (1.01x)\n\nCPython\n239\n237 (1.01x)\n\nPyPy-STM\n371\n116 (3.2x)\n\n\n\nFor comparison, disabling the JIT gives 492ms on PyPy-2.1 and 538ms on\nPyPy-STM.\n\nTry it!\nAll this can be found in the PyPy repository on the stmgc-c4\nbranch.\nTry it for yourself, but keep in mind that this is still experimental\nwith a lot of things yet to come. Only Linux x64 is supported right\nnow, but contributions are welcome.\nYou can download a prebuilt binary from here:\nhttps://bitbucket.org/pypy/pypy/downloads/pypy-oct13-stm.tar.bz2\n(Linux x64 Ubuntu >= 12.04).  This was made at revision bafcb0cdff48.\n\nSummary\nWhat the numbers tell us is that PyPy-STM is, as expected,\nthe only of the three interpreters where multithreading gives a large\nimprovement in speed.  What they also tell us is that, obviously, the\nresult is not good enough yet: it still takes longer on a 8-threaded\nPyPy-STM than on a regular single-threaded PyPy-2.1.  However, as you\nshould know by now, we are good at promising speed and delivering it...\nyears later :-)\nBut it has been two years already since PyPy-STM started, and this is\nour first preview of the JIT integration.  Expect major improvements\nsoon: with STM, the JIT generates code that is completely suboptimal in\nmany cases (barriers, allocation, and more).  Once we improve this, the\nperformance of the STM-JITted code should come much closer to PyPy 2.1.\nCheers\nRemi & Armin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/10/update-on-stm-7145890443443707910.html"
    },
    {
      "title": "Incremental Garbage Collector in PyPy",
      "text": "Hello everyone.\nWe're pleased to announce that as of today,\nthe default PyPy comes with a GC that has much smaller pauses than yesterday.\nLet's start with explaining roughly what GC pauses are. In CPython each\nobject has a reference count, which is incremented each time we create\nreferences and decremented each time we forget them. This means that objects\nare freed each time they become unreachable. That is only half of the story\nthough. First note that when the last reference to a large tree of\nobjects goes away, you have a pause: all the objects are freed. Your\nprogram is not progressing at all during this pause, and this pause's\nduration can be arbitrarily large. This occurs at deterministic times,\nthough. But consider code like this:\n\nclass A(object):\n     pass\n\na = A()\nb = A()\na.item = b\nb.item = a\ndel a\ndel b\n\nThis creates a reference cycle. It means that while we deleted references to\na and b from the current scope, they still have a reference count of 1,\nbecause they point to each other, even though the whole group has no references\nfrom the outside. CPython employs a cyclic garbage collector which is used to\nfind such cycles. It walks over all objects in memory, starting from some known\nroots, such as type objects, variables on the stack, etc. This solves the\nproblem, but can create noticeable, nondeterministic GC pauses as the heap\nbecomes large and convoluted.\nPyPy essentially has only the cycle finder - it does not bother with reference\ncounting, instead it walks alive objects every now and then (this is a big\nsimplification, PyPy's GC is much more complex than this). Although this might\nsound like a missing feature, it is really one of the reasons why PyPy is so\nfast, because at the end of the day the total time spent in managing the\nmemory is lower in PyPy than CPython. However, as a result, PyPy also has the\nproblem of GC pauses.\nTo alleviate this problem, which is essential for\napplications like games, we started to work on incremental GC, which spreads\nthe walking of objects and cleaning them across the execution time in smaller\nintervals. The work was sponsored by the Raspberry Pi foundation, started\nby Andrew Chambers and finished by Armin Rigo and Maciej Fija\u0142kowski.\n\n\nBenchmarks\nEveryone loves benchmarks. We did not measure any significant speed difference\non our quite extensive benchmark suite on speed.pypy.org. The main\nbenchmark that we used for other comparisons was translating the topaz\nruby interpreter using various versions of PyPy and CPython. The exact\ncommand was python <pypy-checkout>/bin/rpython -O2 --rtype targettopaz.py.\nVersions:\n\ntopaz - dce3eef7b1910fc5600a4cd0afd6220543104823\npypy source - defb5119e3c6\npypy compiled with minimark (non-incremental GC) - d1a0c07b6586\npypy compiled with incminimark (new, incremental GC) - 417a7117f8d7\nCPython - 2.7.3\n\nThe memory usage of CPython, PyPy with minimark and PyPy with incminimark is\nshown here. Note that this benchmark is quite bad for PyPy in general, the\nmemory usage is higher and the amount of time taken is longer. This is due\nto the JIT warmup being both memory hungry and inefficient (see below).\nBut first, the new GC is not worse than the old one.\n\n\nEDIT:Red line is CPython, blue is incminimark (new), green is minimark (old)\n\nThe image was obtained by graphing the output of memusage.py.\nHowever, the GC pauses are significantly smaller. For PyPy the way to\nget GC pauses is to measure time between start and stop while running stuff\nwith PYPYLOG=gc-collect:log pypy program.py, for CPython, the magic\nincantation is gc.set_debug(gc.DEBUG_STATS) and parsing the output.\nFor what is worth, the average and total for CPython, as well as the total\nnumber of events are not directly comparable since it only shows the cyclic\ncollector, not the reference counts. The only comparable thing is the\namount of long pauses and their duration. In the table below, pause duration\nis sorted into 8 buckets, each meaning \"below that or equal to the threshold\".\nThe output is generated using the gcanalyze tool.\nCPython:\n\n\n\n\n\n\n\n\n\n\n\n\n150.1ms\n300.2ms\n450.3ms\n600.5ms\n750.6ms\n900.7ms\n1050.8ms\n1200.9ms\n\n5417\n5\n3\n2\n1\n1\n0\n1\n\n\n\nPyPy minimark (non-incremental GC):\n\n\n\n\n\n\n\n\n\n\n\n\n216.4ms\n432.8ms\n649.2ms\n865.6ms\n1082.0ms\n1298.4ms\n1514.8ms\n1731.2ms\n\n27\n14\n6\n4\n6\n5\n3\n3\n\n\n\nPyPy incminimark (new incremental GC):\n\n\n\n\n\n\n\n\n\n\n\n\n15.7ms\n31.4ms\n47.1ms\n62.8ms\n78.6ms\n94.3ms\n110.0ms\n125.7ms\n\n25512\n122\n4\n1\n0\n0\n0\n2\n\n\n\nAs we can see, while there is still work to be done (the 100ms ones could\nbe split among several steps), we did improve the situation quite drastically\nwithout any actual performance difference.\nNote about the benchmark - we know it's a pretty extreme case of JIT\nwarmup, we know we suck on it, we're working on it and we're not afraid of\nshowing PyPy is not always the best ;-)\n\n\nNitty gritty details\nHere are some nitty gritty details for people really interested in\nGarbage Collection.  This was done as a patch to \"minimark\", our current\nGC, and called \"incminimark\" for now.  The former is a generational\nstop-the-world GC.  New objects are allocated \"young\", which means that\nthey initially live in the \"nursery\", a special zone of a few MB of\nmemory.  When the nursery is full, a \"minor collection\" step moves the\nsurviving objects out of the nursery.  This can be done quickly (a few\nmillisecond) because we only need to walk through the young objects that\nsurvive --- usually a small fraction of all young objects; and also by\nfar not all objects that are alive at this point, but only the young\nones.  However, from time to time this minor collection is followed by a\n\"major collection\": in that step, we really need to walk all objects to\nclassify which ones are still alive and which ones are now dead\n(\"marking\") and free the memory occupied by the dead ones (\"sweeping\").\nYou can read more details here.\nThis \"major collection\" is what gives the long GC pauses.  To fix this\nproblem we made the GC incremental: instead of running one complete\nmajor collection, we split its work into a variable number of pieces and\nrun each piece after every minor collection for a while, until there are\nno more pieces.  The pieces are each doing a fraction of marking, or a\nfraction of sweeping.  It adds some few milliseconds after each of these\nminor collections, rather than requiring hundreds of milliseconds in one\ngo.\nThe main issue is that splitting the major collections means that the\nmain program is actually running between the pieces, and so it can\nchange the pointers in the objects to point to other objects.  This is\nnot a problem for sweeping: dead objects will remain dead whatever the\nmain program does.  However, it is a problem for marking.  Let us see\nwhy.\nIn terms of the incremental GC literature, objects are either \"white\",\n\"gray\" or \"black\".  This is called tri-color marking.  See for example\nthis blog post about Rubinius, or this page about LuaJIT or the wikipedia description.  The\nobjects start as \"white\" at the beginning of marking; become \"gray\" when\nthey are found to be alive; and become \"black\" when they have been fully\ntraversed.  Marking proceeds by scanning grey objects for pointers to\nwhite objects.  The white objects found are turned grey, and the grey\nobjects scanned are turned black.  When there are no more grey objects,\nthe marking phase is complete: all remaining white objects are truly\nunreachable and can be freed (by the following sweeping phase).\nIn this model, the important part is that a black object can never point\nto a white object: if the latter remains white until the end, it will be\nfreed, which is incorrect because the black object itself can still be\nreached.  How do we ensure that the main program, running in the middle\nof marking, will not try to write a pointer to white object into a black\nobject?  This requires a \"write barrier\", i.e. a piece of code that runs\nevery time we set a pointer into an object or array.  This piece of code\nchecks if some (hopefully rare) condition is met, and calls a function\nif that is the case.\nThe trick we used in PyPy is to consider minor collections as part of\nthe whole, rather than focus only on major collections.  The existing\nminimark GC had always used a write barrier of its own to do its job,\nlike any generational GC.  This existing write barrier is used to detect\nwhen an old object (outside the nursery) is modified to point to a young\nobject (inside the nursery), which is essential information for minor\ncollections.  Actually, although this was the goal, the actual write\nbarrier code is simpler: it just records all old objects into which we\nwrite any pointer --- to a young or old object.  As we found out over\ntime, doing so is not actually slower, and might actually be a\nperformance improvement: for example, if the main program does a lot of\nwrites into the same old object, we don't need to check over and over\nagain if the written pointer points to a young object or not.  We just\nrecord the old object in some list the first time, and that's it.\nThe trick is that this unmodified write barrier works for incminimark\ntoo.  Imagine that we are in the middle of the marking phase, running\nthe main program.  The write barrier will record all old objects that\nare being modified.  Then at the next minor collection, all surviving\nyoung objects will be moved out of the nursery.  At this point, as we're\nabout to continue running the major collection's marking phase, we\nsimply add to the list of pending gray objects all the objects that we\njust considered --- both the objects listed as \"old objects that are\nbeing modified\", and the objects that we just moved out of the nursery.\nA fraction from the former list were black object; so this mean that\nthey are turned back from the black to the gray color.  This technique\nimplements nicely, if indirectly, what is called a \"backward write\nbarrier\" in the literature.  The backwardness is about the color that\nneeds to be changed in the opposite of the usual direction \"white ->\ngray -> black\", thus making more work for the GC.  (This is as opposed\nto \"forward write barrier\", where we would also detect \"black -> white\"\nwrites but turn the white object gray.)\nIn summary, I realize that this description is less about how we turned\nminimark into incminimark, and more about how we differ from the\nstandard way of making a GC incremental.  What we really had to do to\nmake incminimark was to write logic that says \"if the major collection\nis in the middle of the marking phase, then add this object to the list\nof gray objects\", and put it at a few places throughout minor\ncollection.  Then we simply split a major collection into increments,\ndoing marking or sweeping of some (relatively arbitrary) number of\nobjects before returning.  That's why, after we found that the existing\nwrite barrier would do, it was not much actual work, and could be done\nwithout major changes.  For example, not a single line from the JIT\nneeded adaptation.  All in all it was relatively painless work. ;-)\nCheers,armin and fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/10/incremental-garbage-collector-in-pypy-8956893523842234676.html"
    },
    {
      "title": "Numpy Status Update",
      "text": "Hi everyone\n\nThanks to the people who donated money to the numpy proposal, here is what I've been working on recently :\n\n- Fixed conversion from a numpy complex number to a python complex number\n- Implement the rint ufunc\n-\u00a0Make numpy.character usable as a dtype\n-\u00a0Fix ndarray(dtype=str).fill()\n- Various fixes on boolean and fancy indexing\n\nCheers\nRomain",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/09/numpy-status-update-5160363918470470887.html"
    },
    {
      "title": "PyCon South Africa & sprint",
      "text": "Hi all,\n\nFor those of you that happen to be from South Africa: don't miss\nPyCon ZA 2013, next October 3rd and 4th!\nLike last year, a few of us will be there.  There will be the first talk\nabout STM getting ready (a\nblog post about that should follow soon).\n\nMoreover, general sprints will continue on the weekend (5th and 6th).\nAfterwards, Fijal will host a longer PyPy sprint (marathon?) with me\nuntil around the 21th.  You are welcome to it as well!  Write to the mailing list or to fijal directly (fijall\nat gmail.com), or simply in comments of this post.\n\n--- Armin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/09/pycon-south-africa-sprint-6630788654105016762.html"
    },
    {
      "title": "Slides of the PyPy London Demo Evening",
      "text": "The slides of the London demo evening are now online:",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/08/slides-of-pypy-london-demo-evening-5157052112396009739.html"
    },
    {
      "title": "NumPy road forward",
      "text": "Hello everyone.\nThis is the roadmap for numpy effort in PyPy as discussed on the London sprint.\nFirst, the highest on our priority list is to finish the low-level part\nof the numpy module. What\nwe'll do is to finish the RPython part of numpy and provide a pip installable\nnumpypy repository that includes the pure python part of Numpy. This would\ncontain the original Numpy with a few minor changes.\nSecond, we need to work on the JIT support that will make NumPy on PyPy\nfaster. In detail:\n\nreenable the lazy loop evaluation\noptimize bridges, which is depending on optimizer refactorings\nSSE support\n\nOn the compatibility front, there were some independent attempts into\nmaking the following stuff working:\n\nf2py\nC API (in fact, PyArray_* API is partly present in the nightly builds of\nPyPy)\nmatplotlib (both using PyArray_* API and embedding CPython runtime in PyPy)\nscipy\n\nIn order to make all of the above happen faster, it would be helpful to raise\nmore funds. You can donate to PyPy's NumPy project on our website. Note\nthat PyPy is a member of SFC which is a 501(c)(3) US non-profit, so donations\nfrom US companies can be tax-deducted.\nCheers,\nfijal, arigo, ronan, rguillebert, anto and others",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/08/numpy-road-forward-4210065750776753500.html"
    },
    {
      "title": "Preliminary London Demo Evening Agenda",
      "text": "We now have a preliminary agenda for the demo evening in London next week. It takes place on Tuesday, August 27 2013, 18:30-19:30 (BST) at King's College London, Strand. The preliminary agenda is as follows:\n\n\nLaurence Tratt: Welcome from the Software Development Team\nCarl Friedrich Bolz: A Short Introduction to PyPy\nMaciej Fija\u0142kowski: Numpy on PyPy, Present State and Outlook\nLukas Diekmann: Collection Strategies for Fast Containers in PyPy\nArmin Rigo: Software Transactional Memory for PyPy\nEdd Barrett: Unipycation: Combining Prolog and Python\n \n\nAll the talks are lightning talks. Afterwards there will be plenty of time for discussion.\n\nThere's still free spots, if you want to come, please register on the Eventbrite page. Hope to see you there!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/08/preliminary-london-demo-evening-agenda-5254002451136674320.html"
    },
    {
      "title": "Update on STM",
      "text": "Hi all,\n\nA quick update on Software Transactional Memory.  We are\nworking on two fronts.\n\nOn the one hand, the integration of the \"c4\" C library with PyPy is done\nand works well, but is still subject to improvements.  The \"PyPy-STM\"\nexecutable (without the JIT)\nseems to be stable, as far as it has been tested.  It runs a simple\nbenchmark like Richards with a 3.2x slow-down over a regular JIT-less\nPyPy.\n\nThe main factor of this slow-down: the numerous \"barriers\" in\nthe code --- checks that are needed a bit everywhere to verify that a\npointer to an object points to a recent enough version, and if not, to\ngo to the most recent version.  These barriers are inserted automatically\nduring the translation; there is no need for us to manually put 42 million\nbarriers in the source code of PyPy.  But this automatic insertion uses a\nprimitive algorithm right now, which usually ends up putting more barriers than the\ntheoretical optimum.  I (Armin) am trying to improve that --- and progressing:\nlast week the slow-down was around 4.5x.  This is done in the branch\nstmgc-static-barrier.\n\nOn the other hand, Remi is progressing on the JIT integration in\nthe branch stmgc-c4. \nThis has been working in simple cases since a couple of weeks by now, but the\nresulting \"PyPy-JIT-STM\" often crashes.  This is because while the\nbasics are not really hard, we keep hitting new issues that must be\nresolved.\n\nThe basics are that whenever the JIT is about to generate\nassembler corresponding to a load or a store in a GC object, it must\nfirst generate a bit of extra assembler that corresponds to the barrier\nthat we need.  This works fine by now (but could benefit from the same\nkind of optimizations described above, to reduce the number of barriers).\nThe additional issues are all more subtle.  I will describe the current\none as an example: it is how to write constant pointers inside the assembler.\n\nRemember that the STM library classifies objects as either\n\"public\" or \"protected/private\".  A \"protected/private\" object\nis one which has not been seen by another thread so far.\nThis is essential as an optimization, because we know that no\nother thread will access our protected or private objects in parallel,\nand thus we are free to modify their content in place.  By contrast,\npublic objects are frozen, and to do any change, we first need to\nbuild a different (protected) copy of the object.  See this\nblog\npost for more details.\n\nSo far so good, but the JIT will sometimes (actually often) hard-code\nconstant pointers into the assembler it produces.  For example, this is the\ncase when the Python code being JITted creates an instance of a known class;\nthe corresponding assembler produced by the JIT will reserve the memory for\nthe instance and then write the constant type pointer in it.  This type\npointer is a GC object (in the simple model, it's the Python class object;\nin PyPy it's actually the \"map\" object, which is\na different story).\n\nThe problem right now is that this constant pointer may point to a\nprotected object.  This is a problem because the same piece of assembler\ncan later be executed by a different thread.  If it does, then this\ndifferent thread will create instances whose type pointer is bogus: looking\nlike a protected object, but actually protected by a different thread.\nAny attempt to use this type pointer to change anything on the class\nitself will likely crash: the threads will all think they can safely change it\nin-place.  To fix this, we need to make sure we only write pointers to\npublic objects in the assembler.  This is a bit involved because we need\nto ensure that there is a public version of the object to start with.\n\nWhen this is done, we will likely hit the next problem, and the next one;\nbut at some point it should converge (hopefully!) and we'll give you our first\nPyPy-JIT-STM ready to try.  Stay tuned :-)\n\nA bient\u00f4t,\n\nArmin.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/08/update-on-stm-8705514488940872802.html"
    },
    {
      "title": "NumPyPy Status Update",
      "text": "Hello everyone\n\nAs expected, nditer is a lot of work. I'm going to pause my work on it for now and focus on simpler and more important things, here is a list of what I implemented :\n\nFixed a bug on 32 bit that made int32(123).dtype == dtype(\"int32\") fail\nFixed a bug on the pickling of array slices\nThe external loop flag is implemented on the nditer class\nThe c_index, f_index and multi_index flags are also implemented\nAdd dtype(\"double\") and dtype(\"str\")\nC-style iteration is available for nditer\n\nCheers\nRomain Guillebert",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/08/numpypy-status-update-3401163348519734658.html"
    },
    {
      "title": "PyPy 2.1 - Considered ARMful",
      "text": "We're pleased to announce PyPy 2.1, which targets version 2.7.3 of the Python\nlanguage. This is the first release with official support for ARM processors in the JIT.\nThis release also contains several bugfixes and performance improvements.You can download the PyPy 2.1 release here:https://pypy.org/download.htmlWe would like to thank the Raspberry Pi Foundation for supporting the work\nto finish PyPy's ARM support.The first beta of PyPy3 2.1, targeting version 3 of the Python language, was\njust released, more details can be found here.\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.1 and cpython 2.7.2 performance comparison)\ndue to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\n32. This release also supports ARM machines running Linux 32bit - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like the Beagleboard,\nChromebook, Cubieboard, etc.) that supports VFPv3 should work. Both\nhard-float armhf/gnueabihf and soft-float armel/gnueabi builds are\nprovided. The armhf builds for Raspbian are created using the Raspberry Pi\ncustom cross-compilation toolchain\nbased on gcc-arm-linux-gnueabihf and should work on ARMv6 and\nARMv7 devices running Debian or Raspbian. The armel builds are built\nusing the gcc-arm-linux-gnuebi toolchain provided by Ubuntu and\ncurrently target ARMv7.Windows 64 work is still stalling, we would welcome a volunteer\nto handle that.HighlightsJIT support for ARM, architecture versions 6 and 7, hard- and soft-float ABI\nStacklet support for ARM\nSupport for os.statvfs and os.fstatvfs on unix systems\nImproved logging performance\nFaster sets for objects\nInterpreter improvements\nDuring packaging, compile the CFFI based TK extension\nPickling of numpy arrays and dtypes\nSubarrays for numpy\nBugfixes to numpy\nBugfixes to cffi and ctypes\nBugfixes to the x86 stacklet support\nFixed issue 1533: fix an RPython-level OverflowError for space.float_w(w_big_long_number).\nFixed issue 1552: GreenletExit should inherit from BaseException.\nFixed issue 1537: numpypy __array_interface__\nFixed issue 1238: Writing to an SSL socket in PyPy sometimes failed with a \"bad write retry\" message.\nCheers,David Schneider for the PyPy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/08/pypy-21-considered-armful-7177475722033479233.html"
    },
    {
      "title": "PyPy Demo Evening in London, August 27, 2013",
      "text": "As promised in the London sprint announcement we are organising a PyPy demo\nevening during the London sprint on Tuesday, August 27 2013, 18:30-19:30 (BST). The\ndescription of the event is below. If you want to come, please register on the\nEventbrite page.\n\nPyPy is a fast Python VM. Maybe you've never used PyPy and want to find out\nwhat use it might be for you? Or you and your organisation have been using it\nand you want to find out more about how it works under the hood? If so, this\ndemo session is for you!\nMembers of the PyPy team will give a series of lightning talks on PyPy: its\nbenefits; how it works; research currently being undertaken to make it\nfaster; and unusual uses it can be put to. Speakers will be available\nafterwards for informal discussions. This is the first time an event like\nthis has been held in the UK, and is a unique opportunity to speak to core\npeople. Speakers confirmed thus far include: Armin Rigo, Maciej Fija\u0142kowski,\nCarl Friedrich Bolz, Lukas Diekmann, Laurence Tratt, Edd Barrett.\nThe venue for this talk is the Software Development Team, King's College\nLondon. The main entrance is on the Strand, from where the room for the event\nwill be clearly signposted. Travel directions can be found at\nhttps://www.kcl.ac.uk/campuslife/campuses/directions/strand.aspx\nIf you have any questions about the event, please contact Laurence Tratt",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-demo-evening-in-london-august-27-3640213278969666664.html"
    },
    {
      "title": "PyPy3 2.1 beta 1",
      "text": "We're pleased to announce the first beta of the upcoming 2.1 release of\nPyPy3. This is the first release of PyPy which targets Python 3 (3.2.3)\ncompatibility.We would like to thank all of the people who donated to the py3k proposal\nfor supporting the work that went into this and future releases.You can download the PyPy3 2.1 beta 1 release here:https://pypy.org/download.html#pypy3-2-1-beta-1HighlightsThe first release of PyPy3: support for Python 3, targetting CPython 3.2.3!There are some known issues including performance regressions (issues\n#1540 & #1541) slated to be resolved before the final release.\n\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3 or 3.2.3. It's fast due to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\n32. Also this release supports ARM machines running Linux 32bit - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard,\nChromebook, Cubieboard, etc.) that supports VFPv3 should work.Windows 64 work is still stalling and we would welcome a volunteer to handle\nthat.How to use PyPy?We suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.Cheers,\nthe PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy3-21-beta-1-8647445024868663902.html"
    },
    {
      "title": "PyPy 2.1 beta 2",
      "text": "We're pleased to announce the second beta of the upcoming 2.1 release of PyPy.\nThis beta adds one new feature to the 2.1 release and contains several bugfixes listed below.You can download the PyPy 2.1 beta 2 release here:https://pypy.org/download.htmlHighlightsSupport for os.statvfs and os.fstatvfs on unix systems.\nFixed issue 1533: fix an RPython-level OverflowError for space.float_w(w_big_long_number).\nFixed issue 1552: GreenletExit should inherit from BaseException.\nFixed issue 1537: numpypy __array_interface__\nFixed issue 1238: Writing to an SSL socket in pypy sometimes failed with a \"bad write retry\" message.\ndistutils: copy CPython's implementation of customize_compiler, dont call\nsplit on environment variables, honour CFLAGS, CPPFLAGS, LDSHARED and\nLDFLAGS.\nDuring packaging, compile the CFFI tk extension.\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast due to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\n32. Also this release supports ARM machines running Linux 32bit - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard,\nChromebook, Cubieboard, etc.) that supports VFPv3 should work.Windows 64 work is still stalling, we would welcome a volunteer\nto handle that.How to use PyPy?We suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.Cheers,\nThe PyPy Team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-21-beta-2-264349571160808803.html"
    },
    {
      "title": "PyPy San Francisco Sprint July 27th 2013",
      "text": "The next PyPy sprint will be in San Francisco, California. It is a public\nsprint, suitable for newcomers. It will run on Saturday July 27th.Some possible things people will be hacking on the sprint:running your software on PyPy\nmaking your software fast on PyPy\nimproving PyPy's JIT\nimproving Twisted on PyPy\nany exciting stuff you can think of\nIf there are newcomers, we'll run an introduction to hacking on PyPy.Location\nThe sprint will be held at the Rackspace Office:620 Folsom St, Ste 100The doors will open at 10AM and run until 6PM.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-san-francisco-sprint-july-27th-2012-3064530444396960172.html"
    },
    {
      "title": "PyPy London Sprint (August 26 - September 1 2013)",
      "text": "The next PyPy sprint will be in London, United Kingdom for the first\ntime. This is a fully public sprint. PyPy sprints are a very good way\nto get into PyPy development and no prior PyPy knowledge is necessary.\nGoals and topics of the sprint\nFor newcomers:\n\nbring your application/library and we'll help you port it to PyPy,\nbenchmark and profile\ncome and write your favorite missing numpy function\nhelp us work on developer tools like jitviewer\n\nWe'll also work on:\n\nrefactoring the JIT optimizations\nSTM and STM-related topics\nanything else attendees are interested in\n\nExact times\nThe work days should be August 26 - September 1 2013 (Monday-Sunday).\nThe official plans are for people to arrive on the 26th, and\nto leave on the 2nd. There will be a break day in the middle.\nWe'll typically start at 10:00 in the morning.\nLocation\nThe sprint will happen within a room of King's College's Strand\nCampus in Central London, UK. There are some travel instructions how to\nget there. We are being hosted by Laurence Tratt and the Software\nDevelopment Team.\nDemo Session\nIf you don't want to come to the full sprint, but still want to chat a\nbit, we are planning to have a demo session on Tuesday August 27. We\nwill announce this separately on the blog. If you are interested, please\nleave a comment.\nRegistration\nIf you want to attend, please register by adding yourself to the\n\"people.txt\" file in Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/london-2013\n\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\n\nRemember that you may need a (insert country here)-to-UK power adapter.\nPlease note that UK is not within the Schengen zone, so non-EU and\nnon-Switzerland citizens may require specific visa. Please check travel\nregulations. Also, the UK uses pound sterling (GBP).",
      "tags": "sprint",
      "url": "https://www.pypy.org/posts/2013/07/pypy-london-sprint-august-26-september-5156945690440578388.html"
    },
    {
      "title": "Software Transactional Memory lisp experiments",
      "text": "As covered in the previous blog post, the STM subproject of PyPy has been\nback on the drawing board. The result of this experiment is an STM-aware\ngarbage collector written in C. This is finished by now, thanks to Armin's\nand Remi's work, we have a fully functional garbage collector and a STM system\nthat can be used from any C program with enough effort. Using it is more than\na little mundane, since you have to inserts write and read barriers by hand\neverywhere in your code that reads or writes to garbage collector controlled\nmemory. In the PyPy integration, this manual work is done automatically\nby the STM transformation in the interpreter.\nHowever, to experiment some more, we created a minimal\nlisp-like/scheme-like interpreter\n(called Duhton), that follows closely CPython's implementation strategy.\nFor anyone familiar with CPython's source code, it should be pretty\nreadable. This interpreter works like a normal and very basic lisp variant,\nhowever it comes with a transaction builtin, that lets you spawn transactions\nusing the STM system. We implemented a few demos that let you play with the\ntransaction system. All the demos are running without conflicts, which means\nthere are no conflicting writes to global memory and hence the demos are very\namenable to parallelization. They exercise:\n\narithmetics - demo/many_sqare_roots.duh\nread-only access to globals - demo/trees.duh\nread-write access to local objects - demo/trees2.duh\n\nWith the latter ones being very similar to the classic gcbench. STM-aware\nDuhton can be found in the stmgc repo, while the STM-less Duhton,\nthat uses refcounting, can be found in the duhton repo under the base\nbranch.\nBelow are some benchmarks. Note that this is a little comparing apples to\noranges since the single-threaded duhton uses refcounting GC vs generational\nGC for STM version. Future pypy benchmarks will compare more apples to apples.\nMoreover none of the benchmarks has any conflicts. Time is the total time\nthat the benchmark took (not the CPU time) and there was very little variation\nin the consecutive runs (definitely below 5%).\n\n\n\n\n\n\n\n\n\nbenchmark\n1 thread (refcount)\n1 thread (stm)\n2 threads\n4 threads\n\nsquare\n1.9s\n3.5s\n1.8s\n0.9s\n\ntrees\n0.6s\n1.0s\n0.54s\n0.28s\n\ntrees2\n1.4s\n2.2s\n1.1s\n0.57s\n\n\n\nAs you can see, the slowdown for STM vs single thread is significant\n(1.8x, 1.7x, 1.6x respectively), but still lower than 2x. However the speedup\nfrom running on multiple threads parallelizes the problem almost perfectly.\nWhile a significant milestone, we hope the next blog post will cover\nSTM-enabled pypy that's fully working with JIT work ongoing.\nCheers,\nfijal on behalf of Remi Meier and Armin Rigo",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/07/software-transactional-memory-lisp-7777576128992250197.html"
    },
    {
      "title": "PyPy 2.1 beta",
      "text": "We're pleased to announce the first beta of the upcoming 2.1 release of PyPy.\u00a0This beta contains many bugfixes and improvements, numerous improvements to the\u00a0numpy in pypy effort. The main feature being that the ARM processor support is\u00a0not longer considered alpha level.\n\nWe would like to thank the Raspberry Pi\u00a0Foundation for supporting the work to finish PyPy's ARM support.\n\n\nYou can download the PyPy 2.1 beta release here:\n\nhttps://pypy.org/download.html\n\n\n\n\nHighlights\n\nBugfixes to the ARM JIT backend, so that ARM is now an officially\nsupported processor architecture\nStacklet support on ARM\nInterpreter improvements\nVarious numpy improvements\nBugfixes to cffi and ctypes\nBugfixes to the stacklet support\nImproved logging performance\nFaster sets for objects\n\n\n\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\u00a0CPython 2.7.3. It's fast due to its integrated tracing JIT compiler.\u00a0This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\u00a032. Also this release supports ARM machines running Linux 32bit - anything with\u00a0ARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard,\u00a0Chromebook, Cubieboard, etc.) that supports VFPv3 should work. Both\u00a0hard-float armhf/gnueabihf and soft-float\u00a0armel/gnueabi builds are\u00a0provided. armhf builds for Raspbian are created using the Raspberry Pi\ncustom cross-compilation toolchain\u00a0based on gcc-arm-linux-gnueabihf and should work on ARMv6\u00a0and\u00a0ARMv7 devices running Debian or Raspbian. armel builds are built\u00a0using the gcc-arm-linux-gnuebi toolchain provided by Ubuntu and\u00a0currently target ARMv7.\n\nWindows 64 work is still stalling, we would welcome a volunteer\u00a0to handle that.\n\n\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\u00a0installed, you can follow instructions from pypy documentation on how\u00a0to proceed. This document also covers other installation schemes.\n\nCheers,\n\nthe PyPy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-21-beta-1351105697755187196.html"
    },
    {
      "title": "EuroPython",
      "text": "Hi all,\n\nA short note: if you're at EuroPython right now and wondering if PyPy is\ndead because you don't see the obviously expected talk about PyPy, don't\nworry.  PyPy is still alive and kicking.  The truth is two-fold: (1) we\nmissed the talk deadline (duh!)... but as importantly, (2) for various\nreasons we chose not to travel to Florence this year after our trip to\nPyCon US.  (Antonio Cuni is at Florence but doesn't have a talk about PyPy\neither.)\n\nArmin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/europython-8992114341185888806.html"
    },
    {
      "title": "Py3k status update #11",
      "text": "This is the 11th status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.Here's some highlights of the progress made since the previous update:PyPy py3k now matches CPython 3's hash code for\nint/float/complex/Decimal/Fraction\nVarious outstanding unicode identifier related issues were\nresolved. E.g. test_importlib/pep263/ucn/unicode all now fully pass. Various\nusage of identifiers (in particular type and module names) have been fixed to\nhandle non-ascii names -- mostly around display of reprs and exception\nmessages.\nThe unicodedata database has been upgraded to 6.0.0.\nWindows support has greatly improved, though it could still use some more\nhelp (but so does the default branch to a certain degree).\nProbably the last of the parsing related bugs/features have been taken care\nof.\nOf course various other smaller miscellaneous fixes\nThis leaves the branch w/ only about 5 outstanding failures of the stdlib test\nsuite:test_float1 failing test about containment of floats in collections.\ntest_memoryviewVarious failures: requires some bytes/str changes among other things (Manuel\nJacob's has some progress on this on the py3k-memoryview branch)\ntest_multiprocessing1 or more tests deadlock on some platforms\ntest_sys and test_threading2 failing tests for the New GIL's new API\nProbably the biggest feature left to tackle is the New GIL.We're now pretty close to pushing an initial release. We had planned for one\naround PyCon, but having missed that we've put some more effort into the branch\nto provide a more fully-fledged initial release.Thanks to the following for their contributions: Manuel Jacob, Amaury Forgeot\nd'Arc, Karl Ramm, Jason Chu and Christian Hudon.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/06/py3k-status-update-11-133025715908408072.html"
    },
    {
      "title": "STM on the drawing board",
      "text": "Hi all!\n\nThis is an update about the Software Transactional Memory subproject of\nPyPy.  I have some good news of progress.  Also,\nRemi Meier will\nlikely help me this summer.  He did various\ninvestigations with PyPy-STM for his Master's Thesis and contributed back\na lot of ideas and some code.  Welcome again Remi!\n\nI am also sorry that it seems to advance so slowly.  Beyond the usual\nexcuses --- I was busy with other things, e.g. releasing PyPy 2.0 --- I\nwould like to reassure people: I'm again working on it, and the financial\ncontributions are still there and reserved for STM (almost half the money is\nleft, a big thank you again if you contributed!).\n\nThe real reason for the apparent slowness, though, is that it is really\na research project.  It's possible to either have hard deadlines, or to\nfollow various tracks and keep improving the basics, but not both at the\nsame time.\n\nDuring the past month where I have worked again on STM, I worked still on\nthe second option; and I believe it was worth every second of it.  Let me try\nto convince you :-)\n\nThe main blocker was that the STM subsystem, written in C, and the\nGarbage Collection (GC) subsystem, written in RPython, were getting\nharder and harder to coordinate.  So what I did instead is to give up\nusing RPython in favor of using only C for both.  C is a good language\nfor some things, which includes low-level programming where we must take\ncare of delicate multithreading issues; RPython is not a good fit in\nthat case, and wasn't designed to be.\n\nI started a fresh Mercurial repo\nwhich is basically a stand-alone C library.  This library (in heavy development\nright now!) gives any C\nprogram some functions to allocate and track GC-managed objects, and\ngives an actual STM+GC combination on these objects.  It's possible\n(though rather verbose) to use it directly in C programs, like in a\nsmall example interpreter.  Of course the eventual purpose is to link it\nwith PyPy during translation to C, with all the verbose calls\nautomatically generated.\n\nSince I started this, bringing the GC closer to the STM, I kept finding\nnew ways that the two might interact to improve the performance, maybe\nradically.  Here is a summary of the current ideas.\n\nWhen we run\nmultiple threads, there are two common cases: one is to access (read and write)\nobjects that have only been seen by the current thread; the other is to read\nobjects seen by all threads, like in Python the modules/functions/classes,\nbut not to write to them.  Of course, writing to the same object from\nmultiple threads occurs too, and it is handled correctly (that's the whole\npoint), but it is a relatively rare case.\n\nSo each object is classified as \"public\" or \"protected\" (or \"private\",\nwhen they belong to the current transaction).  Newly created objects, once\nthey are no longer private, remain protected until\nthey are read by a different thread.  Now, the point is to use very\ndifferent mechanisms for public and for protected objects.  Public\nobjects are visible by all threads, but read-only in memory; to change\nthem, a copy must be made, and the changes are written to the copy (the\n\"redolog\" approach to STM).  Protected objects, on the other hand, are\nmodified in-place, with (if necessary) a copy of them being made\nfor the sole purpose of a possible abort of the transaction (the \"undolog\"\napproach).\n\nThis is combined with a generational GC similar to PyPy's --- but here,\neach thread gets its own nursery and does its own \"minor collections\",\nindependently of the others.\n\nSo objects are by default protected; when another thread tries to follow a\npointer to them, then it is that other thread's job to carefully \"steal\"\nthe object and turn it public (possibly making a copy of it if needed,\ne.g. if it was still a young object living in the original nursery).\n\nThe same object can exist temporarily in multiple versions: any number\nof public copies; at most one active protected copy; and optionally one\nprivate copy per thread (this is the copy as currently seen by the\ntransaction in progress on that thread).  The GC cleans up the\nunnecessary copies.\n\nThese ideas are variants and extensions of the same basic idea\nof keeping multiple copies with revision numbers to track them.\nMoreover, \"read barriers\" and \"write barriers\" are used by the C program\ncalling into this library in order to be sure that it is accessing the\nright version of the object.  In the currently investigated variant\nI believe it should be possible to have rather cheap\nread barriers, which would definitely be a major speed improvement over\nthe previous variants.  Actually, as far as I know, it would be a major\nimprovement over most of the other existing STMs: in them, the typical read barrier\ninvolves following chains of pointers, and checking some dictionary to see if this\nthread has a modified local copy of the object.  The difference with a\nread barrier that can resolve most cases in a few CPU cycles should be\nhuge.\n\nSo, this is research :-)  It is progressing, and at some point I'll be\nsatisfied with it and stop rewriting everything; and then the actual\nintegration into PyPy should be straightforward (there is already code\nto detect where the read and write barriers need to be inserted, where\ntransactions can be split, etc.).  Then there is support for the\nJIT to be written, and so on.  But more about it later.\n\nThe purpose of this post was to give you some glimpses into what I'm\nworking on right now.  As usual, no plan for release yet.  But you can\nlook forward to seeing the C library progress.  I'll probably also start\nsoon some sample interpreter in C, to test the waters (likely a\nrevival of duhton).\nIf you know nothing about Python but all about the C-level\nmultithreading issues, now is a good time to get involved :-)\n\nThanks for reading!\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/06/stm-on-drawing-board-1028082727566254104.html"
    },
    {
      "title": "NumPyPy status update",
      "text": "Hello everyone,\n\nMay was the first month I was paid to work on NumPyPy (thanks to all who donated!), here is what I worked on during this period :\n\n\nIt is now possible to use subarrays.\nIt is now possible to pickle ndarrays (including those using subarrays), dtypes and scalars, the pickling protocol is the same as numpy's.\n\n\n\n\nFor June, I plan to work on the nditer class, it seems that there's enough work for an entire month.\n\nCheers\nRomain Guillebert",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/06/numpypy-status-update-3846626188716521472.html"
    },
    {
      "title": "PyPy 2.0.2 - Fermi Panini",
      "text": "We're pleased to announce PyPy 2.0.2.  This is a stable bugfix release\nover 2.0 and 2.0.1.  You can download it here:\n\nhttps://pypy.org/download.html\nIt fixes a crash in the JIT when calling external C functions (with\nctypes/cffi) in a multithreaded context.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.0 and cpython 2.7.3 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Support for ARM is progressing but not bug-free yet.\n\n\nHighlights\nThis release contains only the fix described above.  A crash (or wrong\nresults) used to occur if all these conditions were true:\n\nyour program is multithreaded;\nit runs on a single-core machine or a heavily-loaded multi-core one;\nit uses ctypes or cffi to issue external calls to C functions.\n\nThis was fixed in the branch emit-call-x86 (see the example file\nbug1.py).\nCheers,\narigo et. al. for the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/05/pypy-202-fermi-panini-1917947221142595738.html"
    },
    {
      "title": "PyPy 2.0.1 - Bohr Sm\u00f8rrebr\u00f8d",
      "text": "We're pleased to announce PyPy 2.0.1.  This is a stable bugfix release\nover 2.0.  You can download it here:\n\nhttps://pypy.org/download.html\nThe fixes are mainly about fatal errors or crashes in our stdlib.  See\nbelow for more details.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.0 and cpython 2.7.3 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Support for ARM is progressing but not bug-free yet.\n\n\nHighlights\n\nfix an occasional crash in the JIT that ends in RPython Fatal error:\nNotImplementedError.\nid(x) is now always a positive number (except on int/float/long/complex).\nThis fixes an issue in _sqlite.py (mostly for 32-bit Linux).\nfix crashes of callback-from-C-functions (with cffi) when used together\nwith Stackless features, on asmgcc (i.e. Linux only).  Now gevent should\nwork better.\nwork around an eventlet issue with socket._decref_socketios().\n\nCheers,\narigo et. al. for the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/05/pypy-201-bohr-smrrebrd-6316445093061941482.html"
    },
    {
      "title": "Numpy Status Update",
      "text": "Hello Everyone,\n\nI've started to work on NumPyPy since the end of April and here is a short update :\n\n\nI implemented pickling support on ndarrays and dtypes, it will be compatible with numpy's pickling protocol when the \"numpypy\" module will be renamed to \"numpy\".\nI am now working on subarrays.\n\n\n\n\nI would also like to thank everyone who donated and allowed me to work on this.\n\n\n\nCheers,\n\nRomain Guillebert",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/05/numpy-status-update-4176018422530420763.html"
    },
    {
      "title": "PyPy 2.0 - Einstein Sandwich",
      "text": "We're pleased to announce PyPy 2.0. This is a stable release that brings\na swath of bugfixes, small performance improvements and compatibility fixes.\nPyPy 2.0 is a big step for us and we hope in the future we'll be able to\nprovide stable releases more often.\nYou can download the PyPy 2.0 release here:\n\nhttps://pypy.org/download.html\nThe two biggest changes since PyPy 1.9 are:\n\nstackless is now supported including greenlets, which means eventlet\nand gevent should work (but read below about gevent)\nPyPy now contains release 0.6 of cffi as a builtin module, which\nis preferred way of calling C from Python that works well on PyPy\n\nIf you're using PyPy for anything, it would help us immensely if you fill out\nthe following survey: https://bit.ly/pypysurvey This is for the developers\neyes and we will not make any information public without your agreement.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.0 and cpython 2.7.3 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Windows 64 work is still stalling, we would welcome a volunteer\nto handle that. ARM support is on the way, as you can see from the recently\nreleased alpha for ARM.\n\n\nHighlights\n\nStackless including greenlets should work. For gevent, you need to check\nout pypycore and use the pypy-hacks branch of gevent.\ncffi is now a module included with PyPy.  (cffi also exists for\nCPython; the two versions should be fully compatible.)  It is the\npreferred way of calling C from Python that works on PyPy.\nCallbacks from C are now JITted, which means XML parsing is much faster.\nA lot of speed improvements in various language corners, most of them small,\nbut speeding up some particular corners a lot.\nThe JIT was refactored to emit machine code which manipulates a \"frame\"\nthat lives on the heap rather than on the stack.  This is what makes\nStackless work, and it could bring another future speed-up (not done yet).\nA lot of stability issues fixed.\nRefactoring much of the numpypy array classes, which resulted in removal of\nlazy expression evaluation. On the other hand, we now have more complete\ndtype support and support more array attributes.\n\nCheers,\nfijal, arigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/05/pypy-20-einstein-sandwich-635158782365435530.html"
    },
    {
      "title": "PyPy 2.0 alpha for ARM",
      "text": "Hello.\nWe're pleased to announce an alpha release of PyPy 2.0 for ARM. This is mostly\na technology preview, as we know the JIT is not yet stable enough for the\nfull release. However please try your stuff on ARM and report back.\nThis is the first release that supports a range of ARM devices - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard, Chromebook,\nCubieboard, etc.) that supports VFPv3 should work. We provide builds with\nsupport for both ARM EABI variants: hard-float and some older operating\nsystems soft-float.\nThis release comes with a list of limitations, consider it alpha quality,\nnot suitable for production:\n\nstackless support is missing.\nassembler produced is not always correct, but we successfully managed to\nrun large parts of our extensive benchmark suite, so most stuff should work.\n\nYou can download the PyPy 2.0 alpha ARM release here (including a deb for raspbian):\n\nhttps://pypy.org/download.html\nPart of the work was sponsored by the Raspberry Pi foundation.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast due to its integrated tracing JIT compiler.\nThis release supports ARM machines running Linux 32bit. Both hard-float\narmhf and soft-float armel builds are provided.  armhf builds are\ncreated using the Raspberry Pi custom cross-compilation toolchain based on\ngcc-arm-linux-gnueabihf and should work on ARMv6 and ARMv7 devices running at\nleast debian or ubuntu. armel builds are built using gcc-arm-linux-gnuebi\ntoolchain provided by ubuntu and currently target ARMv7.  If there is interest\nin other builds, such as gnueabi for ARMv6 or without requiring a VFP let us\nknow in the comments or in IRC.\n\n\nBenchmarks\nEverybody loves benchmarks. Here is a table of our benchmark suite\n(for ARM we don't provide it yet on https://speed.pypy.org,\nunfortunately).\nThis is a comparison of Cortex A9 processor with 4M cache and Xeon W3580 with\n8M of L3 cache. The set of benchmarks is a subset of what we run for\nhttps://speed.pypy.org that finishes in reasonable time. The ARM machine\nwas provided by Calxeda.\nColumns are respectively:\n\nbenchmark name\nPyPy speedup over CPython on ARM (Cortex A9)\nPyPy speedup over CPython on x86 (Xeon)\nspeedup on Xeon vs Cortex A9, as measured on CPython\nspeedup on Xeon vs Cortex A9, as measured on PyPy\nrelative speedup (how much bigger the x86 speedup is over ARM speedup)\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\nPyPy vs CPython (arm)\nPyPy vs CPython (x86)\nx86 vs arm (pypy)\nx86 vs arm (cpython)\nrelative speedup\n\nai\n3.61\n3.16\n7.70\n8.82\n0.87\n\nbm_mako\n3.41\n2.11\n8.56\n13.82\n0.62\n\nchaos\n21.82\n17.80\n6.93\n8.50\n0.82\n\ncrypto_pyaes\n22.53\n19.48\n6.53\n7.56\n0.86\n\ndjango\n13.43\n11.16\n7.90\n9.51\n0.83\n\neparse\n1.43\n1.17\n6.61\n8.12\n0.81\n\nfannkuch\n6.22\n5.36\n6.18\n7.16\n0.86\n\nfloat\n5.22\n6.00\n9.68\n8.43\n1.15\n\ngo\n4.72\n3.34\n5.91\n8.37\n0.71\n\nhexiom2\n8.70\n7.00\n7.69\n9.56\n0.80\n\nhtml5lib\n2.35\n2.13\n6.59\n7.26\n0.91\n\njson_bench\n1.12\n0.93\n7.19\n8.68\n0.83\n\nmeteor-contest\n2.13\n1.68\n5.95\n7.54\n0.79\n\nnbody_modified\n8.19\n7.78\n6.08\n6.40\n0.95\n\npidigits\n1.27\n0.95\n14.67\n19.66\n0.75\n\npyflate-fast\n3.30\n3.57\n10.64\n9.84\n1.08\n\nraytrace-simple\n46.41\n29.00\n5.14\n8.23\n0.62\n\nrichards\n31.48\n28.51\n6.95\n7.68\n0.91\n\nslowspitfire\n1.28\n1.14\n5.91\n6.61\n0.89\n\nspambayes\n1.93\n1.27\n4.15\n6.30\n0.66\n\nsphinx\n1.01\n1.05\n7.76\n7.45\n1.04\n\nspitfire\n1.55\n1.58\n5.62\n5.49\n1.02\n\nspitfire_cstringio\n9.61\n5.74\n5.43\n9.09\n0.60\n\nsympy_expand\n1.42\n0.97\n3.86\n5.66\n0.68\n\nsympy_integrate\n1.60\n0.95\n4.24\n7.12\n0.60\n\nsympy_str\n0.72\n0.48\n3.68\n5.56\n0.66\n\nsympy_sum\n1.99\n1.19\n3.83\n6.38\n0.60\n\ntelco\n14.28\n9.36\n3.94\n6.02\n0.66\n\ntwisted_iteration\n11.60\n7.33\n6.04\n9.55\n0.63\n\ntwisted_names\n3.68\n2.83\n5.01\n6.50\n0.77\n\ntwisted_pb\n4.94\n3.02\n5.10\n8.34\n0.61\n\n\n\nIt seems that Cortex A9, while significantly slower than Xeon, has higher\nslowdowns with a large interpreter (CPython) than a JIT compiler (PyPy). This\ncomes as a surprise to me, especially that our ARM assembler is not nearly\nas polished as our x86 assembler. As for the causes, various people mentioned\nbranch predictor, but I would not like to speculate without actually knowing.\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.\nWe would not recommend using in production PyPy on ARM just quite yet,\nhowever the day of a stable PyPy ARM release is not far off.\nCheers,\nfijal, bivab, arigo and the whole PyPy team",
      "tags": "arm,sponsors",
      "url": "https://www.pypy.org/posts/2013/05/pypy-20-alpha-for-arm-2318299473927531503.html"
    },
    {
      "title": "PyPy 2.0 beta 2 released",
      "text": "We're pleased to announce the 2.0 beta 2 release of PyPy. This is a major\nrelease of PyPy and we're getting very close to 2.0 final, however it includes\nquite a few new features that require further testing. Please test and report\nissues, so we can have a rock-solid 2.0 final. It also includes a performance\nregression of about 5% compared to 2.0 beta 1 that we hope to fix before\n2.0 final. The ARM support is not working yet and we're working hard to\nmake it happen before the 2.0 final. The new major features are:\n\nJIT now supports stackless features, that is greenlets and stacklets. This\nmeans that JIT can now optimize the code that switches the context. It enables\nrunning eventlet and gevent on PyPy (although gevent requires some\nspecial support that's not quite finished, read below).\nThis is the first PyPy release that includes cffi as a core library.\nVersion 0.6 comes included in the PyPy library. cffi has seen a lot of\nadoption among library authors and we believe it's the best way to wrap\nC libaries. You can see examples of cffi usage in _curses.py and\n_sqlite3.py in the PyPy source code.\n\nYou can download the PyPy 2.0 beta 2 release here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast (pypy 2.0 beta 2 and cpython 2.7.3\nperformance comparison) due to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32. It also supports ARM machines running Linux, however this is\ndisabled for the beta 2 release.\nWindows 64 work is still stalling, we would welcome a volunteer\nto handle that.\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.\n\n\nHighlights\n\ncffi is officially supported by PyPy. It comes included in the standard\nlibrary, just use import cffi\nstackless support - eventlet just works and gevent requires pypycore\nand pypy-hacks branch of gevent (which mostly disables cython-based\nmodules)\ncallbacks from C are now much faster. pyexpat is about 3x faster, cffi\ncallbacks around the same\n__length_hint__ is implemented (PEP 424)\na lot of numpy improvements\n\n\n\nImprovements since 1.9\n\nJIT hooks are now a powerful tool to introspect the JITting process that\nPyPy performs\nvarious performance improvements compared to 1.9 and 2.0 beta 1\noperations on long objects are now as fast as in CPython (from\nroughly 2x slower)\nwe now have special strategies for dict/set/list which contain\nunicode strings, which means that now such collections will be both faster\nand more compact.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2013/04/pypy-20-beta-2-released-4858660312787995512.html"
    },
    {
      "title": "So, you want to try PyPy",
      "text": "Hello.\nDuring the PyCon trip multiple people asked me how exactly they could run\ntheir stuff on PyPy to get the speedups. Now, in an ideal world,\nyou would just swap CPython with PyPy, everything would run tons of times\nfaster and everyone would live happily ever after. However, we don't live in\nan ideal world and PyPy does not speed up everything you could\npotentially run. Chances are that you can run your stuff quite a bit faster, but\nit requires quite a bit more R&D than just that. This blog post is an attempt to\nexplain certain steps that might help. So here we go:\n\nDownload and install PyPy. 2.0 beta 1 or upcoming 2.0 beta 2 would be a good\ncandidate; it's not called a beta for stability reasons.\nRun your tests on PyPy. There is absolutely no need for fast software that\ndoes not work. There might be some failures. Usually they're harmless (e.g.\nyou forgot to close the file); either fix them or at least inspect them. In\nshort, make sure stuff works.\nInspect your stack. In particular, C extensions, while sometimes working, are\na potential source of instability and slowness. Fortunately,\nsince the introduction of cffi, the ecosystem of PyPy-compatible software\nhas been growing. Things I know are written with PyPy in mind:\nthe new version of pyOpenSSL will support PyPy via cffi\npsycopg2cffi is the most actively maintained postgres binding for PyPy,\nwith pg8000 reported working\nmysql has a ctypes based implementation (although a cffi-based one would\nbe definitely better)\nPyPy 2.0 beta 2 will come with sqlite-using-cffi\nlxml-cffi\nuWSGI, while working, is almost certainly not the best choice. Try\ntornado, twisted.web, cyclone.io, gunicorn or gevent\n(note: gevent support for PyPy is not quite finished; will write about it\nin a separate blog post, but you can't just use the main branch of gevent)\nconsult (and contribute to) pypy compatibility wiki for details (note\nthat it's community maintained, might be out of date)\n\n\n\n\nHave benchmarks. If you don't have benchmarks, then performance does not\nmatter for you. Since PyPy's warm-up time is bad (and yes, we know, we're\nworking on it), you should leave ample time for warm-ups. Five to ten seconds\nof continuous computation should be enough.\nTry them. If you get lucky, the next step might be to deploy and be happy.\nIf you're unlucky, profile and try to isolate bottlenecks. They might be in\na specific library or they might be in your code. The better you can isolate\nthem, the higher your chances of understanding what's going on.\nDon't take it for granted. PyPy's JIT is very good, but there is a variety\nof reasons that it might not work how you expect it to. A lot of times it\nstarts off slow, but a little optimization can improve the speed as much as\n10x. Since PyPy's runtime is less mature than CPython, there are higher\nchances of finding an obscure corner of the standard library that might be\natrociously slow.\nMost importantly, if you run out of options and you have a reproducible\nexample, please report it. A pypy-dev email, popping into #pypy\non irc.freenode.net, or getting hold of me on twitter are good ways.\nYou can also contact me directly at fijall at gmail.com as well. While\nit's cool if the example is slow, a lot of problems only show up on large\nand convoluted examples. As long as I can reproduce it on my machine or I can\nlog in somewhere, I am usually happy to help.\nI typically use a combination of jitviewer, valgrind and\nlsprofcalltree to try to guess what's going on. These tools are all\nuseful, but use them with care. They usually require quite a bit of\nunderstanding before being useful. Also sometimes they're just plain useless\nand you need to write your own analysis.\n\nI hope this summary of steps to take is useful. We hear a lot of stories\nof people trying PyPy, most of them positive, but some of them negative.\nIf you just post \"PyPy didn't work for me\" on your blog, that's\ncool too, but you're missing an opportunity. The reasons may vary from\nsomething serious like \"this is a bad pattern for PyPy GC\" to something\ncompletely hilarious like \"oh, I left this sys._getframe() somewhere\nin my hot loops for debugging\" or \"I used the logging module which uses\nsys._getframe() all over  the place\".\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/03/so-you-want-to-try-pypy-4702482800824669595.html"
    },
    {
      "title": "Numpy status update and developer announcement",
      "text": "Hello, some good news!\nFirst the update:\n\ndtype support - NumPy on PyPy now supports non-native storage formats.\nDue to a lack of true support for longdoubles in rpython, we decided to back\nout the support of longdouble-as-double which was misleading.\nmissing ndarray attributes - work has been made toward supporting the\ncomplete set of attributes\non ndarrays. We are progressing alphabetically, and have made it to d.\nUnsupported attributes, and unsupported arguments to attribute calls\nwill raise a NotImplementedError.\npickling support for numarray - hasn't started yet, but next on the list\nThere has been some work on exposing FFI routines in numpypy.\nBrian Kearns has made progress in improving the numpypy namespace.\nThe python numpypy submodules now more closely resemble their numpy\ncounterparts. Also, translated _numpypy submodules are now more properly\nmapped to the numpy core c-based submodules, furthering the goal of being\nable to install numpy as a pure-python module with few modifications.\n\nAnd now the good news:\nWhile our funding drive over 2012 did not reach our goal, we still managed to\nraise a fair amount of money in donations. So far we only managed to spend around $10 000 of it.\nWe issued a call for additional developers, and are glad to welcome Romain Guillebert and Ronan Lamy\nto the numpypy team. Hopefully we will be able to report on speedier progress soon.\nCheers,\nMatti Picus, Maciej Fijalkowski",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/03/numpy-status-update-and-developer-1503421654591696377.html"
    },
    {
      "title": "Py3k status update #10",
      "text": "This is the tenth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.There's been significant progress since the last update: the linux x86-32\nbuildbot now passes 289 out of approximately 354 modules (with 39 skips) of\nCPython's regression test suite.That means there's only 26 test module failures left! The list of major items\nremaining for 3.2 compatibility are now short enough to list here, with their\nrelated tests:Tokenizer support for non-ascii identifiers\ntest_importlib\ntest_pep263\nmemoryview (Manuel Jacob's tackling this on the py3k-memoryview branch)\ntest_memoryview\nmultiprocessing module currently deadlocks\ntest_multiprocessing\nBuggy handling of the new extended unpacking syntax by the compiler:\ntest_unpack_ex\nThe new Global Interpreter Lock and new thread signal handling\ntest_threading\ntest_threadsignals\ntest_sys\nUpgrade unicodedata to 6.0.0 (requires updates to the actual unicodedata\ngeneration script)\ntest_ucn\ntest_unicode\ntest_unicodedata\nCPyExt\ntest_capi (currently crashes)\nUpdate int's hash code to match to CPython (float's is already updated on the\npy3k-newhash branch. note that PyPy 2.x doesn't even totally match\nCPython's hashing)\ntest_decimal\ntest_fractions\ntest_numeric_tower\nMiscellaneous:\ntest_complex\ntest_float\ntest_peepholer\ntest_range\ntest_sqlite (a new cffi based version seems to be coming)\ntest_ssl\ntest_struct\ntest_subprocess\ntest_sys_settrace\ntest_time\nAdditionally there are still a number of failures in PyPy's internal test\nsuite. These tests are usually ran against untranslated versions of PyPy during\ndevelopment. However we've now began running them against a fully translated\nversion of PyPy on the buildbot too (thanks to Amaury for setting this\nup). This further ensures that our tests and implementation are sane.We're getting closer to producing an initial alpha release. Before that happens\nwe'd like to see:further test fixes\nthe results of test runs on other major platforms (e.g. linux x86-64 and osx\nseem to have some additional failures as of now)\nsome basic real world testing\nFinally I'd like to thank Manuel Jacob for his various contributions over the\npast month, including fixing the array and ctypes modules among other things,\nand also Amaury Forgeot d'Arc for his ongoing excellent contributions.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/03/py3k-status-update-10-6681398990092286007.html"
    },
    {
      "title": "10 years of PyPy",
      "text": "From a software engineering perspective, 10 years is indistinguishable\nfrom infinity, so I don't care what happens 10 years from now -- as\nlong as you don't blame me. :-) - Guido van Rossum, Python creator.\n10 years is indeed a long time. PyPy was created approximately 10 years ago,\nwith the exact date being lost in the annals of the version control system.\nWe've come a long way during those 10 years, from a \"minimal Python\" that\nwas supposed to serve mostly as an educational tool, through to a vehicle for\nacademic research to a high performance VM for Python and beyond.\nSome facts from the PyPy timeline:\n\nIn 2007, at the end of the EU funding period, we promised the JIT was just around the corner.\nIt turned out we misjudged it pretty badly -- the first usable PyPy was released in 2010.\nAt some point we decided to have a JavaScript backend so one could compile RPython programs\nto JavaScript and run them in a browser. Turned out it was a horrible idea.\nAnother option we tried was using RPython to write CPython C extensions. Again, it turned out RPython\nis a bad language and instead we made a fast JIT, so you don't have to write C extensions.\nWe made N attempts to use LLVM.  Seriously, N is 4 or 5.  But we haven't fully given up yet :-)\nThey all run into issues one way or another.\nWe were huge fans of ctypes at the beginning. Up to the point where we tried to make\na restricted subset with static types, called rctypes for RPython. Turned out to be horrible.\nTwice.\nWe were very hopeful about creating a JIT generator from the beginning. But the first one failed miserably,\ngenerating too much assembler. The second failed too. The third first burned down and then failed.\nHowever, we managed to release a working JIT in 2010, against all odds.\nMartijn Faassen used to ask us \"how fast is PyPy\" so we decided to name an option enabling all\noptimizations \"--faassen\".  Then \"--no-faassen\" was naturally added too. Later we\ndecided to grow up and renamed it to \"-O2\", and now \"-Ojit\".\nThe first time the Python interpreter successfully compiled to C, it segfaulted because the code generator used signed chars instead of unsigned chars...\nTo make it more likely to be accepted, the proposal for the EU project contained basically every feature under the sun a language could have. This proved to be annoying, because we had to actually implement all that stuff. Then we had to do a cleanup sprint where we deleted 30% of codebase and 70% of features.\nAt one sprint someone proposed a new software development methodology: 'Terminology-Driven Programming' means to pick a fancy name, then discuss what it could mean, then implement it. Examples: timeshifter, rainbow interpreter, meta-space bubble, hint annotations (all but one of these really existed).\nThere is a conspiracy theory that the reason why translation is so slow is because time is stored away during it, which is later retrieved when an actual program runs to make them appear faster\n\nOverall, it was a really long road.  However, 10 years later we are in\ngood shape.  A quick look on the immediate future: we are approaching\nPyPy 2.0 with stackless+JIT and cffi support,\nthe support for Python 3 is taking shape, non-standard\nextensions like STM are slowly getting ready (more soon), and there are\nseveral non-Python interpreters around the corner (Hippy, Topaz and more).\nCheers,\nfijal, arigo, hodgestar, cfbolz and the entire pypy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/10-years-of-pypy-634401291726575821.html"
    },
    {
      "title": "cppyy status update",
      "text": "The cppyy module\nprovides C++ bindings for PyPy by using the reflection information extracted\nfrom C++ header files by means of the\nReflex package.\nIn order to support C++11, the goal is to move away from Reflex and instead use\ncling, an interactive\nC++ interpreter, as the backend.\nCling is based on llvm's\nclang.\n\nThe use of a real compiler under the hood has the advantage that it is now\npossible to cover every conceivable corner case.\nThe disadvantage, however, is that every corner case actually has to be\ncovered.\nLife is somewhat easier when calls come in from the python interpreter, as\nthose calls have already been vetted for syntax errors and all lookups are\nwell scoped.\nFurthermore, the real hard work of getting sane responses from and for C++\nin an interactive environment is done in cling, not in the bindings.\nNevertheless, it is proving a long road (but for that matter clang does not\nsupport all of C++11 yet), so here's a quick status update showing that good \nprogress is being made.\n\nThe following example is on CPython, not PyPy, but moving a third\n(after Reflex and\nCINT) backend into place\nunderneath cppyy is straightforward compared to developing the backend\nin the first place.\n\nTake this snippet of C++11 code\n(cpp11.C):\n\n    constexpr int data_size() { return 5; }\n\n    auto N = data_size();\n\n    template<class L, class R>\n    struct MyMath {\n       static auto add(L l, R r) -> decltype(l+r) { return l + r; }\n    };\n\n    template class MyMath<int, int>;\n\nAs a practical matter, most usage of new C++11 features will live in\nimplementations, not in declarations, and are thus never seen by the bindings.\nThe above example is therefore somewhat contrived, but it will serve to show\nthat these new declarations actually work.\nThe new features used here are\nconstexpr,\nauto, and\ndecltype.\nHere is how you could use these from CPython, using the\nPyROOT\npackage, which has more than a passing resemblance to cppyy, as one is based\non the other:\n\n    import ROOT as gbl\n    gbl.gROOT.LoadMacro('cpp11.C')\n\n    print 'N =', gbl.N\n    print '1+1 =', gbl.MyMath(int, int).add(1,1)\n\nwhich, when entered into a file\n(cpp11.py) and executed,\nprints the expected results:\n\n    $ python cpp11.py\n    N = 5\n    1+1 = 2\n\nIn the example, the C++ code is compiled on-the-fly, rather than first generating\na dictionary as is needed with Reflex.\nA deployment model that utilizes stored pre-compiled information is foreseen\nto work with larger projects, which may have to pull in headers from many places.\n\nWork is going to continue first on C++03 on cling with CPython (about 85% of\nunit tests currently pass), with a bit of work on C++11 support on the side.\nOnce fully in place, it can be brought into a new backend for cppyy, after \nwhich the remaining parts of C++11 can be fleshed out for both interpreters.\n\nCheers,\nWim Lavrijsen",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/cppyy-status-update-808802896237239604.html"
    },
    {
      "title": "PyCon Silicon Valley and San Francisco visit",
      "text": "Hello everyone.\nWe (Armin Rigo and Maciej Fijalkowski) are visiting San Francisco/Silicon Valley\nfor PyCon and beyond. Alex Gaynor, another core PyPy dev is living there\npermanently. My visiting dates are 12-28 of March, Armin's 11-21st.\nIf you want us to give a  talk at your company or simply catch up with us\nfor a dinner\nplease get in touch. Write to pypy-dev@python.org, if you want this publically\nknown or simply send me a mail at fijall@gmail.com if you don't want it public.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/hello-everyone-4718797989680066222.html"
    },
    {
      "title": "Announcing Topaz, an RPython powered Ruby interpreter",
      "text": "Hello everyone\n\nLast week, Alex Gaynor announced the first public release of\nTopaz,\na Ruby interpreter written in RPython. This is the culmination of a\npart-time effort over the past 10 months to provide a Ruby interpreter\nthat implements enough interesting constructs in Ruby to show that the\nRPython toolchain can produce a Ruby implementation fast enough to\nbeat what is out there.\n\nDisclaimer\n\nObviously the implementation is very incomplete currently in terms of\navailable standard library. We are working on getting it useable. If\nyou want to try it, grab a\nnightly build.\n\nWe have run some benchmarks from the\nRuby benchmark suite\nand the\nmetatracing VMs experiment. The\npreliminary results are promising, but at this point we are missing so\nmany method implementations that most benchmarks won't run yet. So instead of\nperformance, I'm going to talk about the high-level structure of the\nimplementation.\n\nArchitecture\n\nTopaz interprets a custom bytecode set. The basics are similar to\nSmalltalk VMs, with bytecodes for loading and storing locals and\ninstance variables, sending messages, and stack management. Some\nsyntactical features of Ruby, such as defining classes and modules,\nliteral regular expressions, hashes, ranges, etc also have their own\nbytecodes. The third kind of bytecodes are for control flow constructs\nin Ruby, such as loops, exception handling, break, continue, etc.\n\nIn trying to get from Ruby source code to bytecode, we found that the\neasiest way to support all of the Ruby syntax is to write a custom\nlexer and use an RPython port of PLY\n(fittingly called RPly) to create the\nparser from the Ruby yacc grammar.\n\nThe Topaz interpreter uses an ObjectSpace (similar to how PyPy does\nit), to interact with the Ruby world. The object space contains all\nthe logic for wrapping and interacting with Ruby objects from the\nVM. It's __init__ method sets up the core classes, initial globals,\nand creates the main thread (the only one right now, as we do not have\nthreading, yet).\n\nClasses are mostly written in Python. We use ClassDef objects to\ndefine the Ruby hierarchy and attach RPython methods to Ruby via\nClassDef decorators. These two points warrant a little explanation.\n\nHierarchies\n\nAll Ruby classes ultimately inherit from BasicObject. However, most\nobjects are below Object (which is a direct subclass of\nBasicObject). This includes objects of type Fixnum, Float,\nClass, and Module, which may not need all of the facilities of\nfull objects most of the time.\n\nMost VMs treat such objects specially, using tagged pointers to\nrepresent Fixnums, for example. Other VMs (for example from the\nSOM Family)\ndon't. In the latter case, the implementation hierarchy matches the\nlanguage hierarchy, which means that objects like Fixnum share a\nrepresentation with all other objects (e.g. they have class pointers\nand some kind of instance variable storage).\n\nIn Topaz, implementation hierarchy and language hierarchy are\nseparate. The first is defined through the Python inheritance. The\nother is defined through the ClassDef for each Python class, where the\nappropriate Ruby superclass is chosen. The diagram below shows how the\nimplementation class W_FixnumObject inherits directly from\nW_RootObject.  Note that W_RootObject doesn't have any attrs,\nspecifically no storage for instance variables and no map (for\ndetermining the class - we'll get to that). These attributes are\ninstead defined on W_Object, which is what most other implementation\nclasses inherit from. However, on the Ruby side, Fixnum correctly\ninherits (via Numeric and Integer) from Object.\n\n\n\n\nThis simple structural optimization gives a huge speed boost, but\nthere are VMs out there that do not have it and suffer performance\nhits for it.\n\nDecorators\n\nRuby methods can have symbols in its names that are not allowed as\npart of Python method names, for example !, ?, or =, so we\ncannot simply define Python methods and expose them to Ruby by the\nsame name. \n\nFor defining the Ruby method name of a function, as well as argument\nnumber checking, Ruby type coercion and unwrapping of Ruby objects to\ntheir Python equivalents, we use decorators defined on ClassDef. When\nthe ObjectSpace initializes, it builds all Ruby classes from their\nrespective ClassDef objects. For each method in an implementation\nclass that has a ClassDef decorator, a wrapper method is generated and\nexposed to Ruby. These wrappers define the name of the Ruby method,\ncoerce Ruby arguments, and unwrap them for the Python method.\n\nHere is a simple example:\n\n@classdef.method(\"*\", times=\"int\")\ndef method_times(self, space, times):\n    return self.strategy.mul(space, self.str_storage, times)\n\n\nThis defines the method * on the Ruby String class. When this is\ncalled, the first argument is converted into a Ruby Fixnum object\nusing the appropriate coercion method, and then unwrapped into a plain\nPython int and passed as argument to method_times. The wrapper\nmethod also supplies the space argument.\n\nObject Structure\n\nRuby objects have dynamically defined instance variables and may\nchange their class at any time in the program (a concept called\nsingleton class\nin Ruby - it allows each object to have unique behaviour). To still\nefficiently access instance variables, you want to avoid dictionary\nlookups and let the JIT know about objects of the same class that have\nthe same instance variables. Topaz, like PyPy (which got it from\nSelf), implements instances using maps, which transforms dictionary\nlookups into array accesses. See the\nblog post\nfor the details.\n\nThis is only a rough overview of the architecture. If you're\ninterested, get in touch on\n#topaz.freenode.net, follow the\nTopaz Twitter account or contribute\non GitHub.\n\nTim Felgentreff",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/announcing-topaz-rpython-powered-ruby-6662407703061538341.html"
    },
    {
      "title": "CFFI 0.5",
      "text": "Hi all,\n\nA short notice to tell you that CFFI 0.5 was released.  This\ncontains a number of small improvements from 0.4, but seems to otherwise\nbe quite stable since a couple of months --- no change since January 10,\napart from the usual last-minute fixes for Python 3 and for Windows.\n\nHave fun!\n\nArmin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/cffi-05-1630643916751622710.html"
    },
    {
      "title": "NumPyPy 2013 Developer Position",
      "text": "Introduction\nProposed herein is a part-time fellowship for developing NumPy in PyPy.\nThe work will initially consist of 100 hours\nwith the possibility of extension, until the funds run out.\nDevelopment and improvement of PyPy's NumPyPy (as\nwith most Open Source and Free Software) is done as a collaborative process\nbetween volunteer, paid, and academic contributors. Due to a successful funding\ndrive but a lack of contributors willing to work directly for PyPy, we find\nourselves in the enviable situation of being able to offer this position.\n\n\nBackground\nPyPy's developers make all PyPy software available to the public\nwithout charge, under PyPy's Open Source copyright license, the\npermissive MIT License. PyPy's license assures that PyPy is equally\navailable to everyone freely on terms that allow both non-commercial\nand commercial activity. This license allows for academics, for-profit\nsoftware developers, volunteers and enthusiasts alike to collaborate\ntogether to make a better Python implementation for everyone.\nNumPy support for PyPy is licensed similarly, and therefore NumPy in\nPyPy support can directly help researchers and developers who seek to\ndo numeric computing but want an easier programming language to use\nthan Fortan or C, which is typically used for these\napplications. Being licensed freely to the general public means that\nopportunities to use, improve and learn about how NumPy in PyPy works\nitself will be generally available to everyone.\n\n\nThe Need for a Part-Time Developer\nNumPy project in PyPy has seen some slow, but steady progress since we started\nworking about a year ago.  On one hand,\nit's actually impressive what we could deliver with the effort undertaken,\non the other hand, we would like to see the development accelerated.\nPyPy has strict coding, testing, documentation, and review standards,\nwhich ensures excellent code quality, continually improving\ndocumentation and code test coverage, and minimal regressions. A\npart-time developer will be able to bring us closer to the goal of\nfull numpy-api implementation and speed improvements.\n\n\nWork Plan\nThe current proposal is split into two parts:\n\nCompatibility:\nThis part covers the core NumPy Python API. We'll implement most NumPy APIs\nthat are officially documented and we'll pass most of NumPy's tests that\ncover documented APIs and are not implementation details.\nSpecifically, we don't plan to:\n\nimplement NumPy's C API\nimplement other scientific libraries, like SciPy, matplotlib or biopython\nimplement details that are otherwise agreed by consensus to not have a place\nin PyPy's implementation of NumPy or agreed with NumPy community\nto be implementation details\n\n\nSpeed:\nThis part will cover significant speed improvements in the JIT that would\nmake numeric computations faster. This includes, but is not necesarilly\nlimited to:\n\nwrite a set of benchmarks covering various use cases\nteaching the JIT backend (or multiple backends) how to deal with vector\noperations, like SSE\nexperiments with automatic parallelization using multiple threads, akin\nto numexpr\nimproving the JIT register allocator that will make a difference, especially\nfor tight loops\n\nAs with all speed improvements, it's relatively hard to predict exactly\nhow it'll cope, however we expect the results to be withing an order\nof magnitude of handwritten C equivalent.\n\n\n\n\nPosition Candidate\nWe would like people who are proficient in NumPy and PyPy (but don't have to be\ncore developers of either) to step up. The developer selection will be done\nby consensus of PyPy core developers and consulted with the Software Freedom\nConservancy for lack of conflict of interest. The main criterium will be\npast contributions to the PyPy project, but they don't have to be significant\nin size.\nA candidate for the Developer position will demonstrate the following:\n\nThe ability to write clear, stable, suitable and tested code\nThe ability to understand and extend the JIT capabilities used in NumPyPy.\nA positive presence in PyPy's online community on IRC and the mailing\nlist.\n\nIdeally the Developer will also:\n\nHave familiarity with the infrastructure of the PyPy project (including\nbug tracker and buildbot).\nHave Worked to provide education or outreach on PyPy in other forums such as\nworkshops, conferences, and user groups.\n\nConservancy and PyPy are excited to announce the Developer Position.\nRenumeration for the position will be at the rate of 60 USD per hour, through\nthe Software Freedom Conservancy.\nPyPy community is promising to provide necessary guidance and help into\nthe current codebase, however we expect a successful candidate to be able\nto review code and incorporate external patches within two months of the\nstarting date of the contract.\nCandidates should submit their proposal (including their CV) to:\npypy-z@python.org\nThe deadline for this initial round of proposals is February 1, 2013.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/01/numpypy-2013-developer-position-1547805593757893630.html"
    },
    {
      "title": "Py3k status update #9",
      "text": "This is the ninth status update about our work on the py3k branch, which\nwe can work on thanks to all of the people who donated to the py3k\nproposal.Just a very short update on December's work: we're now passing about 223 of\napproximately 355 modules of CPython's regression test suite, up from passing\n194 last month.Some brief highlights:More encoding related issues were addressed. e.g. now most if not all the\nmultibytecodec test modules pass.\nFixed some path handling issues (test_os, test_ntpath and\ntest_posixpath now pass)\nWe now pass test_class, test_descr and almost test_builtin (among\nother things): these are notable as they are fairly extensive test suites of\ncore aspects of the langauge.\nAmaury Forgeot d'Arc continued making progress on CPyExt (thanks again!)\ncheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/01/py3k-status-update-9-98332471264591773.html"
    },
    {
      "title": "PyPy related internship at NCAR",
      "text": "Hello everyone\nI would like to advertise a PyPy-related summer internship at\nthe National Center for Atmospheric Research, which is located in lovely\nBoulder, Colorado. As for the last year, the mentor will be Davide del Vento,\nwith my possible support on the PyPy side.\nThe full details of the application are to be found on\nthe internship description and make sure you read the requirements\nfirst. Important requirements:\n\nMust currently be enrolled in a United States university.\nOnly students authorized to work for any employer in the United\nStates will be considered for the SIParCS program.\nMust be a graduate or under graduate who has completed their sophomore year.\n\nIf you happen to fulfill the requirements, to me this sounds like\na great opportunity to spend a summer at NCAR in Boulder hacking on atmospheric\nmodels using PyPy.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/12/pypy-related-internship-at-ncar-7412729710421119926.html"
    },
    {
      "title": "Py3k status update #8",
      "text": "This is the eight status update about our work on the py3k branch, which\nwe can work on thanks to all of the people who donated to the py3k\nproposal.Just a short update on November's work: we're now passing about 194 of\napproximately 355 modules of CPython's regression test suite, up from passing\n160 last month. Many test modules only fail a small number of individual tests\nnow.We'd like to thank Amaury Forgeot d'Arc for his contributions, in particular he\nhas made significant progress on updating CPyExt for Python 3 this month.Some other highlights:test_marshal now passes, and there's been significant progress on\npickling (thanks Kenny Levinsen and Amaury for implementing\nint.{to,from}_bytes)\nWe now have a _posixsubprocess module\nMore encoding related fixes, which affects many failing tests\n_sre was updated and now test_re almost passes\nException behavior is almost complete per the Python 3 specs, what's mostly\nmissing now are the new __context__ and __traceback__ attributes (PEP\n3134)\nFixed some crashes and deadlocks occurring during the regression tests\nWe merged the unicode-strategies branch both to default and to py3k: now we\nhave versions of lists, dictionaries and sets specialized for unicode\nelements, as we already had for strings.\nHowever, for string-specialized containers are still faster in some cases\nbecause there are shortcuts which have not been implemented for unicode yet\n(e.g., constructing a set of strings from a list of strings). The plan is to\ncompletely kill the shortcuts and improve the JIT to produce the fast\nversion automatically for both the string and unicode versions, to have a\nmore maintainable codebase without sacrificing the speed. The autoreds\nbranch (already merged) was a first step in this direction.\ncheers,\nPhilip&Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/12/py3k-status-update-8-3932232806458251730.html"
    },
    {
      "title": "PyPy San Francisco Sprint Dec 1st - Dec 2nd 2012",
      "text": "The next PyPy sprint will be in San Francisco, California. It is a\npublic sprint, suitable for newcomers. It will run on Saturday December 1st and\nSunday December 2nd. The goals for the sprint are continued work towards the\n2.0 release as well as code cleanup, we of course welcome any topic which\ncontributors are interested in working on.Some other possible topics are:running your software on PyPy\nwork on PyPy's numpy (status)\nwork on STM (status)\nJIT improvements\nany exciting stuff you can think of\nIf there are newcomers, we'll run the usual introduction to hacking on\nPyPy.\nLocationThe sprint will be held at the Rackspace Office:620 Folsom St, Ste 100\nSan FranciscoThe doors will open at 10AM both days, and run until 6PM both days.Thanks to David Reid for helping get everything set up!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/11/pypy-san-francisco-sprint-dec-1st-dec-5133109101989613355.html"
    },
    {
      "title": "PyPy 2.0 beta 1",
      "text": "We're pleased to announce the 2.0 beta 1 release of PyPy. This release is\nnot a typical beta, in a sense the stability is the same or better than 1.9\nand can be used in production. It does however include a few performance\nregressions documented below that don't allow us to label is as 2.0 final.\n(It also contains many performance improvements.)\nThe main features of this release are support for ARM processor and\ncompatibility with CFFI. It also includes\nnumerous improvements to the numpy in pypy effort, cpyext and performance.\nYou can download the PyPy 2.0 beta 1 release here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast (pypy 2.0 beta 1 and cpython 2.7.3\nperformance comparison) due to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32. It also supports ARM machines running Linux.\nWindows 64 work is still stalling, we would welcome a volunteer\nto handle that.\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.\n\n\nRegressions\nReasons why this is not PyPy 2.0:\n\nthe ctypes fast path is now slower than it used to be. In PyPy\n1.9 ctypes was either incredibly faster or slower than CPython depending whether\nyou hit the fast path or not. Right now it's usually simply slower. We're\nprobably going to rewrite ctypes using cffi, which will make it\nuniversally faster.\ncffi (an alternative to interfacing with C code) is very fast, but\nit is missing one optimization that will make it as fast as a native\ncall from C.\nnumpypy lazy computation was disabled for the sake of simplicity.\nWe should reenable this for the final 2.0 release.\n\n\n\nHighlights\n\ncffi is officially supported by PyPy. You can install it normally by\nusing pip install cffi once you have installed PyPy and pip.\nThe corresponding 0.4 version of cffi has been released.\nARM is now an officially supported processor architecture.\nPyPy now work on soft-float ARM/Linux builds.  Currently ARM processors\nsupporting the ARMv7 and later ISA that include a floating-point unit are\nsupported.\nThis release contains the latest Python standard library 2.7.3 and is fully\ncompatible with Python 2.7.3.\nIt does not however contain hash randomization, since the solution present\nin CPython is not solving the problem anyway. The reason can be\nfound on the CPython issue tracker.\ngc.get_referrers() is now faster.\nVarious numpy improvements. The list includes:\naxis argument support in many places\nfull support for fancy indexing\ncomplex128 and complex64 dtypes\n\n\nJIT hooks are now a powerful tool to introspect the JITting process that\nPyPy performs.\n**kwds usage is much faster in the typical scenario\noperations on long objects are now as fast as in CPython (from\nroughly 2x slower)\nWe now have special strategies for dict/set/list which contain\nunicode strings, which means that now such collections will be both faster\nand more compact.\n\n\n\nThings we're working on\nThere are a few things that did not make it to the 2.0 beta 1, which\nare being actively worked on. Greenlets support in the JIT is one\nthat we would like to have before 2.0 final. Two important items that\nwill not make it to 2.0, but are being actively worked on, are:\n\nFaster JIT warmup time.\nSoftware Transactional Memory.\n\nCheers,\nMaciej Fijalkowski, Armin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/11/pypy-20-beta-1-2702952243260181341.html"
    },
    {
      "title": "Py3k status update #7",
      "text": "This is the seventh status update about our work on the py3k branch, which\nwe can work on thanks to all of the people who donated to the py3k\nproposal.The biggest news is that this month Philip started to work on py3k in parallel\nto Antonio. As such, there was an increased amount of activity.The py3k buildbots now fully translate the branch every night and run the\nPython standard library tests.We currently pass 160 out of approximately 355 modules of CPython's standard\ntest suite, fail 144 and skip approximately 51.Some highlights:dictviews (the objects returned by dict.keys/values/items) has been greatly\nimproved, and now they full support set operators\na lot of tests has been fixed wrt complex numbers (and in particular the\n__complex__ method)\n_csv has been fixed and now it correctly handles unicode instead of bytes\nmore parser fixes, py3k list comprehension semantics; now you can no longer\naccess the list comprehension variable after it finishes\n2to3'd most of the lib_pypy modules (pypy's custom standard lib\nreplacements/additions)\npy3-enabled pyrepl: this means that finally readline works at the command\nprompt, as well as builtins.input(). pdb seems to work, as well as\nfancycompleter to get colorful TAB completions :-)\npy3 round\nfurther tightening/cleanup of the unicode handling (more usage of\nsurrogateescape, surrogatepass among other things)\nas well as keeping up with some big changes happening on the default branch\nand of course various other fixes.\nFinally, we would like to thank Amaury Forgeot d'Arc for his significant\ncontributions.cheers,\nPhilip&Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/11/py3k-status-update-7-6182140595418083307.html"
    },
    {
      "title": "NumPy status update #5",
      "text": "Hello.\nI'm quite excited to inform that work on NumPy in PyPy has been restarted\nand there has been quite a bit of progress on the NumPy front in PyPy in the\npast two months. Things that happened:\n\ncomplex dtype support - thanks to matti picus, NumPy on PyPy now supports\ncomplex dtype (only complex128 so far, there is work on the other part)\nbig refactoring - probably the biggest issue we did was finishing\na big refactoring that disabled some speedups (notably lazy computation\nof arrays), but lowered the barrier of implementing cool new features.\nfancy indexing support - all fancy indexing tricks should now work,\nincluding a[b] where b is an array of integers.\nnewaxis support - now you can use newaxis features\nimprovements to ``intp``, ``uintp``, ``void``, ``string`` and record dtypes\n\nFeatures that have active branches, but hasn't been merged:\n\nfloat16 dtype support\nmissing ndarray attributes - this is a branch to finish all attributes\non ndarray, hence ending one chapter.\npickling support for numarray - hasn't started yet, but next on the list\n\nMore importantly, we're getting very close to able to import the python part\nof the original numpy with only import modifications and running it's tests.\nMost tests will fail at this point, however it'll be a good start for another\nchapter :-)\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/11/numpy-status-update-5-5489198414356844587.html"
    },
    {
      "title": "Cape Town 2012 sprint report",
      "text": "Hello.\nWe're about to finish a PyPy sprint in Cape Town, South Africa that was\none of the smallest done so far, only having Armin Rigo and Maciej Fijalkowski\nwith Alex Gaynor joining briefly at the beginning, however also one of the\nlongest, lasting almost 3 weeks. The sprint theme seems to be predominantly\n\"no new features\" and \"spring cleaning\". We overall removed about 20k lines\nof code in the PyPy source tree. The breakdown of things done and worked on:\n\nWe killed SomeObject support in annotation and rtyper. This is a modest\ncode saving, however, it reduces the complexity of RPython and also,\nhopefully, improves compile errors from RPython. We're far from done\non the path to have comprehensible compile-time errors, but the first\nstep is always the hardest :)\n\nWe killed some magic in specifying the interface between builtin functions\nand Python code. It used to be possible to write builtin functions like this:\n\ndef f(space, w_x='xyz'):\n\nwhich will magically wrap 'xyz' into a W_StringObject. Right now, instead,\nyou have to write:\n\n@unwrap_spec(w_x=WrappedDefault('xyz'))\ndef f(space, w_x):\n\nwhich is more verbose, but less magical.\n\nWe killed the CExtModuleBuilder which is the last remaining part of\ninfamous extension compiler that could in theory build C extensions\nfor CPython in RPython. This was never working very well and the main\npart was killed long ago.\n\nWe killed various code duplications in the C backend.\n\nWe killed microbench and a bunch of other small-to-medium unused\ndirectories.\n\nWe killed llgraph JIT backend and rewrote it from scratch. Now the llgraph\nbackend is not translatable, but this feature was rarely used and caused\na great deal of complexity.\n\nWe progressed on continulet-jit-3 branch, up to the point of merging\nit into result-in-resops branch, which also has seen a bit of progress.\nPurpose of those two branches:\n\ncontinulet-jit-3: enable stackless to interact with the JIT by killing\nglobal state while resuming from the JIT into the interpreter. This has\nmultiple benefits. For example it's one of the stones on the path to\nenable STM for PyPy. It also opens new possibilities for other optimizations\nincluding Python-Python calls and generators.\nresult-in-resops: the main goal is to speed up the tracing time of PyPy.\nWe found out the majority of time is spent in the optimizer chain,\nwhich faces an almost complete rewrite. It also simplifies the storage\nof the operations as well as the number of implicit invariants that have\nto be kept in mind while developing.\n\n\nWe finished and merged the excellent work by Ronan Lamy which makes the\nflow object space (used for abstract interpretation during RPython\ncompilation) independent from the Python interpreter. This means\nwe've achieved an important milestone on the path of separating the RPython\ntranslation toolchain from the PyPy Python interpreter.\n\n\nCheers,\nfijal & armin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/10/cape-town-2012-sprint-report-1612771358321767072.html"
    },
    {
      "title": "Py3k status update #6",
      "text": "This is the sixth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.The coolest news is not about what we did in the past weeks, but what we will\ndo in the next: I am pleased to announce that Philip Jenvey has been\nselected by the PyPy communitiy to be funded for his upcoming work on py3k,\nthanks to your generous donations. He will start to work on it shortly, and he\nwill surely help the branch to make faster progress.  I am also particularly\nhappy of this because Philip is the first non-core developer who is getting\npaid with donations: he demonstrated over the past months to be able to work\neffectively on PyPy, and so we were happy to approve his application for the\njob.  This means that anyone can potentially be selected in the future, the\nonly strict requirement is to have a deep interest in working on PyPy and to\nprove to be able to do so by contributing to the project.Back to the status of the branch. Most of the work since the last status\nupdate has been done in the area of, guess what? Unicode strings. As usual,\nthis is one of the most important changes between Python 2 and Python 3, so\nit's not surprising.  The biggest news is that now PyPy internally supports\nunicode identifiers (such as names of variables, functions, attributes, etc.),\nwhereas earlier it supported only ASCII bytes strings.  The changes is still\nbarely visible from the outside, because the parser still rejects non-ASCII\nidentifiers, however you can see it with a bit of creativity:>>>> def foo(x): pass\n>>>> foo(**{'\u00e0\u00e8\u00ec\u00f2\u00f9': 42})\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\nTypeError: foo() got an unexpected keyword argument '\u00e0\u00e8\u00ec\u00f2\u00f9'\nBefore the latest changes, you used to get question marks instead of the\nproper name for the keyword argument.  Although this might seem like a small\ndetail, it is a big step towards a proper working Python 3 interpreter and it\nrequired a couple of days of headaches.  A spin-off of this work is that now\nRPython has better built-in support for unicode (also in the default branch):\nfor example, it now supports unicode string formatting (using the percent\noperator) and the methods .encode/.decode('utf-8').Other than that there is the usual list of smaller issues and bugs that got\nfixed, including (but not limited to):teach the compiler when to emit the new opcode DELETE_DEREF (and\nimplement it!)\ndetect when we use spaces and TABs inconsistently in the source code, as\nCPython does\nfix yet another bug related to the new lexically scoped exceptions (this\nis the last one, hopefully)\nport some of the changes that we did to the standard CPython 2.7 tests to\n3.2, to mark those which are implementation details and should not be run on\nPyPy\nFinally, I would like to thank Amaury Forgeot d'Arc and Ariel Ben-Yehuda for\ntheir work on the branch; among other things, Amaury recently worked on\ncpyext and on the PyPy _cffi_backend, while Ariel submitted a patch to\nimplement PEP 3138.",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/09/py3k-status-update-6-4049281716377789914.html"
    },
    {
      "title": "PyPy Cape Town Sprint Oct 7th - Oct 21st 2012",
      "text": "Hello everyone!\nThe next PyPy sprint will be in Cape Town, South Africa. It is a\npublic sprint, suitable for newcomers. It starts a couple of days\nafter PyCon South Africa, which is on the 4th and 5th of October.\nThis is a relatively unusual sprint in that it is hosted halfway\nacross the world from where most contributors live, so we plan to\nspend some time during those two weeks doing sprinting and some time\ndoing touristy stuff. The goals for the sprint are general progress\nand whatever people are interested in.\nPossible topics:\n\nPyPy release 2.0\nrunning your software on PyPy\nwork on PyPy's numpy (status)\nwork on STM (status)\nJIT improvements\nany exciting stuff you can think of\n\nIf there are newcomers, we'll run the usual introduction to hacking on\nPyPy.\n\nLocation\nThe sprint will be held either in the apartment of fijal, which is in\nTamboerskloof, Cape Town, or in the offices of the Praekelt\nFoundation, located in Woodstock, Cape Town. The Praekelt Foundation\nhas offered to host us, if needed.\nCape Town, as a very touristy place, has tons of accomodation ranging\nin quality from good to amazing. Depending on the sprint location you\nmight need a car.\n\n\nGood to Know\nYou probably don't need visa for South Africa -- consult Wikipedia.\nSouth Africa is a lovely place with lots of stuff to do. You can see\npenguins, elephants, lions and sharks all on one day (or better yet,\non multiple days).\nThere is a wide selection of good restaurants within a reasonable\ndistance of the sprint venue (depending on the venue, either walking\nor driving).\nThe power plug is some weird derivative of an old-english standard,\nbut adapters are easily acquired.\n\n\nWho's Coming?\nIf you'd like to come, please let us know when you will be arriving\nand leaving, as well as what your interests are. We'll keep a list of\npeople which we'll update (or you can do so yourself if you have\nbitbucket pypy commit rights).\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/09/pypy-cape-town-sprint-oct-7th-oct-21st-5757682347636918027.html"
    },
    {
      "title": "NumPy on PyPy status update",
      "text": "Hello everyone.\nIt's been a while since we posted a numpy work update, but I'm pleased to\ninform you that work on it has been restarted. A lot of the work has been\ndone by Matti Picus, who is one of the newest contributors to the PyPy\nproject. None of the work below has been merged so far, it's work in progress:\n\nComplex dtype support.\nFixing incompatibilities between numpy and pypy's version.\nRefactoring numpypy to simplify the code and make it easier for new\ncontributors.\nReuse most of the numpy's pure python code without modifications.\n\nFinishing this is also the plan for the next month.\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/09/numpy-on-pypy-status-update-1605312600799448094.html"
    },
    {
      "title": "CFFI release 0.3",
      "text": "Hi everybody,\nWe released CFFI 0.3.  This is the first release that supports more\nthan CPython 2.x :-)\n\nCPython 2.6, 2.7, and 3.x are supported (3.3 definitely, but maybe 3.2 or earlier too)\nPyPy trunk is supported.\n\nIn more details, the main news are:\n\nsupport for PyPy.  You need to get a trunk version of PyPy, which\ncomes with the built-in module _cffi_backend to use with the CFFI\nrelease.  For testing, you can download the Linux 32/64 versions of\nPyPy trunk.  The OS/X and Windows versions of _cffi_backend\nare not tested at all so far, so probably don't work yet.\nsupport for Python 3.  It is unknown which exact version is\nrequired; probably 3.2 or even earlier, but we need 3.3 to run the\ntests.  The 3.x version is not a separate source; it runs out of the same sources.  Thanks Amaury for starting this port.\nthe main change in the API is that you need to use ffi.string(cdata)\ninstead of str(cdata) or unicode(cdata).  The motivation for this\nchange was the Python 3 compatibility.  If your Python 2 code used to\ncontain str(<cdata 'char *'>), it would interpret the memory content\nas a null-terminated string; but on Python 3 it would just return a\ndifferent string, namely \"<cdata 'char *'>\", and proceed without even\na crash, which is bad.  So ffi.string() solves it by always returning\nthe memory content as an 8-bit string (which is a str in Python 2 and\na bytes in Python 3).\nother minor API changes are documented at\nhttps://cffi.readthedocs.org/ (grep for version 0.3).\n\nUpcoming work, to be done before release 1.0:\n\nexpose to the user the module cffi.model in a possibly refactored\nway, for people that don't like (or for some reason can't easily use)\nstrings containing snippets of C declarations.  We are thinking about\nrefactoring it in such a way that it has a ctypes-compatible\ninterface, to ease porting existing code from ctypes to cffi.  Note\nthat this would concern only the C type and function declarations, not\nall the rest of ctypes.\nCFFI 1.0 will also have a corresponding PyPy release.  We are thinking\nabout calling it PyPy 2.0 and including the whole of CFFI (instead of\njust the _cffi_backend module like now).  In other words it will\nsupport CFFI out of the box --- we want to push forward usage of CFFI\nin PyPy :-)\n\nCheers,\nArmin Rigo and Maciej Fija\u0142kowski",
      "tags": "releasecffi",
      "url": "https://www.pypy.org/posts/2012/08/cffi-release-03-4740491796308953732.html"
    },
    {
      "title": "C++ objects in cppyy, part 1: Data Members",
      "text": "The cppyy module makes it possible to call into C++ from PyPy through the\nReflex package.\nDocumentation and setup instructions are\navailable here.\nRecent work has focused on STL, low-level buffers, and code quality, but also\na lot on pythonizations for the\nCINT backend, which is\nmostly for High Energy Physics (HEP) use only.\nA\nprevious posting walked\nthrough the high-level structure and organization of the module, where it was\nargued why it is necessary to write cppyy in RPython and generate bindings at\nrun-time for the best performance.\nThis posting details how access to C++ data structures is provided and is part\nof a series of 3 postings on C++ object representation in Python: the second\nposting will be about method dispatching, the third will tie up several odds\nand ends by showing how the choices presented here and in part 2 work together\nto make features such as auto-casting possible.\n\n\nWrapping Choices\n\nSay we have a plain old data type (POD), which is the simplest possible\ndata structure in C++.\nLike for example:\n\n    struct A {\n        int    m_i;\n        double m_d;\n    };\n\nWhat should such a POD look like when represented in Python?\nLet's start by looking at a Python data structure that is functionally\nsimilar, in that it also carries two public data members of the desired\ntypes.\nSomething like this:\n\n    class A(object):\n        def __init__(self):\n            self.m_i = 0\n            self.m_d = 0.\n\nAlright, now how to go about connecting this Python class with the former\nC++ POD?\nOr rather, how to connect instances of either.\nThe exact memory layout of a Python\nA\ninstance is up to Python, and likewise the layout of a C++\nA instance is up\nto C++.\nBoth layouts are implementation details of the underlying language, language\nimplementation, language version, and the platform used.\nIt should be no surprise then, that for example an\nint in C++ looks\nnothing like a\nPyIntObject, even\nthough it is perfectly possible, in both cases, to point out in memory where\nthe integer value is.\nThe two representations can thus not make use of the same block of memory\ninternally.\nHowever, the requirement is that the access to C++ from Python looks and feels\nnatural in its use, not that the mapping is exact.\nAnother requirement is that we want access to the actual object from both\nPython and C++.\nIn practice, it is easier to provide natural access to C++ from Python than\nthe other way around, because the choices of memory layout in C++ are far more\nrestrictive: the memory layout defines the access, as the actual class\ndefinition is gone at run-time.\nThe best choice then, is that the Python object will act as a proxy to the C++\nobject, with the actual data always being in C++.\n\nFrom here it follows that if the\nm_i data member\nlives in C++, then Python needs some kind of helper to access it.\nConveniently, since version 2.2, Python has a\nproperty construct\nthat can take a getter and setter function that are called when the property\nis used in Python code, and present it to the programmer as if it were a data\nmember.\nSo we arrive at this (note how the\nproperty instance\nis a variable at the class level):\n\n    class A(object):\n        def __init__(self):\n            self._cppthis = construct_new_A()\n        m_i = property(get_m_i, set_m_i)\n        m_d = property(get_m_d, set_m_d)\n\nThe\nconstruct_new_A\nhelper is not very interesting (the reflection layer can provide for it\ndirectly), and methods are a subject for part 2 of this posting, so focus on\nget_m_i\nand set_m_i.\nIn order for the getter to work, the method needs to have access to the C++\ninstance for which the Python object is a proxy.\nOn access, Python will call the getter function with the proxy instance for\nwhich it is called.\nThe proxy has a\n_cppthis data\nmember from which the C++ instance can be accessed (think of it as a pointer)\nand all is good, at least for\nm_i.\nThe second data member\nm_d, however,\nrequires some more work: it is located at some offset into\n_cppthis.\nThis offset can be obtained from the reflection information, which lets the\nC++ compiler calculate it, so details such as\nbyte padding\nare fully accounted for.\nSince the setter also needs the offset, and since both share some more details\nsuch as the containing class and type information of the data member, it is\nnatural to create a custom property class.\nThe getter and setter methods then become bound methods of an instance of that\ncustom property,\nCPPDataMember, and\nthere is one such instance per data member.\nThink of something along these lines:\n\n    def make_datamember(cppclass, name):\n        cppdm = cppyy.CPPDataMember(cppclass, name)\n        return property(cppdm.get, cppdm.set)\n\nwhere the\nmake_datamember\nfunction replaces the call to\nproperty in the\nclass definition above.\n\nNow hold on a minute!\nBefore it was argued that Python and C++ can not share the same underlying\nmemory structure, because of choices internal to the language.\nBut if on the Python side choices are being made by the developer of the\nlanguage bindings, that is no longer a limitation.\nIn other words, why not go through e.g. the Python extension API, and do\nthis:\n\n    struct A_pyproxy {\n        PyObject_HEAD\n        int    m_i;\n        double m_d;\n    };\n\nDoing so would save on\nmalloc overhead and remove\na pointer indirection.\nThere are some technical issues specific to PyPy for such a choice: there is\nno such thing as\nPyPyObject_HEAD\nand the layout of objects is not a given as that is decided only at\ntranslation time.\nBut assume that those issues can be solved, and also accept that there is no\nproblem in creating structure definitions like this at run-time, since the\nreflection layer can provide both the required size and access to the\nplacement\nnew operator\n(compare e.g. CPython's\nstruct module).\nThere is then still a more fundamental problem: it must be possible to take\nover ownership in Python from instances created in C++ and vice-versa.\nWith a proxy scheme, that is trivial: just pass the pointer and do the\nnecessary bookkeeping.\nWith an embedded object, however, not every use case can be implemented: e.g.\nif an object is created in Python, passed to C++, and deleted in C++, it\nmust have been allocated independently.\nThe proxy approach is therefore still the best choice, although embedding\nobjects may provide for optimizations in some use cases.\n\n\nInheritance\n\nThe next step, is to take a more complicated C++ class, one with inheritance\n(I'm leaving out details such as constructors etc., for brevity):\n\n    class A {\n    public:\n        virtual ~A() {}\n        int    m_i;\n        double m_d;\n    };\n\n    class B : public A {\n    public:\n        virtual ~B() {}\n        int    m_j;\n    };\n\nFrom the previous discussion, it should already be clear what this will look\nlike in Python:\n\n    class A(object):\n        def __init__(self):\n            self._cppthis = construct_new_A()\n        m_i = make_datamember('A', 'm_i')\n        m_d = make_datamember('A', 'm_d')\n\n    class B(A):\n        def __init__(self):\n            self._cppthis = construct_new_B()\n        m_j = make_datamember('B', 'm_j')\n\nThere are some minor adjustments needed, however.\nFor one, the offset of the\nm_i data member\nmay be no longer zero: it is possible that a virtual function dispatch table\n(vtable)\npointer is added at the beginning of\nA (an alternative\nis to have the vtable pointer at the end of the object).\nBut if\nm_i is handled the\nsame way as\nm_d, with the\noffset provided by the compiler, then the compiler will add the bits, if any,\nfor the vtable pointer and all is still fine.\nA real problem could come in however, with a call of the\nm_i property on\nan instance of\nB: in that case,\nthe _cppthis\npoints to a B\ninstance, whereas the getter/setter pair expect an\nA instance.\nIn practice, this is usually not a problem: compilers will align\nA and\nB and calculate\nan offset for\nm_j from the start\nof A.\nStill, that is an implementation detail (even though it is one that can be\ndetermined at run-time and thus taken advantage of by the JIT), so it can not\nbe relied upon.\nThe m_i getter\nthus needs to take into account that it can be called with a derived type,\nand so it needs to add an additional offset.\nWith that modification, the code looks something like this (as you would have\nguessed, this is getting more and more into pseudo-code territory, although it\nis conceptually close to the actual implementation in cppyy):\n\n    def get_m_i(self):\n        return int(self._cppthis + offset(A, m_i) + offset(self.__class__, A))\n\nWhich is a shame, really, because the offset between\nB and\nA is going\nto be zero most of the time in practice, and the JIT can not completely\nelide\nthe offset calculation (as we will see later; it is easy enough to elide if\nself.__class__ is\nA, though).\nOne possible solution is to repeat the properties for each derived class, i.e.\nto have a\nget_B_m_i etc., but\nthat looks ugly on the Python side and anyway\ndoes not work in all cases: e.g. with multiple inheritance where there are\ndata members with the same name in both bases, or if\nB itself has a\npublic data member called\nm_i that shadows\nthe one from A.\nThe optimization then, is achieved by making\nB in charge of the\noffset calculations, by making\noffset a method of\nB, like so:\n\n    def get_m_i(self):\n        return int(self._cppthis + offset(A, m_i) + self.offset(A))\n\nThe insight is that by scanning the inheritance hierarchy of a derived\nclass like B, you\ncan know statically whether it may sometimes need offsets, or whether the\noffsets are always going to be zero.\nHence, if the offsets are always zero, the method\noffset on\nB will\nsimply return the literal\n0 as its\nimplementation, with the JIT taking care of the rest through inlining and\nconstant folding.\nIf the offset could be non-zero, then the method will perform an actual\ncalculation, and it will let the JIT elide the call only if possible.\n\n\nMultiple Virtual Inheritance\n\nNext up would be multiple inheritance, but that is not very interesting: we\nalready have the offset calculation between the actual and base class, which\nis all that is needed to resolve any multiple inheritance hierarchy.\nSo, skip that and move on to multiple virtual inheritance.\nThat that is going to be a tad more complicated will be clear if you show the\nfollowing code snippet to any old C++ hand and see how they respond.\nMost likely you will be told: \"Don't ever do that.\"\nBut if code can be written, it will be written, and so for the sake of the\nargument, what would this look like in Python:\n\n    class A {\n    public:\n        virtual ~A() {}\n        int m_a;\n    };\n\n    class B : public virtual A {\n    public:\n        virtual ~B() {}\n        int m_b;\n    };\n\n    class C : public virtual A {\n    public:\n        virtual ~C() {}\n        int m_c;\n    };\n\n    class D : public virtual B, public virtual C {\n    public:\n        virtual ~D() {}\n        int m_d;\n    };\n\nActually, nothing changes from what we have seen so far: the scheme as laid\nout above is fully sufficient.\nFor example, D\nwould simply look like:\n\n    class D(B, C):\n        def __init__(self):\n            self._cppthis = construct_new_D()\n        m_d = make_datamember('D', 'm_d')\n\nPoint being, the only complication added by the multiple virtual\ninheritance, is that navigation of the C++ instance happens with pointers\ninternal to the instance rather than with offsets.\nHowever, it is still a fixed offset from any location to any other location\nwithin the instance as its parts are laid out consecutively in memory (this is\nnot a requirement, but it is the most efficient, so it is what is used in\npractice).\nBut what you can not do, is determine the offset statically: you need a live\n(i.e. constructed) object for any offset calculations.\nIn Python, everything is always done dynamically, so that is of itself not a\nlimitation.\nFurthermore,\nself is already\npassed to the offset calculation (remember that this was done to put the\ncalculation in the derived class, to optimize the common case of zero\noffset), thus a live C++ instance is there precisely when it is needed.\nThe call to the offset calculation is hard to elide, since the instance will\nbe passed to a C++ helper and so the most the JIT can do is guard on the\ninstance's memory address, which is likely to change between traces.\nInstead, explicit caching is needed on the base and derived types, allowing\nthe JIT to elide the lookup in the explicit cache.\n\n\nStatic Data Members and Global Variables\n\nThat, so far, covers all access to instance data members.\nNext up are static data members and global variables.\nA complication here is that a Python\nproperty needs to\nlive on the class in order to work its magic.\nOtherwise, if you get the property, it will simply return the getter function,\nand if you set it, it will dissappear.\nThe logical conclusion then, is that a\nproperty\nrepresenting a static or global variable, needs to live on the class of the\nclass, or the metaclass.\nIf done directly though, that would mean that every static data member is\navailable from every class, since all Python classes have the same metaclass,\nwhich is class\ntype (and which is\nits own metaclass).\nTo prevent that from happening and because\ntype is actually\nimmutable, each proxy class needs to have its own custom metaclass.\nFurthermore, since static data can also be accessed on the instance, the\nclass, too, gets a\nproperty object\nfor each static data member.\nExpressed in code, for a basic C++ class, this looks as follows:\n\n    class A {\n    public:\n        static int s_i;\n    };\n\nPaired with some Python code such as this, needed to expose the static\nvariable both on the class and the instance level:\n\n    meta_A = type(CppClassMeta, 'meta_A', [CPPMetaBase], {})\n    meta_A.s_i = make_datamember('A', 's_i')\n\n    class A(object):\n        __metaclass__ = meta_A\n        s_i = make_datamember('A', 's_i')\n\nInheritance adds no complications for the access of static data per se, but\nthere is the issue that the metaclasses must follow the same hierarchy as the\nproxy classes, for the Python method resolution order (MRO) to work.\nIn other words, there are two complete, parallel class hierarchies that map\none-to-one: a hierarchy for the proxy classes and one for their metaclasses.\n\nA parallel class hierarchy is used also in other highly dynamic,\nobject-oriented environments, such as for example\nSmalltalk.\nIn Smalltalk as well, class-level constructs, such as class methods and data\nmembers, are defined for the class in the metaclass.\nA metaclass hierarchy has further uses, such as lazy loading of nested\nclasses and member templates (this would be coded up in the base class of all\nmetaclasses:\nCPPMetaBase), and\nmakes it possible to distribute these over different reflection libraries.\nWith this in place, you can write Python codes like so:\n\n    >>>> from cppyy.gbl import A\n    >>>> a = A()\n    >>>> a.s_i = 42\n    >>>> print A.s_i == a.s_i\n    True\n    >>>> # etc.\n\nThe implementation of the getter for\ns_i is a lot\neasier than for instance data: the static data lives at a fixed, global,\naddress, so no offset calculations are needed.\nThe same is done for global data or global data living in namespaces:\nnamespaces are represented as Python classes, and global data are implemented\nas properties on them.\nThe need for a metaclass is one of the reasons why it is easier for namespaces\nto be classes: module objects are too restrictive.\nAnd even though namespaces are not modules, you still can, with\nsome limitations,\nimport from\nthem anyway.\n\nIt is common that global objects themselves are pointers, and therefore it\nis allowed that the stored\n_cppthis is not a\npointer to a C++ object, but rather a pointer to a pointer to a C++ object.\nA double pointer, as it were.\nThis way, if the C++ code updates the global pointer, it will automatically\nreflect on the Python side in the proxy.\nLikewise, if on the Python side the pointer gets set to a different variable,\nit is the pointer that gets updated, and this will be visible on the C++ side.\nIn general, however, the same caveat as for normal Python code applies: in\norder to set a global object, it needs to be set within the scope of that\nglobal object.\nAs an example, consider the following code for a C++ namespace\nNS with\nglobal variable\ng_a, which behaves\nthe same as Python code for what concerns the visibility of changes to the\nglobal variable:\n\n    >>>> from cppyy.gbl import NS, A\n    >>>> from NS import g_a\n    >>>> g_a = A(42)                     # does NOT update C++ side\n    >>>> print NS.g_a.m_i\n    13                                   # the old value happens to be 13\n    >>>> NS.g_a = A(42)                  # does update C++ side\n    >>>> print NS.g_a.m_i\n    42\n    >>>> # etc.\n\n\nConclusion\n\nThat covers all there is to know about data member access of C++ classes in\nPython through a reflection layer!\nA few final notes: RPython does not support metaclasses, and so the\nconstruction of proxy classes (code like\nmake_datamember\nabove) happens in Python code instead.\nThere is an overhead penalty of about 2x over pure RPython code associated\nwith that, due to extra guards that get inserted by the JIT.\nA factor of 2 sounds like a lot, but the overhead is tiny to begin with, and\n2x of tiny is still tiny and it's not easy to measure.\nThe class definition of the custom property,\nCPPDataMember, is\nin RPython code, to be transparent to the JIT.\nThe actual offset calculations are in the reflection layer.\nHaving the proxy class creation in Python, with structural code in RPython,\ncomplicates matters if proxy classes need to be constructed on-demand.\nFor example, if an instance of an as-of-yet unseen type is returned by a\nmethod.\nExplaining how that is solved is a topic of part 2, method calls, so stay\ntuned.\n\nThis posting laid out the reasoning behind the object representation of C++\nobjects in Python by cppyy for the purpose of data member access.\nIt explained how the chosen representation of offsets gives rise to a very\npythonic representation, which allows Python introspection tools to work as\nexpected.\nIt also explained some of the optimizations done for the benefit of the JIT.\nNext up are method calls, which will be described in part 2.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/08/c-objects-in-cppyy-part-1-data-members-1105848719513737614.html"
    },
    {
      "title": "Multicore Programming in PyPy and CPython",
      "text": "Hi all,\nThis is a short \"position paper\" kind of post about my view (Armin\nRigo's) on the future of multicore programming in high-level languages.\nIt is a summary of the\nkeynote presentation at EuroPython.  As I learned by talking with people\nafterwards, I am not a good enough speaker to manage to convey a deeper\nmessage in a 20-minutes talk.  I will try instead to convey it in a\n250-lines post...\nThis is about three points:\n\nWe often hear about people wanting a version of Python running without\nthe Global Interpreter Lock (GIL): a \"GIL-less Python\".  But what we\nprogrammers really need is not just a GIL-less Python --- we need a\nhigher-level way to write multithreaded programs than using directly\nthreads and locks.  One way is Automatic Mutual Exclusion (AME), which\nwould give us an \"AME Python\".\nA good enough Software Transactional Memory (STM) system can be used\nas an internal tool to do that.\nThis is what we are building into an \"AME PyPy\".\nThe picture is darker for CPython, though there is a way too.  The\nproblem is that when we say STM, we think about either GCC 4.7's STM\nsupport, or Hardware Transactional Memory (HTM).  However, both\nsolutions are enough for a \"GIL-less CPython\", but not\nfor \"AME CPython\", due to capacity limitations.  For the latter, we\nneed somehow to add some large-scale STM into the compiler.\n\nLet me explain these points in more details.\n\nGIL-less versus AME\nThe first point is in favor of the so-called Automatic Mutual Exclusion\napproach.  The issue with using threads (in any language with or without\na GIL) is that threads are fundamentally non-deterministic.  In other\nwords, the programs' behaviors are not reproductible at all, and worse,\nwe cannot even reason about it --- it becomes quickly messy.  We would\nhave to consider all possible combinations of code paths and timings,\nand we cannot hope to write tests that cover all combinations.  This\nfact is often documented as one of the main blockers towards writing\nsuccessful multithreaded applications.\nWe need to solve this issue with a higher-level solution.  Such\nsolutions exist theoretically, and Automatic Mutual Exclusion (AME) is\none of them.  The idea of AME is that we divide the execution of each\nthread into a number of \"atomic blocks\".  Each block is well-delimited\nand typically large.  Each block runs atomically, as if it acquired a\nGIL for its whole duration.  The trick is that internally we use\nTransactional Memory, which is a technique that lets the system run the\natomic blocks from each thread in parallel, while giving the programmer\nthe illusion that the blocks have been run in some global serialized\norder.\nThis doesn't magically solve all possible issues, but it helps a lot: it\nis far easier to reason in terms of a random ordering of large atomic\nblocks than in terms of a random ordering of lines of code --- not to\nmention the mess that multithreaded C is, where even a random ordering\nof instructions is not a sufficient model any more.\nHow do such atomic blocks look like?  For example, a program might\ncontain a loop over all keys of a dictionary, performing some\n\"mostly-independent\" work on each value.  This is a typical example:\neach atomic block is one iteration through the loop.  By using the\ntechnique described here, we can run the iterations in parallel\n(e.g. using a thread pool) but using AME to ensure that they appear to\nrun serially.\nIn Python, we don't care about the order in which the loop iterations\nare done, because we are anyway iterating over the keys of a dictionary.\nSo we get exactly the same effect as before: the iterations still run in\nsome random order, but --- and that's the important point --- they\nappear to run in a\nglobal serialized order.  In other words, we introduced parallelism, but\nonly under the hood: from the programmer's point of view, his program\nstill appears to run completely serially.  Parallelisation as a\ntheoretically invisible optimization...  more about the \"theoretically\"\nin the next paragraph.\nNote that randomness of order is not fundamental: they are techniques\nbuilding on top of AME that can be used to force the order of the\natomic blocks, if needed.\n\n\nPyPy and STM/AME\nTalking more precisely about PyPy: the current prototype pypy-stm is\ndoing precisely this.  In pypy-stm, the length of the atomic blocks is\nselected in one of two ways: either explicitly or automatically.\nThe automatic selection gives blocks corresponding to some small number\nof bytecodes, in which case we have merely a GIL-less Python: multiple\nthreads will appear to run serially, with the execution randomly\nswitching from one thread to another at bytecode boundaries, just like\nin CPython.\nThe explicit selection is closer to what was described in the previous\nsection: someone --- the programmer or the author of some library that\nthe programmer uses --- will explicitly put with thread.atomic: in\nthe source, which delimitates an atomic block.  For example, we can use\nit to build a library that can be used to iterate over the keys of a\ndictionary: instead of iterating over the dictionary directly, we would\nuse some custom utility which gives the elements \"in parallel\".  It\nwould give them by using internally a pool of threads, but enclosing\nevery handling of an element into such a with thread.atomic block.\nThis gives the nice illusion of a global serialized order, and thus\ngives us a well-behaving model of the program's behavior.\nRestating this differently,\nthe only semantical difference between pypy-stm and\na regular PyPy or CPython is that it has thread.atomic, which is a\ncontext manager that gives the illusion of forcing the GIL to not be\nreleased during the execution of the corresponding block of code.  Apart\nfrom this addition, they are apparently identical.\nOf course they are only semantically identical if we ignore performance:\npypy-stm uses multiple threads and can potentially benefit from that\non multicore machines.  The drawback is: when does it benefit, and how\nmuch?  The answer to this question is not immediate.  The programmer\nwill usually have to detect and locate places that cause too many\n\"conflicts\" in the Transactional Memory sense.  A conflict occurs when\ntwo atomic blocks write to the same location, or when A reads it,\nB writes it, but B finishes first and commits.  A conflict\ncauses the execution of one atomic block to be aborted and restarted,\ndue to another block committing.  Although the process is transparent,\nif it occurs more than occasionally, then it has a negative impact on\nperformance.\nThere is no out-of-the-box perfect solution for solving all conflicts.\nWhat we will need is more tools to detect them and deal with them, data\nstructures that are made aware of the risks of \"internal\" conflicts when\nexternally there shouldn't be one, and so on.  There is some work ahead.\nThe point here is that from the point of view of the final programmer,\nwe gets conflicts that we should resolve --- but at any point, our\nprogram is correct, even if it may not be yet as efficient as it could\nbe.  This is the opposite of regular multithreading, where programs are\nefficient but not as correct as they could be.  In other words, as we\nall know, we only have resources to do the easy 80% of the work and not\nthe remaining hard 20%.  So in this model we get a program that has 80%\nof the theoretical maximum of performance and it's fine.  In the regular\nmultithreading model we would instead only manage to remove 80% of the\nbugs, and we are left with obscure rare crashes.\n\n\nCPython and HTM\nCouldn't we do the same for CPython?  The problem here is that\npypy-stm is implemented as a transformation step during translation,\nwhich is not directly possible in CPython.  Here are our options:\n\nWe could review and change the C code everywhere in CPython.\nWe use GCC 4.7, which supports some form of STM.\nWe wait until Intel's next generation of CPUs comes out (\"Haswell\")\nand use HTM.\nWe write our own C code transformation within a compiler (e.g. LLVM).\n\nI will personally file the first solution in the \"thanks but no thanks\"\ncategory.  If anything, it will give us another fork of CPython that\nwill painfully struggle to keep not more than 3-4 versions behind, and\nthen eventually die.  It is very unlikely to be ever merged into the\nCPython trunk, because it would need changes everywhere.  Not to\nmention that these changes would be very experimental: tomorrow we might\nfigure out that different changes would have been better, and have to\nstart from scratch again.\nLet us turn instead to the next two solutions.  Both of these solutions\nare geared toward small-scale transactions, but not long-running ones.\nFor example, I have no clue how to give GCC rules about performing I/O\nin a transaction --- this seems not supported at all; and moreover\nlooking at the STM library that is available so far to be linked with\nthe compiled program, it assumes short transactions only.  By contrast,\nwhen I say \"long transaction\" I mean transactions that can run for 0.1\nseconds or more.  To give you an idea, in 0.1 seconds a PyPy program\nallocates and frees on the order of ~50MB of memory.\nIntel's Hardware Transactional Memory solution is both more flexible and\ncomes with a stricter limit.  In one word, the transaction boundaries\nare given by a pair of special CPU instructions that make the CPU enter\nor leave \"transactional\" mode.  If the transaction aborts, the CPU\ncancels any change, rolls back to the \"enter\" instruction and causes\nthis instruction to return an error code instead of re-entering\ntransactional mode (a bit like a fork()).  The software then detects\nthe error code.  Typically, if transactions are rarely cancelled, it is\nfine to fall back to a GIL-like solution just to redo these cancelled\ntransactions.\nAbout the implementation: this is done by recording all the changes that\na transaction wants to do to the main memory, and keeping them invisible\nto other CPUs.  This is \"easily\" achieved by keeping them inside this\nCPU's local cache; rolling back is then just a matter of discarding a\npart of this cache without committing it to memory.  From this point of\nview, there is a lot to bet that we are actually talking about the\nregular per-core Level 1 and Level 2 caches --- so any transaction that\ncannot fully store its read and written data in the 64+256KB of the L1+L2\ncaches will abort.\nSo what does it mean?  A Python interpreter overflows the L1 cache of\nthe CPU very quickly: just creating new Python function frames takes a\nlot of memory (on the order of magnitude of 1/100 of the whole L1\ncache).  Adding a 256KB L2 cache into the picture helps, particularly\nbecause it is highly associative and thus avoids a lot of fake conflicts.\nHowever, as long as the HTM support is limited to L1+L2 caches,\nit is not going to be enough to run an \"AME Python\" with any sort of\nmedium-to-long transaction.  It can\nrun a \"GIL-less Python\", though: just running a few hundred or even\nthousand bytecodes at a time should fit in the L1+L2 caches, for most\nbytecodes.\nI would vaguely guess that it will take on the order of 10 years until\nCPU cache sizes grow enough for a CPU in HTM mode to actually be able to\nrun 0.1-second transactions.  (Of course in 10 years' time a lot of other\nthings may occur too, including the whole Transactional Memory model\nbeing displaced by something else.)\n\n\nWrite your own STM for C\nLet's discuss now the last option: if neither GCC 4.7 nor HTM are\nsufficient for an \"AME CPython\", then we might want to\nwrite our own C compiler patch (as either extra work on GCC 4.7, or an\nextra pass to LLVM, for example).\nWe would have to deal with the fact that we get low-level information,\nand somehow need to preserve interesting high-level bits through the\ncompiler up to the point at which our pass runs: for example, whether\nthe field we read is immutable or not.  (This is important because some\ncommon objects are immutable, e.g. PyIntObject.  Immutable reads don't\nneed to be recorded, whereas reads of mutable data must be protected\nagainst other threads modifying them.)  We can also have custom code to\nhandle the reference counters: e.g. not consider it a conflict if\nmultiple transactions have changed the same reference counter, but just\nresolve it automatically at commit time.  We are also free to handle I/O\nin the way we want.\nMore generally, the advantage of this approach over both the current GCC\n4.7 and over HTM is that we control the whole process.  While this still\nlooks like a lot of work, it looks doable.  It would be possible to come\nup with a minimal patch of CPython that can be accepted into core\nwithout too much troubles (e.g. to mark immutable fields and tweak the\nrefcounting macros), and keep all the cleverness inside the compiler\nextension.\n\n\nConclusion\nI would assume that a programming model specific to PyPy and not\napplicable to CPython has little chances to catch on, as long as PyPy is\nnot the main Python interpreter (which looks unlikely to change anytime\nsoon).  Thus as long as only PyPy has AME, it looks like it will not\nbecome the main model of multicore usage in Python.  However, I can\nconclude with a more positive note than during the EuroPython\nconference: it is a lot of work, but there is a more-or-less reasonable\nway forward to have an AME version of CPython too.\nIn the meantime, pypy-stm is around the corner, and together with\ntools developed on top of it, it might become really useful and used.  I\nhope that in the next few years this work will trigger enough motivation\nfor CPython to follow the ideas.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/08/multicore-programming-in-pypy-and-6595343388141556320.html"
    },
    {
      "title": "NumPyPy non-progress report",
      "text": "Hello everyone.\nNot much has happened in the past few months with numpypy development. A part\nof the reason was doing other stuff for me, a part of the reason was\nvarious unexpected visa-related admin, a part of the reason was EuroPython\nand a part was long-awaited holiday.\nThe thing that's maybe worth mentioning is that it does not mean the donations\ndisappeared in the mist. PyPy developers are being paid to work on NumPyPy on\nan hourly basis - that means if I decide to take holidays or work on something\nelse, the money is simply staying in the account until later.\nThanks again for all the donations, I hope to get back to this topic soon!\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/08/hello-everyone-5492331040603503642.html"
    },
    {
      "title": "CFFI release 0.2.1",
      "text": "Hi everybody,We released CFFI 0.2.1 (expected to be 1.0 soon).  CFFI is a way to call C from Python.EDIT: Win32 was broken in 0.2.  Fixed.This release is only for CPython 2.6 or 2.7.  PyPy support is coming in\nthe ffi-backend branch, but not finished yet.  CPython 3.x would be\neasy but requires the help of someone.The package is available on bitbucket as well as documented. You\ncan also install it straight from the python package index: pip install cffiContains numerous small changes and support for more C-isms.\nThe biggest news is the support for installing packages that use\nffi.verify() on machines without a C compiler.  Arguably, this\nlifts the last serious restriction for people to use CFFI.\nPartial list of smaller changes:mappings between 'wchar_t' and Python unicodes\nthe introduction of ffi.NULL\na possibly clearer API for ffi.new(): e.g. to allocate a single int and obtain a pointer to it, use ffi.new(\"int *\") instead of the old\nffi.new(\"int\")\nand of course a plethora of smaller bug fixes\n\nCFFI uses pkg-config to install itself if available.  This helps\nlocate libffi on modern Linuxes.  Mac OS/X support is available too\n(see the detailed installation instructions).  Win32 should work out\nof the box.  Win64 has not been really tested yet.\nCheers,\nArmin Rigo and Maciej Fija\u0142kowski",
      "tags": "releasecffi",
      "url": "https://www.pypy.org/posts/2012/07/cffi-release-02-4800000428934604295.html"
    },
    {
      "title": "Prototype PHP interpreter using the PyPy toolchain - Hippy VM",
      "text": "Hello everyone.\nI'm proud to release the result of a Facebook-sponsored study on the feasibility of\nusing the RPython toolchain to produce a PHP interpreter. The rules were\nsimple: two months; one person; get as close to PHP as possible, implementing\nenough warts and corner cases to be reasonably sure that it answers hard\nproblems in the PHP language. The outcome is called Hippy VM and implements\nmost of the PHP 1.0 language (functions, arrays, ints, floats and strings).\nThis should be considered an alpha release.\nThe resulting interpreter is obviously incomplete \u2013 it does not support all\nmodern PHP constructs (classes are completely unimplemented), builtin functions,\ngrammar productions, web server integration, builtin libraries\netc., etc.. It's just complete enough for me to reasonably be able to\nsay that \u2013 given some engineering effort \u2013 it's possible to provide a rock-solid\nand fast PHP VM using PyPy technologies.\nThe result is available in a Bitbucket repo and is released under the MIT\nlicense.\n\nPerformance\nThe table below shows a few benchmarks comparing Hippy VM to Zend (a standard\nPHP interpreter available in Linux distributions) and HipHop VM (a PHP-to-C++\noptimizing compiler developed by Facebook).  The versions used were Zend 5.3.2\n(Zend Engine v2.3.0) and HipHop VM heads/vm-0-ga4fbb08028493df0f5e44f2bf7c042e859e245ab\n(note that you need to check out the vm branch to get the newest version).\nThe run was performed on 64-bit Linux running on a Xeon W3580 with 8M of\nL2 cache, which was otherwise unoccupied.\nUnfortunately, I was not able to run it on the JITted version of HHVM, the new effort by Facebook,\nbut people involved with the project told me it's usually slower or comparable with the compiled HipHop.\nTheir JITted VM is still alpha software, so I'll update it as soon as I have the info.\n\n\n\n\n\n\n\n\n\n\n\nbenchmark\nZend\nHipHop VM\nHippy VM\nHippy / Zend\nHippy / HipHop\n\narr\n2.771\n0.508+-0%\n0.274+-0%\n10.1x\n1.8x\n\nfannkuch\n21.239\n7.248+-0%\n1.377+-0%\n15.4x\n5.3x\n\nheapsort\n1.739\n0.507+-0%\n0.192+-0%\n9.1x\n2.6x\n\nbinary_trees\n3.223\n0.641+-0%\n0.460+-0%\n7.0x\n1.4x\n\ncache_get_scb\n3.350\n0.614+-0%\n0.267+-2%\n12.6x\n2.3x\n\nfib\n2.357\n0.497+-0%\n0.021+-0%\n111.6x\n23.5x\n\nfasta\n1.499\n0.233+-4%\n0.177+-0%\n8.5x\n1.3x\n\n\n\n\nThe PyPy compiler toolchain provides a way to implement a dynamic\nlanguage interpreter in a high-level language called RPython. This is\na language which is lower-level than Python, but still higher-level than\nC or C++: for example, RPython is a garbage-collected language. The killer\nfeature is that the toolchain will generate a JIT for your interpreter which\nwill be able to leverage most of the work that has been done on speeding up Python\nin the PyPy project.  The resulting JIT is generated for your interpreter, and is not Python-specific.\nThis was one of the toolchain's original design decisions \u2013 in contrast to e.g. the JVM,\nwhich was initially only used to interpret Java and later adjusted to serve as a platform for\ndynamic languages.\nAnother important difference is that there is no common bytecode to which you compile both your\nlanguage and Python, so you don't inherit problems presented when implementing language X on top of,\nsay, Parrot VM or the JVM.  The PyPy toolchain does not impose constraints on the semantics of\nyour language, whereas the benefits of the JVM only apply to languages that map well onto Java concepts.\nTo read more about creating your own interpreters using the PyPy toolchain,\nread more blog posts or an excellent article by Laurence Tratt.\n\n\nPHP deviations\nThe project's biggest deviation from the PHP specification is probably\nthat GC is no longer reference counting. That means that the object finalizer, when\nimplemented, will not be called directly at the moment of object death, but\nat some later point. There are possible future developments to alleviate that\nproblem, by providing \"refcounted\" objects when leaving the current scope.\nResearch has to be done in order to achieve that.\n\n\nAssessment\nThe RPython toolchain seems to be a cost-effective choice for writing\ndynamic language VMs.  It both provides a fast JIT and gives you\naccess to low-level primitives when you need them. A good example is\nin the directory hippy/rpython which contains the implementation\nof an ordered dictionary. An ordered dictionary is not a primitive\nthat RPython provides \u2013 it's not necessary for the goal of\nimplementing Python.  Now, implementing it on top of a normal dictionary\nis possible, but inefficient. RPython provides a way to work\ndirectly at a lower level, if you desire to do so.\nThings that require improvements in RPython:\n\nLack of mutable strings on the RPython level ended up being a problem.\nI ended up using lists of characters; which are efficient, but inconvenient,\nsince they don't support any string methods.\nFrame handling is too conservative and too Python-specific, especially around\nthe calls. It's possible to implement less general, but simpler and faster\nframe handling implementation in RPython.\n\n\n\nStatus of the implementation\nDon't use it! It's a research prototype intended to assess the feasibility\nof using RPython to create dynamic language VMs. The most notable\nfeature that's missing is reasonable error reporting. That said, I'm\nconfident it implements enough of the PHP language to prove that the full\nimplementation will present the same performance characteristics.\n\n\nBenchmarks\nThe benchmarks are a selection of computer language shootout benchmarks, as well\nas cache_get_scb, which is a part of old Facebook code. All benchmarks other\nthan this one (which is not open source, but definitely the most interesting :( ) are\navailable in the bench directory. The Python program to run them is called\nrunner.py and is in the same directory. It runs them 10 times, cutting off the first\n3 runs (to ignore the JIT warm-up time) and averaging the rest. As you can see\nthe standard deviation is fairly minimal for all interpreters and runs; if\nit's omitted it means it's below 0.5%.\nThe benchmarks were not selected for their ease of optimization \u2013 the optimizations\nin the interpreter were written specifically for this set of benchmarks. No special JIT\noptimizations were added, and barring what's mentioned below a vanilla PyPy 1.9 checkout\nwas used for compilation.\n\n\nSo, how fast will my website run if this is completed?\nThe truth is that I lack the benchmarks to be able to answer that right now. The core\nof the PHP language is implemented up to the point where I'm confident\nthat the performance will not change as we get more of the PHP going.\n\n\nHow do I run it?\nGet a PyPy checkout, apply the diff if you want to squeeze out the last\nbits of performance and run pypy-checkout/pypy/bin/rpython targethippy.py to\nget an executable that resembles a PHP interpreter. You can also directly run\npython targethippy.py file.php, but this will be about 2000x slower.\n\n\nRPython modifications\nThere was a modification that I did to the PyPy source code; the diff\nis available. It's trivial, and should simply be made optional in the\nRPython JIT generator, but it was easier just to do it, given the very constrained time\nframe.\n\ngen_store_back_in_virtualizable was disabled. This feature is\nnecessary for Python frames but not for PHP frames. PHP frames\ndo not have to be kept alive after we exit a function.\n\n\n\nFuture\nHippy is a cool prototype that presents a very interesting path towards a fast\nPHP VM.  However, at the moment I have too many other open source commitments\nto take on the task of completing it in my spare time.  I do think that this project\nhas a lot of potential, but I will not commit to any further development at\nthis time.  If you send pull requests I'll try to review them.  I'm also open\nto having further development on this project funded, so if you're interested\nin this project and the potential of a fast PHP interpreter, please get in\ntouch.\n\nCheers,\nfijal\nEDIT: Fixed the path to the rpython binary",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/07/hello-everyone-6869934374873967346.html"
    },
    {
      "title": "Py3k status update #5",
      "text": "This is the fifth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.Apart from the usual \"fix shallow py3k-related bugs\" part, most of my work in\nthis iteration has been to fix the bootstrap logic of the interpreter, in\nparticular to setup the initial sys.path.Until few weeks ago, the logic to determine sys.path was written entirely\nat app-level in pypy/translator/goal/app_main.py, which is automatically\nincluded inside the executable during translation.  The algorithm is more or\nless like this:find the absolute path of the executable by looking at sys.argv[0]\nand cycling through all the directories in PATH\nstarting from there, go up in the directory hierarchy until we find a\ndirectory which contains lib-python and lib_pypy\nThis works fine for Python 2 where the paths and filenames are represented as\n8-bit strings, but it is a problem for Python 3 where we want to use unicode\ninstead.  In particular, whenever we try to encode a 8-bit string into an\nunicode, PyPy asks the _codecs built-in module to find the suitable\ncodec. Then, _codecs tries to import the encodings package, to list\nall the available encodings. encodings is a package of the standard\nlibrary written in pure Python, so it is located inside\nlib-python/3.2. But at this point in time we yet have to add\nlib-python/3.2 to sys.path, so the import fails.  Bootstrap problem!The hard part was to find the problem: since it is an error which happens so\nearly, the interpreter is not even able to display a traceback, because it\ncannot yet import traceback.py. The only way to debug it was through some\ncarefully placed print statement and the help of gdb. Once found the\nproblem, the solution was as easy as moving part of the logic to RPython,\nwhere we don't have bootstrap problems.Once the problem was fixed, I was able to finally run all the CPython test\nagainst the compiled PyPy.  As expected there are lots of failures, and fixing\nthem will be the topic of my next months.",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/07/py3k-status-update-5-359698189825543897.html"
    },
    {
      "title": "EuroPython sprint",
      "text": "Hi all,\n\nEuroPython is next week.  We will actually be giving a presentation on Monday, in one of the plenary talks: PyPy: current status and GIL-less future.  This is the first international PyPy keynote we give, as far as I know, but not the first keynote about PyPy [David Beazley's video] :-)\n\nThe other talks are PyPy JIT under the hood and to some extent Performance analysis tools for JITted VMs.  This year we are also trying out a help desk.  Finally, we will have the usual sprint after EuroPython on Saturday and Sunday.\n\nSee you soon!\n\nArmin.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/06/europython-sprint-5668923199392472912.html"
    },
    {
      "title": "Architecture of Cppyy",
      "text": "The cppyy module makes it possible to call into C++ from PyPy through the\nReflex package.\nWork started about two years ago, with a follow-up sprint a year later.\nThe module has now reached an acceptable level of maturity and initial\ndocumentation with setup instructions, as well as a list of the currently\nsupported language features, are now available here.\nThere is a sizable (non-PyPy) set of unit and application tests that is still\nbeing worked through, not all of them of general applicability, so development\ncontinues its current somewhat random walk towards full language coverage.\nHowever, if you find that cppyy by and large works for you except for certain\nspecific features, feel free to ask for them to be given higher priority.\nCppyy handles bindings differently than what is typically found in other\ntools with a similar objective, so this update walks through some of these\ndifferences, and explains why choices were made as they are.\nThe most visible difference, is from the viewpoint of the Python programmer\ninteracting with the module.\nThe two canonical ways of making Python part of a larger environment, are to\neither embed or extend it.\nThe latter is done with so-called extension modules, which are explicitly\nconstructed to be very similar in their presentation to the Python programmer\nas normal Python modules.\nIn cppyy, however, the external C++ world is presented from a single entrance\npoint, the global C++ namespace (in the form of the variable cppyy.gbl).\nThus, instead of importing a package that contains your C++ classes, usage\nlooks like this (assuming class MyClass in the global namespace):\n\n>>>> import cppyy\n>>>> m = cppyy.gbl.MyClass()\n>>>> # etc.\n\nThis is more natural than it appears at first: C++ classes and functions are,\nonce compiled, represented by unique linker symbols, so it makes sense to give\nthem their own unique place on the Python side as well.\nThis organization allows pythonizations of C++ classes to propagate from one\ncode to another, ensures that all normal Python introspection (such as\nissubclass and isinstance) works as expected in all cases, and that it\nis possible to represent C++ constructs such as typedefs simply by Python\nreferences.\nAchieving this unified presentation would clearly require a lot of internal\nadministration to track all C++ entities if they each lived in their own,\npre-built extension modules.\nSo instead, cppyy generates the C++ bindings at run-time, which brings us to\nthe next difference.\nThen again, that is not really a difference: when writing or generating a\nPython extension module, the result is some C code that consists of calls into\nPython, which then gets compiled.\nHowever, it is not the bindings themselves that are compiled; it is the code\nthat creates the bindings that gets compiled.\nIn other words, any generated or hand-written extension module does exactly\nwhat cppyy does, except that they are much more specific in that the bound\ncode is hard-wired with e.g. fixed strings and external function calls.\nThe upshot is that in Python, where all objects are first-class and run-time\nconstructs, there is no difference whatsoever between bindings generated at\nrun-time, and bindings generated at ... well, run-time really.\nThere is a difference in organization, though, which goes back to the first\npoint of structuring the C++ class proxies in Python: given that a class will\nsettle in a unique place once bound, instead of inside a module that has no\nmeaning in the C++ world, it follows that it can also be uniquely located in\nthe first place.\nIn other words, cppyy can, and does, make use of a class loader to\nauto-load classes on-demand.\nIf at this point, this all reminds you of a bit ctypes, just with some extra\nbells and whistles, you would be quite right.\nIn fact, internally cppyy makes heavy use of the RPython modules that form the\nguts of ctypes.\nThe difficult part of ctypes, however, is the requirement to annotate\nfunctions and structures.\nThat is not very pleasant in C, but in C++ there is a whole other level of\ncomplexity in that the C++ standard specifies many low-level details, that are\nrequired for dispatching calls and understanding object layout, as\n\"implementation defined.\"\nOf course, in the case of Open Source compilers, getting at those details is\ndoable, but having to reverse engineer closed-source compilers gets old rather\nquickly in more ways than one.\nMore generally, these implementation defined details prevent a clean interface,\ni.e. without a further dependency on the compiler, into C++ like the one that\nthe CFFI module provides for C.\nStill, once internal pointers have been followed, offsets have been calculated,\nthis objects have been provided, etc., etc., the final dispatch into binary\nC++ is no different than that into C, and cppyy will therefore be able to make\nuse of CFFI internally, like it does with ctypes today.\nThis is especially relevant in the CLang/LLVM world, where stub functions\nare done away with.\nTo get the required low-level details then, cppyy relies on a back-end, rather\nthan getting it from the programmer, and this is where Reflex (together with\nthe relevant C++ compiler) comes in, largely automating this tedious process.\nThere is nothing special about Reflex per se, other than that it is relatively\nlightweight, available, and has proven to be able to handle huge code bases.\nIt was a known quantity when work on cppyy started, and given the number\nof moving parts in learning PyPy, that was a welcome relief.\nReflex is based on gccxml, and can therefore handle pretty much any C or\nC++ code that you care to throw at it.\nIt is also technically speaking obsolete as it will not support C++11, since\ngccxml won't, but its expected replacement, based on CLang/LLVM, is not\nquite there yet (we are looking at Q3 of this year).\nIn cppyy, access to Reflex, or any back-end for that matter, is through a\nthin C API (see the schematic below): cppyy asks high level questions to the\nback-end, and receives low-level results, some of which are in the form of\nopaque handles.\nThis ensures that cppyy is not tied to any specific back-end.\nIn fact, currently it already supports another, CINT, but that back-end is\nof little interest outside of High Energy Physics (HEP).\nThe Python side is always the same, however, so any Python code based on cppyy\ndoes not have to change if the back-end changes.\nTo use the system,  a back-end specific tool (genreflex for Reflex) is\nfirst run on a set of header files with a selection file for choosing the\nrequired classes.\nThis produces a C++ file that must be compiled into a shared library, and a\ncorresponding map file for the class loader.\nThese shared libraries, with their map files alongside, can be put anywhere\nas long as they can be located through the standard paths for the dynamic\nloader.\nWith that in place, the setup is ready, and the C++ classes are available to\nbe used from cppyy.\n\nSo far, nothing that has been described is specific to PyPy.\nIn fact, most of the technologies described have been used for a long time\non CPython already, so why the need for a new, PyPy-specific, module?\nTo get to that, it is important to first understand how a call is mediated\nbetween Python and C++.\nIn Python, there is the concept of a PyObject, which has a reference count, a\npointer to a type object, and some payload.\nThere are APIs to extract the low-level information from the payload for use\nin the C++ call, and to repackage any results from the call.\nThis marshalling is where the bulk of the time is spent when dispatching.\nTo be absolutely precise, most C++ extension module generators produce slow\ndispatches because they don't handle overloads efficiently, but even in there,\nthey still spend most of their time in the marshalling code, albeit in calls\nthat fail before trying the next overload.\nIn PyPy, speed is gained by having the JIT unbox objects into the payload only,\nallowing it to become part of compiled traces.\nIf the same marshalling APIs were used, the JIT is forced to rebox the payload,\nhand it over through the API, only to have it unboxed again by the binding.\nDoing so is dreadfully inefficient.\nThe objective of cppyy, then, is to keep all code transparent to the JIT until\nthe absolute last possible moment, i.e. the call into C++ itself, therefore\nallowing it to (more or less) directly pass the payload it already has, with\nan absolute minimal amount of extra work.\nIn the extreme case when the binding is not to a call, but to a data member of\nan object (or to a global variable), the memory address is delivered to the\nJIT and this results in direct access with no overhead.\nNote the interplay: cppyy in PyPy does not work like a binding in the CPython\nsense that is a back-and-forth between the interpreter and the extension.\nInstead, it does its work by being transparent to the JIT, allowing the JIT to\ndissolve the binding.\nAnd with that, we have made a full circle: if to work well with the JIT, and\nin so doing achieve the best performance, you can not have marshalling or do\nany other API-based driving, then the concept of compiled extension modules is\nout, and the better solution is in run-time generated bindings.\nThat leaves one final point.\nWhat if you do want to present an extension module-like interface to\nprogrammers that use your code?\nBut of course, this is Python: everything consists of first-class objects,\nwhose behavior can be changed on the fly.\nIn CPython, you might hesitate to make such changes, as every overlay or\nindirection results in quite a bit of overhead.\nWith PyPy, however, these layers are all optimized out of existences, making\nthat a non-issue.\nThis posting laid out the reasoning behind the organization of cppyy.\nA follow-up is planned, to explain how C++ objects are handled and\nrepresented internally.\nWim Lavrijsen",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/06/architecture-of-cppyy-9077100041707701102.html"
    },
    {
      "title": "Release 0.1 of CFFI",
      "text": "Hi.We're pleased to announce the first public release, 0.1 of CFFI, a way to call C from Python.\n(This release does not support PyPy yet --- but we announce it here as it is planned for the\nnext release :-)The package is available on bitbucket as well as documented. You can also install it\nstraight from the python package index (pip).The aim of this project is to provide a convenient and reliable way of calling C code from Python.\nThe interface is based on LuaJIT's FFI and follows a few principles:The goal is to call C code from Python.  You should be able to do so\nwithout learning a 3rd language: every alternative requires you to learn\ntheir own language (Cython, SWIG) or API (ctypes).  So we tried to\nassume that you know Python and C and minimize the extra bits of API that\nyou need to learn.\nKeep all the Python-related logic in Python so that you don't need to\nwrite much C code (unlike CPython native C extensions).\nWork either at the level of the ABI (Application Binary Interface)\nor the API (Application Programming Interface).  Usually, C\nlibraries have a specified C API but often not an ABI (e.g. they may\ndocument a \"struct\" as having at least these fields, but maybe more).\n(ctypes works at the ABI level, whereas Cython or native C extensions\nwork at the API level.)\nWe try to be complete.  For now some C99 constructs are not supported,\nbut all C89 should be, including macros (and including macro \"abuses\",\nwhich you can manually wrap in saner-looking C functions).\nWe attempt to support both PyPy and CPython (although PyPy support is not\ncomplete yet) with a reasonable path for other Python implementations like\nIronPython and Jython.\nNote that this project is not about embedding executable C code in\nPython, unlike Weave.  This is about calling existing C libraries\nfrom Python.\nStatus of the projectConsider this as a beta release. Creating CPython extensions is fully supported and the API should\nbe relatively stable; however, minor adjustements of the API are possible.PyPy support is not yet done and this is a goal for the next release. There are vague plans to make this the\npreferred way to call C from Python that can reliably work between PyPy and CPython.Right now CFFI's verify() requires a C compiler and header files to be available at run-time.\nThis limitation will be lifted in the near future and it'll contain a way to cache the resulting binary.Cheers,\n\nArmin Rigo and Maciej Fija\u0142kowski",
      "tags": "releasecffi",
      "url": "https://www.pypy.org/posts/2012/06/release-01-of-cffi-4760622823232463868.html"
    },
    {
      "title": "STM with threads",
      "text": "Hi all,A quick update.  The first version of pypy-stm based on regular\nthreads is ready.  Still having no JIT and a 4-or-5-times performance\nhit, it is not particularly fast, but I am happy that it turns out not\nto be much slower than the previous thread-less attempts.  It is at\nleast fast enough to run faster (in real time) than an equivalent no-STM\nPyPy, if fed with an eight-threaded program on an eight-core machine\n(provided, of course, you don't mind it eating all 8 cores' CPU power\ninstead of just one :-).You can download and play around with this binary for Linux 64.  It\nwas made from the stm-thread branch of the PyPy repository (translate.py --stm -O2 targetpypystandalone.py).  (Be sure\nto put it where it can find its stdlib, e.g. by putting it inside the\ndirectory from the official 1.9 release.)This binary supports the thread module and runs without the GIL.\nSo, despite the factor-of-4 slow-down issue, it should be the fourth\ncomplete Python interpreter in which we can reasonably claim to have\nresolved the problem of the GIL.  (The first one was Greg Stein's Python\n1.4, re-explored here; the second one is Jython; the third one is\nIronPython.)  Unlike the previous three, it is also the first one to\noffer full GIL semantics to the programmer, and additionally\nthread.atomic (see below).  I should also add that we're likely to\nsee in the next year a 5th such interpreter, too, based on Hardware\nTransactional Memory (same approach as with STM, but using e.g.\nIntel's HTM).The binary I linked to above supports all built-in modules from PyPy,\napart from signal, still being worked on (which can be a bit\nannoying because standard library modules like subprocess depend on\nit).  The sys.get/setcheckinterval() functions can be used to tweak\nthe frequency of the automatic commits.  Additionally, it offers\nthread.atomic, described in the previous blog post as a way to\ncreate longer atomic sections (with the observable effect of preventing\nthe \"GIL\" to be released during that time).  A complete\ntransaction.py module based on it is available from the sources.The main missing features are:the signal module;\nthe Garbage Collector, which does not do major collections so far, only\nminor ones;\nand finally, the JIT, which needs some amount of integration to generate\nthe correctly-tweaked assembler.\nHave fun!Armin.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/06/stm-with-threads-7818875111634541910.html"
    },
    {
      "title": "PyPy 1.9 - Yard Wolf",
      "text": "We're pleased to announce the 1.9 release of PyPy. This release brings mostly\nbugfixes, performance improvements, other small improvements and overall\nprogress on the numpypy effort.\nIt also brings an improved situation on Windows and OS X.You can download the PyPy 1.9 release here:https://pypy.org/download.htmlWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 1.9 and cpython 2.7.2 performance comparison)\ndue to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Windows 64 work is still stalling, we would welcome a volunteer\nto handle that.Thanks to our donorsBut first of all, we would like to say thank you to all people who\ndonated some money to one of our four calls:NumPy in PyPy (got so far $44502 out of $60000, 74%)\nPy3k (Python 3) (got so far $43563 out of $105000, 41%)\nSoftware Transactional Memory (got so far $21791 of $50400, 43%)\nas well as our general PyPy pot.\nThank you all for proving that it is indeed possible for a small team of\nprogrammers to get funded like that, at least for some\ntime.  We want to include this thank you in the present release\nannouncement even though most of the work is not finished yet.  More\nprecisely, neither Py3k nor STM are ready to make it in an official release\nyet: people interested in them need to grab and (attempt to) translate\nPyPy from the corresponding branches (respectively py3k and\nstm-thread).HighlightsThis release still implements Python 2.7.2.\nMany bugs were corrected for Windows 32 bit.  This includes new\nfunctionality to test the validity of file descriptors; and\ncorrect handling of the calling convensions for ctypes.  (Still not\nmuch progress on Win64.) A lot of work on this has been done by Matti Picus\nand Amaury Forgeot d'Arc.\nImprovements in cpyext, our emulator for CPython C extension modules.\nFor example PyOpenSSL should now work.  We thank various people for help.\nSets now have strategies just like dictionaries. This means for example\nthat a set containing only ints will be more compact (and faster).\nA lot of progress on various aspects of numpypy. See the numpy-status\npage for the automatic report.\nIt is now possible to create and manipulate C-like structures using the\nPyPy-only _ffi module.  The advantage over using e.g. ctypes is that\n_ffi is very JIT-friendly, and getting/setting of fields is translated\nto few assembler instructions by the JIT. However, this is mostly intended\nas a low-level backend to be used by more user-friendly FFI packages, and\nthe API might change in the future. Use it at your own risk.\nThe non-x86 backends for the JIT are progressing but are still not\nmerged (ARMv7 and PPC64).\nJIT hooks for inspecting the created assembler code have been improved.\nSee JIT hooks documentation for details.\nselect.kqueue has been added (BSD).\nHandling of keyword arguments has been drastically improved in the best-case\nscenario: proxy functions which simply forwards *args and **kwargs\nto another function now performs much better with the JIT.\nList comprehension has been improved.\nJitViewerThere will be a corresponding 1.9 release of JitViewer which is guaranteed to work\nwith PyPy 1.9. See the JitViewer docs for details.Cheers,\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/06/pypy-19-yard-wolf-7006180436602667005.html"
    },
    {
      "title": "Py3k status update #4",
      "text": "This is the fourth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.For various reasons, less work than usual has been done since the last status\nupdate. However, some interesting things happened anyway.As readers know, so far we spent most of the effort in fixing all PyPy's own\ntests which started to fail for various py2/py3 differences.  Most of them\nfailed for shallow reasons, e.g. syntactic changes or the int/long\nunifications. Others failed for subtle differences and needed a bit more care,\nfor example the fact that unbound methods are gone in Py3k.The good news is that finally we are seeing the light at the end of the\ntunnel. Most of them have been fixed. For sine other tests, we introduced the\nconcept of \"py3k-skipping\": some optimizations and modules are indeed failing,\nbut right now we are concentrating on completing the core language and so we\nare not interested in those.  When the core language will be done, we will be\nable to easily find and work on the py3k-skipped tests.  In particular, for\nnow we disabled the Int and String dict strategies, which are broken\nbecause of the usual int/long unification and str vs bytes.  As for modules,\nfor now _continuation (needed for stackless) and _multiprocessing do\nnot work yet.Another non-trivial feature we implemented is the proper cleaning of exception\nvariables when we exit except blocks.  This is a feature which touches\nlots of levels of PyPy, starting from astcompiler, down to the bytecode\ninterpreter. It tooks two days of headache, but at the end we made it :-).Additionally, Amaury did a lot of improvements to cpyext, which had been\nbroken since forever on this branch.As for the next plans, now that things are starting to work and PyPy's own\ntests mostly pass, we can finally start to run the compiled PyPy against\nCPython's test suite.  It is very likely that we will have tons of failures at\nthe beginning, but once we start to fix them one by one, a Py3k-compatible\nPyPy will be closer and closer.",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/06/py3k-status-update-4-4834053219477515637.html"
    },
    {
      "title": "STM update: back to threads?",
      "text": "Hi again,\n\nHere is another update on the status of Software Transactional Memory on PyPy.\n\nThose of you who have been closely following this blog since last year know that, from the very first post about STM, I explored various design ideas about the API that we should get when programming in Python.\n\nI went a full circle, and now I am back to where I started (with, important difference, a very roughly working implementation of pypy-stm).\n\nWhat I realized is that the \"thread\" module is not that bad after all --- I mean, yes, it is a horribly low-level interface, but it is general enough to build various interesting things on top of it.  What the \"stm-thread\" branch of PyPy contains is, basically, the regular \"thread\" module in which the GIL was replaced with STM.  It gives multicore capabilities to any program based on multiple threads.  (This is so far exactly the idea same than the one being investigated for Hardware Transactional Memory.  It is roughly also what you would get if you managed to convince GCC 4.7 to compile CPython using STM.)\n\nNow while this might already be quite interesting to some people, here is how it relates to all I said previously: namely, threads are bad, and some new \"transaction\" module would be a better idea.\n\nThere is one new core functionality in the \"stm-thread\" branch: it is \"thread.atomic\", a context manager that can be used in a \"with\" statement (exact name subject to change).  In terms of the GIL, it prevents the GIL from being released in the \"with\" block.  In terms of STM, it prevents a \"transaction break\", which means that the whole \"with\" statement runs in one single transaction.  (From the Python programmer's point of view, the net effect is the same.)\n\nSo far, no ground-breaking news.  But what I missed previously is that this is enough to give multicore capabilities even to a program that is not using threads so far.  It is possible to rewrite an equivalent of the old transaction module in a few pages of pure Python, using \"thread.atomic\".  Something along the following lines: start N threads that each reads from a Queue.Queue() the next job to do, and does it in a \"with thread.atomic\" block.  The STM version of PyPy is then able to run these atomic blocks concurrently.  The key point is that the slightly delicate handling of threads should be nicely hidden inside the new \"transaction\" module, and from outside the observed behavior would be exactly as if the transactions that we schedule are run serially.\n\nThe point I kept missing was that, yes, this sounds like nonsense, because it seems that we create N threads just to serialize their work again in \"thread.atomic\" sections.  In fact this would be nonsense in any model that would \"just\" remove the GIL to let multiple threads run concurrently without crashing.  Indeed, you have multiple threads, but their atomic blocks would be again a sort of GIL: only one of them would run at a time.  And this is indeed the simple model of execution that you get even with STM --- but not the model of performance.  The performance with STM scales with the number of cores, as long as there is enough non-conflicting work to do.\n\nSo in summary the complete circle back to the starting point is that threads might be a good low-level model.  It mends itself naturally to, say, a kind of program in which the main thread polls file descriptors using select() or the Linux epoll(), and the work received is split along N other threads --- which is the kind of program you would naturally write in other languages that don't have a GIL, say Java.  The other threads can then use \"thread.atomic\" blocks to protect sections of their work.  The traditional Transactional Memory point of view is that you use such blocks to guard the short sections of code that communicate with other threads or modify global state, but nothing prevents you from using much larger sections: you should be able to scale them up to the size of a native \"unit of work\", so that every unit is naturally atomic.  And then it's only a matter of design: you can tweak an existing module that does the thread pooling to add one \"with thread.atomic\"; or do it yourself from scratch; or (if the design is compatible enough) just plug in the proposed pure-Python \"transaction\" module.  Or if you feel like it you can even use threads directly (but keep in mind that using threads too explicitly is not a composable abstraction, whereas higher-level designs typically are).\n\nAt the end of the day, you can write or reuse programs whose global structure you are already familiar with, for example with a thread pool (that can be hidden in a library if you prefer), or any other structure with or without explicit threads.  But you can do so without all the mess that comes with threads like locks and deadlocks.  From that angle it is really similar to Garbage Collection: e.g. the Boehm GC (now used by GCC itself) lets you write C code like you are used to, but forgeting all you had to learn about careful explicit memory management.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/05/stm-update-back-to-threads-6622746581767639355.html"
    },
    {
      "title": "STM update (and thanks everybody)",
      "text": "A short update on the Software Transactional Memory (STM) side.  Let me remind you that the work is to add STM internally into PyPy, with the goal of letting the user's programs run on multiple cores after a minor adaptation.  (The goal is not to expose STM to the user's program.)  I will soon write some official documentation that explains in more details exactly what you get.  For now you can read the previous blog posts, and you can also find technical details in the call for donation itself; or directly look at how I adapted the examples linked to later in this post.I have now reached the point where the basics seem to work.  There is no integration with the JIT so far; moreover the integration with the Garbage Collection subsystem is not finished right now, but at least it is \"not crashing in my simple tests and not leaking memory too quickly\". (It means that it is never calling __del__ so far, although it releases memory; and when entering transactional mode or when going to the next transaction, all live objects become immortal.  This should still let most not-too-long-running programs work.)If you want to play with it, you can download this binary (you need to put it in a place with the paths lib-python and lib_pypy, for example inside the main directory from a regular nightly tarball or from a full checkout). This version was compiled for Linux x86 32-bit from the stm-gc branch on the 25th of April.  It runs e.g. the modified version of richards. This branch could also be translated for Linux x86-64, but not for other OSes nor other CPUs for now.The resulting pypy-stm exposes the same interface as the pure Python transaction module, which is an emulator (running on CPython or any version of PyPy) which can be used to play around and prepare your programs.  See the comments in there.  A difference is that the real pypy-stm doesn't support epoll right now, so it cannot be used yet to play with a branch of Twisted that was already adapted (thanks Jean-Paul Calderone); but that's coming soon.  For now you can use it to get multi-core usage on purely computational programs.I did for example adapt PyPy's own translate.py: see the tweak in rpython/rtyper.py.  Lines 273-281 are all that I needed to add, and they are mostly a \"simplification and parallelization\" of the lines above.  There are a few more places in the whole translate.py that could be similarly modified, but overall it is just that: a few places. I did not measure performance, but I checked that it is capable of using multiple cores in the RTyping step of translation, with --- as expected --- some still-reasonable number of conflicts, particularly at the beginning when shared data structures are still being built.On a few smaller, more regular examples like richards, I did measure the performance.  It is not great, even taking into account that it has no JIT so far.  Running pypy-stm with one thread is roughly 5 times slower than running a regular PyPy with no JIT (it used to be better in previous versions, but they didn't have any GC; nevertheless, I need to investigate).  However, it does seem to scale.  At least, it scales roughly as expected on my 2-real-cores, 4-hyperthreaded-cores laptop (i.e. for N between 1 and 4, the N-threaded pypy-stm performs similarly to N independent pypy-stm's running one thread each).And finally......a big thank you to everyone who contributed some money to support this!  As you see on the PyPy site, we got more than 6700$ so far in only 5 or 6 weeks.  Thanks to that, my contract started last Monday, and I am now paid a small salary via the Software Freedom Conservancy (thanks Bradley M. Kuhn for organizational support from the SFC). Again, thank you everybody!UPDATE: The performance regression was due to disabling an optimization, the method cache, which caused non-deterministic results --- the performance could vary from simple to double.  Today, as a workaround, I made the method cache transaction-local for now; it is only effective for transactions that run for long enough (maybe 0.1ms or 1ms), but at least it is there in this situation.  In the version of richards presented above, the transactions are too short to make a difference (around 0.015ms).",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/04/stm-update-and-thanks-everybody-6071745734932940294.html"
    },
    {
      "title": "NumPy on PyPy progress report",
      "text": "Hello.\nA lot of things happened in March, like pycon. I was also busy doing other\nthings (pictured), so apologies for the late numpy status update.\nHowever, a lot of things have happened and numpy continues to be one of the\nmain points of entry for hacking on PyPy. Apologies to all the people whose\npatches I don't review in timely manner, but seriously, you do a lot of\nwork.\nThis list of changes is definitely not exhaustive, and I might be forgetting\nimportant contributions. In a loose order:\n\nMatti Picus made out parameter work for a lot of (but not all)\nfunctions.\n\nWe merged record dtypes support. The only missing dtypes left are complex\n(important), datetime (less important) and object (which will probably\nnever be implemented because it makes very little sense and is a mess with moving GCs).\n\nTaavi Burns and others implemented lots of details, including lots of ufuncs.\nOn the completely unscientific measure of \"implemented functions\" on\nnumpypy status page, we're close to 50% of numpy working. In reality\nit might be more or less, but after complex dtypes we're getting very close\nto running real programs.\n\nBool indexing of arrays of the same size should work, leaving only\narrays-of-ints indexing as the last missing element of fancy indexing.\n\nI did some very early experiments on SSE. This work is seriously\npreliminary - in fact the only implemented operation is addition of\nfloat single-dimension numpy arrays. However, results are encouraging,\ngiven that our assembler generator is far from ideal:\n\n\n\n\n\n\n\n\n\n\n\u00a0\nNumpy\n\nPyPy SSE\n\nPyPy\n\nGCC non-looped\n\nGCC looped\n\n\na+b\n\n0.6s\n\n0.3s\n\n0.4s\n\n0.3s\n\n0.25s\n\n\na+b+c\n\n1.9s\n\n0.35s\n\n0.5s\n\n0.7s\n\n0.32s\n\n\na+b+c+d+e\n\n3.2s\n\n0.36s\n\n0.8s\n\n1.7s\n\n0.51s\n\n\n\n\nThe benchmark repo is available. GCC was run with -O3, no further\noptions specified. PyPy was run with default options, the SSE branch is under\nbackend-vector-ops, but it's not working completely yet.\nOne might argue that C and Python is not the same code - indeed it is not.\nIt just shows some possible approach to writing numeric code.\n\n\nNext step would be to just continue implementing missing features such as\n\nspecialised arrays i.e. masked arrays and matrixes\ncore modules such as fft, linalg, random.\nnumpy's testing framework\n\nThe future is hard to predict, but we're not far off!\nCheers,fijal\n\nUPDATE:Indeed, string and unicode dtypes are not supported yet. They're as important as complex dtype",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/04/numpy-on-pypy-progress-report-6048076549081013253.html"
    },
    {
      "title": "PyCon 2012 wrap up",
      "text": "So, PyCon happened. This was the biggest PyCon ever and probably the biggest\ngathering of Python hackers ever.\nFrom the PyPy perspective, a lot at PyCon was about PyPy. Listing things:\n\nDavid Beazley presented an excellent keynote describing his experience\ndiving head-first into PyPy and at least partly failing. He, however, did\nnot fail to explain bits and pieces about PyPy's architecture.\nVideo is available.\nWe gave tons of talks, including the tutorial, why pypy by example\nand pypy's JIT architecture\nWe had a giant influx of new commiters, easily doubling the amount of pull\nrequests ever created for PyPy. The main topics for newcomers were numpy and\npy3k, disproving what David said about PyPy being too hard to dive into ;)\nGuido argued in his keynote that Python is not too slow. In the meantime,\nwe're trying to prove him correct :-)\n\nWe would like to thank everyone who talked to us, shared ideas and especially\nthose who participated in sprints - we're always happy to welcome newcomers!\nI'm sure there are tons of things I forgot, but thank you all!\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/04/pycon-2012-wrap-up-559575896040055505.html"
    },
    {
      "title": "Py3k status update #3",
      "text": "This is the third status update about my work on the py3k branch, which I can work on thanks to all of the people who donated to the py3k proposal.\n\nA lot of work has been done during the last month: as usual, the list of changes is too big to be reported in a detalied way, so this is just a summary of what happened.\n\nOne of the most active areas was killing old and deprecated features. In particular, we killed support for the __cmp__ special method and its counsins, the cmp builtin function and keyword argument for list.sort() and sorted().  Killing is easy, but then you have to fix all the places which breaks because of this, including all the types which relied on __cmp__ to be comparable,, fixing all the tests which tried to order objects which are no longer ordeable now, or implementing new behavior like forbidding calling hash() on objects which implement __eq__ but not __hash__.\n\nAmong the other features, we killed lots of now-gone functions in the operator module, the builtins apply(), reduce() and buffer, and the os.* functions to deal with temporary files, which has been deprecated in favour of the new tempfile module.\n\nThe other topic which can't miss in a py3k status update is, as usual, string-vs-unicode. At this round, we fixed bugs in string formatting (in particular to teach format() to always use unicode strings) and various corner cases about when calling the (possibly overridden) __str__ method on subclasses of str. Believe me, you don't want to know the precise rules :-).\n\nOther features which we worked on and fixed tests include, but are not limited to, marshal, hashlib, zipimport, _socket and itertools, plus the habitual endless lists of tests which fail for shallow reasons such as the syntactic differences, int vs long, range() vs list(range()) etc. As a result, the number of failing tests dropped from 650 to 235: we are beginning to see the light at the end of the tunnel :-)\n\nBenjamin finished implementing Python 3 syntax. Most of it was small cleanups and tweaks to be compatible with CPython such as making True and False keywords and preventing . . . (note spaces between dots) from being parsed as Ellipsis. Larger syntax additions included keyword only arguments and function annotations.\n\nFinally, we did some RPython fixes, so that it is possible again to translate PyPy in the py3k branch. However, the resuling binary is a strange beast which mixes python 2 and python 3 semantics, so it is unusable for anything but showing friends how cool it is.\n\nI would like to underline that I was not alone in doing all this work. In particular, a lot of people joined the PyPy sprint at Pycon and worked on the branch, as you can clearly see in this activity graph. I would like to thank all who helped!\n\ncheers,\nAntonio and Benjamin",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/04/py3k-status-update-3-6975588144646689872.html"
    },
    {
      "title": "PyPy sprint in Leipzig, Germany (June 22-27)",
      "text": "The next PyPy sprint will be held --- for the first time in a while ---\nin a place where we haven't been so far: Leipzig, Germany, at the\nPython Academy's Teaching Center.  It will take place from the 22nd\nto the 27th of June 2012, before EuroPython.  Thanks to Mike M\u00fcller for\norganizing it!\nThis is a fully public sprint, everyone is welcome to join us.  All days are\nfull sprint days, so it is recommended to arrive the 21st and leave the 28th.\nTopics and goals\nOpen.  Here are some goals:\n\nnumpy: progress towards completing the numpypy module; try to\nuse it in real code\nstm: progress on Transactional Memory; try out the transaction module on real code.\njit optimizations: there are a number of optimizations we can still\ntry out or refactor.\nwork on various, more efficient data structures for Python language.\nA good example would be lazy string slicing/concatenation or more efficient\nobjects.\nany other PyPy-related topic is fine too.\n\nGrants\nFor students, we have the possibility to support some costs via PyPy\nfunds.  Additionally, we can support you applying for grants from the\nPSF and other sources.\nRegistration\nIf you'd like to come, please sign up either by announcing yourself on\npypy-dev, or by directly adding yourself to the list of people.\n(We need to have a head count for the organization.)  If you are new to\nthe project please drop a note about your interests and post any\nquestions.\nMore...\nFor more information, please see the sprint announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/04/pypy-sprint-in-leipzig-june-22-27-6450601012927549960.html"
    },
    {
      "title": "Call for donations for Software Transactional Memory",
      "text": "Hi all,\n\nThe Software Transactional Memory\ncall for donations is up.  From the proposal:\n\n\nPrevious attempts on Hardware Transactional Memory focused on parallelizing existing programs written using the thread or threading modules. However, as argued here, this may not be the most practical way to achieve real multithreading; it seems that better alternatives would offer good scalability too. Notably, Transactional Memory could benefit any event-based system that is written to dispatch events serially (Twisted-based, most GUI toolkit, Stackless, gevent, and so on). The events would internally be processed in parallel, while maintaining the illusion of serial execution, with all the corresponding benefits of safety. This should be possible with minimal changes to the event dispatchers. This approach has been described by the Automatic Mutual Exclusion work at Microsoft Research, but not been implemented anywhere (to the best of our knowledge).\n\nNote that, yes, this gives you both sides of the coin: you keep using your non-thread-based program (without worrying about locks and their drawbacks like deadlocks, races, and friends), and your programs benefit from all your cores.\n\nIn more details, a low-level built-in module will provide the basics to start transactions in parallel; but this module will be only used internally in a tweaked version of, say, a Twisted reactor. Using this reactor will be enough for your existing Twisted-based programs to actually run on multiple cores. You, as a developer of the Twisted-based program, have only to care about improving the parallelizability of your program (e.g. by splitting time-consuming transactions into several parts; the exact rules will be published in detail once they are known).\n\n\nThe point is that your program is always correct, and can be tweaked to improve performance.  This is the opposite from what explicit threads and locks give you, which is a performant program which you need to tweak to remove bugs.  Arguably, this approach is the reason for why you use Python in the first place :-)\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/03/call-for-donations-for-software-8853699867109654713.html"
    },
    {
      "title": "Py3k status update #2",
      "text": "This is the second status update about my work on the py3k branch, which I can work on thanks to all of the people who donated to the py3k proposal.Since my previous status update, things have improved a lot: first of all, I fixed the syntax of many more tests, which were failing on the branch because they used constructs which are no longer valid in Python 3, such as u'' strings, the print statement or the old except Exception, e syntax.  I have to say that this work is tedious and not very rewarding, but it has to be done anyway, so that the real failures can stand up.Then, I spent most of the rest of the time by killing features which are present in Python 2 and are gone in Python 3.Some of them were easy and mechnical: for example, I removed all the function attributes such as func_code and func_closure, which has been renamed to __code__ and __closure__, and then I had to find and fix all the places which still expected the old ones.Some were trickier: I removed support for the cmp function and the __cmp__ special method, but this also meant that I had to fix a few types which relied on it to be comparable (for example, did you know that the cells contained in __closure__ are comparable?). At the same time, I also removed the old behavior which in Python 2 allows us to compare arbitrary objects with <, > & co.: in Python 3 the only comparisons allowed between incompatible types are == and !=.Speaking of old special methods, __hex__ and __oct__ are gone as well (and I didn't even know about their existence before removing them :-))But the most important breakthrough was the removal of the _file module, containing the implementation of the file type in Python 2, which is now gone since in Python 3 files are handled by the _io module.  Killing the module was not straightforward, because some of the importing logic was tightly tied to the internal implementation of files, so it needed some refactoring. Finally, I had to fix the marshal module to correctly detect text files vs. byte files.Among these things, I fixed tons of smaller issues here and there. As a result, there are many fewer failing tests than a few weeks ago.  Obviously the number itself does not mean much, because sometimes fixing a single test takes hours, and some other times by changing one line one fixes tens of tests. But at the end, seeing it dropping from 999 to 650 always is nice and rewarding :-).The road for having a pypy3k is still long, but everything is going fine so far. Stay tuned for more updates!cheers, Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/03/py3k-status-update-2-4018939509128176130.html"
    },
    {
      "title": "Py3k status update",
      "text": "Thank to all the people who donated to the py3k proposal, we managed to collect enough money to start to work on the first step.  This is a quick summary of what I did since I began working on this.\nFirst of all, many thanks to Amaury Forgeot d'Arc, who started the py3k branch months ago, and already implemented lots of features including e.g. switching to \"unicode everywhere\" and the int/long unification, making my job considerably easier :-)\nI started to work on the branch at the last Leysin sprint together with Romain Guillebert, where we worked on various syntactical changes such as extended tuple unpacking and keyword-only arguments.  Working on such features is a good way to learn about a lot of the layers which the PyPy Python interpreter is composed of, because often you have to touch the tokenizer, the parser, the ast builder, the compiler and finally the interpreter.\nThen I worked on improving our test machinery in various way, e.g. by optimizing the initialization phase of the object space created by tests, which considerably speeds up small test runs, and adding the possibility to automatically run our tests against CPython 3, to ensure that what we are not trying to fix a test which is meant to fail :-). I also setup our buildbot to run the py3k tests nightly, so that we can have an up to date overview of what is left to do.\nFinally I started to look at all the tests in the interpreter/ directory, trying to unmangle the mess of failing tests. Lots of tests were failing because of simple syntax errors (e.g., by using the no longer valid except Exception, e syntax or the old print statement), others for slightly more complex reasons like unicode vs bytes or the now gone int/long distinction.  Others were failing simply because they relied on new features, such as the new lexical exception handlers.\nTo give some numbers, at some point in january we had 1621 failing tests in the branch, while today we are under 1000 (to be exact: 999, and this is why I've waited until today to post the status update :-)).\nBefore ending this blog post, I would like to thank once again all the people who donated to PyPy, who let me to do this wonderful job.  That's all for now, I'll post more updates soon.\ncheers, Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/02/py3k-status-update-8840622949715145821.html"
    },
    {
      "title": "A Larger Example for the Flow Graph Language",
      "text": "Part 4 of Comparing Partial Evaluation to Tracing\nThis is the fourth and final blog post in a series about comparing partial evaluation and\ntracing. We've come a long way: In the first post of the series I showed an interpreter for a small flow-graph\nlanguage together with a partial evaluator it. In the second post I showed how a tracer for\nthe same language works and how it relates to both execution and to partial\nevaluation. The third post described an optimizer for traces.\nIn this final post we can compare and contrast the two different approaches of\ntracing and partial evaluation by means of an example. The programs in the flow\nchart language seen so far have been rather small, so I want to give an example\nof a larger program: an interpreter for an extremely simple bytecode\ninstruction set. I will look at how the partial evaluator deals with that\ninterpreter, and\nwhat the tracer does with it. The code for\nthat, as well as all the code of the series can be found here: https://paste.pocoo.org/show/550282/ (some small\nadditions have been made, such as a nicer way to print traces).\nA Bytecode Interpreter\nWriting programs in the flow graph language is painful, but I still want to give\nan example that is a bit more interesting than the tiny ones that we've seen so\nfar. The example is an interpreter for the bytecode of a very trivial\nregister-based language. The language has four registers, one of which is an\naccumulator on which all the actual operations are performed.\nThe opcodes of the language are:\n\njump_if_a, jumps to a target address when the accumulator is non-zero\nmov_a_r0, mov_a_r1, mov_a_r2 move the value of the accumulator to\nthe respective register\nmov_r0_a, mov_r1_a, mov_r2_a move the value of a register to\nthe accumulator\nadd_r0_to_a, add_r1_to_a, add_r2_to_a add the value of the\nregister to the accumulator\ndecr_a decrement the accumulator\nreturn_a stop the program and print the accumulator\n\nThe interpreter has a main loop that reads the opcode at the current program\ncounter, does a (lengthy) dispatch to the right bytecode via a series of if\nstatements and then executes the right opcode. Afterwards the next opcode is\ntreated equivalently.\nHere is a part of the source code in the flow graph language. As pseudocode:\n\nbytecode_loop:\n    opcode = bytecode[pc]\n    pc = pc + 1\n    c = opcode == 'jump_if_a'\n    if c goto op_jump_if_a else goto not_jump_if_a\n\n# select the right bytecode via a long series of if statements\nnot_jump_if_a:\n    c = opcode == 'mov_a_r0'\n    if y goto op_mov_a_r0 else goto not_mov_a_r0\nnot_mov_a_r0:\n    c = opcode == 'mov_a_r0'\n    if y goto op_mov_a_r1 else goto not_mov_a_r1\n...\n\n# bytecode implementations\nop_mov_a_r0:\n    r0 = a\n    goto bytecode_loop\n\nop_jump_if_a:\n    c = a == 0\n    target = bytecode[pc]\n    pc += 1\n    if c goto bytecode_loop else goto op_jump_if_a_jump\n\nop_jump_if_a_jump:\n    pc = target\n    goto bytecode_loop\n...\n\nAnd actually working, as Prolog facts (the full implementation can be found at\nthe link above):\n% bytecode dispatch loop\nblock(bytecode_loop,\n      op2(opcode, readlist, var(bytecode), var(pc),\n      op2(pc, add, var(pc), const(1),\n      op2(c, eq, var(opcode), const(jump_if_a),\n      if(c, op_jump_if_a, not_jump_if_a))))).\n\n% select the right bytecode via a long series of if statements\nblock(not_jump_if_a,\n      op2(c, eq, var(opcode), const(mov_a_r0),\n      if(c, op_mov_a_r0, not_mov_a_r0))).\nblock(not_mov_a_r0,\n      op2(c, eq, var(opcode), const(mov_a_r1),\n      if(c, op_mov_a_r1, not_mov_a_r1))).\n...\n\n% bytecode implementations\nblock(op_jump_if_a,\n      op2(c, eq, var(a), const(0),\n      op2(target, readlist, var(bytecode), var(pc),\n      op2(pc, add, var(pc), const(1),\n      if(c, bytecode_loop, op_jump_if_a_jump))))).\nblock(op_jump_if_a_jump,\n      op1(pc, same, var(target),\n      promote(bytecode, bytecode_loop))).\nblock(op_mov_a_r0,\n      op1(r0, same, var(a), jump(bytecode_loop))).\n...\n\nThe bytecode_loop block is the main dispatch loop. It reads an opcode out of the\nbytecode list at the program counter position, then has a long series of if\nstatements that compares the current opcode to the various existing opcodes.\nThe full code of the interpreter can be found under the link above.\nThe bytecodes of the interpreter don't really permit hugely complex\nprograms, but it can be used to write a program that computes the square of a\nnumber with the following program:\n\nmov_a_r0     # r0 = a\nmov_a_r1     # r1 = a\n# 2:\nmov_r0_a     # r0--\ndecr_a\nmov_a_r0\nmov_r2_a     # r2 += a\nadd_r1_to_a\nmov_a_r2\nmov_r0_a     # if r0!=0: goto 2\njump_if_a 2\nmov_r2_a     # return r2\nreturn_a\n\nPartially Evaluating the Bytecode Interpreter\nThe partial evaluator from the first blog post can be easily used to partially\nevaluate the bytecode interpreter. The static input is the bytecode for\ncomputing the square and the initial program counter value, as given above. The\ndynamic input are the content of the accumulator (the number to be squared).\nThis can be done as follows:\n?- bytecode_square(B),\nEnv = [bytecode/B, pc/0],\ndo_pe(bytecode_loop, Env, Label),\nREnv = [a/16, r0/0, r1/0, r2/0],\ninterp(jump(Label), REnv), listing(block).\n256\n:- dynamic block/2.\n\n<lots of generated code>\n\nThe code that is generated by the partial evaluation process is somewhat hard to\nread. It contains a lot of passages like this:\n...\nblock(op_return_a1, print_and_stop(var(a))).\nblock(not_decr_a1, jump(op_return_a1)).\nblock(not_add_r2_to_a2, jump(not_decr_a1)).\nblock(not_add_r1_to_a2, jump(not_add_r2_to_a2)).\nblock(not_add_r0_to_a3, jump(not_add_r1_to_a2)).\nblock(not_mov_r2_a3, jump(not_add_r0_to_a3)).\nblock(not_mov_r1_a5, jump(not_mov_r2_a3)).\nblock(not_mov_r0_a5, jump(not_mov_r1_a5)).\nblock(not_mov_a_r27, jump(not_mov_r0_a5)).\nblock(not_mov_a_r18, jump(not_mov_a_r27)).\nblock(not_mov_a_r09, jump(not_mov_a_r18)).\nblock(not_jump_if_a11, jump(not_mov_a_r09)).\nblock(bytecode_loop12, jump(not_jump_if_a11)).\nblock(op_mov_r2_a2, op1(a, same, var(r2), jump(bytecode_loop12))).\n...\n\nI.e. lots of blocks that do nothing but jump to another block, interspersed with\nsome blocks that contain an actual operation. I cleaned the output up manually\nand got something like the following (this sort of cleanup is something a good\npartial evaluation system would do itself, after partial evaluation has\noccurred):\nblock(bytecode_loop1,\n    op1(r0, same, var(a),\n    op1(r1, same, var(a),\n    op1(a, same, var(r0),\n    op2(a, sub, var(a), const(1),\n    op1(r0, same, var(a),\n    op1(a, same, var(r2),\n    op2(a, add, var(a), var(r1),\n    op1(r2, same, var(a),\n    op1(a, same, var(r0),\n    op2(c, eq, var(a), const(0),\n    if(c, bytecode_loop11, op_jump_if_a_jump1)))))))))))).\n\nblock(bytecode_loop11,\n    op1(a, same, var(r2),\n    print_and_stop(var(a))).\n\nblock(op_jump_if_a_jump1,\n    op1(a, same, var(r0),\n    op2(a, sub, var(a), const(1),\n    op1(r0, same, var(a),\n    op1(a, same, var(r2),\n    op2(a, add, var(a), var(r1),\n    op1(r2, same, var(a),\n    op1(a, same, var(r0),\n    op2(c, eq, var(a), const(0),\n    if(c, bytecode_loop11, op_jump_if_a_jump1)))))))))).\n\nWhat do we see here? The partial evaluator has generated a block bytecode_loop1,\nwhich corresponds to the initialization opcodes mov_a_r0 and mov_a_r1 together\nwith one iteration of the loop. Then it either jumps to a copy of the main loop\n(label op_jump_if_a_jump1) or to block bytecode_loop11, which prints the result\nand then stops. The residual code does exactly what the bytecode did: It\nsquares the accumulator then prints that. All the uses of the bytecode and\npc variable are gone.\nWhy did the partial evaluator produce two copies of the main loop that\nlook the same? The reason for that is that in the second copy, the additional\nstatic information target = 2 is known, where target is a variable in\nthe interpreter source that stores the jump target, for very brief periods of\ntime. This additional static information does not have any effect on the\nresidual code, so the same code is uselessly generated twice. This is an\nexample of overspecialization.\nTracing the Interpreter\nIn this section we will look at what happens if we try to trace the interpreter.\nThe naive way of doing that yields traces that are not very useful, because they\nabort after one iteration. We will look at a way of avoiding this problem. The\nproblems described in this section are at the core of the paper Tracing the\nmeta-level: PyPy's tracing JIT compiler (that paper uses a slightly more\nadvanced version of the bytecode interpreter as an example).\nTo trace the interpreter, it is useful to change the bytecode_loop block from above\nto always promote the bytecode and the pc variables, because without\nknowing them the trace produced is not really interesting. This is similar to\nmaking these variables static in the partial evaluation example above:\nblock(bytecode_loop,\n      promote(bytecode, bytecode_loop_promote_bytecode)).\nblock(bytecode_loop_promote_bytecode,\n      promote(pc, bytecode_loop_promote_pc)).\nblock(bytecode_loop_promote_pc,\n      op2(opcode, readlist, var(bytecode), var(pc),\n      op2(pc, add, var(pc), const(1),\n      op2(c, eq, var(opcode), const(0),\n      if(c, op_jump_if_a, not_jump_if_a))))).\n...\n\nThe rest of the interpreter stays unchanged.\nTo trace the interpreter we would start naively at the bytecode_loop label, because\nthat's the label in the interpreter that is jumped to most often (which a\nprofiler could establish easily). The following command can be used for that\n(this output prints traces in a slightly more readable way than in previous blog\nposts):\n?- bytecode_square(B),\n        A = 16, Env = [bytecode/B, pc/2, a/A, r0/A, r1/A, r2/0],\n        do_trace(bytecode_loop, Env).\ntrace\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,2,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  op2(c,eq,var(opcode),const(jump_if_a))\n  guard_false(c,[],op_jump_if_a)\n  op2(c,eq,var(opcode),const(mov_a_r0))\n  guard_false(c,[],op_mov_a_r0)\n  op2(c,eq,var(opcode),const(mov_a_r1))\n  guard_false(c,[],op_mov_a_r1)\n  op2(c,eq,var(opcode),const(mov_a_r2))\n  guard_false(c,[],op_mov_a_r2)\n  op2(c,eq,var(opcode),const(mov_r0_a))\n  guard_true(c,[],not_mov_r0_a)\n  op1(a,same,var(r0))\n  loop\n\nopttrace\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,2,[bytecode/[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]],bytecode_loop_promote_pc)\n  op1(a,same,var(r0))\n  op1(bytecode,same,const([mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]))\n  op1(pc,same,const(3))\n  op1(opcode,same,const(mov_r0_a))\n  op1(c,same,const(1))\n  loop\n\n256\nB = [mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a, mov_a_r2, mov_r0_a|...],\nA = 16,\nEnv = [bytecode/[mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a|...], pc/2, a/16, r0/16, r1/16, r2/0]\n\nThese traces are very short. They start with promoting the bytecode and the\npc, followed by the execution of the opcode mov_r0_a, which is the\none at position 2 in the given bytecode. Then they increment the pc and\nloop back to the beginning. Looking at the optimized trace, it is clear that the\ntrace is essentially useless. It will run only for one iteration, because in the\nsecond iteration the pc is 3, thus the guard_value at the beginning\nwill fail.\nThis problem can be solved by tracing more than just one iteration of the\nbytecode dispatch loop, which is called meta-tracing. To get this behaviour, in\nthis simple example it is enough to start (and thus end) tracing at a different\nlabel, op_jump_if_a_jump. This label is hit when the interpreter executes a\njump_if_a bytecode and the jump is taken. In a loop on the level of the\nexecuted bytecode program there is one such jump. Thus tracing from this label,\na full loop in the bytecode program is traced, containing potentially many\niterations of the bytecode dispatch loop in the control flow graph language.\nDoing that yields the following:\n?- bytecode_square(B),\n        A = 16, Env = [bytecode/B, pc/11, a/A, r0/A, r1/A, r2/0, target/2],\n        do_trace(op_jump_if_a_jump, Env).\ntrace\n  op1(pc,same,var(target))\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop)\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,2,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  op2(c,eq,var(opcode),const(jump_if_a))\n  guard_false(c,[],op_jump_if_a)\n  op2(c,eq,var(opcode),const(mov_a_r0))\n  guard_false(c,[],op_mov_a_r0)\n  op2(c,eq,var(opcode),const(mov_a_r1))\n  guard_false(c,[],op_mov_a_r1)\n  op2(c,eq,var(opcode),const(mov_a_r2))\n  guard_false(c,[],op_mov_a_r2)\n  op2(c,eq,var(opcode),const(mov_r0_a))\n  guard_true(c,[],not_mov_r0_a)\n  op1(a,same,var(r0))\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,3,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  ...\n  lots of operations ommitted\n  ...\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,9,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  op2(c,eq,var(opcode),const(jump_if_a))\n  guard_true(c,[],not_jump_if_a)\n  op2(c,eq,var(a),const(0))\n  op2(target,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  guard_false(c,[],bytecode_loop)\n  loop\n\nopttrace\n  op1(pc,same,var(target))\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop)\n  guard_value(pc,2,[bytecode/[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]],bytecode_loop_promote_pc)\n  op1(a,same,var(r0))\n  op2(a,sub,var(a),const(1))\n  op1(r0,same,var(a))\n  op1(a,same,var(r2))\n  op2(a,add,var(a),var(r1))\n  op1(r2,same,var(a))\n  op1(a,same,var(r0))\n  op2(c,eq,var(a),const(0))\n  guard_false(c,[bytecode/[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],pc/11,opcode/jump_if_a,target/2],bytecode_loop)\n  op1(bytecode,same,const([mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]))\n  op1(pc,same,const(11))\n  op1(opcode,same,const(jump_if_a))\n  op1(target,same,const(2))\n  op1(c,same,const(0))\n  loop\n\n256\nB = [mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a, mov_a_r2, mov_r0_a|...],\nA = 16,\nEnv = [bytecode/[mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a|...], pc/11, a/16, r0/16, r1/16, r2/0, target/2] .\n\nThat looks better. The trace corresponds to the interpreter running all the\nbytecodes in the loop of the squaring function in the example bytecode above.\nThe optimized code starts with\ntwo guards (checking that the bytecode is still the one for the squaring\nfunction, checking that the pc is 2) and then only does the operations\nthat actually do the computation. No bytecode dispatching is performed, thus the\ninterpretation overhead is fully removed, apart from the two guard_value\noperations at the beginning.\nMany of the assignments in the trace are superfluous, e.g. all the copying back\nand forth between registers r1, r1, r2 and accumulator a. This\ncould be easily solved by an even more intelligent optimization utilizing SSA\nform.\nConclusion About the Interpreter\nBoth partial evaluation and meta-tracing can be used to transform the example\nbytecode computing a square into a form that shows the essential computation\nthat is going on, without the interpretation overhead. The naive partial evaluator\nproduces lots of extra blocks that just jump around, which could be solved with\na post-processing step. The tracer by itself produces uselessly short traces,\nbut with a simple trick of starting the trace at a different point the results\nbecome a lot better.\nIn a real meta-tracing system, the meta-tracer would need a way for the author\nof the interpreter\nto mark which bytecode corresponds to a backward jump. It would also need better\nintegration with the interpreter to start tracing automatically, as well as\ncache the traces. Additionally, it would have to deal better with guards that fail a\nlot, attaching new traces to the failing guards. However, all that is \"just\"\nengineering on top of the ideas presented in this series of blog posts.\nHigh-Level Conclusion\nSome concluding high-level thoughts about the similarities of tracing and\npartial evaluation: Tracing and partial evaluation try to tackle a similar\nproblem, that of automatically reducing the interpreter overhead, their\napproaches are slightly different though.\nTracing is very close to normal evaluation, only keeping some extra information\nin the process. But then, the optimizer that is used in a tracer\nis again very similar in structure to a partial evaluator. The task of the\noptimizer is much simpler though, because it does not need to deal with control\nflow at all, just a linear list of operations.\nSo in a sense tracing is taking those parts of partial evaluation that work (the\n\"just evaluate those things that you can, and leave the others\") and replacing\nthe parts that don't (controlling unfolding) by a much more pragmatic mechanism.\nThat mechanism observes actual execution runs of the program to choose control\nflow paths that are typical. At the same time, the tracer's focus is on loops,\nbecause they are where most programs spend significant amounts of time.\nAnother point of view of tracing is that it is a form of partial evaluation that\nreplaces the control components of a partial evaluator with an oracle (the\nactual execution runs) that provide the information which paths to look at.\nAlready in the quite trivial interpreter here the effects of this are visible.\nThe simple partial evaluator over-specializes the loop and produces two\nidentical versions of it, that aren't different. The tracer doesn't, and it\nalso generates only code for the loop itself, not for the initialization\nopcodes.\nThat's it for this series. To those that made it, thanks for following along.\nAlso thanks to Samuele and Sven, who consistently gave me good feedback on the\nposts before I put them here.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/larger-example-for-flow-graph-language-6139699450091061040.html"
    },
    {
      "title": "PyPy 1.8 - business as usual",
      "text": "We're pleased to announce the 1.8 release of PyPy. As habitual this\nrelease brings a lot of bugfixes, together with performance and memory\nimprovements over the 1.7 release. The main highlight of the release\nis the introduction of list strategies which makes homogenous lists\nmore efficient both in terms of performance and memory. This release\nalso upgrades us from Python 2.7.1 compatibility to 2.7.2. Otherwise\nit's \"business as usual\" in the sense that performance improved\nroughly 10% on average since the previous release.\nyou can download the PyPy 1.8 release here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 1.8 and cpython 2.7.1 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 32/64 or\nWindows 32. Windows 64 work has been stalled, we would welcome a volunteer\nto handle that.\n\n\nHighlights\n\nList strategies. Now lists that contain only ints or only floats should\nbe as efficient as storing them in a binary-packed array. It also improves\nthe JIT performance in places that use such lists. There are also special\nstrategies for unicode and string lists.\n\nAs usual, numerous performance improvements. There are many examples\nof python constructs that now should be faster; too many to list them.\n\nBugfixes and compatibility fixes with CPython.\n\nWindows fixes.\n\nNumPy effort progress; for the exact list of things that have been done,\nconsult the numpy status page. A tentative list of things that has\nbeen done:\n\nmulti dimensional arrays\nvarious sizes of dtypes\na lot of ufuncs\na lot of other minor changes\n\nRight now the numpy module is available under both numpy and numpypy\nnames. However, because it's incomplete, you have to import numpypy first\nbefore doing any imports from numpy.\n\nNew JIT hooks that allow you to hook into the JIT process from your python\nprogram. There is a brief overview of what they offer.\n\nStandard library upgrade from 2.7.1 to 2.7.2.\n\n\n\n\nOngoing work\nAs usual, there is quite a bit of ongoing work that either didn't make it to\nthe release or is not ready yet. Highlights include:\n\nNon-x86 backends for the JIT: ARMv7 (almost ready) and PPC64 (in progress)\nSpecialized type instances - allocate instances as efficient as C structs,\nincluding type specialization\nMore numpy work\nSince the last release there was a significant breakthrough in PyPy's\nfundraising. We now have enough funds to work on first stages of numpypy\nand py3k. We would like to thank again to everyone who donated.\nIt's also probably worth noting, we're considering donations for the\nSoftware Transactional Memory project. You can read more about our plans\n\nCheers,\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/pypy-18-business-as-usual-7266036404915945090.html"
    },
    {
      "title": "Introductory Article About RPython",
      "text": "Laurence Tratt from King's College London has written a long and detailed introduction to the goals and significance of RPython over on his blog. Laurie has been implementing his Converge Language in RPython in the last months. He is one of the first people external to the PyPy team who have pushed a sizeable RPython-based VM quite far, adding and tuning JIT hints. The post describes some of that work and his impressions of RPython and PyPy.\n\n\n\"RPython, to my mind, is an astonishing project. It has, almost single-handedly, opened up an entirely new approach to VM implementation. As my experience shows, creating a decent RPython VM is not a huge amount of work (despite some frustrations). In short: never again do new languages need come with unusably slow VMs. That the the PyPy / RPython team have shown that these ideas scale up to a fast implementation of a large, real-world language (Python) is another feather in their cap.\"",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/introductionary-article-about-rpython-5386281283454207551.html"
    },
    {
      "title": "Optimizing Traces of the Flow Graph Language",
      "text": "Part 3 of Comparing Partial Evaluation to Tracing\nThis is the third blog post in a series about comparing partial evaluation and\ntracing. In the first post of the series I introduced a small flow-graph\nlanguage together with an interpreter for it. Then I showed a partial evaluator\nfor the language. In the second post of the series I showed how a tracer for\nthe same language works and how it relates to both execution and to partial\nevaluation. Then I added support for promotion to that tracer.\nIn this post I will show how to optimize the traces that are produced by the\ntracer and compare the structure of the optimizer to that of partial\nevaluation.\nThe code from this post can be found here: https://paste.pocoo.org/show/547304/\nOptimizing Traces\nIn the last post we saw how to produce a linear trace with guards by\ninterpreting a control flow graph program in a special mode. A trace always end with\na loop statement, which jumps to the beginning. The tracer is just logging\nthe operations that are done while interpreting, so the trace can contain\nsuperfluous operations. On the other hand, the trace also contains some of the\nruntime values through promotions and some decisions made on them which can be\nexploited by optimization. An example for this is the trace produced by the\npromotion example from the last post:\nop2(c,ge,var(i),const(0),\nguard_true(c,[],l_done,\nguard_value(x,5,[],b2,\nop2(x2,mul,var(x),const(2),\nop2(x3,add,var(x2),const(1),\nop2(i,sub,var(i),var(x3),\nloop))))))\n\nAfter the guard_value(x, 5, ...) operation, x is know to be 5: If\nit isn't 5, execution falls back to the interpreter. Therefore, operations\non x after the guard can be constant-folded. To do that sort of\nconstant-folding,\nan extra optimization step is needed. That optimization step walks along the\ntrace, remembers which variables are constants and what their values are using a\npartial environment. The opimizer removes operations that have only constant\narguments and leaves the others in the trace. This process is actually\nremarkably similar to partial evaluation: Some variables are known to be\nconstants, operations on only constant arguments are optimized away, the rest\nremains.\nThe code for optimizing operations looks as follows:\noptimize(op1(ResultVar, Op, Arg, Rest), PEnv, NewOp) :-\n    presolve(Arg, PEnv, RArg),\n    (RArg = const(C) ->\n        do_op(Op, C, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        NewOp = RestResidual\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op1(ResultVar, Op, RArg, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\noptimize(op2(ResultVar, Op, Arg1, Arg2, Rest), PEnv, NewOp) :-\n    presolve(Arg1, PEnv, RArg1),\n    presolve(Arg2, PEnv, RArg2),\n    (RArg1 = const(C1), RArg2 = const(C2) ->\n        do_op(Op, C1, C2, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        NewOp = RestResidual\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op2(ResultVar, Op, RArg1, RArg2, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\nJust like partial evaluation! It even reuses the helper functions presolve\nfrom the partial evaluator and a partial environment PEnv. When the\narguments of the operation are known constants in the partial environment, the\noperation can be executed at optimization time and removed from the trace.\nOtherwise, the operation has to stay in the output trace. The result variable\n(as in the partial evaluator) needs to be removed from the partial environment,\nbecause it was just overwritten by an unknown result.\nNow we need to deal with guards in the trace.\noptimize(guard_true(V, [], L, Rest), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C) ->\n        NewOp = RestResidual\n    ;\n        NewOp = guard_true(V, PEnv, L, RestResidual)\n    ),\n    optimize(Rest, PEnv, RestResidual).\n\noptimize(guard_false(V, [], L, Rest), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C) ->\n        NewOp = RestResidual,\n        NEnv = PEnv\n    ;\n        write_env(PEnv, V, 0, NEnv),\n        NewOp = guard_false(V, PEnv, L, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\nWhen the variable that is being guarded is actually known to be a constant, we\ncan remove the guard. Note that it is not possible that the guard of that\nconstant fails: The tracer recorded the operation while running with real\nvalues, therefore the guards have to succeed for values the optimizer\ndiscovers to be constant.\nguard_false is slightly different from guard_true: after the former we\nknow that the argument is actually 0. After guard_true we only know that\nit is not equal to zero, but not which precise value it has.\nAnother point to note in the optimization of guards is that the second argument\nof the guard operation, which was so far always just an empty list, is now\nreplaced by the partial environment PEnv. I will discuss further down why\nthis is needed.\nOptimizing guard_value is very similar, except that it really gives precise\ninformation about the variable involved:\noptimize(guard_value(V, C, [], L, Rest), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C1) ->\n        NewOp = RestResidual,\n        NEnv = PEnv\n    ;\n        write_env(PEnv, V, C, NEnv),\n        NewOp = guard_value(V, C, PEnv, L, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\nThis operation is the main way how the optimizer gains constant variables that\nit then exploits to do constant-folding on later operations. This is a chief\ndifference from partial evaluation: There the optimizer knows the value of some\nvariables from the start. When optimizing traces, at the beginning the value of\nno variable is known. Knowledge about some variables is only later gained\nthrough guards.\nNow we are missing what happens with the loop statement. In principle, it is\nturned into a loop statement again. However, at the loop statement a few\nadditional operations need to be emitted. The reason is that we optimized away\noperations and thus assignments when the result value of the variable was a\nconstant. That means the involved variable still potentially has some older\nvalue. The next iteration of the loop would continue with this older value,\nwhich is obviously wrong. Therefore we need to emit some assignments before the\nloop statement, one per entry in the partial environment:\noptimize(loop, PEnv, T) :-\n    generate_assignments(PEnv, T).\n\ngenerate_assignments([], loop).\ngenerate_assignments([Var/Val | Tail], op1(Var, same, const(Val), T)) :-\n    generate_assignments(Tail, T).\n\nAs an example of how generate_assignments assignments works, let's look at\nthe following example. When the partial environment is, [x/5, y/10] the\nfollowing assignments are generated:\n?- generate_assignments([x/5, y/10], Out).\nOut = op1(x, same, const(5), op1(y, same, const(10), loop)).\n\nThat's all the code of the optimizer. While the basic structure is quite similar to partial evaluation,\nit's a lot less complex as well. What made the partial evaluator hard was that\nit needs to deal with control flow statements and with making sure that code is\nreused if the same block is partially evaluated with the same constants. Here,\nall these complexities go away. The tracer has already removed all control flow\nand replaced it with guards and one loop operation at the end. Thus, the\noptimizer can simply do one pass over the operations, removing some (with some\nextra care around the loop statement).\nWith this machinery in place, we can optimize the trace from the promotion\nexample of the last post:\n?- optimize(\n    guard_value(x,3,[],b2,\n    op2(x2,mul,var(x),const(2),\n    op2(x3,add,var(x2),const(1),\n    op2(i,sub,var(i),var(x3),\n    op2(c,ge,var(i),const(0),\n    guard_true(c,[],l_done, loop)))))),\n    [],\n    LoopOut).\nLoopOut = guard_value(x, 3, [], b2, op2(i, sub, var(i), const(7), op2(c, ge, var(i), const(0), guard_true(c, [x/3, x2/6, x3/7], l_done, op1(x, same, const(3), op1(x2, same, const(6), op1(x3, same, const(7), loop)))))))\n\nMore readably, the optimized version is:\nguard_value(x, 3, [], b2,\nop2(i, sub, var(i), const(7),\nop2(c, ge, var(i), const(0),\nguard_true(c, [x/3, x2/6, x3/7], l_done,\nop1(x, same, const(3),\nop1(x2, same, const(6),\nop1(x3, same, const(7),\nloop)))))))\n\nAs intended, the operations on x after the guard_value have all been\nremoved. However, some additional assignments (to x, x2, x3) at the end have been generated as\nwell. The assignments look superfluous, but the optimizer does not have\nenough information to easily recognize this. That can be fixed, but only at the\ncost of additional complexity. (A real system would transform the trace into\nstatic single assignment form to answer such questions.)\nResuming to the Interpreter\nWhy does the code above need to add the partial environment to\nthe guards that cannot be optimized away? The reason is related to why we needed\nto generate assignments before the loop statement. The problem is that the optimizer\nremoves assignments to variables when it knows the values of these variables.\nThat means that when switching back from running the optimized trace to the\ninterpreter, a number of variables are not updated in the environment, making\nthe execution in the interpreter incorrect.\nIn the example above, this applies to the variables x2 and x3. When the\nsecond guard fails, they have not been assigned in the optimized case.\nTherefore, the guard lists them and their (always constant) values.\nWhen switching back these assignments need to be made. Thus we need to adapt the\nresume_interp function from the last blog post as follows:\nwrite_resumevars([], Env, Env).\nwrite_resumevars([Key / Value | Rest], Env, NEnv) :-\n    write_env(Env, Key, Value, Env1),\n    write_resumevars(Rest, Env1, NEnv).\n\nresume_interp(Env, ResumeVars, L) :-\n    write_resumevars(ResumeVars, Env, NEnv),\n    block(L, Block),\n    interp(Block, NEnv).\n\nOn resuming, the ResumeVars (a former partial environment) are simply added\nback to the normal environment before going back to the interpreter.\nThe data attached to guards about what needs to be done to resume to the\ninterpreter when the guard fails is often a very complex part of a tracing\nsystem. The data can become big, yet most guards never fail. Therefore, most\nreal systems try hard to compress the attached data or try to share it between\nsubsequent guards.\nSummary\nIn this post we have shown how to optimize traces by applying a variant of the\npartial evaluation principle: Perform all the operations that have only constant\narguments, leave the others alone. However, optimizing traces is much simpler,\nbecause no control flow is involved. All the questions about control flow have\nalready been solved by the tracing component.\nIn the next and final post of the series I will show a larger example of how\ntracing and partial evaluation can be used to optimize a small bytecode\ninterpreter.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/optimizing-traces-of-flow-graph-4169388883059419385.html"
    },
    {
      "title": "Almost There - PyPy's ARM Backend",
      "text": "In this post I want to give an update on the status of the ARM backend for PyPy's JIT and describe some of the issues and details of the backend.\n\n\n\n\n\n\n\nCurrent Status\nIt has been a more than a year that I have been working on the ARM backend. Now it is in a shape, that we can measure meaningful numbers and also ask for some feedback.\u00a0Since the last post about the backend we have added support floating point operations as well as for PyPy's framework GC's. Another area of work was to keep up with the constant improvements done in the main development branch, such as out-of-line guards, labels, etc.\u00a0It has been possible for about a year to cross-translate the PyPy Python interpreter and other interpreters such as Pyrolog, with a JIT, to run benchmarks on ARM. Up until now there remained some hard to track bugs that would cause the interpreter to crash with a segmentation fault in certain cases when running with the JIT on ARM. Lately it was possible to run all benchmarks without problems, but when running the translation toolchain itself it would crash.\u00a0During the last PyPy sprint in Leysin Armin and I managed to fix several of these hard to track bugs in the ARM backend with the result that, it is now possible to run the PyPy translator on ARM itself (at least unless until it runs out of memory), which is a kind of litmus test for the backend itself and used to crash before. Just to point it out, we are not able to complete a PyPy translation on ARM, because on the hardware we have currently available there is not enough memory. But up to the point we run out of memory the JIT does not hit any issues.\n\n\n\n\n\n\n\nImplementation Details\nThe hardware requirements to run the JIT on ARM follow those for Ubuntu on ARM which targets ARMv7 with a VFP unit running in little endian mode. The JIT can be translated without floating point support, but there might be a few places that need to be fixed to fully work in this setting.\u00a0We are targeting the ARM instruction set, because at least at the time we decided to use it seemed to be the best choice in terms of speed while having some size overhead compared to the Thumb2 instruction set. It appears that the Thumb2 instruction set should give comparable speed with better code density but has a few restriction on the number of registers available and the use of conditional execution. Also the implementation is a bit easier using a fixed width instruction set and we can use the full set of registers in the generated code when using the ARM instruction set.\n\n\n\n\n\n\n\nThe calling convention on ARM\nThe calling convention on ARM uses 4 of the general purpose registers to pass arguments to functions, further arguments are passed on the stack. The presence of a floating point unit is not required for ARM cores, for this reason there are different ways of handling floats with relation to the calling convention. There is a so called soft-float calling convention that is independent of the presence of a floating point unit. For this calling convention floating point arguments to functions are stored in the general purpose registers and on the stack. Passing floats around this way works with software and hardware floating point implementations. But in presence of a floating point unit it produces some overhead, because floating point numbers need to be moved from the floating point unit to the core registers to do a call and moved back to the floating point registers by the callee. The alternative calling convention is the so-called hard-float calling convention which requires the presence of a floating point unit but has the advantage of getting rid of the overhead of moving floating point values around when performing a call. Although it would be better in the long term to support the hard-float calling convention, we need to be able to interoperate with external code compiled for the operating system we are running on. For this reason at the moment we only support the soft-float to interoperate with external code.\u00a0We implemented and tested the backend on a BeagleBoard-xM with a Cortex-A8 processor running Ubuntu 11.04 for ARM.\n\n\n\n\n\n\n\nTranslating for ARM\nThe toolchain used to translate PyPy currently is based on a Scratchbox2. Scratchbox2 is a cross-compiling environment. Development had stopped for a while, but it seems to have revived again. We run a 32-bit Python interpreter on the host system and perform all calls to the compiler using a Scratchbox2 based environment. A description on how to setup the cross translation toolchain can be found here.\n\n\n\n\n\n\n\nResults\nThe current results on ARM, as shown in the graph below, show that the JIT currently gives a speedup of about 3.5 times compared to CPython on ARM. The benchmarks were run on the before mentioned BeagleBoard-xM with a 1GHz ARM Cortex-A8 processor and 512MB of memory. The operating system on the board is Ubuntu 11.04 for ARM. We measured the PyPy interpreter with the JIT enabled and disabled comparing each to CPython Python 2.7.1+ (r271:86832) for ARM. The graph shows the speedup or slowdown of both PyPy versions for the different benchmarks from our benchmark suite normalized to the runtime of CPython. The data used for the graph can be seen below.\n\n\n\nThe speedup is less than the speedup of 5.2 times we currently  get on x86 on our own benchmark suite (see https://speed.pypy.org for details). There are several possible reasons for this. Comparing the results for the interpreter without the JIT on ARM and x86 suggests that the interpreter generated by PyPy, without the JIT, has a worse performance when compared to CPython that it does on x86. Also it is quite possible that the code we are generating with the JIT is not yet optimal. Also there are some architectural constraints produce some overhead. One of these differences is the handling of constants, most ARM instructions only support 8 bit (that can be shifted) immediate values, larger constants need to be loaded into a register, something that is not necessary on x86.\n\n\nBenchmarkPyPy JITPyPy no JIT\nai0.4844397800473.72756749625\nchaos0.08072916919342.2908692212\ncrypto_pyaes0.07111148322453.30112318509\ndjango0.09777432455192.56779947601\nfannkuch0.2104237356982.49163632938\nfloat0.1542753346752.12053281495\ngo0.3304830342025.84628320479\nhtml5lib0.6292643898623.60333138526\nmeteor-contest0.9847474269122.93838610037\nnbody_modified0.2369695930821.40027234936\npyflate-fast0.3674471918072.72472422146\nraytrace-simple0.02905274614371.97270054339\nrichards0.0345755735533.29767342015\nslowspitfire0.7866425519083.7397367403\nspambayes0.6603243794563.29059863111\nspectral-norm0.0636107837314.01788986233\nspitfire0.436171311652.72050579076\nspitfire_cstringio0.2555387021341.7418593111\ntelco0.1029189304133.86388866047\ntwisted_iteration0.1227239868054.33632475491\ntwisted_names2.423677971352.99878698076\ntwisted_pb1.309918374314.48877805486\ntwisted_tcp0.9270333540552.8161624665\nwaf1.020598119321.03793427321\n\n\n\n\n\n\n\n\n\n\nThe next steps and call for help\nAlthough there probably still are some remaining issues which have not surfaced yet, the JIT backend for ARM is working. Before we can merge the backend into the main development line there are some things that we would like to do first, in particular it we are looking for a way to run the all PyPy tests to verify that things work on ARM before we can merge.\u00a0Additionally\u00a0there are some other longterm ideas. To do this we are looking for people willing to help, either by contributing to implement the open features or that can help us with hardware to test.\n\nThe incomplete list of open topics:\n\nWe are looking for a better way to translate PyPy for ARM, than the one describe above. I am not sure if there currently is hardware with enough memory to directly translate PyPy on an ARM based system, this would require between 1.5 or 2 Gig of memory. A fully QEMU based approach could also work, instead of Scratchbox2 that uses QEMU under the hood.\nTest the JIT on different hardware.\nExperiment with the JIT settings to find the optimal thresholds for ARM.\nContinuous integration: We are looking for a way to run the PyPy test suite to make sure everything works as expected on ARM, here QEMU also might provide an alternative.\nA long term plan would be to port the backend to ARMv5 ISA and improve the support for systems without a floating point unit. This would require to implement the ISA and create different code paths and improve the instruction selection depending on the target architecture.\nReview of the generated machine code the JIT generates on ARM to see if the instruction selection makes sense for ARM.\nBuild a version that runs on Android.\nImprove the tools, i.e. integrate with jitviewer.\n\nSo if you are interested or willing to help in any way contact us.",
      "tags": "arm,jit,pypy",
      "url": "https://www.pypy.org/posts/2012/02/almost-there-pypys-arm-backend_01-3216759488618774525.html"
    },
    {
      "title": "A Simple Tracer for the Flow Graph Language",
      "text": "Part 2 of Comparing Partial Evaluation to Tracing\nThis is the second blog post in a series about comparing partial evaluation and\ntracing. In the first post of the series I introduced a small flow-graph\nlanguage together with an interpreter for it. Then I showed a partial evaluator\nfor the language. In this post I will show how a tracer for the same language\nworks and how it relates to both execution and to partial evaluation.\nThe code from this post can be found here: https://paste.pocoo.org/show/543542/\nTracing Execution\nThe idea of a tracer (for the described language and also in general) is to do completely normal\ninterpretation but at the same time keep a log of all the normal operations\n(i.e. non-control-flow operations) that were performed. This continues until the\ntracer executes the code block where it started at, in which case the trace\ncorresponds to a closed loop. Then tracing stops and the last operation is\nreplaced by a jump to the start. After tracing has ended, the trace can be\nexecuted, optionally optimizing it before that.\nTo write a tracer, we start from the rules of the interpreter, rename the\npredicate to trace and add some extra arguments. Thus, the following rules\nin the interpreter:\ninterp(op1(ResultVar, Op, Arg, Rest), Env) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\ninterp(op2(ResultVar, Op, Arg1, Arg2, Rest), Env) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\nbecome the following rules in the tracer:\ntrace(op1(ResultVar, Op, Arg, Rest), Env, op1(ResultVar, Op, Arg, T), TraceAnchor) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    trace(Rest, NEnv, T, TraceAnchor).\n\ntrace(op2(ResultVar, Op, Arg1, Arg2, Rest), Env, op2(ResultVar, Op, Arg1, Arg2, T), TraceAnchor) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    trace(Rest, NEnv, T, TraceAnchor).\n\nNote how the bodies of the trace rules correspond exactly to the bodies of\nthe interp rules, the only difference is the recursive call to trace.\nThe meaning of the arguments of trace is as follows: The first and second argument are\nthe operation currently executed and the environment,\nlike in the interpreter. The argument\nafter that is an output argument that collects the currently traced operation,\nin the example above it is exactly like the operation that was executed.\nTraceAnchor is additional information about the trace that is being built\nright now, most of the time it is just handed on to the recursive call of\ntrace. We will see later what it contains.\nThe rule for print_and_stop is very simple, as execution (and therefore also\ntracing) simply stops there:\ntrace(print_and_stop(V), Env, print_and_stop(V), _) :-\n    resolve(V, Env, Val),\n    print(Val), nl.\n\nLeft are the rules for the control operations jump and if. A trace\nlinearizes one execution path, it contains no jumps. However, when a jump to the\nstarting label is reached, tracing should stop. Therefore, the implementation of\njump contains two cases:\ntrace(jump(L), Env, T, TraceAnchor) :-\n    (TraceAnchor = traceanchor(L, FullTrace) ->\n        T = loop,\n        write(trace), nl, write(FullTrace), nl,\n        do_optimize(FullTrace, OptTrace),\n        write(opttrace), nl, write(OptTrace), nl,\n        runtrace(OptTrace, Env, OptTrace)\n    ;\n        block(L, Block),\n        trace(Block, Env, T, TraceAnchor)\n    ).\n\nLet's disect this code in small increments. First, we see what TraceAnchor\nis. It is a term of the form\ntraceanchor(StartLabel, FullTrace). StartLabel is a label in the program\nwhere tracing started (and where it should end as well, when the loop is\nclosed). The argument FullTrace is an accumulator which contains the full\ntrace that is being built right now.\nThe condition at the start of the rule checks whether the taget-label L is\nthe same as the one stored in the trace anchor. If that is the case, we can stop\ntracing. The rest of the trace T is assigned the operation loop, which\njumps back to the beginning of the trace. Afterwards we print and optimize the\ntrace, then run it, using the FullTrace part of the traceanchor.\nIf the label we jump to is not the StartLabel we simply continue tracing\nwithout recording any operation. This part of the rule is again extremely\nsimilar to the interpretation of jump.\nFor now, we will not use any interesting optimizations, just return the\nunoptimized trace unchanged:\ndo_optimize(FullTrace, FullTrace).\n\nThe missing operation now is if. An if statement needs special treatment,\nbecause it is a way where control flow can diverge from the trace. The trace is\nlinear, therefore it can only record one of the two possible paths. When\nexecuting the trace it is possible for the other path to be taken. Therefore\nwe need to make sure that the same conditions that were true or false during\ntracing are still true or false during the execution of the trace. This is done\nwith a guard operation, which checks for this condition. The following rule\nimplements it:\ntrace(if(V, L1, L2), Env, T, TraceAnchor) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        L = L2, T = guard_false(V, [], L1, NT)\n    ;\n        L = L1, T = guard_true(V, [], L2, NT)\n    ),\n    trace(jump(L), Env, NT, TraceAnchor).\n\nIt is very similar to the interp rule of if. The rule inserts a\nguard_true into the case, if the condition is true, and a guard_false if\nthe condition is false. The arguments of the guard are: The variable that is\nbeing guarded, an empty list (the reason for that will be explained in a later\npost), the label where execution needs to continue when the guard fails and the\nrest of the trace.\nLet's also add a small helper predicate that can be used to conveniently start\ntracing:\ndo_trace(L, Env) :-\n    block(L, StartBlock),\n    trace(StartBlock, Env, ProducedTrace, traceanchor(L, ProducedTrace)).\n\nThe predicate takes a label and an environment and executes the label with the\ngiven environment by first producing a trace, then executing the trace and\neventually jumping back to interpretation, if a guard fails. It does this by\nreading the code at label L with the block statement, and then calling\ntrace with an unbound variable ProducedTrace to hold the trace and a trace\nanchor that contains the label where tracing started and the produced trace\nvariable.\nWith that predicate and the trace so far we can already trace the power\nimplementation from the last blog post, just not execute the trace (which we\nwill do in the next section):\n?- do_trace(power_rec, [res/1, x/10, y/20]).\ntrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\nopttrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\n...\n\nThe computed trace is:\n\nop2(res,mul,var(res),var(x),\nop2(y,sub,var(y),const(1),\nguard_true(y,[],power_done,\nloop)))\n\nwhich is exactly the content of the loop from power_rec. Note how the if\nis turned into a guard_true which jumps to power_done if the guard\nfails.\nA real tracing system would need a way for the tracer to get started, e.g. by\ndoing profiling in an interpreter and starting the tracer for labels that are\njumped to often. Also, traces for the same label are usually cached in some way.\nThese details are left out in this simple model.\nExecuting Traces\nIn a real tracing system, the traces would be turned into machine code and\nexecuted by the CPU. In our small model, we will simply write another\ninterpreter for them. This interpreter is very simple and looks again very\nsimilar to interp.\nruntrace(op1(ResultVar, Op, Arg, Rest), Env, TraceFromStart) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    runtrace(Rest, NEnv, TraceFromStart).\n\nruntrace(op2(ResultVar, Op, Arg1, Arg2, Rest), Env, TraceFromStart) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    runtrace(Rest, NEnv, TraceFromStart).\n\nThese rules are completely equivalent to the interp rules for op1 and\nop2. runtrace needs an extra argument, TraceFromStart, which is\nalways just handed over to the recursive call of runtrace.\nWhen the end of the trace is reached and the loop statement is encountered,\nwe simply start from the beginning:\nruntrace(loop, Env, TraceFromStart) :-\n    runtrace(TraceFromStart, Env, TraceFromStart).\n\nThe remaining question is what to do when encountering guards. In that case the\nguard condition needs to be checked. If the guard succeeds, executing the trace can\ncontinue. Otherwise the trace is aborted and the interpreter resumes execution:\nruntrace(guard_true(V, ResumeVars, L, Rest), Env, TraceFromStart) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        resume_interp(Env, ResumeVars, L)\n    ;\n        runtrace(Rest, Env, TraceFromStart)\n    ).\n\nruntrace(guard_false(V, ResumeVars, L, Rest), Env, TraceFromStart) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        runtrace(Rest, Env, TraceFromStart)\n    ;\n        resume_interp(Env, ResumeVars, L)\n    ).\n\n\nresume_interp(Env, [], L) :-\n    block(L, Block),\n    interp(Block, Env).\n\nNote how the execution is handed over to the interpreter at the label that was\nencoded as the third argument in the guard operation.\nWhat the ResumeVars are for we will see in a later post. For now we assume\nthat it is always an empty list.\nWith this interpreter for traces we can now trace and then execute the example:\n:- do_trace(power_rec, [res/1, x/10, y/20]).\ntrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\nopttrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\n100000000000000000000\n\nOf course this is example is not very exciting, because the trace looks more or less exactly\nlike the original code as well. There will be more exciting examples in a later\npost.\nExtension: Promotion\nAs it is, the tracer does not actually add much to the interpreter. It\nlinearizes control flow, but nothing deeply advanced happens. In this section I\nwill add a crucial but simple to implement extension to the control flow language that allows the tracer\nto do more interesting things. This extension is called promotion.\nPromotion is basically a hint that the programmer can add to her control flow\ngraph program. A promotion is an operation promote(V, L) that takes a\nvariable V and a label L. When the interpreter runs this statement, it\nsimply jumps to the label L and ignores the variable:\ninterp(promote(_, L), Env) :-\n    interp(jump(L), Env).\n\nHowever, the tracer does something much more interesting. For the tracer, the\npromote statement is a hint that it would be very useful to know the value\nof V and that the rest of the trace should keep that value as a constant.\nTherefore, when the tracer encounters a promotion, it inserts a special kind of\nguard called guard_value\ntrace(promote(V, L), Env, guard_value(V, Val, [], L, T), TraceAnchor) :-\n    lookup(V, Env, Val),\n    trace(jump(L), Env, T, TraceAnchor).\n\nThe guard_value is an interesting operation, because it freezes the current\nvalue FVal of variable V into the trace. When the trace is executed, the\nguard checks that the current value of the variable and the frozen value are the\nsame. If yes, execution continues, if not, the trace is aborted:\nruntrace(guard_value(V, FVal, ResumeVars, L, Rest), Env, TraceFromStart) :-\n    lookup(V, Env, Val),\n    (Val == FVal ->\n        runtrace(Rest, Env, TraceFromStart)\n    ;\n        resume_interp(Env, ResumeVars, L)\n    ).\n\nWhat can this operation be used for? It's a way to communicate to the tracer\nthat variable V is not changing very often and that it is therefore useful\nto freeze the current value into the trace. This can be done even without\nknowing the value of V in advance.\nLet's look at a (slightly contrived) example:\n\nl:\n    c = i >= 0\n    if c goto b else goto l_done\n\nl_done:\n    print_and_stop(var(i))\n\nb:\n    promote(x, b2)\n\nb2:\n    x2 = x * 2\n    x3 = x2 + 1\n    i = i - x3\n    goto l\n\nEncoded in Prolog syntax:\nblock(l, op2(c, ge, var(i), const(0),\n         if(c, b, l_done))).\nblock(l_done, print_and_stop(var(i))).\n\nblock(b, promote(x, b2)).\nblock(b2, op2(x2, mul, var(x), const(2),\n          op2(x3, add, var(x2), const(1),\n          op2(i, sub, var(i), var(x3),\n          jump(l))))).\n\nThis is a simple loop that counts down in steps of x * 2 + 1, whatever x\nmight be, until i >= 0 is no longer true. Assuming that x doesn't change\noften, it is worth to promote it to be able to constant-fold x * 2 + 1 to\nnot have to redo it every iteration. This is done with the promotion of x\n(of course optimizing this loop with loop invariant code motion would work as\nwell, because x doesn't actually change during the loop).\nTo trace this, we can run the following query:\n?- do_trace(b, [i/100, x/5]).\ntrace\nguard_value(x,5,[],b2,op2(x2,mul,var(x),const(2),op2(x3,add,var(x2),const(1),op2(i,sub,var(i),var(x3),op2(c,ge,var(i),const(0),guard_true(c,[],l_done,loop))))))\nopttrace\nguard_value(x,5,[],b2,op2(x2,mul,var(x),const(2),op2(x3,add,var(x2),const(1),op2(i,sub,var(i),var(x3),op2(c,ge,var(i),const(0),guard_true(c,[],l_done,loop))))))\n-10\n\nWriting the trace in a more readable way:\nguard_value(x,3,[],b2,\nop2(x2,mul,var(x),const(2),\nop2(x3,add,var(x2),const(1),\nop2(i,sub,var(i),var(x3),\nop2(c,ge,var(i),const(0),\nguard_true(c,[],l_done,\nloop))))))\n\nAfter the guard_value the operations performed on x could be\nconstant-folded away, because the guard ensures that x is 5 before\nexecution continues. To actually do the constant-folding we would need some\noptimization component that optimizes traces. This will be done in the next blog\npost.\nIn this section I mostly talked about how promotion is realized in the tracer,\nnot what and how to use to use it for. Promotion is one of the most important\ningredients that's responsible for the success of PyPy's tracing approach. How\nthis works is discussed in detail in the paper \"Runtime feedback in a\nmeta-tracing JIT for efficient dynamic languages\".\nConclusion\nIn this blog post we have seen a very minimalistic tracer and an interpreter for\nthe produced traces. The tracer is very much like the original interpreter, it\njust also keeps track of which operations were executed, in addition to\nexecuting the program. Tracing stops when a loop is closed, then the trace can\nbe optimized and run. Running a trace continues until a failing guard is hit. At\nthat point, execution goes back to the normal interpreter (and stays there, in\nthis very simple implementation).\nI also presented an extension of tracing that makes it possible to add a hint\ncalled promote to the original program that tells the tracer to feed back a\nruntime value into the trace and freeze it there. This extension would be\nimpossible to do in the partial evaluator from the last post, because partial\nevaluation is done strictly before run time, so if a variable isn't already\nknown, its likely runtime value cannot be found out.\nIn the next post I will show how to optimize traces before executing them and\nhow the optimizer for traces is related to partial evaluation.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/01/simple-tracer-for-flow-graph-language-6930951890987229484.html"
    },
    {
      "title": "NumPyPy status update",
      "text": "Hello.\nThis is just a quick status update on the NumPy in PyPy project that very\nrecently became my day job. I should give my thanks once again to Getco,\nNate Lawson and other contributors who donated above $40000 towards the goal.\nRecently we (Alex Gaynor, Matti Picus and me) implemented a few interesting things\nthat a lot of people use:\n\nmore ufuncs\nmost ufuncs now accept the axis parameter (except all and any)\nfixed string representation of arrays, now it's identical to numpy (uses\npretty much the same code)\nndarray.flat should be working correctly\nndarray.flatten, ndarray.ravel, ndarray.take\nindexing arrays by boolean arrays of the same size\nand various bugfixes.\n\nWe would also like to introduce the nightly report of numpy status. This\nis an automated tool that does package introspection. While it gives some\nsort of idea how much of numpy is implemented, it's not by far the authority.\nYour tests should be the authority. It won't report whether functions\nsupport all kinds of parameters (for example masked arrays and out parameter\nare completely unsupported) or that functions work at all. We also\nreserve the right to incorporate jokes in that website, so don't treat it\nthat seriously overall :-)\nThanks, and stay tuned.  We hope to post here regular updates on the\nprogress.\nCheers,\nfijal & the PyPy team",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/01/numpypy-status-update-6434340612277938795.html"
    },
    {
      "title": "Py3k and Numpy First Stage: Thanks to all who Gave",
      "text": "Last year was quite successful for PyPy fundraising through the Software Freedom Conservancy, and Conservancy and PyPy are very excited to announce that enough was raised to begin the first stages on the Py3k and Numpy grant proposals.As of the end of 2011, 135 different individuals gave to the Py3k campaign, and 114 to the Numpy campaign.  We thank each of you who donated to help make this work possible.  Meanwhile, if you haven't given to support these projects, we do hope you'll give generously now to help fund their second stages later this year!We're also particularly excited that a few donors gave particularly large donations to support this work; those big donations really filled in the gap to help us get started!Specifically, we're pleased to announce that Google  donated $35000 towards implementing Python 3 in PyPy.   Google's general support of the Python community is well known, and their specific support of our grant proposal is much appreciated.Meanwhile, Numpy was supported in part by contributions from Nate Lawson, Cantab Capital Partners, and Getco, as well as more than a hundred other contributors.With these donations combined with many others, we're now starting work on both projects.  This week, the Conservancy signed contracts with Antonio Cuni and Benjamin Peterson to work towards the Stage 1.1 goals in Py3k proposal (and is negotiating for another contractor as well), and with Maciej Fija\u0142kowski to work towards the Stage 1 goals in the Numpy proposal.In 2012, PyPy will continue regular sprint meetings, at which Py3K and Numpy efforts will certainly have a place.  We have some limited funds to fund travels of contributors to those meetings.We're very thankful for all who donated so far to support these efforts, and we hope that now that work has begun, even more donors will come forward to help us finish the job.  In the meantime, watch for the commits showing up from these developers and other contributors in the PyPy repositories!Cheers, The PyPy Team",
      "tags": "numpy,pypy3",
      "url": "https://www.pypy.org/posts/2012/01/py3k-and-numpy-first-stage-thanks-to-3008917396290059758.html"
    },
    {
      "title": "Comparing Partial Evaluation and Tracing, Part 1",
      "text": "As part of writing my PhD I am currently thinking about the relationship\nbetween PyPy's meta-tracing approach with various previous ideas to\nautomatically get a (JIT-)compiler from only an interpreter of a language. One\nof the most-researched ideas along these lines is that of partial evaluation.\nPartial evaluation has basically the same goals as PyPy when it comes to\ncompilers: Write an interpreter, and get a compiler for free. The methods for\nreaching that goal are a bit different. In this series of blog posts, I am\ntrying to explore the similarities and differences of partial evaluation and\nPyPy's meta-tracing.\nA Flowgraph Language\nTo be able to clearly understand what \"partial evaluation\" is and what\n\"meta-tracing\" is I will show an \"executable model\" of both. To that end, I am\ndefining a small imperative language and will then show what a partial evaluator\nand a tracer for that language look like. All this code will be\nimplemented in Prolog. (Any pattern-matching functional language would do, but I\nhappen to know Prolog best. Backtracking is not used, so you can read things\nsimply as functional programs.) In this post I will start with\nthe definition of the language, and a partial evaluator for it. The code\nwritten in this blog post can be found fully here: https://paste.pocoo.org/show/541004/\nThe language is conceptionally similar to PyPy's flow graphs, but a bit more\nrestricted. It does not have function calls, only labelled basic blocks\nthat consist of a series of linearly executed operations, followed by a\nconditional or an unconditional jump. Every operation is assigning a value to a\nvariable, which is computed by applying some operation to some arguments.\nA simple program to raise x to the yth power in that language looks like\nthis:\n\npower:\n    res = 1\n    if y goto power_rec else goto power_done\n\npower_rec:\n    res = res * x\n    y = y - 1\n    if y goto power_rec else goto power_done\n\npower_done:\n    print_and_stop(res)\n\nTo represent the same program as Prolog data structures, we use the\nfollowing Prolog code:\nblock(power, op1(res, same, const(1),\n             if(y, power_rec, power_done))).\nblock(power_rec, op2(res, mul, var(res), var(x),\n                 op2(y, sub, var(y), const(1),\n                 if(y, power_rec, power_done)))).\nblock(power_done, print_and_stop(var(res))).\n\nEvery rule of block declares one block by first giving the label of the\nblock, followed by the code. Code is a series of op1 or op2 statements\nterminated by a jump, an if or a print_and_stop. op1 statements\nare operations with one argument of the form op1(res_variable,\noperation_name, argument, next_statement). Arguments can be either variables\nin the form var(name) or constants in the form const(value).\nTo run programs in this flowgraph language, we first need some helper\nfunctionality. The first few helper functions are concerned with the handling of\nenvironments, the data structures the interpreter uses to map variable\nnames occuring in the program to the variables' current values. In Python\ndictionaries would be used for this purpose, but in Prolog we have to emulate\nthese by lists of key/value pairs (not very efficient, but good enough):\nlookup(X, [], _) :- throw(key_not_found(X)).\nlookup(Key, [Key/Value | _], Value) :- !.\nlookup(Key, [_ | Rest], Value) :- lookup(Key, Rest, Value).\n\nwrite_env([], X, V, [X/V]).\nwrite_env([Key/_ | Rest], Key, Value, [Key/Value | Rest]) :- !.\nwrite_env([Pair | Rest], Key, Value, [Pair | NewRest]) :- write_env(Rest, Key, Value, NewRest).\n\nremove_env([], _, []).\nremove_env([Key/_ | Rest], Key, Rest) :- !.\nremove_env([Pair | Rest], Key, [Pair | NewRest]) :- remove_env(Rest, Key, NewRest).\n\nresolve(const(X), _, X).\nresolve(var(X), Env, Y) :- lookup(X, Env, Y).\n\nThe implementation of these functions is not too important. The lookup\nfunction finds a key in an environment list, the write_env function adds a\nnew key/value pair to an environment, remove_env removes a key. The\nresolve function is used to take either a constant or a variable and return\na value. If it's a constant, the value of that constant is returned, if it's a\nvariable it is looked up in the environment. Note how the last argument of\nlookup and resolve is actually a return value, which is the typical\napproach in Prolog.\nSo far we have not specified what the primitive operations that can occur in the\nprogram actually mean. For that we define a do_op function which\nexecutes primitive operations:\ndo_op(same, X, X).\ndo_op(mul, X, Y, Z) :- Z is X * Y.\ndo_op(add, X, Y, Z) :- Z is X + Y.\ndo_op(sub, X, Y, Z) :- Z is X - Y.\ndo_op(eq, X, Y, Z) :- X == Y -> Z = 1; Z = 0.\ndo_op(ge, X, Y, Z) :- X >= Y -> Z = 1; Z = 0.\ndo_op(readlist, L, I, X) :- nth0(I, L, X).\ndo_op(Op, _, _, _) :- throw(missing_op(Op)).\n\nAgain the last argument is an output variable.\nNow we can start executing simple operations. For that an interp predicate\nis defined. It takes as its first argument the current environment and as the\nsecond argument the operation to execute. E.g. to execute primitive operations\nwith one or two arguments:\ninterp(op1(ResultVar, Op, Arg, Rest), Env) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\ninterp(op2(ResultVar, Op, Arg1, Arg2, Rest), Env) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\nFirst the arguments are resolved into values. Afterwards the operation is executed,\nand the result is written back into the environment. Then interp is called on\nthe rest of the program. Similarly easy are the unconditional jump and\nprint_and_stop:\ninterp(jump(L), Env) :-\n    block(L, Block),\n    interp(Block, Env).\n\n\ninterp(print_and_stop(Arg), Env) :-\n    resolve(Arg, Env, Val),\n    print(Val), nl.\n\nIn the unconditional jump we simply get the target block and continue executing\nthat. To execute print_and_stop we resolve the argument, print the value and\nthen are done.\nThe conditional jump is only slightly more difficult:\ninterp(if(V, L1, L2), Env) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        block(L2, Block)\n    ;\n        block(L1, Block)\n    ),\n    interp(Block, Env).\n\nFirst the variable is looked up in the environment. If the variable is zero,\nexecution continues at the second block, otherwise it continues at the first\nblock.\nGiven this interpreter, we can execute the above example program like this, on a\nProlog console:\n$ swipl -s cfglang.pl\n?- block(power, Block), interp(Block, [x/10, y/10]).\n10000000000\n\nPartial Evaluation of the Flowgraph Language\nLet's look at what a partial evaluator for this simple flowgraph language would\nlook like. Partial evaluation (PE), also called specialization, is a program\nmanipuation technique. PE takes an input program and transforms it into a\n(hopefully) simpler and faster output program. It does this by assuming that\nsome variables in the input program are constants. All operations that act only\non such constants can be folded away. All other operations need to remain in the\noutput program (called residual program). Thus the partial evaluator proceeds\nmuch like an interpreter, just that it cannot actually execute some operations.\nAlso, its output is not just a value, but also list of remaining operations that\ncould not be optimized away.\nThe partial evaluator cannot use normal environments, because unlike the\ninterpreter not all variables' values are known to it. It will therefore work on\npartial environments, which store just the know variables. For these partial\nenvironments, some new helper functions are needed:\nplookup(Key, [], var(Key)).\nplookup(Key, [Key/Value | _], const(Value)) :- !.\nplookup(Key, [_ | Rest], Value) :- plookup(Key, Rest, Value).\n\npresolve(const(X), _, const(X)).\npresolve(var(V), PEnv, X) :- plookup(V, PEnv, X).\n\nThe function plookup takes a variable and a partial environment and returns\neither const(Value) if the variable is found in the partial environment or\nvar(Key) if it is not. Equivalently, presolve is like resolve,\nexcept that it uses plookup instead of lookup.\nWith these helpers we can start writing a partial evaluator. The following two\nrules are where the main optimization in the form of constant folding happens.\nThe idea is that when the partial evaluator sees an operation that involves\nonly constant arguments, it can constant-fold the operation, otherwise it\ncan't:\npe(op1(ResultVar, Op, Arg, Rest), PEnv, NewOp) :-\n    presolve(Arg, PEnv, RArg),\n    (RArg = const(C) ->\n        do_op(Op, C, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        RestResidual = NewOp\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op1(ResultVar, Op, RArg, RestResidual)\n    ),\n    pe(Rest, NEnv, RestResidual).\n\npe(op2(ResultVar, Op, Arg1, Arg2, Rest), PEnv, NewOp) :-\n    presolve(Arg1, PEnv, RArg1),\n    presolve(Arg2, PEnv, RArg2),\n    (RArg1 = const(C1), RArg2 = const(C2) ->\n        do_op(Op, C1, C2, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        RestResidual = NewOp\n\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op2(ResultVar, Op, RArg1, RArg2, RestResidual)\n    ),\n    pe(Rest, NEnv, RestResidual).\n\nThe pe predicate takes a partial environment, the current operations and\npotentially returns a new operation. To partially evaluate a simple operation, its arguments are\nlooked up in the partial environment. If all the arguments are constants, the\noperation can be executed, and no new operation is produced. Otherwise, we need\nto produce a new residual operation which is exactly like the one currently\nlooked at. Also, the result variable needs to be removed from the partial\nenvironment, because it was just overwritten by an unknown value.\nThe potentially generated residual operation is stored into the output argument\nNewOp. The output argument of the recursive call is the last argument of\nthe newly created residual operation, which will then be filled by the\nrecursive call. This is a typical approach in Prolog, but may look strange if\nyou are not familiar with it.\nNote how the first case of these two rules is just like interpretation. The\nsecond case doesn't really do anything, it just produces a residual operation.\nThis relationship between normal evaluation and partial evaluation is very\ntypical.\nThe unconditional jump and print_and_stop are not much more complex:\npe(jump(L), PEnv, jump(LR)) :-\n    do_pe(L, PEnv, LR).\n\npe(print_and_stop(Arg), Env, print_and_stop(RArg)) :-\n    presolve(Arg, Env, RArg).\n\nTo partially evaluate an unconditional jump we again produce a jump. The target\nlabel of that residual jump is computed by asking the partial evaluator to\nproduce residual code for the label L with the given partial environment.\nprint_and_stop is simply turned into a print_and_stop. We will see the\ncode for do_pe soon.\nConditional jumps are more interesting:\npe(if(V, L1, L2), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C) ->\n        (C = 0 ->\n            L = L2\n        ;\n            L = L1\n        ),\n        do_pe(L, PEnv, LR),\n        NewOp = jump(LR)\n    ;\n        do_pe(L1, PEnv, L1R),\n        do_pe(L2, PEnv, L2R),\n        NewOp = if(V, L1R, L2R)\n    ).\n\nFirst we look up the value of the condition variable. If it is a constant, we\ncan produce better code, because we know statically that only one path is\nreachable. Thus we produce code for that path, and then emit an unconditional\njump there. If the condition variable is not known at partial evaluation time,\nwe need to partially evaluate both paths and produce a conditional jump in the\nresidual code.\nThis rule is the one that causes the partial evaluator to potentially do much\nmore work than the interpreter, because after an if sometimes both paths\nneed to be explored. In the worst case this process never stops, so a real\npartial evaluator would need to ensure somehow that it terminates. There are\nmany algorithms for doing that, but I will ignore this problem here.\nNow we need to understand what the do_pe predicate is doing. Its most\nimportant task is to make sure that we don't do the same work twice by\nmemoizing code that was already partially evaluated in the past. For that it\nkeeps a mapping of Label, Partial Environment to Label of the residual\ncode:\ndo_pe(L, PEnv, LR) :-\n    (code_cache(L, PEnv, LR) ->\n        true\n    ;\n        gensym(L, LR),\n        assert(code_cache(L, PEnv, LR)),\n        block(L, Code),\n        pe(Code, PEnv, Residual),\n        assert(block(LR, Residual))\n    ).\n\nIf the code cache indicates that label L was already partially evaluated\nwith partial environment PEnv, then the previous residual code label\nLPrevious\nis returned. Otherwise, a new label is generated with gensym, the code cache\nis informed of that new label with assert, then the block is partially\nevaluated and the residual code is added to the database.\nFor those who know partial evaluation terminology: This partial evaluator is a\npolyvariant online partial evaluator. \"Polyvariant\" means that for every label,\nseveral specialized version of the block can be generated. \"Online\" means that\nno preprocessing is done before the partial evaluator runs.\n\nPartial Evaluation Example\nWith this code we can look at the classical example of partial evaluation (it's\nprobably the \"Hello World\" of partial evaluation). We\ncan ask the partial evaluator to compute a power function, where the exponent\ny is a fixed number, e.g. 5, and the base x is unknown:\n?- do_pe(power, [y/5], LR).\nLR = power1.\n\nTo find out which code was produced, we can use listing:\n?- listing(code_cache)\ncode_cache(power, [y/5], power1).\ncode_cache(power_rec, [y/5, res/1], power_rec1).\ncode_cache(power_rec, [y/4], power_rec2).\ncode_cache(power_rec, [y/3], power_rec3).\ncode_cache(power_rec, [y/2], power_rec4).\ncode_cache(power_rec, [y/1], power_rec5).\ncode_cache(power_done, [y/0], power_done1).\n\n?- listing(block)\n.... the block definition of the user program ....\nblock(power_done1, print_and_stop(var(res))).\nblock(power_rec5, op2(res, mul, var(res), var(x), jump(power_done1))).\nblock(power_rec4, op2(res, mul, var(res), var(x), jump(power_rec5))).\nblock(power_rec3, op2(res, mul, var(res), var(x), jump(power_rec4))).\nblock(power_rec2, op2(res, mul, var(res), var(x), jump(power_rec3))).\nblock(power_rec1, op2(res, mul, const(1), var(x), jump(power_rec2))).\nblock(power1, jump(power_rec1)).\n\nThe code_cache tells which residual labels correspond to which original\nlabels under which partial environments. Thus, power1 contains the code of\npower under the assumption that y is 5. Looking at the block listing,\nthe label power1 corresponds to code that simply multiplies res by x\nfive times without using the variable x at all. The loop that was present\nin the original program has been fully unrolled, the loop variable y has\ndisappeared. Hopefully this is faster than the original program.\n\nConclusion\nIn this blog post we saw an interpreter for a simple flow graph language in\nProlog, together with a partial evaluator for it. The partial evaluator\nessentially duplicates every rule of the interpreter. If all the arguments of\nthe current operation are known, it acts like the interpreter, otherwise it\nsimply copies the operation into the residual code.\nPartial evaluation can be used for a variety of applications, but the most\ncommonly cited one is that of applying it to an interpreter. To do that, the\nprogram that the interpreter runs is assumed to be constant by the partial\nevaluator. Thus a specialized version of the interpreter is produced that does\nnot use the input program at all. That residual code can be seen as a compiled\nversion of the input program.\nIn the next blog post in this series we will look at writing a simple tracer for\nthe same flowgraph language.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/01/comparing-partial-evaluation-and-7255412724168990164.html"
    },
    {
      "title": "PyPy internship at NCAR",
      "text": "Hello, everyone\nI would like to inform you that there is a very interesting opportunity\nfor doing an internship at NCAR in the lovely town of Boulder, situated\non the foothils of Rocky Mountains. Before you read on, make sure you:\n\nare a student of a US University, who is legally eligible to work in the US\nare at least finishing second year this year\napply before February 3rd.\n\nThe internship itself will focus on using PyPy (in some way) to provide\na high performance numeric kernel for an atmospheric model, and measuring how\nfast we can go. This is very much in line with what the current effort on\nNumPy in PyPy is about. The internship will be mentored by Davide del Vento\nand I hope to have some influence over where it goes myself :-)\nA few interesting links:\n\nprogram website\ninternship proposal - note that the actual roadmap is very flexible, as\nlong as it's a numeric kernel of an atmospheric model using PyPy.\n\nFeel free to contact Davide for details about the proposal and pypy-dev or\nme directly for details about PyPy.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/01/pypy-internship-at-ncar-2244162842744077724.html"
    },
    {
      "title": "Transactional Memory (II)",
      "text": "Here is an update about the previous blog post about the\nGlobal Interpreter Lock (GIL).  In 5 months, the point of view\nchanged quite a bit.\nLet me remind you that the GIL is the technique used in both CPython and\nPyPy to safely run multi-threaded programs: it is a global lock that\nprevents multiple threads from actually running at the same time.  The\nreason to do that is that it would have disastrous effects in the\ninterpreter if several threads access the same object concurrently --- to\nthe point that in CPython even just manipulating the object's reference\ncounter needs to be protected by the lock.\nSo far, the ultimate goal to enable true multi-CPU usage has been to\nremove the infamous GIL from the interpreter, so that multiple threads\ncould actually run in parallel.  It's a lot of work, but this has been\ndone in Jython.  The reason that it has not been done in CPython so far\nis that it's even more work: we would need to care not only about\ncarefully adding fine-grained locks everywhere, but also about reference\ncounting; and there are a lot more C extension modules that would need\ncare, too.  And we don't have locking primitives as performant as\nJava's, which have been hand-tuned since ages (e.g. to use help from the\nJIT compiler).\nBut we think we have a plan to implement a different model for using\nmultiple cores.  Believe it or not, this is better than just removing\nthe GIL from PyPy.  You might get to use all your cores without ever\nwriting threads.\nYou would instead just use some event dispatcher, say from Twisted, from\nStackless, or from your favorite GUI; or just write your own.  From\nthere, you (or someone else) would add some minimal extra code to the\nevent dispatcher's source code, to exploit the new transactional features\noffered by PyPy.  Then you would run your program on a\nspecial version of PyPy, and voil\u00e0: you get some form of automatic parallelization.\nSounds magic, but the basic idea is simple: start handling multiple\nevents in parallel, giving each one its own transaction.  More about\nit later.\n\nThreads or Events?\nFirst, why would this be better than \"just\" removing the GIL?  Because\nusing threads can be a mess in any complex program.  Some authors (e.g.\nLee) have argued that the reason is that threads are fundamentally\nnon-deterministic.  This makes it very hard to reason about them.\nBasically the programmer needs to \"trim\" down the non-determinism (e.g.\nby adding locks, semaphores, etc.), and it's hard to be sure when he's\ngot a sufficiently deterministic result, if only because he can't write\nexhaustive tests for it.\nBy contrast, consider a Twisted program.  It's not a multi-threaded\nprogram, which means that it handles the \"events\" one after the other.\nThe exact ordering of the events is not really deterministic, because\nthey often correspond to external events; but that's the only source of\nnon-determinism.  The actual handling of each event occurs in a nicely\ndeterministic way, and most importantly, not in parallel with the\nhandling of other events.  The same is true about other libraries like\nGUI toolkits, gevent, or Stackless.\n(Of course the Twisted and the Stackless models, to cite only these two,\nare quite different from each other; but they have in common the fact\nthat they are not multi-threaded, and based instead on \"events\" ---\nwhich in the Stackless case means running a tasklet from one switch()\npoint to the next one.)\nThese two models --- threads or events --- are the two main models we\nhave right now.  The latter is more used in Python, because it is much\nsimpler to use than the former, and the former doesn't give any benefit\nbecause of the GIL.  A third model, which is the only one that gives\nmulti-core benefits, is to use multiple processes, and do inter-process\ncommunication.\n\nThe problem\nConsider the case of a big program that has arbitrary complicated\ndependencies.  Even assuming a GIL-less Python, this is likely enough to\nprevent the programmer from even starting a multi-threaded rewrite,\nbecause it would require a huge mess of locks.  He could also consider\nusing multiple processes instead, but the result is annoying as well:\nthe complicated dependencies translate into a huge mess of inter-process\nsynchronization.\nThe problem can also be down-sized to very small programs, like the kind\nof hacks that you do and forget about.  In this case, the dependencies\nmight be simpler, but you still have to learn and use subtle locking\npatterns or a complex inter-process library, which is overkill for the\npurpose.\n(This is similar to how explicit memory management is not very hard for\nsmall programs --- but still, nowadays a lot of people agree that\nautomatic memory management is easier for programs of all sizes.  I\nthink the same will eventually be true for using multiple CPUs, but the\ncorrect solution will take time to mature, like garbage collectors did.\nThis post is a step in hopefully the right direction :-))\n\nEvents in Transactions\nLet me introduce the notion of independent events: two events are\nindependent if they don't touch the same set of objects. In a multi-threaded\nworld, it means that they can be executed in parallel without needing any lock\nto ensure correctness.\nEvents might also be mostly independent, i.e. they rarely access the same\nobject concurrently.  Of course, in a multi-threaded world we would still need\nlocks to ensure correctness, but the point is that the locks are rarely causing\npauses: lock contention is low.\nConsider again the Twisted example I gave above.  There are often several\nevents pending in the dispatch queue (assuming the program is using 100%\nof our single usable CPU, otherwise the whole discussion is moot).  The case I am\ninterested in is the case in which these events are generally mostly\nindependent, i.e. we expect few conflicts between them.  However\nthey don't have to be proved independent.  In fact it is fine if\nthey have arbitrary complicated dependencies as described above.  The\npoint is the expected common case.  Imagine that you have a GIL-less\nPython and that you can, by a wave of your hand, have all the careful\nlocking mess magically done.  Then what I mean here is the case in which\nsuch a theoretical program would run mostly in parallel on multiple\ncore, without waiting too often on the locks.\nIn this case, the solution I'm proposing is that with minimal tweaks\nin the event dispatch loop, we can\nhandle multiple events on multiple threads, each in its own transaction.\nA transaction is basically a tentative execution of the corresponding\npiece of code: if we detect conflicts with other concurrently executing\ntransactions, we abort the whole transaction and restart it from\nscratch.\nBy now, the fact that it can basically work should be clear: multiple\ntransactions will only get into conflict when modifying the same data\nstructures, which is the case where the magical wand above would have\nput locks.  If the magical program could progress without too many\nlocks, then the transactional program can progress without too many\nconflicts.  In a way, you get even more than what the magical program\ncan give you: each event is dispatched in its own transaction, which\nmeans that from each event's point of view, we have the illusion that\nnobody else is running concurrently.  This is exactly what all existing\nTwisted-/Stackless-/etc.-based programs are assuming.\nNote that this solution, without transactions, already exists in some\nother languages: for example, Erlang is all about independent events.\nThis is the simple case where we can just run them on multiple cores,\nknowing by construction of the language that you can't get conflicts.\nOf course, it doesn't work for Python or for a lot of other languages.\nFrom that point of view, what I'm suggesting is merely that\ntransactional memory could be a good model to cope with the risks of\nconflicts that come from not having a special-made language.\n\nNot a perfect solution\nOf course, transactional memory\n(TM) is not a perfect solution either.  Right now, the biggest issue is\nthe performance hit that comes from the software implementation (STM).\nIn time, hardware support (HTM) is likely to show up and help\nmitigate the problem; but I won't deny the fact that in some cases,\nbecause it's simple enough and/or because you really need the top\nperformance, TM is not the best solution.\nAlso, the explanations above are silent on what is a hard point for TM,\nnamely system calls.  The basic general solution is to suspend other\ntransactions as soon as a transaction does its first system call, so\nthat we are sure that the transaction will succeed.  Of course this\nsolution is far from optimal.  Interestingly, it's possible to do better\non a case-by-case basis: for example, by adding in-process buffers, we\ncan improve the situation for sockets, by having recv() store in a\nbuffer what is received so that it can be re-recv()-ed later if the\ntransaction is aborted; similarly, send() or writes to log files can be\ndelayed until we are sure that the transaction will commit.\nFrom my point of view, the most important point is that the TM solution\ncomes from the correct side of the \"determinism\" scale.  With threads,\nyou have to prune down non-determinism.  With TM, you start from a\nmostly deterministic point, and if needed, you add non-determinism.  The\nreason you would want to do so is to make the transactions shorter:\nshorter transactions have less risks of conflicts, and when there are\nconflicts, less things to redo.  So making transactions shorter\nincreases the parallelism that your program can achieve, while at the\nsame time requiring more care.\nIn terms of an event-driven model, the equivalent would be to divide the\nresponse of a big processing event into several events that are handled\none after the other: for example, the first event sets things up and fires the second\nevent, which does the actual computation; and afterwards a third event\nwrites the results back.  As a result, the second event's transaction\nhas little risks of getting aborted.  On the other hand, the writing\nback needs to be aware of the fact that it's not in the same transaction\nas the original setting up, which means that other unrelated\ntransactions may have run in-between.\n\nOne step towards the future?\nThese, and others, are the problems of the TM approach.  They are \"new\"\nproblems, too, in the sense that the existing ways of programming don't\nhave these problems.\nStill, as you have guessed, I think that it is overall a win, and\npossibly a big win --- a win that might be on the same scale for the age\nof multiple CPUs as automatic garbage collection was 20 years ago for\nthe age of RAM size explosion.\nStay tuned for more!\n--- Armin (and reviews by Antonio and Fijal)\n\n\nUPDATE: please look at the tiny transaction module I wrote as an example.  The idea is to have the same interface as this module, but implemented differently.  By making use of transactional memory internally, it should be possible to safely run on multiple CPUs while keeping the very same programmer interface.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/01/transactional-memory-ii-7225309560970774590.html"
    },
    {
      "title": "NumPyPy progress report - running benchmarks",
      "text": "Hello.\nWe're excited to let you know about some of the great progress we've made on\nNumPyPy: both completeness and performance. In this blog entry we mostly\nwill talk about performance and how much progress we have made so far.\nWord of warning: this\nwork is in progress -- we're maybe half way to where we want to be and there are\nmany trivial and not so trivial optimizations to be written. (For example, we\nhaven't even started to implement important optimizations, like vectorization.)\n\nBenchmark\nWe chose a laplace equation solver, based on SciPy's PerformancePython wiki.\nUnfortunately, the different implementations on the wiki page accidentally use\ntwo different algorithms, which have different convergences, and very different\nperformance characteristics on modern computers. As a result, we implemented\nour own versions in both C and Python (with and without NumPy). The full source\ncan be found in fijal's hack repo, all these benchmarks were performed at\nrevision 18502dbbcdb3.\nFirst, let me describe various algorithms used. Note that some of them contain\nPyPy-specific hacks to work around limitations in the current implementation.\nThese hacks will go away eventually and the performance will improve.\nNumerically the algorithms used are identical, however exact data layout in\nmemory differs between them.\nA note about all the benchmarks: they each were run once, but the\nperformance is very stable across runs.\nStarting with the C version, it implements a trivial laplace transform\nusing two loops and double-reference memory (array of int*). The double\nreference does not matter for performance and the two algorithms are\nimplemented in inline-laplace.c and laplace.c. They were both compiled\nwith gcc 4.4.5 at -O3. The inline version modifies array in-place while the non-inline version stores results in a copy. That makes them converge at different rate, hence different number of iterations\nA straightforward version of those in Python is implemented in laplace.py\nusing, respectively, inline_slow_time_step and slow_time_step.\nslow_2_time_step does the same thing, except it copies arrays in-place\ninstead of creating new copies. Table below compares running PyPy against C:\n\n\n\n\n\n\n\nbench\nnumber of iterations\ntime per iteration\n\nlaplace C\n219\n6.3ms\n\ninline-laplace C\n278\n20ms\n\nslow python\n219\n17ms\n\nslow 2 python\n219\n14ms\n\ninline_slow python\n278\n23.7ms\n\n\n\nAn important thing to notice is the data dependency of the inline\nversion causes a huge slowdown for the C versions. This is not a severe\ndisadvantage for us though -- the brain-dead Python version takes longer\nand PyPy is not able to take advantage of the knowledge that the data is\nindependent. The results are in the same ballpark as the C versions --\n15% - 170% slower, but the algorithm\none chooses matters more than the language. By comparison, the slow versions\ntake about 5.75s each on CPython 2.6 per iteration and, by estimation,\nare about 200x slower than the PyPy equivalent, if I had the patience to\nmeasure the full run.\nThe next step is to use NumPy expressions. The first problem we run into is\nthat computing the error requires walking the entire array a second time. This\nis fairly inefficient in terms of cache access, so I took the liberty of\ncomputing the errors every 15 steps. This results in the convergence being\nrounded to the nearest 15 iterations, but speeds things up considerably.\nnumeric_time_step takes the most braindead approach of replacing the array\nwith itself, like this:\n\nu[1:-1, 1:-1] = ((u[0:-2, 1:-1] + u[2:, 1:-1])*dy2 +\n                       (u[1:-1,0:-2] + u[1:-1, 2:])*dx2)*dnr_inv\n\nWe need 3 arrays here -- one is an intermediate (PyPy only needs one, for all of\nthose subexpressions), one is a copy for computing the error, and one is the\nresult. This works automatically because in NumPy + or * creates an\nintermediate, while NumPyPy avoids allocating the intermediate if possible.\nnumeric_2_time_step works in pretty much the same way:\n\nsrc = self.u\nself.u = src.copy()\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nexcept the copy is now explicit rather than implicit.\nnumeric_3_time_step does the same thing, but notice one doesn't have to copy\nthe entire array, it's enough to copy the border pieces and fill rest with\nzeros:\n\nsrc = self.u\nself.u = numpy.zeros((self.nx, self.ny), 'd')\nself.u[0] = src[0]\nself.u[-1] = src[-1]\nself.u[:, 0] = src[:, 0]\nself.u[:, -1] = src[:, -1]\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nnumeric_4_time_step is the one that tries hardest to resemble the C version.\nInstead of doing an array copy, it actually notices that one can alternate\nbetween two arrays. This is exactly what the C version does. The\nremove_invalidates call is a PyPy specific hack - we hope to remove this\ncall in the near future, but, in short, it promises \"I don't have any unbuilt\nintermediates that depend on the value of the argument\", which means one doesn't\nhave to compute sub-expressions one is not actually using:\n\nremove_invalidates(self.old_u)\nremove_invalidates(self.u)\nself.old_u[:,:] = self.u\nsrc = self.old_u\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nThis one is the most comparable to the C version.\nnumeric_5_time_step does the same thing, but notices one doesn't have to copy\nthe entire array, it's enough to just copy the edges. This is an optimization\nthat was not done in the C version:\n\nremove_invalidates(self.old_u)\nremove_invalidates(self.u)\nsrc = self.u\nself.old_u, self.u = self.u, self.old_u\nself.u[0] = src[0]\nself.u[-1] = src[-1]\nself.u[:, 0] = src[:, 0]\nself.u[:, -1] = src[:, -1]\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nLet's look at the table of runs. As before, gcc 4.4.5, compiled at -O3,\nand PyPy nightly 7bb8b38d8563, on an x86-64 machine. All of the numeric methods\nrun for 226 steps, slightly more than the 219, rounding to the next 15 when the\nerror is computed.\n\n\n\n\n\n\n\nbenchmark\nPyPy\nCPython\n\nnumeric\n21ms\n35ms\n\nnumeric 2\n14ms\n37ms\n\nnumeric 3\n13ms\n29ms\n\nnumeric 4\n11ms\n31ms\n\nnumeric 5\n9.3ms\n21ms\n\n\n\nWe think that these preliminary results are pretty good. They're not as fast as\nthe C version (or as fast as we'd like them to be), but we're already much\nfaster than NumPy on CPython -- almost always by more than 2x on this relatively\nreal-world example. This is not the end, though. In fact, it's hardly the\nbeginning! As we continue work, we hope to make even more use of the\nhigh level information that we have. Looking at the assembler generated by\ngcc for this example, it's pretty clear we can outperform it thanks to better\naliasing information and hence better possibilities for vectorization.\nStay tuned.\nEDIT: fixed the benchmark name\n\nEDIT2: added info that first table is about PyPy\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/01/numpypy-progress-report-running-3336055571122066974.html"
    },
    {
      "title": "Leysin Winter Sprint",
      "text": "PyPy Leysin Winter Sprint: 15-22nd January 2012\n\n\nThe next PyPy sprint will be in Leysin, Switzerland, for the\neighth time.  This is a fully public sprint: newcomers and topics\nother than those proposed below are welcome.\n\nGoals and topics of the sprint\n\n\nPy3k: work towards supporting Python 3 in PyPy\n\nNumPyPy: work towards supporting the numpy module in PyPy\n\nJIT backends: integrate tests for ARM; look at the PowerPC 64;\n  maybe try again to write an LLVM- or GCC-based one\n\nSTM and STM-related topics; or the Concurrent Mark-n-Sweep GC\n\nAnd as usual, the main side goal is to have fun in winter sports :-)\n  We can take a day off for ski.\n\n\nExact times\n\nThe work days should be 15-21 January 2011 (Sunday-Saturday).  The\nofficial plans are for people to arrive on the 14th or the 15th, and to\nleave on the 22nd.\n\nInterested? Read more...",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/12/leysin-winter-sprint-6862532189897876336.html"
    },
    {
      "title": "Come see us at PyCon 2012",
      "text": "PyCon 2012 is coming up in just a few short months, and PyPy will be well\nrepresented there.  We'll be delivering a tutorial, two talks, plus we'll be\naround for the sprints.Here are the abstracts for the tutorials and talks:How to get the most out of your PyPy, by Maciej Fijalkowski, Alex Gaynor\nand Armin Rigo: For many applications PyPy can provide performance benefits\nright out of the box. However, little details can push your application to\nperform much better. In this tutorial we'll give you insights on how to push\nPyPy to its limits. We'll focus on understanding the performance\ncharacteristics of PyPy, and learning the analysis tools in order to maximize\nyour applications' performance. This is the tutorial.\nWhy PyPy by example, by Maciej Fijalkowski, Alex Gaynor and Armin Rigo:\nOne of the goals of PyPy is to make existing Python code faster; however an\neven broader goal was to make it possible to write things in Python that\npreviously would needed to be written in C or other low-level language. This\ntalk will show examples of this, and describe how they represent the\ntremendous progress PyPy has made, and what it means for people looking at\nusing PyPy.\nHow the PyPy JIT works, by Benjamin Peterson: The Python community is\nabuzz about the major speed gains PyPy can offer for pure Python code. But how\ndoes the PyPy JIT actually work? This talk will discuss how the PyPy JIT is\nimplemented. It will include descriptions of the tracing, optimization, and\nassembly generation phases. I will demonstrate each step with an example loop.\nIf you have any questions let us know!  We look forward to seeing people at\nPyCon and chatting about PyPy and the entire Python ecosystem.See you there,\nMaciej Fijalkowski, Alex Gaynor, Benjamin Peterson, Armin Rigo, and the entire PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/12/come-see-us-at-pycon-2012-610420698450130659.html"
    },
    {
      "title": "Plotting using matplotlib from PyPy",
      "text": "Big fat warning This is just a proof of concept. It barely works. There are\nmissing pieces left and right, which were replaced with hacks so I can get this\nto run and prove it's possible. Don't try this at home, especially your home.\nYou have been warned.\nThere has been a lot of talking about PyPy not integrating well with the\ncurrent scientific Python ecosystem, and numpypy (a NumPy reimplementation\non top of pypy) was dubbed \"a fancy array library\". I'm going to show that\nintegration with this ecosystem is possible with our design.\nFirst, the demo:\n\n#!/usr/bin/env pypy\n\n# numpy, pypy version\nimport numpypy as numpy\n# DRAGONS LIVE THERE (fortunately hidden)\nfrom embed.emb import import_mod\n\npylab = import_mod('matplotlib.pylab')\n\nif __name__ == '__main__':\n    a = numpy.arange(100, dtype=int)\n    b = numpy.sin(a)\n    pylab.plot(a, b)\n    pylab.show()\n\nAnd you get:\n\n\n\nNow, how to reproduce it:\n\nYou need a PyPy without cpyext, I did not find a linker that would support\noverriding symbols. Right now there are no nightlies like this, so you have\nto compile it yourself, like:\n\n./translate.py -Ojit targetpypystandalone.py --withoutmod-cpyext\n\nThat would give you a PyPy that's unable to load some libraries like PIL, but\nperfectly working otherwise.\n\nSpeaking of which, you need a reasonably recent PyPy.\n\nThe approach is generally portable, however the implementation has been\ntested only on 64bit linux. Few tweaks might be required.\n\nYou need to install python2.6, the python2.6 development headers, and have\nnumpy and matplotlib installed on that python.\n\nYou need a checkout of my hacks directory and put embedded on your\nPYTHONPATH, your pypy checkout also has to be on the PYTHONPATH.\n\n\n\nEr wait, what happened?\nWhat didn't happen is we did not reimplement matplotlib on top of PyPy. What\ndid happen is we embed CPython inside of PyPy using ctypes. We instantiate it.\nand follow the embedding tutorial for CPython. Since numpy arrays are not\nmovable, we're able to pass around an integer that's represents the memory\naddress of the array data and reconstruct it in the embedded interpreter. Hence\nwith a relatively little effort we managed to reuse the same array data on both\nsides to plot at array. Easy, no?\nThis approach can be extended to support anything that's not too tied with\npython objects. SciPy and matplotlib both fall into the same category\nbut probably the same strategy can be applied to anything, like GTK or QT.\nIt's just a matter of extending a hack into a working library.\nTo summarize, while we're busy making numpypy better and faster, it seems\nthat all external libraries on the C side can be done using an embedded Python\ninterpreter with relatively little effort. To get to that point, I spent\na day and a half to learn how to embed CPython, with very little prior\nexperience in the CPython APIs. Of course you should still keep as much as\npossible in PyPy to make it nice and fast :)\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/12/plotting-using-matplotlib-from-pypy-6389240123679375092.html"
    },
    {
      "title": "PyPy 1.7 on Win32",
      "text": "Hi all,\n\nWe have fixed _continuation on Win32 (thanks Stakkars), and so we have now a Win32 version of PyPy 1.7.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/11/pypy-17-on-win32-4962523601794245248.html"
    },
    {
      "title": "PyPy 1.7 - widening the sweet spot",
      "text": "We're pleased to announce the 1.7 release of PyPy. As became a habit, this\nrelease brings a lot of bugfixes and performance improvements over the 1.6\nrelease. However, unlike the previous releases, the focus has been on widening\nthe \"sweet spot\" of PyPy. That is, classes of Python code that PyPy can greatly\nspeed up should be vastly improved with this release. You can download the 1.7\nrelease here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 1.7 and cpython 2.7.1 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 32/64 or\nWindows 32. Windows 64 work is ongoing, but not yet natively supported.\nThe main topic of this release is widening the range of code which PyPy\ncan greatly speed up. On average on\nour benchmark suite, PyPy 1.7 is around 30% faster than PyPy 1.6 and up\nto 20 times faster on some benchmarks.\n\n\nHighlights\n\nNumerous performance improvements. There are too many examples which python\nconstructs now should behave faster to list them.\n\nBugfixes and compatibility fixes with CPython.\n\nWindows fixes.\n\nPyPy now comes with stackless features enabled by default. However,\nany loop using stackless features will interrupt the JIT for now, so no real\nperformance improvement for stackless-based programs. Contact pypy-dev for\ninfo how to help on removing this restriction.\n\nNumPy effort in PyPy was renamed numpypy. In order to try using it, simply\nwrite:\n\nimport numpypy as numpy\n\nat the beginning of your program. There is a huge progress on numpy in PyPy\nsince 1.6, the main feature being implementation of dtypes.\n\nJSON encoder (but not decoder) has been replaced with a new one. This one\nis written in pure Python, but is known to outperform CPython's C extension\nup to 2 times in some cases. It's about 20 times faster than\nthe one that we had in 1.6.\n\nThe memory footprint of some of our RPython modules has been drastically\nimproved. This should impact any applications using for example cryptography,\nlike tornado.\n\nThere was some progress in exposing even more CPython C API via cpyext.\n\n\n\n\nThings that didn't make it, expect in 1.8 soon\nThere is an ongoing work, which while didn't make it to the release, is\nprobably worth mentioning here. This is what you should probably expect in\n1.8 some time soon:\n\nSpecialized list implementation. There is a branch that implements lists of\nintegers/floats/strings as compactly as array.array. This should drastically\nimprove performance/memory impact of some applications\nNumPy effort is progressing forward, with multi-dimensional arrays coming\nsoon.\nThere are two brand new JIT assembler backends, notably for the PowerPC and\nARM processors.\n\n\n\nFundraising\nIt's maybe worth mentioning that we're running fundraising campaigns for\nNumPy effort in PyPy and for Python 3 in PyPy. In case you want to see any\nof those happen faster, we urge you to donate to numpy proposal or\npy3k proposal. In case you want PyPy to progress, but you trust us with\nthe general direction, you can always donate to the general pot.\n\nCheers,Maciej Fija\u0142kowki, Armin Rigo and the entire PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/11/pypy-17-widening-sweet-spot-4260962828394182017.html"
    },
    {
      "title": "Gothenburg sprint report",
      "text": "In the past week, we have been busy hacking on PyPy at the Gothenburg sprint, the second of this 2011.  The sprint was hold at Laura's and Jacob's place, and here is a brief report of what happened.\n\n\nIn the first day we welcomed Mark Pearse, who was new to PyPy and at his first sprint.  Mark worked the whole sprint in the new SpecialisedTuple branch, whose aim is to have a special implementation for small 2-items and 3-items tuples of primitive types (e.g., ints or floats) to save memory.  Mark paired with Antonio for a couple of days, then he continued alone and did an amazing job.  He even learned how to properly do Test Driven Development :-).\nAntonio spent a couple of days investigating whether it is possible to use application checkpoint libraries such as BLCR and DMTCP to save the state of the PyPy interpreter between subsequent runs, thus saving also the JIT-compiled code to reduce the warmup time.  The conclusion is that these are interesting technologies, but more work would be needed (either on the PyPy side or on the checkpoint library side) before it can have a practical usage for PyPy users.\nThen, Antonio spent most of the rest of the sprint working on his ffistruct branch, whose aim is to provide a very JIT-friendly way to interact with C structures, and eventually implement ctypes.Structure on top of that.  The \"cool part\" of the branch is already done, and the JIT already can compile set/get of fields into a single fast assembly instruction, about 400 times faster than the corresponding ctypes code.  What is still left to do is to add a nicer syntax (which is easy) and to implement all the ctypes peculiarities (which is tedious, at best :-)).\nAs usual, Armin did tons of different stuff, including fixing a JIT bug, improving the performance of file.readlines() and working on the STM branch (for Software Transactional Memory), which is now able to run RPython multithreaded programs using software transaction (as long as they don't fill up all the memory, because support for the GC is still missing :-)).  Finally, he worked on improving the Windows version of PyPy. While doing so he discovered together with Anto a terrible bug which lead to a continuous leak of stack space because the JIT called some functions using the wrong calling convention.\nH\u00e5kan, with some help from Armin, worked on the jit-targets branch, whose goal is to heavily refactor the way the traces are internally represented by the JIT, so that in the end we can produce (even :-)) better code than what we do nowadays.  More details in this mail.\nAndrew Dalke worked on a way to integrate PyPy with FORTRAN libraries, and in particular the ones which are wrapped by Numpy and Scipy: in doing so, he wrote f2pypy, which is similar to the existing f2py but instead of producing a CPython extension module it produces a pure python modules based on ctypes.  More work is needed before it can be considered complete, but f2pypy is already able to produce a wrapper for BLAS which passes most of the tests under CPython, although there's still work left to get it working for PyPy.\n\n\nArmin and H\u00e5kan with Laura's \"5x faster\" cake\nChristian Tismer worked the whole sprint on the branch to make PyPy compatible with Windows 64 bit.  This needs a lot of work because a lot of PyPy is written under the assumption that the long type in C has the same bit size than void*, which is not true on Win64.  Christian says that in the past Genova-Pegli sprint he completed 90% of the work, and in this sprint he did the other 90% of the work.  Obviously, what is left to complete the task is the third 90% :-).  More seriously, he estimated a total of 2-4 person-weeks of work to finish it.\nBut, all in all, the best part of the sprint has been the cake that Laura baked to celebrate the \"5x faster than CPython\" achievement. Well, actually our speed page reports \"only\" 4.7x, but that's because in the meantime we switched from comparing against CPython 2.6 to comparing against CPython 2.7, which is slightly faster.  We are confident that we will reach the 5x goal again, and that will be the perfect excuse to eat another cake :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/11/gothenburg-sprint-report-8371395613874909242.html"
    },
    {
      "title": "Speeding up JSON encoding in PyPy",
      "text": "Hi\nRecently I spent a bit of effort into speeding up JSON in PyPy. I started with\nwriting a benchmark, which is admittedly not a very good one, but it's\nbetter than nothing (suggestions on how to improve it are welcome!).\nFor this particular benchmark, the numbers are as follow. Note that CPython by\ndefault uses the optimized C extension, while PyPy uses the pure Python one.\nPyPy trunk contains another pure Python version which has been optimized\nspecifically for the PyPy JIT. Detailed optimizations are described later in\nthis post.\nThe number reported is the time taken for the third run, when things are\nwarmed up. Full session here.\n\n\n\n\n\n\nCPython 2.6\n22s\n\nCPython 2.7\n3.7s\n\nCPython 2.7 no C extension\n44s\n\nPyPy 1.5\n34s\n\nPyPy 1.6\n22s\n\nPyPy trunk\n3.3s\n\n\n\nLessons learned:\n\nExpectations are high\nA lot of performance critical stuff in Python world is already written in a hand\noptimized C. Writing C (especially when you interface with CPython C API) is\nugly and takes significant effort. This approach does not scale well when\nthere is a lot of code to be written or when there is a very tight coupling\nbetween the part to be rewritten and the rest of the code. Still, people would\nexpect PyPy to be better at \"tasks\" and not precisely at running equivalent\ncode, hence a comparison between the C extension and the pure python version\nis sound. Fortunately it's possible to outperform the C extension, but requires\na bit of effort on the programmer side as well.\n\n\nOften interface between the C and Python part is ugly\nThis is very clear if you look at json module as implemented in CPython's\nstandard library. Not everything is in C (it would probably be just too\nmuch effort) and the interface to what is in C is guided via profiling not\nby what kind of interface makes sense. This especially is evident comparing CPython 2.6 to 2.7.\nJust adapting the code to an interface with C made the Python version slower.\nRemoving this clutter improves the readability a lot and improves PyPy's version\na bit, although I don't have hard numbers.\n\n\nJitViewer is crucial\nIn case you're fighting with PyPy's performance, jitviewer is worth a shot.\nWhile it's not completely trivial to understand what's going on, it'll\ndefinitely show you what kind of loops got compiled and how.\n\n\nNo nice and fast way to build strings in Python\nPyPy has a custom thing called __pypy__.builders.StringBuilder. It has\na few a features that make it much easier to optimize than other ways like\nstr.join() or cStringIO.\n\nYou can specify the start size, which helps a lot if you can even provide\na rough estimate on the size of the string (less copying)\nOnly append and build are allowed. While  the string is being built you\ncan't seek or do anything else. After it's built you can never append any more.\nUnicode version available as well as __pypy__.builders.UnicodeBuilder.\n\n\n\nMethod calls are ok, immutable globals are ok\nPyPy's JIT seems to be good enough for at least the simple cases. Calling\nmethods for common infrastructure or loading globals (instead of rebinding as\nlocals) is fast enough and improves code readability.\n\n\nString copying is expensive\nEdit: see the comment at the end\nIf you use re.sub, the current implementation will always create a copy\nof the string even if there was no match to replace.\nIf you know your regexp is simple, first try to check if there is\nanything to replace. This is a pretty hard optimization to\ndo automatically -- simply matching the regular expression can be too costly\nfor it to make sense. In our particular example however, the regexp is really\nsimple, checking ranges of characters. It also seems that this is by far the\nfastest way to escape characters as of now.\n\n\nGenerators are slower than they should be\nI changed the entire thing to simply call builder.append instead of\nyielding to the main loop where it would be gathered. This is kind of a PyPy\nbug that using generators extensively is slower, but a bit hard to fix.\nEspecially in cases where there is relatively little data being passed around\n(few bytes), it makes sense to gather it first. If I were to implement an\nefficient version of iterencode, I would probably handle chunks of\npredetermined size, about 1000 bytes instead of yielding data every few bytes.\n\n\nI must admit I worked around PyPy's performance bug\nFor obscure (although eventually fixable) reasons, this:\n\nfor c in s: # s is string\n  del c\n\nis faster than:\n\nfor c in s:\n  pass\n\nThis is a PyPy performance bug and should be fixed, but on a different branch ;-)\n\n\nPyPy's JIT is good\nI was pretty surprised, but the JIT actually did make stuff work nicely.\nThe changes that were done were relatively minor and straightforward, once\nthe module was cleaned to the normal \"pythonic\" state.\nIt is worth noting that it's possible to write code in Python and make it\nrun really fast, but you have to be a bit careful. Again, jitviewer is your\nfriend when determining why things are slow. I hope we can write more tools\nin the future that would more automatically guide people through potential\nperformance pitfals.\nCheers,\nfijal\nEdit: I was wrong about re.sub. It just seems to be that the JIT is figuring match better than sub, will be fixed soon",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/10/speeding-up-json-encoding-in-pypy-8937643890263223898.html"
    },
    {
      "title": "PyPy G\u00f6teborg Post-Hallowe'en Sprint Nov 2nd - Nov 9th",
      "text": "The next PyPy sprint will be in Gothenburg, Sweden. It is a public sprint,\nsuitable for newcomers.  We'll focus on making a public kickoff for\nboth the numpy/pypy integration project\nand the Py3k support project,\nas well as whatever interests the Sprint attendees.  Since both of these\nprojects are very new, there will be plenty of work suitable for newcomers\nto PyPy.\nOther topics might include:\n\nHelping people get their code running with PyPy\nwork on a FSCons talk?\nstate of the STM Vinnova project (We most likely, but not for certain will\nknow whether or not we are approved by this date.)\n\n\nOther Useful dates\nGothPyCon - Saturday Oct 29.\nFSCONS Friday Nov 11 - Sunday Nov 12.\n\n\nLocation\nThe sprint will be held in the apartment of Laura Creighton and Jacob Hall\u00e9n\nwhich is at G\u00f6tabergsgatan 22 in Gothenburg, Sweden.  Here is a map.  This is\nin central Gothenburg.  It is between the tram stops of Vasaplatsen and\nValand, (a distance of 4 blocks) where many lines call -- the 2, 3, 4, 5,\n7, 10 and 13.\nProbably cheapest and not too far away is to book accomodation at SGS\nVeckobostader. The  Elite Park Avenyn Hotel is a luxury hotel just a\nfew blocks away. There are scores of hotels a short walk away from the\nsprint location, suitable for every budget, desire for luxury, and desire\nfor the unusual.  You could, for instance, stay on a boat.  Options are\ntoo numerous to go into here. Just ask in the mailing list or on the blog.\nHours will be\nfrom 10:00 until people have had enough.  It's a good idea to arrive a\nday before the sprint starts and leave a day later.  In the middle of\nthe sprint there usually is a break day and it's usually ok to take\nhalf-days off if you feel like it.  Of course, many of you may be interested\nin sticking around for FSCons, held the weekend after the sprint.\n\n\nGood to Know\nSweden is not part of the Euro zone. One SEK (krona in singular, kronor\nin plural) is roughly 1/10th of a Euro (9.36 SEK to 1 Euro).\nThe venue is central in Gothenburg.  There is a large selection of\nplaces to get food nearby, from edible-and-cheap to outstanding.  We\noften cook meals together, so let us know if you have any food allergies,\ndislikes, or special requirements.\nSweden uses the same kind of plugs as Germany. 230V AC.\n\n\nGetting Here\nIf are coming train, you will arrive at the Central Station.  It is\nabout 12 blocks to the site from there, or you can take a tram.\nThere are two airports which are local to G\u00f6teborg, Landvetter (the main\none) and Gothenburg City Airport (where some budget airlines fly).\nIf you arrive at Landvetter  the airport bus stops right downtown at\nElite Park Avenyn Hotel which is the second stop, 4 blocks from the\nSprint site, as well as the end of the line, which is the Central Station.\nIf you arrive at Gothenburg City Airport take the bus to the end of the\nline.  You will be at the  Central Station.\nYou can also arrive by ferry, from either Kiel in Germany or Frederikshavn\nin Denmark.\n\n\nWho's Coming?\nIf you'd like to come, please let us know when you will be arriving and\nleaving, as well as letting us know your interests  We'll keep a list\nof people which we'll update (which you can do so yourself if you\nhave bitbucket pypy commit rights).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/10/pypy-goteborg-post-halloween-sprint-nov-7335004338996313725.html"
    },
    {
      "title": "Numpy funding and status update",
      "text": "Hi everyone,\nIt's been a little while since we wrote about NumPy on PyPy, so we wanted to\ngive everyone an update on what we've been up to, and what's up next for us.\nWe would also like to note that we're launching a funding campaign\nfor NumPy support in PyPy. Details can be found on the donation page.\nSome of the things that have happened since last we wrote are:\n\nWe added dtype support, meaning you can now create arrays of a bunch of\ndifferent types, including bools, ints of a various sizes, and floats.\nMore array methods and ufuncs, including things like comparison methods\n(==, >, etc.)\nSupport for more and more argument types, for example you can index by a\ntuple now (only works with tuples of length one, since we only have\nsingle-dimension arrays thus far).\n\nSome of the things we're working on at the moment:\n\nMore dtypes, including complex values and user-defined dtypes.\nSubscripting arrays by other array as indices, and by bool arrays as masks.\nStarting to reuse Python code from the original numpy.\n\nSome of the things on the near horizon are:\n\nBetter support for scalar data, for example did you know that\nnumpy.array([True], dtype=bool)[0] doesn't return a bool object?\nInstead it returns a numpy.bool_.\nMulti-dimensional array support.\n\nIf you're interested in helping out, we always love more contributors,\nAlex, Maciej, Justin, and the whole PyPy team",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2011/10/numpy-funding-and-status-update-2380711174693638392.html"
    },
    {
      "title": "More Compact Lists with List Strategies",
      "text": "Since we come closer to merging the list-strategy branch I want to try to explain this memory optimization today. Datatypes in PyPy are stored as W_<type>Objects (e.g. W_StringObject to represent strings, W_IntObject to represent ints). This is necessary due to the dynamic nature of Python. So the actual value (e.g. string, integer) is stored inside that box, resulting in an indirection. When having a large amount of such boxed objects, for example in a list, the wasted memory can become quite large.   If you have a closer look at such lists, you will see that in many of them only one type of data is stored and only few (and smaller) lists store mixed types. Another thing to observe is that those lists often won't change the types of the objects they contain at runtime very often. For instance a list of a million integers is very unlikely to suddenly get a string appended to it. List StrategiesThe goal of this work is to write an optimization that exploits this behaviour. Instead of wrapping all items in a list, we implement lists in a way that they are optimized for storing certain (primitive) datatypes. These implementations store the content of the list in unwrapped form, getting rid of the extra indirection and wrapper objects. One approach would be to add a level of indirection, making each W_ListObject instance point to another object that stores the actual content. For this other object, several implementations would exist, for every datatype we want to store without wrapping it (as well as a general one that deals with arbitrary content). The data layout would look something like this:  This approach has the problem that we need two indirections to get to the data and that the implementation instances need memory themselves.What we would like to do is to make the W_ListObject point to an RPython list directly, that contains either wrapped or unwrapped data. This plan has the problem that storing different unwrapped data is not directly possible in RPython.  To solve the problem, we use the rerased RPython library module. It allows us to erase the type of an object, in this case lists, and returns something similar to void-star in C, or Object in Java. This object is then stored on the W_ListObject in the field storage. If we want to work with the list, for example to append or delete items, we need to unerase the storage again.Example for rerase: storage = erase([1 ,2 ,3 ,4])\n# storage is an opaque object that you can do nothing with\n....\nl = unerase(storage)\nl.clear()\nNow that we know how to make the W_ListObject point directly to wrapped or unwrapped data, we need to find out how to actually do any operations on this data. This can be accomplished by adding another field to our W_ListObject. This field points to a ListStrategy object. The actual implementation of W_ListObject is now deferred to those ListStrategy classes. For instance, a W_ListObject which holds only integers will use the IntegerListStrategy.When the type of content is being changed, we need to change the used strategy as well as the storage in compatible ways. For example when we add a string to the list of integers we need to switch to the ObjectListStrategy and change the storage to be a list of wrapped objects. Thus the currently used strategy always knows what to do with what is currently in the storage.  As you can see, we now save one level of indirections by storing some of the data unwrapped. Of course each operation on a list needs to go via the strategy, but since we save one indirection for each element stored in that list and the Strategy classes are singletons, the benefits outweigh the costs.Currently there are only strategies for integers and strings since many lists seem to have these datatypes. Other strategies i.e for floats and unicode strings are planned. We also implemented two special strategies for empty lists and range-lists. The EmptyListStrategy's storage is None. If objects are added to the list we just switch to the appropriate strategy (determined by the item's type). RangeListsStrategies do not store any items at all. Instead they only store values describing the range of the list, i.e. start, step and length. On any operations that changes the data of the list we switch to the IntegerStrategy.A nice side-effect of storing unwrapped datatypes is that we can implement optimized methods for certain cases. For instance, since comparison of unwrapped integers is now much faster than comparison between arbitrary objects, we can rewrite the sorting methods for lists containing integers.MicrobenchmarksFinally here is an early overview of the memory consumption of different Python implementations: CPython, PyPy and PyPy-list which uses list-strategies. To demonstrate how powerful list-strategies can be in the best case, we wrote benchmarks that create a list of integers, a list of strings and a range-list each with one million elements each and then reads out the heap size of the process as reported by the OS. The results are as follows:    The savings on integers and strings in this ideal case are quite big.The benchmark for range-lists is a little unfair, since in CPython one could accomplish the same memory behaviour using xrange. However, in PyPy users won't notice that internally the list does not store all items, making it still possible to use all list methods, such as append or delete.ConclusionWe hope that list strategies bring memory savings for applications that use homogeneous lists of primitive types. Furthermore, operations on such lists tend to be somewhat faster as well. This also integrates well with the JIT. The list strategies optimizations will be merged to the PyPy's default branch at some point in the next months. An equivalent optimization for dictionaries has already been merged (and is part of PyPy 1.6), one for sets is coming in the future.Lukas Diekmann and Carl Friedrich Bolz",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/10/more-compact-lists-with-list-strategies-8229304944653956829.html"
    },
    {
      "title": "Py3k for PyPy fundraiser",
      "text": "Hi,We would like to announce a donation campaign for implementing Python 3 in PyPy.\nPlease read our detailed plan for all the details and donate using the\nbutton on that page!Thanks,\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/09/py3k-for-pypy-fundraiser-8139653689520709617.html"
    },
    {
      "title": "Wrapping C++ Libraries with Reflection \u2014 Status Report One Year Later",
      "text": "Well over a year ago, work was started on the cppyy module which lives in the\nreflex-support branch.\nSince then, work has progressed at a varying pace and has included a recent\nsprint in D\u00fcsseldorf, last July.\nLet's first take a step back and recap why we're interested in doing this,\ngiven that it is perfectly possible to use C++ through generated bindings and\ncpyext.\ncppyy makes use of reflection information generated for the C++ classes of\ninterest, and has that reflection information available at run time.\nTherefore, it is able to open up complex C++ types to the JIT in a\nconceptually similar manner as simple types are open to it.\nThis means that it is possible to get rid of a lot of the marshalling layers\nwhen making cross-language calls, resulting in much lower call overhead than\nis possible when going through the CPython API, or other methods of wrapping.\nThere are two problems that need to be solved: C++ language constructs need to\nbe presented on the Python side in a natural way; and cross-language impedance\nmismatches need to be minimized, with some hints of the user if need be.\nFor the former, the list of mapped features has grown to a set that is\nsufficient to do real work.\nThere is now support for:\n\n\nbuiltin, pointer, and array types\nnamespaces, classes, and inner classes\nglobal functions, global data\nstatic/instance data members and methods\ndefault variables, object return by value\nsingle and multiple (virtual) inheritance\ntemplated classes\nbasic STL support and pythonizations\nbasic (non-global) operator mapping\n\n\nThe second problem is harder and will always be an on-going process.\nBut one of the more important issues has been solved at the recent D\u00fcsseldorf\nsprint, namely, that of reclaiming C++ objects instantiated from the Python\nside by the garbage collector.\nPerformance has also improved, especially that of the nicer \"pythonized\"\ninterface that the user actually sees, although it still misses out on\nabout a factor of 2.5 in comparison to the lower-level interface (which has\ngotten uglier, so you really don't want to use that).\nMost of this improvement is due to restructuring so that it plays nicer with\nthe JIT and libffi, both of which themselves have seen improvements.\nWork is currently concentrated on the back-ends: a CINT back-end is underway\nand a LLVM/CLang pre-compiled headers (PCH) back-end is planned.\nThe latter is needed for this code to be released in the wild, rather than\njust used in high energy physics (HEP), as that would be easier to support.\nAlso, within HEP, CLang's PCH are foreseen to be the future format of\nreflection information.\nAt the end of the D\u00fcsseldorf sprint, we tried a little code that did something\nactually \"useful,\" namely the filling of a histogram with some random values.\nWe did get it to work, but trying cppyy on a large class library showed\nthat a good warning system for such things like missing classes was sorely\nneeded.\nThat has been added since, and revisiting the histogram example later, here is\nan interesting note: the pypy-c run takes 1.5x the amount of time of that\nof the compiled, optimized, C++ code.\nThe run was timed start to finish, including the reflection library loading\nand JIT warm-up that is needed in the case of Python, but not for the compiled\nC++ code.\nHowever, in HEP, scientists run many short jobs while developing their\nanalysis codes, before submitting larger jobs on the GRID to run during lunch\ntime or overnight.\nThus, a more realistic comparison is to include the compilation time needed\nfor the C++ code and with that, the Python code needs only 55% of the time\nrequired by C++.\nThe choice of a programming language is often a personal one, and such\narguments like the idea that C++ is hard to use typically do not carry much\nweight with the in-crowd that studies quantum field dynamics for fun.\nHowever, getting the prompt with your analysis results back faster is a sure\nwinner. We hope that cppyy will soon have progressed far enough to make it\nuseful first to particle physicists and then other uses for wrapping C++\nlibraries.\n\nWim Lavrijsen, Carl Friedrich Bolz, Armin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/wrapping-c-libraries-with-reflection-3916959558080483711.html"
    },
    {
      "title": "We need Software Transactional Memory",
      "text": "Hi all.  Here is (an extract of) a short summary paper about my current position on\nSoftware Transactional Memory as a general tool in the implementation\nof Python or Python-like languages.  Thanks to people on IRC for discussion on making\nthis blog post better (lucian, Alex Gaynor, rguillebert, timonator, Da_Blitz).\nFor the purpose of the present discussion, we are comparing Java with Python\nwhen it comes to multi-threading.\n\nThe problem in complex high-level languages\nLike Java, the Python language gives guarantees: it is not acceptable\nfor the Python virtual machine to crash due to incorrect usage of\nthreads.  A primitive operation in Java is something like reading or\nwriting a field of an object; the corresponding guarantees are along the\nlines of: if the program reads a field of an object, and another thread\nwrites to the same field of the same object, then the program will see\neither the old value, or the new value, but not something else entirely,\nand the virtual machine will not crash.\nHigher-level languages like Python differ from Java by the fact that a\n\"primitive operation\" is far more complex.  It may for example involve\nlooking in several hash maps, perhaps doing updates.  In general, it is\ncompletely impossible to map every operation that must be atomic to a\nsingle processor instruction.\n\nJython: fine-grained locking\nThis problem has been solved \"explicitly\" in the Jython interpreter that\nruns on top of Java.  The solution is explicit in the following sense:\nthroughout the Jython interpreter, every single operation makes careful\nuse of Java-level locking mechanisms.  This is an application of\n\"fine-grained locking\".  For example, operations like attribute lookup,\nwhich need to perform look-ups in a number of hash maps, are protected\nby acquiring and releasing locks (in __getattribute__).\nA draw-back of this solution is the attention to detail required.\nIf even one place misses a lock, then there is either a\nbug --- and such bugs occur in cases that are increasingly rare and hard\nto debug as the previous bugs are fixed --- or we just file it under \"differences\nfrom CPython\".  There is however the risk of\ndeadlock, if two threads attempt to lock the same objects in different\norder.\n\nIn practice, the situation is actually not as bad as\nI may paint it: the number of locks in Jython is reasonable, and allows for\nall the \"common cases\" to work as expected.\n(For the uncommon cases, see below.)\n\nPerformance-wise, the Java virtual machine itself comes with locks that\nhave been heavily optimized over a long period of time, so the\nperformance is acceptable.  However if this solution were coded in C, it\nwould need a lot of extra work to optimize the locks manually (possibly\nintroducing more of the subtle bugs).\n\nCPython: coarse-grained locking\nCPython, the standard implementation of Python in C, took a different\nand simpler approach: it has a single global lock, called the Global\nInterpreter Lock (GIL).  It uses \"coarse-grained locking\": the lock is\nacquired and released around the whole execution of one bytecode (or\nactually a small number of bytecodes, like 100).  This solution is\nenough to ensure that no two operations can conflict with each other,\nbecause the two bytecodes that invoke them are themselves\nserialized by the GIL.  It is a solution which avoids --- unlike Jython\n--- writing careful lock-acquiring code all over the interpreter.  It\nalso offers even stronger guarantees: every bytecode runs entirely\natomically.\nNowadays, the draw-back of the GIL approach is obvious on multi-core\nmachines: by serializing the execution of bytecodes, starting multiple\nthreads does not actually let the interpreter use of more than one core.\nPyPy, the Python implementation in Python, takes the same approach so\nfar.\n\nExisting usage\nAs we have seen, we have the following situation: the existing Python\nlanguage, as CPython implements it, offers very strong guarantees about\nmulti-threaded usage.  It is important to emphasize that most existing\nmulti-threaded Python programs actually rely on such strong guarantees.\nThis can be seen for example in a problem that takes a populated list\nand does in several threads:\n\nnext_item = global_list.pop()\n\nThis implicitly relies on the fact that pop() will perform atomic\nremoval from the list.  If two threads try to pop() from the same list\nat the same time, then the two operations will occur in one order or the\nother; but they will not e.g. return the same object to both threads or\nmess up the internal state of the list object.\nWith such an example in mind, it should be clear that we do not want a\nsolution to the multi-core issue that involves dropping these strong\nguarantees.  It is ok however to lower the barrier, as Jython does; but\nany Python implementation must offer some guarantees, or not offer\nmulti-threading at all.  This includes the fact that a lot of methods on\nbuilt-in types are supposed to be atomic.\n\n(It should be noted that not offering multi-threading at all is actually\nalso a (partial) solution to the problem.  Recently, several \"hacks\"\nhave appeared that give a programmer more-or-less transparent access to\nmultiple independent processes (e.g. multiprocessing).  While these provide appropriate\nsolutions in some context, they are not as widely applicable as\nmulti-threading.  As a typical example, they fail to apply when the\nmutiple cores need to process information that cannot be serialized at\nall --- a requirement for any data exchange between several processes.)\n\nHere is an example of how Jython's consistency is weaker than CPython's GIL.\nIt takes uncommon examples to show it, and the fact that it does not work\nlike a CPython programmer expect them to is generally considered as an\nimplementation detail.  Consider:\nThread 1:  set1.update(set2)\nThread 2:  set2.update(set3)\nThread 3:  set3.update(set1)\nEach operation is atomic in the case of CPython, but decomposed in two steps\n(which can each be considered atomic) in the case of Jython: reading from the\nargument, and then updating the target set.  Suppose that initially\nset1 = {1}, set2 = {2}, set3 = {3}.  On CPython, independently on\nthe order in which the threads run, we will end up with at least one of the\nsets being {1, 2, 3}.  On Jython, it is possible that all\nthree sets end up as containing two items only.  The example is a bit\nfar-fetched but should show that CPython's consistency is strictly stronger\nthan Jython's.\n\nPyPy\nPyPy is a Python interpreter much like CPython or Jython, but the way it\nis produced is particular.  It is an interpreter written in RPython, a\nsubset of Python, which gets turned into a complete virtual machine (as\ngenerated C code) automatically by a step called the \"translation\".  In\nthis context, the trade-offs are different from the ones in CPython and\nin Jython: it is possible in PyPy, and even easy, to apply arbitrary\nwhole-program transformations to the interpreter at \"translation-time\".\nWith this in mind, it is possible to imagine a whole-program\ntransformation that would add locking on every object manipulated in\nRPython by the interpreter.  This would end up in a situation similar to\nJython.  However, it would not automatically solve the issue of\ndeadlocks, which is avoided in the case of Jython by careful manual\nplacement of the locks.  (In fact, being deadlock-free is a global\nprogram property that cannot be automatically ensured or verified; any\nchange to Jython can in theory break this property, and thus introduce\nsubtle deadlocks.  The same applies to non-atomicity.)\nIn fact, we can easily check that if the interpreter accesses (for\nboth reading and writing)\nobjects A and B in a bytecode of thread 1, and objects B and A (in the\nopposite order) in a bytecode of thread 2 --- and moreover if you need to\nhave accessed the first object before you can decide that you will need\nto access the second object --- then there is no way (apart from the GIL) to avoid\na deadlock while keeping the strong guarantee of atomicity.  Indeed, if\nboth threads have progressed to the middle of the execution of their\nbytecode, then A has already been mutated by thread 1 and similarly B\nhas already been mutated by thread 2.  It is not possible to\nsuccessfully continue running the threads in that case.\n\nUsing Software Transactional Memory\nSoftware Transactional Memory (STM) is an approach that gives a solution\nto precisely the above problem.  If a thread ended up in a situation\nwhere continuing to run it would be wrong, then we can abort and\nrollback.  This is similar to the notion of transaction on databases.\nIn the above example, one or both threads would notice that they are\nabout to run into troubles and abort.  This means more concretely that\nthey need to have a way to restart execution at the start of the\nbytecode, with all the side-effects of what they did so far being either\ncancelled or just not committed yet.\nWe think that this capacity to abort and rollback is the missing piece\nof the puzzle of multi-threaded implementations of Python.\nActually, according to the presentation of the problem given\nabove, it is unavoidable that any solution that wants to offer the\nsame level of consistency and atomicity as CPython would involve\nthe capacity of aborting and rolling back --- which means precisely\nthat STM cannot be avoided.\n\nOk, but why not settle down with Jython's\napproach and put careful locks left and right throughout the interpreter?\nBecause (1) we would have to consider every operation's atomicity and make decisions\n(or steal Jython's) and document them\nhere;\n(2) it would also be really a lot of work, to optimize these locks e.g. with the\nJIT as well as the JVM does; and (3) it is not the PyPy way to require manually\ntweaking your code everywhere for a feature that should be orthogonal.  Point\n(3) is probably the most important here: you need to redo the work for every\nlanguage you implement in PyPy.\nIt also implies my own point (4): it is not fun :-)\n\nIn more details, the process would work as follows.  (This gives an\noverview of one possible model; it is possible that a different model\nwill end up being better.)  In every thread:\n\nAt the start of a bytecode, we start a \"transaction\".  This means\nsetting up a thread-local data structure to record a log of what\noccurs in the transaction.\nWe record in the log all objects that are read, as well as the\nmodifications that we would like to make.\nDuring this time, we detect \"read\" inconsistencies, shown by the\nobject's \"last-modified\" timestamp being later than the start time\nof the current transaction, and abort.  This prevents the rest of\nthe code from running with inconsistent values.\nIf we reach the end of the bytecode without a \"read\" inconsistency,\nthen we atomically check for \"write\" inconsistencies.  These are\ninconsistencies which arise from concurrent updates to objects\nin the other threads --- either our \"write\" objects, or our \"read\"\nobjects.\nIf no inconsistency is found, we \"commit\" the transaction by copying\nthe delayed writes from the log into main memory.\n\n\nThe points at which a transaction starts or ends are exactly the\npoints at which, in CPython, the Global Interpreter Lock is\nrespectively acquired and released.  If we ignore the fact that (purely for\nperformance) CPython acquires and releases the GIL only every N bytecodes,\nthen this means:\n\nBefore any bytecode we acquire the GIL (start a transaction), and after\nthe bytecode we release it (ends the transaction); and\nBefore doing an external call to the C library or the OS we release the GIL\n(ends the transaction) and afterwards re-acquire it (start the next transaction).\n\nSo in particular this model is well suited to the STM condition that we cannot\ndo anything in a transaction that cannot be rolled back, like --- precisely ---\nsystem calls.  Indeed, by construction, these system calls occur outside a\ntransaction, because in CPython they occur with the GIL released.\n\nPerformance\nA large number of implementation details are still open for now.\nFrom a user's point of view (i.e. the programmer using Python),\nthe most relevant one is the overall performance impact.  We\ncannot give precise numbers so far, and we expect the initial\nperformance to be abysmally bad (maybe 10x slower); however, with\nsuccessive improvements to the locking mechanism, to the global\nprogram transformation inserting the locks, to the garbage \ncollector (GC), and to the Just-in-Time (JIT) compiler, we\nbelieve that it should be possible to get a roughly reasonable\nperformance (up to maybe 2x slower).  For example, the GC can\nmaintain flags on the objects to know that they did not escape\ntheir creation thread, and do not need any logging; and the JIT\ncompiler can aggregate several reads or writes to an object into\none.  We believe that these are the kind of optimizations that\ncan give back a lot of the performance lost.\n\nThe state of STM\nTransactional Memory is itself a relatively old idea, originating\nfrom a 1986 paper by Tom Knight.  At first based on hardware\nsupport, the idea of software-only transactional memory (STM) was\npopularized in 1995 and has recently been the focus of intense \nresearch.\nThe approach outlined above --- using STM to form the core of the\nimplementation of a language --- is new, as far as we know.  So\nfar, most implementations provide STM as a library feature.  It\nrequires explicit usage, often in the form of explicitly\ndeclaring which objects must be protected by STM (object-based\nSTMs).  It is only recently that native STM support has started\nto appear, notably in the Clojure language.\nSTM is described on Wikipedia as an approach that \"greatly\nsimplifies conceptual understanding of multithreaded programs and\nhelps make programs more maintainable by working in harmony with\nexisting high-level abstractions such as objects and modules.\"\nWe actually think that these benefits are important enough to\nwarrant being exposed to the Python programmer as well, instead\nof being used only internally.  This would give the Python\nprogrammer a very simple interface:\n\nwith atomic:\n    <these operations are executed atomically>\n\n(This is an old idea.  Funny how back in 2003 people, including me, thought that this was a hack.  Now I'm writing a blog post to say \"it was not a hack; it's explicitly using locks that is a hack.\"  I'm buying the idea of composability.)\n\nFrom a practical point of view, I started looking seriously at\nthe University of Rochester STM (RSTM), a C++ library that has\nbeen a focus of --- and a collection of results from --- recent\nresearch.  One particularly representative paper is\nA\nComprehensive Strategy for Contention Management in Software\nTransactional Memory by Michael F. Spear, Luke Dalessandro,\nVirendra J. Marathe and Michael L. Scott.\n\nConclusion\nTaking these ideas and applying them in the context of an\nimplementation of a complex high-level language like Python comes\nwith its own challanges.  In this context, using PyPy makes sense\nas both an experimentation platform and as a platform that is\nrecently gaining attention for its performance.  The alternatives\nare unattractive: doing it in CPython for example would mean\nglobally rewriting the interpreter.  In PyPy instead, we write it\nas a transformation that is applied systematically at translation-time.\nAlso, PyPy is a general platform for generating fast interpreters\nfor dynamic languages; the STM implementation in PyPy would work\nout of the box for other language implementations as well, instead\nof just for Python.\n\nUpdate:\n\nThis is mostly me (Armin Rigo) ranting aloud and trying experiments;\nthis post should not be confused as meaning that the whole PyPy team\nwill now spend the next years working on it full-time.\nAs I said it is orthogonal to the actual Python interpreter, and it is in\nany case a feature that can be turned on or off during translation; I know\nthat in many or most use cases, people are more interested in getting a\nfast PyPy rather than one which is twice as slow but scales well.\nNothing I said is really new.  For proof, see\nRiley and Zilles (2006)\nas well as Tabba (2010) who both experimented with Hardware Transactional Memory, turning CPython or PyPy interpreter's GIL into start/end transactions, as I describe here.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/we-need-software-transactional-memory-6513983438425039230.html"
    },
    {
      "title": "PyPy 1.6 - kickass panda",
      "text": "We're pleased to announce the 1.6 release of PyPy. This release brings a lot\nof bugfixes and performance improvements over 1.5, and improves support for\nWindows 32bit and OS X 64bit. This version fully implements Python 2.7.1 and\nhas beta level support for loading CPython C extensions.  You can download it\nhere:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.1. It's fast (pypy 1.6 and cpython 2.6.2 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64 or Mac OS X.  Windows 32\nis beta (it roughly works but a lot of small issues have not been fixed so\nfar).  Windows 64 is not yet supported.\nThe main topics of this release are speed and stability: on average on\nour benchmark suite, PyPy 1.6 is between 20% and 30% faster than PyPy 1.5,\nwhich was already much faster than CPython on our set of benchmarks.\nThe speed improvements have been made possible by optimizing many of the\nlayers which compose PyPy.  In particular, we improved: the Garbage Collector,\nthe JIT warmup time, the optimizations performed by the JIT, the quality of\nthe generated machine code and the implementation of our Python interpreter.\n\n\nHighlights\n\nNumerous performance improvements, overall giving considerable speedups:\nbetter GC behavior when dealing with very large objects and arrays\nfast ctypes: now calls to ctypes functions are seen and optimized\nby the JIT, and they are up to 60 times faster than PyPy 1.5 and 10 times\nfaster than CPython\nimproved generators(1): simple generators now are inlined into the caller\nloop, making performance up to 3.5 times faster than PyPy 1.5.\nimproved generators(2): thanks to other optimizations, even generators\nthat are not inlined are between 10% and 20% faster than PyPy 1.5.\nfaster warmup time for the JIT\nJIT support for single floats (e.g., for array('f'))\noptimized dictionaries: the internal representation of dictionaries is now\ndynamically selected depending on the type of stored objects, resulting in\nfaster code and smaller memory footprint.  For example, dictionaries whose\nkeys are all strings, or all integers. Other dictionaries are also smaller\ndue to bugfixes.\n\n\nJitViewer: this is the first official release which includes the JitViewer,\na web-based tool which helps you to see which parts of your Python code have\nbeen compiled by the JIT, down until the assembler. The jitviewer 0.1 has\nalready been release and works well with PyPy 1.6.\nThe CPython extension module API has been improved and now supports many\nmore extensions. For information on which one are supported, please refer to\nour compatibility wiki.\nMultibyte encoding support: this was of of the last areas in which we were\nstill behind CPython, but now we fully support them.\nPreliminary support for NumPy: this release includes a preview of a very\nfast NumPy module integrated with the PyPy JIT.  Unfortunately, this does\nnot mean that you can expect to take an existing NumPy program and run it on\nPyPy, because the module is still unfinished and supports only some of the\nnumpy API. However, barring some details, what works should be\nblazingly fast :-)\nBugfixes: since the 1.5 release we fixed 53 bugs in our bug tracker, not\ncounting the numerous bugs that were found and reported through other\nchannels than the bug tracker.\n\nCheers,\nHakan Ardo, Carl Friedrich Bolz, Laura Creighton, Antonio Cuni,\nMaciej Fijalkowski, Amaury Forgeot d'Arc, Alex Gaynor,\nArmin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/pypy-16-kickass-panda-559424594592497545.html"
    },
    {
      "title": "Visualization of JITted code",
      "text": "Hello.\nWe're proud to announce the first public release of the jitviewer. As of now,\njitviewer is a slightly internal tool that helps understanding how your Python\nsource code is compiled by the PyPy's JIT all the way down to machine code.\nTo install it, you need a very recent version of PyPy\n(newer than 9th of August), for example one of the nightly builds:\n\n\ninstall pip and distribute either by creating a PyPy virtualenv\nor by following the installation instructions.\nmake sure to have a source code checkout of PyPy and put it in your\nPYTHONPATH.\npip install jitviewer.  Note that you need to run the pip\nexecutable which belongs to PyPy, not the globally installed one.\n\n\nHave a look at the README for how to start it, or try the online demo if\nyou just want to play with it.\nThe jitviewer is a web application written with flask and jinja2.  If\nyou have experience with web development and you want to help PyPy, don't\nhesitate to contact us, there are plenty of things to improve in it :-).\n\nWhat does the jitviewer really do?\nAt the top of the page, you will see the list of pieces of code which has been\ncompiled by the JIT.  You will see entries for both normal loops and for\n\"entry bridges\".  This is not the right place to discuss the difference\nbetween those, but you most probably want to look at loops, because usually\nit's where most of the time is spent.\nNote that for each loop, you will see the name of the function which contains\nthe first instruction of the loop.  However, thanks to the inlining done\nby the JIT, it will contain also the code for other functions.\nOnce you select a loop, the jitviewer shows how the JIT has compiled the\nPython source code into assembler in a hierarchical way. It displays four\nlevels:\n\nPython source code: only the lines shown in azure have been compiled for\nthis particular loop, the ones in gray have not.\n\nPython bytecode, the one you would get by doing:\n\ndef f(a, b):\n   return a + b\n\nimport dis\ndis.dis(f)\n\nThe opcodes are e.g. LOAD_FAST, LOAD_GLOBAL etc.  The opcodes\nwhich are not in bold have been completely optimized aways by the JIT.\n\nIntermediate representation of jit code (IR). This is a combination of\noperations (like integer addition, reading fields out of structures) and\nguards (which check that the assumptions we made are actually true). Guards\nare in red.  These operations are \"at the same level as C\": so, for example,\n+ takes two unboxed integers which can be stored into the register\nof the CPU.\n\nAssembler: you can see it by clicking on \"Show assembler\" in the menu on the\nright.\n\n\nSometimes you'll find that a guard fails often enough that a new piece of\nassembler is required to be compiled. This is an alternative path through the\ncode and it's called a bridge. You can see bridges in the jitviewer when\nthere is a link next to a guard. For more information about purpose look up\nthe jit documentation.\n\n\nI'm still confused\nJitviewer is not perfect when it comes to explaining what's going on. Feel free\nto pop up on IRC or send us a mail to the mailing list, we'll try to explain\nand/or improve the situation. Consult the contact page for details.\nCheers,\nfijal & antocuni",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/visualization-of-jitted-code-6202490807361942120.html"
    },
    {
      "title": "PyPy is faster than C, again: string formatting",
      "text": "String formatting is probably something you do just about every day in Python,\nand never think about.  It's so easy, just \"%d %d\" % (i, i) and you're\ndone.  No thinking about how to size your result buffer, whether your output\nhas an appropriate NULL byte at the end, or any other details.  A C\nequivalent might be:\n\nchar x[44];\nsprintf(x, \"%d %d\", i, i);\n\nNote that we had to stop for a second and consider how big numbers might get\nand overestimate the size (44 = length of the biggest number on 64bit (20) +\n1 for the sign * 2 + 1 (for the space) + 1 (NUL byte)), it took the authors of\nthis post, fijal and alex, 3 tries to get the math right on this :-)\nThis is fine, except you can't even return x from this function, a more\nfair comparison might be:\n\nchar *x = malloc(44 * sizeof(char));\nsprintf(x, \"%d %d\", i, i);\n\nx is slightly overallocated in some situations, but that's fine.\nBut we're not here to just discuss the implementation of string\nformatting, we're here to discuss how blazing fast PyPy is at it, with\nthe new unroll-if-alt branch.  Given the Python code:\n\ndef main():\n    for i in xrange(10000000):\n        \"%d %d\" % (i, i)\n\nmain()\n\nand the C code:\n\n#include <stdio.h>\n#include <stdlib.h>\n\n\nint main() {\n    int i = 0;\n    char x[44];\n    for (i = 0; i < 10000000; i++) {\n        sprintf(x, \"%d %d\", i, i);\n    }\n}\n\nRun under PyPy, at the head of the unroll-if-alt branch, and\ncompiled with GCC 4.5.2 at -O4 (other optimization levels were tested,\nthis produced the best performance). It took 0.85 seconds to\nexecute under PyPy, and 1.63 seconds with the compiled binary. We\nthink this demonstrates the incredible potential of dynamic\ncompilation, GCC is unable to inline or unroll the sprintf call,\nbecause it sits inside of libc.\nBenchmarking the C code:\n\n#include <stdio.h>\n#include <stdlib.h>\n\n\nint main() {\n    int i = 0;\n    for (i = 0; i < 10000000; i++) {\n        char *x = malloc(44 * sizeof(char));\n        sprintf(x, \"%d %d\", i, i);\n        free(x);\n    }\n}\n\nWhich as discussed above, is more comperable to the Python, gives a\nresult of 1.96 seconds.\nSummary of performance:\n\n\n\n\n\n\n\n\n\nPlatform\nGCC (stack)\nGCC (malloc)\nCPython\nPyPy (unroll-if-alt)\n\nTime\n1.63s\n1.96s\n10.2s\n0.85s\n\nrelative to C\n1x\n0.83x\n0.16x\n1.9x\n\n\n\nOverall PyPy is almost 2x faster. This is clearly win for dynamic\ncompilation over static - the sprintf function lives in libc and so\ncannot be specializing over the constant string, which has to be parsed\nevery time it's executed. In the case of PyPy, we specialize\nthe assembler if we detect the left hand string of the modulo operator\nto be constant.\nCheers,\nalex & fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/pypy-is-faster-than-c-again-string-6756589731691762127.html"
    },
    {
      "title": "Realtime image processing in Python",
      "text": "Image processing is notoriously a CPU intensive task.  To do it in realtime,\nyou need to implement your algorithm in a fast language, hence trying to do it\nin Python is foolish: Python is clearly not fast enough for this task. Is it?\n:-)\nActually, it turns out that the PyPy JIT compiler produces code which is fast\nenough to do realtime video processing using two simple algorithms implemented\nby H\u00e5kan Ard\u00f6.\nsobel.py implements a classical way of locating edges in images, the\nSobel operator. It is an approximation of the magnitude of the image\ngradient. The processing time is spend on two convolutions between the\nimage and 3x3-kernels.\nmagnify.py implements a pixel coordinate transformation that rearranges\nthe pixels in the image to form a magnifying effect in the center.\nIt consists of a single loop over the pixels in the output image copying\npixels from the input image.\nYou can try by yourself by downloading the appropriate demo:\n\n\npypy-image-demo.tar.bz2: this archive contains only the source code,\nuse this is you have PyPy already installed\npypy-image-demo-full.tar.bz2: this archive contains both the source\ncode and prebuilt PyPy binaries for linux 32 and 64 bits\n\n\nTo run the demo, you need to have mplayer installed on your system.  The\ndemo has been tested only on linux, it might (or not) work also on other\nsystems:\n$ pypy pypy-image-demo/sobel.py\n\n$ pypy pypy-image-demo/magnify.py\n\nBy default, the two demos uses an example AVI file.  To have more fun, you can\nuse your webcam by passing the appropriate mplayer parameters to the scripts,\ne.g:\n$ pypy demo/sobel.py tv://\n\nBy default magnify.py uses nearest-neighbor interpolation.  By adding the\noption -b, bilinear interpolation will be used instead, which gives\nsmoother result:\n$ pypy demo/magnify.py -b\n\nThere is only a single implementation of the algorithm in\nmagnify.py. The two different interpolation methods are implemented by\nsubclassing the class used to represent images and embed the\ninterpolation within the pixel access method. PyPy is able to achieve good\nperformance with this kind of abstractions because it can inline\nthe pixel access method and specialize the implementation of the algorithm.\nIn C++ that kind of pixel access method would be virtual and you'll need to use\ntemplates to get the same effect without incurring in runtime overhead.\n\n\n\n\n\n\nThe video above shows PyPy and CPython running sobel.py side by\nside (PyPy taking input from the webcam, CPython from the test\nfile). Alternatively, to have a feeling on how much PyPy is faster than\nCPython, try to run the demo with the latter.  These are the the average fps\n(frames per second) that I get on my machine (Ubuntu 64 bit, Intel i7 920, 4GB\nRAM) when processing the default test.avi video and using the prebuilt\nPyPy binary found in the full tarball alinked above.  For sobel.py:\n\n\nPyPy: ~47.23 fps\nCPython: ~0.08 fps\n\n\nFor magnify.py:\n\n\nPyPy: ~26.92 fps\nCPython: ~1.78 fps\n\n\nThis means that on sobel.py, PyPy is 590 times faster.  On\nmagnify.py the difference is much less evident and the speedup is \"only\"\n15x.\nIt must be noted that this is an extreme example of what PyPy can do.  In\nparticular, you cannot expect (yet :-)) PyPy to be fast enough to run an\narbitrary video processing algorithm in real time, but the demo still proves\nthat PyPy has the potential to get there.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/07/realtime-image-processing-in-python-6985924592886873374.html"
    },
    {
      "title": "Global Interpreter Lock, or how to kill it",
      "text": "People that listened to my (Armin Rigo) lightning talk at EuroPython know that\nsuddenly, we have a plan to remove the Global Interpreter Lock --- the\ninfamous GIL, the thing in CPython that prevents multiple threads from\nactually running in your Python code in parallel.\nThat's not actually new, because Jython has been doing it all along.\nJython works by very carefully adding locks to\nall the mutable built-in types, and by relying on the underlying Java\nplatform to be efficient about them (so that the result is faster than,\nsay, very carefully adding similar locks in CPython).  By \"very\ncarefully\", I mean really really carefully; for example,\n'dict1.update(dict2)' needs to lock both dict1 and dict2, but if you do\nit naively, then a parallel 'dict2.update(dict1)' might cause a\ndeadlock.\nAll of PyPy, CPython and IronPython have a GIL.  But for PyPy we are considering\na quite different approach than Jython's, based on Software\nTransactional Memory.  This is a recent development in computer\nscience, and it gives a nicer solution than locking.  Here is a short\nintroduction to it.\nSay you want to atomically pop an item from 'list1' and append it to\n'list2':\n\ndef f(list1, list2):\n    x = list1.pop()\n    list2.append(x)\n\nThis is not safe in multithreaded cases (even with the GIL).  Say that\nyou call f(l1, l2) in thread 1 and f(l2, l1) in thread 2.  What\nyou want is that it has no effect at all (x is moved from one list to\nthe other, then back).  But what can occur is that instead the top of\nthe two lists are swapped, depending on timing issues.\nOne way to fix it is with a global lock:\n\ndef f(list1, list2):\n    global_lock.acquire()\n    x = list1.pop()\n    list2.append(x)\n    global_lock.release()\n\nA finer way to fix it is with locks that come with the lists:\n\ndef f(list1, list2):\n    acquire_all_locks(list1.lock, list2.lock)\n    x = list1.pop()\n    list2.append(x)\n    release_all_locks(list1.lock, list2.lock)\n\nThe second solution is a model for Jython's, while the first is a model\nfor CPython's.  Indeed, in CPython's interpreter, we acquire the GIL,\nthen we do one bytecode (or actually a number of them, like 100), then\nwe release the GIL; and then we proceed to the next bunch of 100.\nSoftware Transactional Memory (STM) gives a third solution:\n\ndef f(list1, list2):\n    while True:\n        t = transaction()\n        x = list1.pop(t)\n        list2.append(t, x)\n        if t.commit():\n            break\n\nIn this solution, we make a transaction object and use it in all\nreads and writes we do to the lists.  There are actually several\ndifferent models, but let's focus on one of them.  During a transaction,\nwe don't actually change the global memory at all.  Instead, we use the\nthread-local transaction object.  We store in it which objects we\nread from, which objects we write to, and what values we write.  It is\nonly when the transaction reaches its end that we attempt to \"commit\"\nit.  Committing might fail if other commits have occurred in between,\ncreating inconsistencies; in that case, the transaction aborts and\nmust restart from the beginning.\nIn the same way as the previous two solutions are models for CPython and\nJython, the STM solution looks like it could be a model for PyPy in the\nfuture.  In such a PyPy, the interpreter would start a transaction, do\none or several bytecodes, and then end the transaction; and repeat.\nThis is very similar to what is going on in CPython with the GIL.  In\nparticular, it means that it gives programmers all the same guarantees\nas the GIL does.  The only difference is that it can actually run\nmultiple threads in parallel, as long as their code does not interfere\nwith each other.  (In particular, if you need not just the GIL but actual\nlocks in your existing multi-threaded program, then this will not\nmagically remove the need for them.  You might get an additional built-in\nmodule that exposes STM to your Python programs, if you prefer it over\nlocks, but that's another question.)\nWhy not apply that idea to CPython?  Because we would need to change\neverything everywhere.  In the example above, you may have noted that I\nno longer call 'list1.pop()', but 'list1.pop(t)'; this is a way to tell\nthat the implementation of all the methods needs to be changed, in order\nto do their work \"transactionally\".  This means that instead of really\nchanging the global memory in which the list is stored, it must instead\nrecord the change in the transation object.  If our interpreter is\nwritten in C, as CPython is, then we need to write it explicitly\neverywhere.  If it is written instead in a higher-level language, as\nPyPy is, then we can add this behavior as as set of translation rules, and\napply them automatically wherever it is necessary.  Moreover, it can be\na translation-time option: you can either get the current \"pypy\" with a\nGIL, or a version with STM, which would be slower due to the extra\nbookkeeping.  (How much slower?  I have no clue, but as a wild guess,\nmaybe between 2 and 5 times slower.  That is fine if you have enough\ncores, as long as it scales nicely :-)\nA final note: as STM research is very recent (it started around 2003),\nthere are a number of variants around, and it's not clear yet which one\nis better in which cases.  As far as I can tell, the approach described\nin \"A Comprehensive Strategy for Contention Management in Software\nTransactional Memory\" seems to be one possible state-of-the-art; it also\nseems to be \"good enough for all cases\".\nSo, when will it be done?  I cannot say yet.  It is still at the idea\nstage, but I think that it can work.  How long would it take us to\nwrite it?  Again no clue, but we are looking at many months rather\nthan many days.  This is the sort of thing that I would\nlike to be able to work on full time after the Eurostars funding\nruns out on September 1.  We are currently looking at ways to use\ncrowdfunding to raise money so that I can do exactly that.  Expect\na blog post about that very soon.  But this looks like a perfect\ncandidate for crowdfunding -- there are at least thousands of you who\nwould be willing to pay 10s of Euros to Kill the GIL.  Now we only\nhave to make this happen.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/06/global-interpreter-lock-or-how-to-kill-8270246310848099963.html"
    },
    {
      "title": "Report back from our survey",
      "text": "Hi all,\nI'm here to report back the results of our survey. First, we're very pleased to\nreport that a number of you guys are happilly running PyPy in production! Most\n(97%) of the respondants using PyPy are using it because it's faster, but a\nfurther 26% (respondants could choose multiple answers) are using it because of\nlower memory usage. Of users who aren't using PyPy, the most common reason was\nC extensions, followed by \"Other\".\nFrom reading the extra comments section there are a few things we've learned:\n\nGoogle docs needs a better UI for this stuff\nA huge number of people want NumPy and SciPy, it was easily the most\nrequested C extension (25% of respondants said somthing about NumPy). We've\nalready blogged on the topic of our plans for NumPy.\nHaving packages in the various OS's repositories would be a big help in\ngetting users up and running.\n\nA huge thanks to everyone who responded! Finally, if you're using PyPy in\nproduction we'd love to get a testimonial from you, if you're willing to spare\na few minutes to give us a quote or two please get in contact with us via our\nmailing list.\nThanks,\nAlex",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/06/report-back-from-our-survey-2083371215707583264.html"
    },
    {
      "title": "PyPy Genova-Pegli Post-EuroPython Sprint June 27 - July 2 2011",
      "text": "The next PyPy sprint will be in Genova-Pegli, Italy, the week after EuroPython\n(which is in Florence, about 3h away by train). This is a fully public sprint:\nnewcomers and topics other than those proposed below are welcome.\n\n\n\nGoals and topics of the sprint\n\n\nNow that we have released 1.5, the sprint itself is going to be mainly\nworking on fixing issues reported by various users.  Possible topics\ninclude, but are not limited to:\n\n\nfixing issues in the bug tracker\nimprove cpyext, the C-API compatibility layer, to support more extension\nmodules\nfinish/improve/merge jitypes2, the branch which makes ctypes JIT friendly\ngeneral JIT improvements\nimprove our tools, like the jitviewer or the buildbot infrastructure\nmake your favorite module/application working on PyPy, if it doesn't yet\n\n\n\n\nOf course this does not prevent people from showing up with a more precise\ninterest in mind  If there are newcomers, we will gladly give introduction\ntalks.\n\n\nSince we are almost on the beach, we can take one day off for summer\nrelaxation and/or tourist visits nearby :-).\n\n\n\n\n\n\nExact times\nThe work days should be 27 June - 2 July 2011.  People may arrive on\nthe 26th already and/or leave on the 3rd.\n\n\n\nLocation & Accomodation\nBoth the sprint venue and the lodging will be at Albergo Puppo in\nGenova-Pegli, Italy.  Pegli is a nice and peaceful little quarter of Genova,\nand the hotel is directly on the beach, making it a perfect place for those\nwho want to enjoy the sea in the middle of the Italian summer, as a quick\nsearch on Google Images shows :-)\n\nThe place has a good ADSL Internet connexion with wireless installed.  You can\nof course arrange your own lodging anywhere but I definitely recommend lodging\nthere too.\nPlease confirm that you are coming so that we can adjust the reservations as\nappropriate.  The prices are as follows, and they include breakfast and a\nparking place for the car, in case you need it:\n\n\nsingle room:  70 \u20ac\ndouble room:  95 \u20ac\ntriple room: 105 \u20ac\n\n\nPlease register by hg:\n\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/genova-pegli-2011/people.txt\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nIn case you want to share a room with someone else but you don't know who,\nplease let us know (either by writing it directly in people.txt or by writing\non the mailing list) and we will try to arrange it.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/pypy-genova-pegli-post-europython-4004229800858530064.html"
    },
    {
      "title": "PyPy Usage Survey",
      "text": "We've been working on PyPy for a long time. But readers of this blog will know\nthat in the past year something has changed: we think PyPy is production ready.\nAnd it's not just us, this week LWN.net wrote an article about how PyPy\nsped up one of their scripts by a factor of three, noting that, \"plans are to\nrun gitdm under PyPy from here on out\". All in all we think PyPy is pretty\ngreat, but not everyone is using it yet, and we want to know why. We want your\nfeedback on why PyPy isn't ready to be your only Python yet, and how we can\nimprove it to make that happen.\nTherefore, we've put together a quick survey, whether you're using PyPy or not\nif you could take a few minutes to fill it out and let us know how we're doing\nwe'd really appreciate it. You can find the form here.\nThanks,\nThe PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/pypy-usage-survey-1402303968715807009.html"
    },
    {
      "title": "Server migration in progress",
      "text": "Hi all,\n\nWe are in the process of migrating the hosting machine for PyPy, moving away from codespeak.net and towards a mixture of custom servers (e.g. for buildbot.pypy.org) and wide-scale services (e.g. for the docs, now at readthedocs.org).\n\nWhen this is done, a proper announce will be posted here.  In the meantime, we have already moved the mailing lists, now hosted on python.org.  The subscribers' list have been copied, so if you didn't notice anything special for the past week, then everything works fine :-)  This concerns pypy-dev, pypy-issue and pypy-commit.  Two notes:\nSome settings have not been copied, notably if you used to disable mail delivery.  Sorry about that; you have to re-enter such settings.\nFollowing the move, about 50 addresses have been dropped for being invalid.  I'm unsure why they were not dropped earlier, but in case sending mail to you from python.org instead of codespeak.net fails, then you have been dropped from the mailing lists, and you need to subscribe again.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/server-migration-in-progress-2113491786141182920.html"
    },
    {
      "title": "Playing with Linear Programming on PyPy",
      "text": "Fancy hi-level interfaces often come with a high runtime overhead\nmaking them slow. Here is an experiment with building such an\ninterface using constructions that PyPy should be good at\noptimizing. The idea is to allow the JIT in PyPy to remove the\noverhead introduced by using a fancy high-level python interface\non top of a low-level C interface. The application considered is\nLinear\nprogramming. It is a tool used to solve linear optimization\nproblems. It can for example be used to find the nonnegative values\nx, y and z that gives the maximum value of\n\n\n\n\n\nwithout violating the constraints\n\n\n\n\n\n\n\nThere exists general purpose solvers for these kind of problems that\nare very fast and can literally handle millions of variables. To use\nthem however the problem has to be transformed into some specific\nmatrix form, and the coefficients of all the matrices\nhas to be passed to the solver using some API. This transformation is\na tedious and error prone step that forces you to work with matrix\nindexes instead of readable variable names. Also it makes maintaining\nan implementation hard since any modification has to be transformed\ntoo.\n\n\nThe example above comes from the manual of\nthe glpk library. That\nmanual continues by describing how to convert this problem into the\nstandard form of glpk (which involves introducing three new variables)\nand then gives the c-code needed to call the\nlibrary. Relating that c-code to the problem above without the\nintermediate explanation of the manual is not easy. A common\nsolution here is to build a hi-level interface that allows a more\nnatural way of defining the matrices and/or allow the equations to be\nentered symbolically. Unfortunately, such interfaces often become\nslow. For the benchmark below for example, \ncvxopt\nrequires 20 minutes to setup a problem that takes 9.43 seconds to solve\n(this seems a bit extreme, am I doing something wrong?).\n\n\nThe high-level interface I constructed on top of the\nglpk library is \npplp and it allows\nthe equations to be entered symbolically. The above problem can be\nsolved using\n\n    lp = LinearProgram()\n    x, y, z = lp.IntVar(), lp.IntVar(), lp.IntVar()\n    lp.objective = 10*x + 6*y + 4*z\n    lp.add_constraint( x + y + z <= 100 )\n    lp.add_constraint( 10*x + 4*y + 5*z <= 600 )\n    lp.add_constraint( 2*x + 2*y + 6*z <= 300 )\n    lp.add_constraint( x >= 0 )\n    lp.add_constraint( y >= 0 )\n    lp.add_constraint( z >= 0 )\n\n    maxval = lp.maximize()\n    print maxval\n    print x.value, y.value, z.value\n\n\n\nTo benchmark the API I used it to solve a \nminimum-cost\n  flow problem with 154072 nodes and 390334 arcs. The C library\n  needs 9.43 s to solve this and the pplp interface adds another 5.89\n  s under PyPy and 28.17 s under CPython. A large amount of time is\n  still spend setting up the problem, but it's a significant\n  improvement over the 20 minutes required on CPython by\n  cvxopt. It is\n  probably not designed to be fast on this kind of benchmark. I have\n  not been able to get cvxopt to work under PyPy. The benchmark used is\n  available here",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/playing-with-linear-programming-on-pypy-4040572987275633047.html"
    },
    {
      "title": "NumPy Follow up",
      "text": "Hi everyone.  Since yesterday's blog post we got a ton of feedback, so we want\nto clarify a few things, as well as share some of the progress we've made, in\nonly the 24 hours since the post.\nReusing the original NumPy\nFirst, a lot of people have asked why we cannot just reuse the original NumPy\nthrough cpyext, our CPython C-API compatibility layer.  We believe this is\nnot the best approach, for a few reasons:\n\n\ncpyext is slow, and always will be slow. It has to emulate far too many\ndetails of the CPython object model that don't exist on PyPy (e.g.,\nreference counting). Since people are using NumPy primarily for speed this\nwould mean that even if we could have a working NumPy, no one would want to\nuse it.  Also, as soon as the execution crosses the cpyext boundary, it\nbecomes invisible to the JIT, which means the JIT has to assume the worst\nand deoptimize stuff away.\nNumPy uses many obscure documented and undocumented details of the CPython\nC-API. Emulating these is often difficult or impossible (e.g. we can't fix\naccessing a struct field, as there's no function call for us to intercept).\nIt's not much fun. Frankly, working on cpyext, debugging the crashes,\nand everything else that goes with it is not terribly fun, especially when\nyou know that the end result will be slow. We've demonstrated we can build\na much faster NumPy, in a way that's more fun, and given that the people\nworking on this are volunteers, it's important to keep us motivated.\n\n\nFinally, we are not proposing to rewrite the entirety of NumPy or, god\nforbid, BLAST, or any of the low level stuff that operates on C-level arrays,\nonly the parts that interface with Python code directly.\nC bindings vs. CPython C-API\nThere are two issues on C code, one has a very nice story, and the other not so\nmuch. First is the case of arbitrary C-code that isn't Python related, things\nlike libsqlite, libbz2, or any random C shared library on your system.\nPyPy will quite happily call into these, and bindings can be developed either\nat the RPython level (using rffi) or in pure Python, using ctypes.\nWriting bindings with ctypes has the advantage that they can run on every\nalternative Python implementation, such as Jython and IronPython.  Moreover,\nonce we merge the jittypes2 branch ctypes calls will even be smoking\nfast.\nOn the other hand there is the CPython C-extension API. This is a very specific\nAPI which CPython exposes, and PyPy tries to emulate. It will never be fast,\nbecause there is far too much overhead in all the emulation that needs to be\ndone.\nOne of the reasons people write C extensions is for speed.  Often, with PyPy\nyou can just forget about C, write everything in pure python and let the JIT to\ndo its magic.\nIn case the PyPy JIT alone isn't fast enough, or you just want to\nuse existing C code then it might make sense to split\nyour C-extension into 2 parts, one which doesn't touch the CPython C-API and\nthus can be loaded with ctypes and called from PyPy, and another which does\nthe interfacing with Python for CPython (where it will be faster).\nThere are also libraries written in C to interface with existing C codebases,\nbut for whom performance is not the largest goal, for these the right solution\nis to try using CPyExt, and if it works that's great, but if it fails the\nsolution will be to rewrite using ctypes, where it will work on all Python\nVMs, not just CPython.\nAnd finally there are rare cases where rewriting in RPython makes more sense,\nNumPy is one of the few examples of these because we need to be able to give\nthe JIT hints on how to appropriately vectorize all of the operations on an\narray.  In general writing in RPython is not necessary for almost any\nlibraries, NumPy is something of a special case because it is so ubiquitous\nthat every ounce of speed is valuable, and makes the way people use it leads to\ncode structure where the JIT benefits enormously from extra hints and the\nability to manipulate memory directly, which is not possible from Python.\nProgress\nOn a more positive note, after we published the last post, several new people\ncame and contributed improvements to the numpy-exp branch. We would like to\nthank all of them:\n\n\nnightless_night contributed: An implementation of __len__, fixed bounds\nchecks on __getitem__ and __setitem__.\nbrentp contributed: Subtraction and division on NumPy arrays.\nMostAwesomeDude contributed: Multiplication on NumPy arrays.\nhodgestar contributed: Binary operations between floats and NumPy arrays.\n\n\nThose last two were technically an outstanding branch we finally merged, but\nhopefully you get the picture. In addition there was some exciting work done by\nregular PyPy contributors. I hope it's clear that there's a place to jump in\nfor people with any level of PyPy familiarity. If you're interested in\ncontributing please stop by #pypy on irc.freenode.net, the pypy-dev mailing\nlist, or send us pull requests on bitbucket.\nAlex",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2011/05/numpy-follow-up-6928627691060102514.html"
    },
    {
      "title": "Numpy in PyPy - status and roadmap",
      "text": "Hello.\nNumPy integration is one of the single most requested features for PyPy. This\npost tries to describe where we are, what we plan (or what we don't plan), and\nhow you can help.\nShort version for the impatient: we are doing experiments, which show that\nPyPy+numpy can be faster and better than CPython+numpy.  We have a plan on how\nto move forward, but at the moment there is lack of dedicated people or money\nto tackle it.\n\nThe slightly longer version\nIntegrating numpy in PyPy has been my pet project on an on-and-off (mostly off)\nbasis over the past two years. There were some experiments, then a long\npause, and then some more experiments which are documented below.\nThe general idea is not to use the existing CPython module, but to\nreimplement numpy in RPython (i.e. the language PyPy is implemented in), thus\nletting our JIT achieve extra speedups. The really cool thing about this part\nis that numpy will automatically benefit of any general JIT improvements,\nwithout any need of extra tweaking.\nAt the moment, there is branch called numpy-exp which contains a\ntranslatable version of a very minimal version of numpy in the module called\nmicronumpy. Example benchmarks show the following:\n\n\n\n\n\n\n\n\u00a0\nadd\niterate\n\nCPython 2.6.5 with numpy 1.3.0\n0.260s (1x)\n4.2 (1x)\n\nPyPy numpy-exp @ 3a9d77b789e1\n0.120s (2.2x)\n0.087 (48x)\n\n\n\nThe add benchmark spends most of the time inside the + operator on\narrays (doing a + a + a + a + a), , which in CPython is implemented in C.\nAs you can see from the table above, the PyPy version is already ~2 times\nfaster. (Although numexpr is still faster than PyPy, but we're working on it).\nThe exact way array addition is implemented is worth another blog post, but in\nshort it lazily evaluates the expression and computes it at the end, avoiding\nintermediate results. This approach scales much better than numexpr\nand can lead to speeding up all the operations that you can perform on matrices.\nThe next obvious step to get even more speedups would be to extend the JIT to\nuse SSE operations on x86 CPUs, which should speed it up by about additional\n2x, as well as using multiple threads to do operations.\niterate is also interesting, but for entirely different reasons. On CPython\nit spends most of the time inside a Python loop; the PyPy version is ~48 times\nfaster, because the JIT can optimize across the python/numpy boundary, showing\nthe potential of this approach, users are not grossly penalized for writing\ntheir loops in Python.\nThe drawback of this approach is that we need to reimplement numpy in RPython,\nwhich takes time.  A very rough estimate is that it would be possible to\nimplement an useful subset of it (for some definition of useful) in a period\nof time comprised between one and three man-months.\nIt also seems that the result will be faster for most cases and the same speed\nas original numpy for other cases. The only problem is finding the dedicated\npersons willing to spend quite some time on this and however, I am willing to\nboth mentor such a person and encourage him or her.\nThe good starting point for helping would be to look at what's already\nimplemented in micronumpy modules and try extending it. Adding a - operator\nor adding integers would be an interesting start. Drop by on #pypy on\nirc.freenode.net or get in contact with developers via some other channel (such\nas the pypy-dev mailing list) if you want to help.\nAnother option would be to sponsor NumPy development. In case you're\ninterested, please get in touch with us or leave your email in comments.\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2011/05/numpy-in-pypy-status-and-roadmap-8332894230779779992.html"
    },
    {
      "title": "PyPy 1.5 Released: Catching Up",
      "text": "We're pleased to announce the 1.5 release of PyPy. This release updates\nPyPy with the features of CPython 2.7.1, including the standard library. Thus\nall the features of CPython 2.6 and CPython 2.7 are now supported. It\nalso contains additional performance improvements. You can download it here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.1. It's fast (pypy 1.5 and cpython 2.6.2 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release includes the features of CPython 2.6 and 2.7. It also includes a\nlarge number of small improvements to the tracing JIT compiler. It supports\nIntel machines running Linux 32/64 or Mac OS X.  Windows is beta (it roughly\nworks but a lot of small issues have not been fixed so far).  Windows 64 is\nnot yet supported.\nNumerous speed achievements are described on our blog. Normalized speed\ncharts comparing pypy 1.5 and pypy 1.4 as well as pypy 1.5 and cpython\n2.6.2 are available on our benchmark website. The speed improvement over 1.4\nseems to be around 25% on average.\n\n\nMore highlights\n\nThe largest change in PyPy's tracing JIT is adding support for loop invariant\ncode motion, which was mostly done by H\u00e5kan Ard\u00f6. This feature improves the\nperformance of tight loops doing numerical calculations.\nThe CPython extension module API has been improved and now supports many more\nextensions. For information on which one are supported, please refer to our\ncompatibility wiki.\nThese changes make it possible to support Tkinter and IDLE.\nThe cProfile profiler is now working with the JIT. However, it skews the\nperformance in unstudied ways. Therefore it is not yet usable to analyze\nsubtle performance problems (the same is true for CPython of course).\nThere is an external fork which includes an RPython version of the\npostgresql.  However, there are no prebuilt binaries for this.\nOur developer documentation was moved to Sphinx and cleaned up.\nand many small things :-)\n\nCheers,\nCarl Friedrich Bolz, Laura Creighton, Antonio Cuni, Maciej Fijalkowski,\nAmaury Forgeot d'Arc, Alex Gaynor, Armin Rigo and the PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2011/04/pypy-15-released-catching-up-302997959079576809.html"
    },
    {
      "title": "Using Tkinter and IDLE with PyPy",
      "text": "We are pleased to announce that Tkinter, the GUI library based on TCL/TK, now\nworks with PyPy.\nTkinter is composed of two parts:\n\n\n_tkinter, a module written in C which interfaces with the TCL world\nTkinter, a pure Python package which wraps _tkinter to expose the\npythonic API we are used to\n\n\n\n\n\n\nThe PyPy version of _tkinter reuses the C code of as found in CPython and\ncompile it through the PyPy C-API compatibility layer, cpyext.  To make it\nwork with PyPy, we had to modify it slightly, in order to remove the\ndependency on some API functions which are not supported by PyPy.  In particular, we\nremoved the dependency on the PyOS_InputHook variable, which allows a nice\nintegration of Tkinter and the Python interactive prompt: the result is that,\nunlike CPython, in PyPy Tk windows created at the interactive prompt are not\nshown until we manually call the mainloop method.  Apart from this\ninconvenience, all the rest works fine.\nAt the moment, _tkinter is not distributed with PyPy because our build\nsystem does not support automatic compilation of C extension.  Instead, it is\nnecessary to install it manually, either directly from source or by\neasy_installing/pip installing tkinter-pypy from PyPI.\nFor everything to work correctly, you need a recent build of PyPy: the\nfollowing is a step-by-step guide to install _tkinter in a PyPy nightly\nbuild for Linux 64 bit; for other architectures, look at the nightly build\npage:\n$ wget https://buildbot.pypy.org/nightly/trunk/pypy-c-jit-43485-1615dfd7d8f1-linux64.tar.bz2\n\n$ tar xfv pypy-c-jit-43485-1615dfd7d8f1-linux64.tar.bz2\n\n$ cd pypy-c-jit-43485-1615dfd7d8f1-linux64/\n\n$ wget https://peak.telecommunity.com/dist/ez_setup.py\n\n$ ./bin/pypy ez_setup.py    # install setuptools\n\n$ ./bin/easy_install tkinter-pypy\n\nOnce you complete the steps above, you can start using Tkinter from your\npython programs.  In particular, you can use IDLE, the IDE which is part of\nthe Python standard library.  To start IDLE, type:\n$ ./bin/pypy -m idlelib.idle\n\nHave fun :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/using-tkinter-and-idle-with-pypy-6156563216925585965.html"
    },
    {
      "title": "Tutorial Part 2: Adding a JIT",
      "text": "This is the second part of a tutorial written by Andrew Brown. The first\npart described how to write an interpreter with PyPy.\n\nAdding JIT\nTranslating RPython to C is pretty cool, but one of the best features of PyPy\nis its ability to generate just-in-time compilers for your interpreter.\nThat's right, from just a couple hints on how your interpreter is structured,\nPyPy will generate and include a JIT compiler that will, at runtime, translate\nthe interpreted code of our BF language to machine code!\nSo what do we need to tell PyPy to make this happen? First it needs to know\nwhere the start of your bytecode evaluation loop is. This lets it keep track of\ninstructions being executed in the target language (BF).\nWe also need to let it know what defines a particular execution frame. Since\nour language doesn't really have stack frames, this boils down to what's\nconstant for the execution of a particular instruction, and what's not. These\nare called \"green\" and \"red\" variables, respectively.\nRefer back to example2.py for the following.\nIn our main loop, there are four variables used: pc, program, bracket_map, and\ntape. Of those, pc, program, and bracket_map are all green variables. They\ndefine the execution of a particular instruction. If the JIT routines see the\nsame combination of green variables as before, it knows it's skipped back and\nmust be executing a loop.  The variable \"tape\" is our red variable, it's what's\nbeing manipulated by the execution.\nSo let's tell PyPy this info. Start by importing the JitDriver class and making\nan instance:\nfrom pypy.rlib.jit import JitDriver\njitdriver = JitDriver(greens=['pc', 'program', 'bracket_map'],\n        reds=['tape'])\n\nAnd we add this line to the very top of the while loop in the mainloop\nfunction:\njitdriver.jit_merge_point(pc=pc, tape=tape, program=program,\n        bracket_map=bracket_map)\n\nWe also need to define a JitPolicy. We're not doing anything fancy, so this is\nall we need somewhere in the file:\ndef jitpolicy(driver):\n    from pypy.jit.codewriter.policy import JitPolicy\n    return JitPolicy()\n\nSee this example at example3.py\nNow try translating again, but with the flag --opt=jit:\n\n$ python ./pypy/pypy/translator/goal/translate.py --opt=jit example3.py\n\nIt will take significantly longer to translate with JIT enabled, almost 8\nminutes on my machine, and the resulting binary will be much larger. When it's\ndone, try having it run the mandelbrot program again. A world of difference,\nfrom 12 seconds compared to 45 seconds before!\nInterestingly enough, you can see when the JIT compiler switches from\ninterpreted to machine code with the mandelbrot example. The first few lines of\noutput come out pretty fast, and then the program gets a boost of speed and\ngets even faster.\n\n\nA bit about Tracing JIT Compilers\nIt's worth it at this point to read up on how tracing JIT compilers work.\nHere's a brief explanation: The interpreter is usually running your interpreter\ncode as written. When it detects a loop of code in the target language (BF) is\nexecuted often, that loop is considered \"hot\" and marked to be traced. The next\ntime that loop is entered, the interpreter gets put in tracing mode where every\nexecuted instruction is logged.\nWhen the loop is finished, tracing stops. The trace of the loop is sent to an\noptimizer, and then to an assembler which outputs machine code. That machine\ncode is then used for subsequent loop iterations.\nThis machine code is often optimized for the most common case, and depends on\nseveral assumptions about the code. Therefore, the machine code will contain\nguards, to validate those assumptions. If a guard check fails, the runtime\nfalls back to regular interpreted mode.\nA good place to start for more information is\nhttps://en.wikipedia.org/wiki/Just-in-time_compilation\n\n\nDebugging and Trace Logs\nCan we do any better? How can we see what the JIT is doing? Let's do two\nthings.\nFirst, let's add a get_printable_location function, which is used during debug\ntrace logging:\ndef get_location(pc, program, bracket_map):\n    return \"%s_%s_%s\" % (\n            program[:pc], program[pc], program[pc+1:]\n            )\njitdriver = JitDriver(greens=['pc', 'program', 'bracket_map'], reds=['tape'],\n        get_printable_location=get_location)\n\nThis function is passed in the green variables, and should return a string.\nHere, we're printing out the BF code, surrounding the currently executing\ninstruction with underscores so we can see where it is.\nDownload this as example4.py and translate it the same as example3.py.\nNow let's run a test program (test.b, which just prints the letter \"A\" 15 or so\ntimes in a loop) with trace logging:\n\n$ PYPYLOG=jit-log-opt:logfile ./example4-c test.b\n\nNow take a look at the file \"logfile\". This file is quite hard to read, so\nhere's my best shot at explaining it.\nThe file contains a log of every trace that was performed, and is essentially a\nglimpse at what instructions it's compiling to machine code for you. It's\nuseful to see if there are unnecessary instructions or room for optimization.\nEach trace starts with a line that looks like this:\n\n[3c091099e7a4a7] {jit-log-opt-loop\n\nand ends with a line like this:\n\n[3c091099eae17d jit-log-opt-loop}\n\nThe next line tells you which loop number it is, and how many ops are in it.\nIn my case, the first trace looks like this:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29  [3c167c92b9118f] {jit-log-opt-loop\n  # Loop 0 : loop with 26 ops\n  [p0, p1, i2, i3]\n  debug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\n  debug_merge_point('+<[>[>_+_<-]>.[<+>-]<<-]++++++++++.', 0)\n  i4 = getarrayitem_gc(p1, i2, descr=<SignedArrayDescr>)\n  i6 = int_add(i4, 1)\n  setarrayitem_gc(p1, i2, i6, descr=<SignedArrayDescr>)\n  debug_merge_point('+<[>[>+_<_-]>.[<+>-]<<-]++++++++++.', 0)\n  debug_merge_point('+<[>[>+<_-_]>.[<+>-]<<-]++++++++++.', 0)\n  i7 = getarrayitem_gc(p1, i3, descr=<SignedArrayDescr>)\n  i9 = int_sub(i7, 1)\n  setarrayitem_gc(p1, i3, i9, descr=<SignedArrayDescr>)\n  debug_merge_point('+<[>[>+<-_]_>.[<+>-]<<-]++++++++++.', 0)\n  i10 = int_is_true(i9)\n  guard_true(i10, descr=<Guard2>) [p0]\n  i14 = call(ConstClass(ll_dict_lookup__dicttablePtr_Signed_Signed), ConstPtr(ptr12), 90, 90, descr=<SignedCallDescr>)\n  guard_no_exception(, descr=<Guard3>) [i14, p0]\n  i16 = int_and(i14, -9223372036854775808)\n  i17 = int_is_true(i16)\n  guard_false(i17, descr=<Guard4>) [i14, p0]\n  i19 = call(ConstClass(ll_get_value__dicttablePtr_Signed), ConstPtr(ptr12), i14, descr=<SignedCallDescr>)\n  guard_no_exception(, descr=<Guard5>) [i19, p0]\n  i21 = int_add(i19, 1)\n  i23 = int_lt(i21, 114)\n  guard_true(i23, descr=<Guard6>) [i21, p0]\n  guard_value(i21, 86, descr=<Guard7>) [i21, p0]\n  debug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\n  jump(p0, p1, i2, i3, descr=<Loop0>)\n  [3c167c92bc6a15] jit-log-opt-loop}\n\nI've trimmed the debug_merge_point lines a bit, they were really long.\nSo let's see what this does. This trace takes 4 parameters: 2 object pointers\n(p0 and p1) and 2 integers (i2 and i3). Looking at the debug lines, it seems to\nbe tracing one iteration of this loop: \"[>+<-]\"\nIt starts executing the first operation on line 4, a \">\", but immediately\nstarts executing the next operation. The \">\" had no instructions, and looks\nlike it was optimized out completely.  This loop must always act on the same\npart of the tape, the tape pointer is constant for this trace. An explicit\nadvance operation is unnecessary.\nLines 5 to 8 are the instructions for the \"+\" operation. First it gets the\narray item from the array in pointer p1 at index i2 (line 6), adds 1 to it and\nstores it in i6 (line 7), and stores it back in the array (line 8).\nLine 9 starts the \"<\" instruction, but it is another no-op. It seems that i2\nand i3 passed into this routine are the two tape pointers used in this loop\nalready calculated. Also deduced is that p1 is the tape array. It's not clear\nwhat p0 is.\nLines 10 through 13 perform the \"-\" operation: get the array value (line 11),\nsubtract (line 12) and set the array value (line 13).\nNext, on line 14, we come to the \"]\" operation. Lines 15 and 16 check whether\ni9 is true (non-zero). Looking up, i9 is the array value that we just\ndecremented and stored, now being checked as the loop condition, as expected\n(remember the definition of \"]\").  Line 16 is a guard, if the condition is not\nmet, execution jumps somewhere else, in this case to the routine called\n<Guard2> and is passed one parameter: p0.\nAssuming we pass the guard, lines 17 through 23 are doing the dictionary lookup\nto bracket_map to find where the program counter should jump to.  I'm not too\nfamiliar with what the instructions are actually doing, but it looks like there\nare two external calls and 3 guards. This seems quite expensive, especially\nsince we know bracket_map will never change (PyPy doesn't know that).  We'll\nsee below how to optimize this.\nLine 24 increments the newly acquired instruction pointer. Lines 25 and 26 make\nsure it's less than the program's length.\nAdditionally, line 27 guards that i21, the incremented instruction pointer, is\nexactly 86. This is because it's about to jump to the beginning (line 29) and\nthe instruction pointer being 86 is a precondition to this block.\nFinally, the loop closes up at line 28 so the JIT can jump to loop body <Loop0>\nto handle that case (line 29), which is the beginning of the loop again. It\npasses in parameters (p0, p1, i2, i3).\n\n\nOptimizing\nAs mentioned, every loop iteration does a dictionary lookup to find the\ncorresponding matching bracket for the final jump. This is terribly\ninefficient, the jump target is not going to change from one loop to the next.\nThis information is constant and should be compiled in as such.\nThe problem is that the lookups are coming from a dictionary, and PyPy is\ntreating it as opaque. It doesn't know the dictionary isn't being modified or\nisn't going to return something different on each query.\nWhat we need to do is provide another hint to the translation to say that the\ndictionary query is a pure function, that is, its output depends only on its\ninputs and the same inputs should always return the same output.\nTo do this, we use a provided function decorator pypy.rlib.jit.purefunction,\nand wrap the dictionary call in a decorated function:\n@purefunction\ndef get_matching_bracket(bracket_map, pc):\n    return bracket_map[pc]\n\nThis version can be found at example5.py\nTranslate again with the JIT option and observe the speedup. Mandelbrot now\nonly takes 6 seconds!  (from 12 seconds before this optimization)\nLet's take a look at the trace from the same function:\n[3c29fad7b792b0] {jit-log-opt-loop\n# Loop 0 : loop with 15 ops\n[p0, p1, i2, i3]\ndebug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\ndebug_merge_point('+<[>[>_+_<-]>.[<+>-]<<-]++++++++++.', 0)\ni4 = getarrayitem_gc(p1, i2, descr=<SignedArrayDescr>)\ni6 = int_add(i4, 1)\nsetarrayitem_gc(p1, i2, i6, descr=<SignedArrayDescr>)\ndebug_merge_point('+<[>[>+_<_-]>.[<+>-]<<-]++++++++++.', 0)\ndebug_merge_point('+<[>[>+<_-_]>.[<+>-]<<-]++++++++++.', 0)\ni7 = getarrayitem_gc(p1, i3, descr=<SignedArrayDescr>)\ni9 = int_sub(i7, 1)\nsetarrayitem_gc(p1, i3, i9, descr=<SignedArrayDescr>)\ndebug_merge_point('+<[>[>+<-_]_>.[<+>-]<<-]++++++++++.', 0)\ni10 = int_is_true(i9)\nguard_true(i10, descr=<Guard2>) [p0]\ndebug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\njump(p0, p1, i2, i3, descr=<Loop0>)\n[3c29fad7ba32ec] jit-log-opt-loop}\n\nMuch better! Each loop iteration is an add, a subtract, two array loads, two\narray stores, and a guard on the exit condition. That's it! This code doesn't\nrequire any program counter manipulation.\nI'm no expert on optimizations, this tip was suggested by Armin Rigo on the\npypy-dev list. Carl Friedrich has a series of posts on how to optimize your\ninterpreter that are also very useful: https://bit.ly/bundles/cfbolz/1\n\n\nFinal Words\nI hope this has shown some of you what PyPy is all about other than a faster\nimplementation of Python.\nFor those that would like to know more about how the process works, there are\nseveral academic papers explaining the process in detail that I recommend. In\nparticular: Tracing the Meta-Level: PyPy's Tracing JIT Compiler.\nSee https://readthedocs.org/docs/pypy/en/latest/extradoc.html",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/tutorial-part-2-adding-jit-8121732841568309472.html"
    },
    {
      "title": "Tutorial: Writing an Interpreter with PyPy, Part 1",
      "text": "This is a guest blog post written by Andrew Brown, with help from the PyPy developers\non the pypy-dev mailing list.\nThis tutorial's master copy and supporting files live at\nhttps://bitbucket.org/brownan/pypy-tutorial/\n\nWhen I first learned about the PyPy project, it took me a while to figure out\nexactly what it was about. For those that don't already know, it's two things:\n\nA set of tools for implementing interpreters for interpreted languages\nAn implementation of Python using this toolchain\n\nThe second part is probably what most people think PyPy is, but this tutorial\nis not about their Python interpreter.  It is about writing your own\ninterpreter for your own language.\nThis is the project I undertook to help myself better understand how PyPy works\nand what it's all about.\nThis tutorial assumes you know very little about PyPy, how it works, and even\nwhat it's all about. I'm starting from the very beginning here.\n\nWhat PyPy Does\nHere's a brief overview of what PyPy can do. Let's say you want to write an\ninterpreted language. This involves writing some kind of source code parser, a\nbytecode interpretation loop, and lots of standard library code.\nThat's quite a bit of work for moderately complicated languages, and there's a\nlot of low level work involved. Writing the parser and compiler code usually\nisn't fun, that's why there are tools out there to generate parsers and\ncompilers for you.\nEven then, you still must worry about memory management in your interpreter,\nand you're going to be re-implementing a lot if you want data types like\narbitrary precision integers, nice general hash tables, and such. It's enough\nto put someone off from implementing their idea for a language.\nWouldn't it be nice if you could write your language in an existing high level\nlanguage like, for example, Python? That sure would be ideal, you'd get all the\nadvantages of a high level language like automatic memory management and rich\ndata types at your disposal.  Oh, but an interpreted language interpreting\nanother language would be slow, right? That's twice as much interpreting going\non.\nAs you may have guessed, PyPy solves this problem. PyPy is a sophisticated\ntoolchain for analyzing and translating your interpreter code to C code (or JVM\nor CLI). This process is called \"translation\", and it knows how to translate\nquite a lot of Python's syntax and standard libraries, but not everything. All\nyou have to do is write your interpreter in RPython, a subset of the Python\nlanguage carefully defined to allow this kind of analysis and translation, and\nPyPy will produce for you a very efficient interpreter.\nBecause efficient interpreters should not be hard to write.\n\n\nThe Language\nThe language I've chosen to implement is dead simple. The language runtime\nconsists of a tape of integers, all initialized to zero, and a single pointer\nto one of the tape's cells. The language has 8 commands, described here:\n\n>\nMoves the tape pointer one cell to the right\n\n\n<\nMoves the tape pointer one cell to the left\n+\nIncrements the value of the cell underneath the pointer\n-\nDecrements the value of the cell underneath the pointer\n\n\n[\nIf the cell under the current pointer is 0, skip to the instruction after\nthe matching ]\n\n\n]\nSkip back to the matching [ (evaluating its condition)\n\n\n.\nPrint out a single byte to stdout from the cell under the pointer\n\n\n,\nRead in a single byte from stdin to the cell under the pointer\n\nAny unrecognized bytes are ignored.\nSome of you may recognize this language. I will be referring to it as BF.\nOne thing to notice is that the language is its own bytecode; there is no\ntranslation from source code to bytecode. This means that the language can be\ninterpreted directly: the main eval loop of our interpreter will operate right\non the source code. This simplifies the implementation quite a bit.\n\n\nFirst Steps\nLet's start out by writing a BF interpreter in plain old Python. The first step\nis sketching out an eval loop:\ndef mainloop(program):\n    tape = Tape()\n    pc = 0\n    while pc < len(program):\n        code = program[pc]\n\n        if code == \">\":\n            tape.advance()\n        elif code == \"<\":\n            tape.devance()\n        elif code == \"+\":\n            tape.inc()\n        elif code == \"-\":\n            tape.dec()\n        elif code == \".\":\n            sys.stdout.write(chr(tape.get()))\n        elif code == \",\":\n            tape.set(ord(sys.stdin.read(1)))\n        elif code == \"[\" and value() == 0:\n            # Skip forward to the matching ]\n        elif code == \"]\" and value() != 0:\n            # Skip back to the matching [\n\n        pc += 1\n\nAs you can see, a program counter (pc) holds the current instruction index. The\nfirst statement in the loop gets the instruction to execute, and then a\ncompound if statement decides how to execute that instruction.\nThe implementation of [ and ] are left out here, but they should change the\nprogram counter to the value of the matching bracket. (The pc then gets\nincremented, so the condition is evaluated once when entering a loop, and once\nat the end of each iteration)\nHere's the implementation of the Tape class, which holds the tape's values as\nwell as the tape pointer:\nclass Tape(object):\n    def __init__(self):\n        self.thetape = [0]\n        self.position = 0\n\n    def get(self):\n        return self.thetape[self.position]\n    def set(self, val):\n        self.thetape[self.position] = val\n    def inc(self):\n        self.thetape[self.position] += 1\n    def dec(self):\n        self.thetape[self.position] -= 1\n    def advance(self):\n        self.position += 1\n        if len(self.thetape) <= self.position:\n            self.thetape.append(0)\n    def devance(self):\n        self.position -= 1\n\nAs you can see, the tape expands as needed to the right, indefinitely. We\nshould really add some error checking to make sure the pointer doesn't go\nnegative, but I'm not worrying about that now.\nExcept for the omission of the \"[\" and \"]\" implementation, this code will work\nfine.  However, if the program has a lot of comments, it will have to skip over\nthem one byte at a time at runtime. So let's parse those out once and for all.\nAt the same time, we'll build a dictionary mapping between brackets, so that\nfinding a matching bracket is just a single dictionary lookup. Here's how:\ndef parse(program):\n    parsed = []\n    bracket_map = {}\n    leftstack = []\n\n    pc = 0\n    for char in program:\n        if char in ('[', ']', '<', '>', '+', '-', ',', '.'):\n            parsed.append(char)\n\n            if char == '[':\n                leftstack.append(pc)\n            elif char == ']':\n                left = leftstack.pop()\n                right = pc\n                bracket_map[left] = right\n                bracket_map[right] = left\n            pc += 1\n\n    return \"\".join(parsed), bracket_map\n\nThis returns a string with all invalid instructions removed, and a dictionary\nmapping bracket indexes to their matching bracket index.\nAll we need is some glue code and we have a working BF interpreter:\ndef run(input):\n    program, map = parse(input.read())\n    mainloop(program, map)\n\nif __name__ == \"__main__\":\n    import sys\n    run(open(sys.argv[1], 'r'))\n\nIf you're following along at home, you'll also need to change the signature of\nmainloop() and implement the bracket branches of the if statement. Here's the\ncomplete example: example1.py\nAt this point you can try it out to see that it works by running the\ninterpreter under python, but be warned, it will be very slow on the more\ncomplex examples:\n\n$ python example1.py 99bottles.b\n\nYou can find mandel.b and several other example programs (not written by me) in\nmy repository.\n\n\nPyPy Translation\nBut this is not about writing a BF interpreter, this is about PyPy. So what\ndoes it take to get PyPy to translate this into a super-fast executable?\nAs a side note, there are some simple examples in the pypy/translator/goal\ndirectory of the PyPy source tree that are helpful here. My starting point for\nlearning this was the example \"targetnopstandalone.py\", a simple hello world\nfor PyPy.\nFor our example, the module must define a name called \"target\" which returns the\nentry point. The translation process imports your module and looks for that\nname, calls it, and the function object returned is where it starts the\ntranslation.\ndef run(fp):\n    program_contents = \"\"\n    while True:\n        read = os.read(fp, 4096)\n        if len(read) == 0:\n            break\n        program_contents += read\n    os.close(fp)\n    program, bm = parse(program_contents)\n    mainloop(program, bm)\n\ndef entry_point(argv):\n    try:\n        filename = argv[1]\n    except IndexError:\n        print \"You must supply a filename\"\n        return 1\n\n    run(os.open(filename, os.O_RDONLY, 0777))\n    return 0\n\ndef target(*args):\n    return entry_point, None\n\nif __name__ == \"__main__\":\n    entry_point(sys.argv)\n\nThe entry_point function is passed the command line arguments when you run the\nresulting executable.\nA few other things have changed here too. See the next section...\n\n\nAbout RPython\nLet's talk a bit about RPython at this point. PyPy can't translate arbitrary\nPython code because Python is a bit too dynamic. There are restrictions on what\nstandard library functions and what syntax constructs one can use. I won't be\ngoing over all the restrictions, but for more information see\nhttps://readthedocs.org/docs/pypy/en/latest/coding-guide.html#restricted-python\nIn the example above, you'll see a few things have changed.  I'm now using low\nlevel file descriptors with os.open and os.read instead of file objects.\nThe implementation of \".\" and \",\" are similarly tweaked (not shown above).\nThose are the only changes to make to this code, the rest is simple enough for\nPyPy to digest.\nThat wasn't so hard, was it? I still get to use dictionaries, expandable lists,\nand even classes and objects! And if low level file descriptors are too low for\nyou, there are some helpful abstractions in the rlib.streamio module included\nwith PyPy's \"RPython standard library.\"\nFor the example thus far, see example2.py\n\n\nTranslating\nIf you haven't already, check yourself out the latest version of PyPy from\ntheir bitbucket.org repository:\n\n$ hg clone https://bitbucket.org/pypy/pypy\n\n(A recent revision is necessary because of a bugfix that makes my example\npossible)\nThe script to run is in \"pypy/translator/goal/translate.py\". Run this script,\npassing in our example module as an argument.\n[A note added much later: this script has been moved to \"rpython/bin/rpython\".]\n\n$ python ./pypy/pypy/translator/goal/translate.py example2.py\n\n(You can use PyPy's python interpreter for extra speed, but it's not necessary)\nPyPy will churn for a bit, drawing some nice looking fractals to your console\nwhile it works. It takes around 20 seconds on my machine.\nThe result from this is an executable binary that interprets BF programs.\nIncluded in my repository are some example BF programs, including a mandelbrot\nfractal generator, which takes about 45 seconds to run on my computer. Try it\nout:\n\n$ ./example2-c mandel.b\n\nCompare this to running the interpreter un-translated on top of python:\n\n$ python example2.py mandel.b\n\nTakes forever, doesn't it?\nSo there you have it. We've successfully written our own interpreter in RPython\nand translated it with the PyPy toolchain.\n\n(more in the next blog post...)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/tutorial-writing-interpreter-with-pypy-3785910476193156295.html"
    },
    {
      "title": "PyPy G\u00f6teborg Post-Easter Sprint April 25 - May 1 2011",
      "text": "The next PyPy sprint will be in Gothenburg, Sweden. It is a public sprint,\nvery suitable for newcomers.  We'll focus on making the 1.5 release (if\nit hasn't already happened) and whatever interests the Sprint attendees.\n\nTopics and goals\nThe main goal is to polish and release PyPy 1.5, supporting Python 2.7\nas well as the last few months' improvements in the JIT (provided that\nit hasn't already happened).  Other topics:\n\nGoing over our documentation, and classifying our docs in terms of\nmouldiness.  Deciding what needs writing, and maybe writing it.\nHelping people get their code running with PyPy\nmaybe work on EuroPython Training, and talks\nSummer of Code preparation\nspeed.pypy.org\nany other programming task is welcome too -- e.g. tweaking the\nPython or JavaScript interpreter, Stackless support, and so on.\n\n\n\nLocation\nThe sprint will be held in the apartment of Laura Creighton and Jacob Hall\u00e9n\nwhich is at G\u00f6tabergsgatan 22 in Gothenburg, Sweden.  Here is a map.  This is\nin central Gothenburg.  It is between the tram stops of Vasaplatsen and\nValand, (a distance of 4 blocks) where many lines call -- the 2, 3, 4, 5,\n7, 10 and 13.\nProbably cheapest and not too far away is to book accomodation at SGS\nVeckobostader. The  Elite Park Avenyn Hotel is a luxury hotel just a\nfew blocks away. There are scores of hotels a short walk away from the\nsprint location, suitable for every budget, desire for luxury, and desire\nfor the unusual.  You could, for instance, stay on a boat.  Options are\ntoo numerous to go into here. Just ask in the mailing list or on the blog.\nHours will be\nfrom 10:00 until people have had enough.  It's a good idea to arrive a\nday before the sprint starts and leave a day later.  In the middle of\nthe sprint there usually is a break day and it's usually ok to take\nhalf-days off if you feel like it.\n\n\nGood to Know\nSweden is not part of the Euro zone. One SEK (krona in singular, kronor\nin plural) is roughly 1/10th of a Euro (9.36 SEK to 1 Euro).\nThe venue is central in Gothenburg.  There is a large selection of\nplaces to get food nearby, from edible-and-cheap to outstanding.  We\noften cook meals together, so let us know if you have any food allergies,\ndislikes, or special requirements.\nSweden uses the same kind of plugs as Germany. 230V AC.\nThe Sprint will be held the week following Easter.  This means, as always,\nthat Gothcon will be taking place the weekend before (Easter weekend).\nGothcon, now in its 35 year, is the largest European game players conference.\nSome of you may be interested in arriving early for the board games.\nThe conference site is only in Swedish, alas.  You don't need to register\nin advance unless you are planning to host a tournament, (and it's too\nlate for that anyway).\n\n\nGetting Here\nIf are coming train, you will arrive at the Central Station.  It is\nabout 12 blocks to the site from there, or you can take a tram.\nThere are two airports which are local to G\u00f6teborg, Landvetter (the main\none) and Gothenburg City Airport (where some budget airlines fly).\nIf you arrive at Landvetter  the airport bus stops right downtown at\nElite Park Avenyn Hotel which is the second stop, 4 blocks from the\nSprint site, as well as the end of the line, which is the Central Station.\nIf you arrive at Gothenburg City Airport take the bus to the end of the\nline.  You will be at the  Central Station.\nYou can also arrive by ferry, from either Kiel in Germany or Frederikshavn\nin Denmark.\n\n\nWho's Coming?\nIf you'd like to come, please let us know when you will be arriving and\nleaving, as well as letting us know your interests  We'll keep a list\nof people which we'll update (which you can do so yourself if you\nhave bitbucket pypy commit rights).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/pypy-goteborg-post-easter-sprint-april-16274563331982977.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 4: Benchmarks",
      "text": "This is part 4 and the final part of the series on how to speed up an interpreter\nwritten with PyPy by adding JIT hints to the interpreter. Part 1 described how\nto control the extent of tracing. Part 2 described how to influence the\noptimizer with promotion and pure functions. Part 3 described a simple object\nmodel and how it can be optimized by doing small rewrites. In this (short) post\nI present some benchmarks.\n\nBenchmarks\nFor the benchmarks I ran a subset of the benchmarks on https://speed.pypy.org\nwith CPython and four different executables of PyPy's Python interpreter (all\nwith a JIT). The executables contain all combinations of enabling maps (which\nmake instance attributes fast) and type versions (which makes method lookup\nfast).\n\npypy-slow: contains neither maps nor type versions.\npypy-map: contains maps but not type versions.\npypy-version: contains type versions but not maps.\npypy-full: contains both maps and type versions\n\nThe results are as follows:\n\nThe graph shows the speedup over CPython's numbers. The results are quite\ninteresting. Maps by themselves do not speed up much over the bare JIT, whereas\ntyped versions alone improve on the JIT baseline in many cases. However, maps\nare not useless. In combination with type versions they add a nice improvement\nover just type versions in a number of benchmarks (most notably\nraytrace-simple and richards but also in crypto-pyaes, django\nand go).\nIt's clear that type versions can be arbitrarily effective. A method lookup on a\nclass can be arbitrarily slow, if the inheritance hierarchy becomes deeper and\ndeeper. The full lookup is replaced by one promotion if type versions are\nenabled.\nMaps on the other hand always replace one dict lookup with one promotion. Since\ndict lookups are already very fast, this by itself does not lead to a gigantic\nimprovement. Only in combination with type versions do they show their full\npotential.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with_26-3072929156700508140.html"
    },
    {
      "title": "A thank you to the PSF",
      "text": "This year's PyCon was an incredible time; several members of the PyPy team were\nthere, and we'll be blogging more about our experiences in the coming days.\nHowever, we quickly wanted to extend a thank you to the Python Software\nFoundation (PSF).\nAs you may have heard, on Friday morning at PyCon Jesse Noller handed the PyPy\nteam a check for $10,000, on behalf of the PSF.  This was in recognition of our\nsuccess over the past few years in bringing PyPy from a research project\nto a fast, compliant, production-ready Python implementation, and to allow us\nto continue our work on making it faster and more up-to-date with upstream\nversion changes.\nBeyond the large check, we're grateful for the endorsement this represents,\nnot only of our work on PyPy, but also of all alternatve Python VMs.\nThe PSF has shifted its focus from representing just CPython to representing\nthe Python Language, reguardless of its implementation, something we are very\nappreciative of.\n\nFrom left to right, PyPy people present at PyCon 2011: Maciej Fija\u0142kowski, Armin Rigo, Alex Gaynor, Laura Creighton and Jacob Hall\u00e9n\n\nThank you, PSF.",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2011/03/thank-you-to-psf-5934275567667314914.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 3: Putting it All Together",
      "text": "This is part 3 of the series on how to speed up an interpreter written with\nPyPy by adding JIT hints to the interpreter. Part 1 described how to control\nthe extent of tracing. Part 2 described how to influence the optimizer with\npromotion and pure functions. In this post I describe a worked-out example of\na small object model for a dynamic language and how to make it efficient using\nthe hints described in the previous posts.\n\nA Simple Object Model\nTo implement a dynamic language efficiently, the operations on its objects need\nto be fast. Most dynamic languages have object models that are made by using\ndictionaries everywhere. Let's look at an example of how the JIT can be made to\noptimize such operations.\nFor the purpose of this blog post we will use a very simple and bare-bones\nobject model that just supports very simple classes and instances, without any\ninheritance or any fancy features. The model has classes, which contain methods.\nInstances have a class. Instances have their own attributes. When looking up an\nattribute on an instance, the instances attributes are searched. If the\nattribute is not found there, the class' attributes are searched.\nTo implement this object model, we could use the following RPython code as part\nof the interpreter source code:\nclass Class(object):\n    def __init__(self, name):\n        self.name = name\n        self.methods = {}\n\n    def instantiate(self):\n        return Instance(self)\n\n    def find_method(self, name):\n        result = self.methods.get(name)\n        if result is not None:\n            return result\n        raise AttributeError(name)\n\n    def change_method(self, name, value):\n        self.methods[name] = value\n\n\nclass Instance(object):\n    def __init__(self, cls):\n        self.cls = cls\n        self.attributes = {}\n\n    def getfield(self, name):\n        result = self.attributes.get(name)\n        if result is not None:\n            return result\n        raise AttributeError(name)\n\n    def write_attribute(self, name, value):\n        self.attributes[name] = value\n\n    def getattr(self, name):\n        try:\n            return self.getfield(name)\n        except AttributeError:\n            return self.cls.find_method(name)\n\nIn this straightforward implementation the methods and attributes are just\nstored in dictionaries on the classes/instances. While this object model is very\nsimple it already contains all the hard parts of Python's object model. Both\ninstances and classes can have arbitrary fields, and they are changeable at\nany time.  Moreover, instances can change their class after they have been\ncreated.\nWhen using this object model in\nan interpreter, a huge amount of time will be spent doing lookups in these\ndictionaries. To make the language efficient using a tracing JIT, we need to\nfind a way to get rid of these dictionary lookups somehow.\nLet's assume we trace through code that sums three attributes, such as:\ninst.getattr(\"a\") + inst.getattr(\"b\") + inst.getattr(\"c\")\n\nThe trace could look like this:\n# inst.getattr(\"a\")\nattributes1 = inst.attributes\nresult1 = dict.get(attributes1, \"a\")\nguard(result1 is not None)\n\n# inst.getattr(\"b\")\nattributes2 = inst.attributes\nv1 = dict.get(attributes2, \"b\")\nguard(v1 is None)\ncls1 = inst.cls\nmethods1 = cls.methods\nresult2 = dict.get(methods1, \"b\")\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\nattributes3 = inst.attributes\nv3 = dict.get(attributes3, \"c\")\nguard(v3 is None)\ncls1 = inst.cls\nmethods2 = cls.methods\nresult3 = dict.get(methods2, \"c\")\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nIn this example, the attribute a is found on the instance, but the\nattributes b and c are found on the class. The trace indeed contains\nfive calls to dict.get, which is slow.\n\n\nMaking Instance Attributes Faster Using Maps\nThe first step in making getattr faster in our object model is to optimize\naway the dictionary lookups on the instances. The hints we have looked at in the\ntwo earlier blog posts don't seem to help with the current object model. There is\nno pure function to be seen, and the instance is not a candidate for promotion,\nbecause there tend to be many instances.\nThis is a common problem when trying to apply hints. Often, the interpreter\nneeds a small rewrite to expose the pure functions and nearly-constant objects\nthat are implicitly there. In the case of instance fields this rewrite is not\nentirely obvious. The basic idea is as follows. In theory instances can have\narbitrary fields. In practice however many instances share their layout (i.e.\ntheir set of keys) with many other instances.\nTherefore it makes sense to factor the layout information out of the instance\nimplementation into a shared object. This shared layout object is called a\nmap. Maps are an old idea that comes originally from the SELF language. They are\nalso used by many JavaScript implementations such as V8. I've written about maps\nbefore, so I won't explain them fully again.\nThe rewritten Instance class using maps looks like this:\nclass Map(object):\n    def __init__(self):\n        self.attribute_indexes = {}\n        self.other_maps = {}\n\n    @purefunction\n    def getindex(self, name):\n        return self.attribute_indexes.get(name, -1)\n\n    @purefunction\n    def new_map_with_additional_attribute(self, name):\n        if name not in self.other_maps:\n            newmap = Map()\n            newmap.attribute_indexes.update(self.attribute_indexes)\n            newmap.attribute_indexes[name] = len(self.attribute_indexes)\n            self.other_maps[name] = newmap\n        return self.other_maps[name]\n\n\nEMPTY_MAP = Map()\n\nclass Instance(object):\n    def __init__(self, cls):\n        self.cls = cls\n        self.map = EMPTY_MAP\n        self.storage = []\n\n    def getfield(self, name):\n        map = hint(self.map, promote=True)\n        index = map.getindex(name)\n        if index != -1:\n            return self.storage[index]\n        raise AttributeError(name)\n\n    def write_attribute(self, name, value):\n        map = hint(self.map, promote=True)\n        index = map.getindex(name)\n        if index != -1:\n            self.storage[index] = value\n            return\n        self.map = map.new_map_with_additional_attribute(name)\n        self.storage.append(value)\n\n    def getattr(self, name):\n        try:\n            return self.getfield(name)\n        except AttributeError:\n            return self.cls.find_method(name)\n\nInstances no longer use dictionaries to store their fields. Instead, they have a\nreference to a map, which maps field names to indexes into a storage list. The\nstorage list contains the actual field values. The maps are shared between\nobjects with the same layout. Therefore they have to be immutable, which means\nthat their getindex method is a pure function. When a new attribute is added\nto an instance, a new map needs to be chosen, which is done with the\nnew_map_with_additional_attribute method on the previous map. Now that we have\nintroduced maps, it is safe to promote the map everywhere, because we assume\nthat the number of different instance layouts is small.\nWith this changed instance implementation, the trace we had above changes to the\nfollowing, where 0xb74af4a8 is the memory address of the Map instance that\nhas been promoted:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nindex1 = Map.getindex(map1, \"a\")\nguard(index1 != -1)\nstorage1 = inst.storage\nresult1 = storage1[index1]\n\n# inst.getattr(\"b\")\nmap2 = inst.map\nguard(map2 == 0xb74af4a8)\nindex2 = Map.getindex(map2, \"b\")\nguard(index2 == -1)\ncls1 = inst.cls\nmethods1 = cls.methods\nresult2 = dict.get(methods1, \"b\")\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\nmap3 = inst.map\nguard(map3 == 0xb74af4a8)\nindex3 = Map.getindex(map3, \"c\")\nguard(index3 == -1)\ncls1 = inst.cls\nmethods2 = cls.methods\nresult3 = dict.get(methods2, \"c\")\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nThe calls to Map.getindex can be optimized away, because they are calls to\na pure function and they have constant arguments. That means that index1/2/3\nare constant and the guards on them can be removed. All but the first guard on\nthe map will be optimized away too, because the map cannot have changed in\nbetween. The optimized trace looks like this:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nstorage1 = inst.storage\nresult1 = storage1[0]\n\n# inst.getattr(\"b\")\ncls1 = inst.cls\nmethods1 = cls1.methods\nresult2 = dict.get(methods1, \"b\")\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\ncls2 = inst.cls\nmethods2 = cls2.methods\nresult3 = dict.get(methods2, \"c\")\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nThe index 0 that is used to read out of the storage array is the result\nof the constant-folded getindex call. This trace is already much better than\nthe original one. Now we are down from five dictionary lookups to just two.\n\n\nVersioning of Classes\nInstances were optimized making the assumption that the total number of\nInstance layouts is small compared to the number of instances. For classes we\nwill make an even stronger assumption. We simply assume that it is rare for\nclasses to change at all. This is not totally reasonable (sometimes classes contain\ncounters or similar things) but for this simple example it is good enough.\nWhat we would really like is if the Class.find_method method were pure.\nBut it cannot be, because it is always possible to change the class itself.\nEvery time the class changes, find_method can potentially return a\nnew value.\nTherefore, we give every class a version number, which is increased every time a\nclass gets changed (i.e., the content of the methods dictionary changes).\nThis means that the result of methods.get() for a given (name,\nversion) pair will always be the same, i.e. it is a pure operation.  To help\nthe JIT to detect this case, we factor it out in a helper method which is\nexplicitly marked as @purefunction. The refactored Class looks like\nthis:\nclass VersionTag(object):\n    pass\n\nclass Class(object):\n    def __init__(self, name):\n        self.name = name\n        self.methods = {}\n        self.version = VersionTag()\n\n    def find_method(self, name):\n        self = hint(self, promote=True)\n        version = hint(self.version, promote=True)\n        result = self._find_method(name, version)\n        if result is not None:\n            return result\n        raise AttributeError(name)\n\n    @purefunction\n    def _find_method(self, name, version):\n        return self.methods.get(name)\n\n    def change_method(self, name, value):\n        self.methods[name] = value\n        self.version = VersionTag()\n\nWhat is interesting here is that _find_method takes the version\nargument but it does not use it at all. Its only purpose is to make the call\npure (because when the version number changes, the result of the call might be\ndifferent than the previous one).\nThe trace with this new class implementation looks like this:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nindex1 = Map.getindex(map1, \"a\")\nguard(index1 != -1)\nstorage1 = inst.storage\nresult1 = storage1[index1]\n\n# inst.getattr(\"b\")\nmap2 = inst.map\nguard(map2 == 0xb74af4a8)\nindex2 = Map.getindex(map2, \"b\")\nguard(index2 == -1)\ncls1 = inst.cls\nguard(cls1 == 0xb7aaaaf8)\nversion1 = cls1.version\nguard(version1 == 0xb7bbbb18)\nresult2 = Class._find_method(cls, \"b\", version1)\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\nmap3 = inst.map\nguard(map3 == 0xb74af4a8)\nindex3 = Map.getindex(map3, \"c\")\nguard(index3 == -1)\ncls2 = inst.cls\nguard(cls2 == 0xb7aaaaf8)\nversion2 = cls2.version\nguard(version2 == 0xb7bbbb18)\nresult3 = Class._find_method(cls, \"c\", version2)\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nThe calls to Class._find_method can now be optimized away, also the\npromotion of the class and the version, except for the first one. The final\noptimized trace looks like this:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nstorage1 = inst.storage\nresult1 = storage1[0]\n\n# inst.getattr(\"b\")\ncls1 = inst.cls\nguard(cls1 == 0xb7aaaaf8)\nversion1 = cls1.version\nguard(version1 == 0xb7bbbb18)\nv2 = result1 + 41\n\n# inst.getattr(\"c\")\nv4 = v2 + 17\nreturn(v4)\n\nThe constants 41 and 17 are the results of the folding of the\n_find_method` calls. This final trace is now very good. It no longer performs any\ndictionary lookups. Instead it contains several guards. The first guard\nchecks that the map is still the same. This guard will fail if the same\ncode is executed with an instance that has another layout. The second guard\nchecks that the class of inst is still the same. It will fail if trace is\nexecuted with an instance of another class. The third guard checks that the\nclass did not change since the trace was produced. It will fail if somebody\ncalls the change_method method on the class.\n\n\nReal-World Considerations\nThe techniques used above for the simple object model are used for the object\nmodel of PyPy's Python interpreter too. Since Python's object model is\nconsiderably more complex, some additional work needs to be done.\nThe first problem that needs to be solved is that Python supports (multiple)\ninheritance. Therefore looking up a method in a class needs to consider the\nwhole method resolution order. This makes the versioning of classes more\ncomplex. If a class is changed its version changes. At the same time, the\nversions of all the classes inheriting from it need to be changed as well,\nrecursively. This makes class changes expensive, but they should be rare.  On the\nother hand, a method lookup in a complex class hierarchy is as optimized in the\ntrace as in our object model here.\nA downside of the versioning of classes that we haven't yet fixed in PyPy, is\nthat some classes do change a lot. An example would be a class that keeps a\ncounter of how many instances have been created so far. This is very slow right\nnow, but we have ideas about how to fix it in the future.\nAnother optimization is that in practice the shape of an instance is correlated\nwith its class. In our code above, we allow both to vary independently.\nIn PyPy's Python interpreter we act somewhat more cleverly. The class of\nan instance is not stored on the instance itself, but on the map. This means\nthat we get one fewer promotion (and thus one fewer guard) in the trace, because the class doesn't need to\nbe promoted after the map has been.\n\n\nMore General Patterns\nThe techniques we used above to make instance and class lookups faster are\napplicable in more general cases than the one we developed them for. A more\nabstract view of maps is that of splitting a data-structure into a part that\nchanges slowly, and a part that changes quickly. In the concrete example of maps\nwe split the original dictionary into the map (the slow-changing part) and the\nstorage array (the quick-changing part). All the computation on the\nslow-changing part can be constant-folded during tracing so that only the\nmanipulation of the quick-changing part remains.\nSimilarly, versions can be used to constant-fold arbitrary functions of large data\nstructures. The version needs to be updated carefully every time the result of\nthis function can change. Therefore this is useful only if the data structure is\nexpected to change slowly.\n\n\nConclusion\nIn this post I showed how to use purefunction and promote to make a\nsmall but still relevant dynamic object model no longer use any dictionary lookups\nafter tracing. Instead a number of guards are inserted into the\ntrace to check whether the assumptions about the objects are still true. This\nmakes operations on objects seriously faster. I plan to write another small post\nthat shows the speed benefits for PyPy's Python interpreter for exactly these\noperations.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with_21-6524148550848694588.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 2: Controlling Optimization",
      "text": "This is part 2 of a series on how to speed up an interpreter written with PyPy\nby adding JIT hints to the interpreter. Part 1 described how to control the\nextent of tracing. In this post I will describe how to add hints that\ninfluence the optimizer.  If applied correctly these techniques can give\nreally big speedups by pre-computing parts of what happens at runtime. On the other\nhand, if applied incorrectly they might lead to code bloat, thus making the\nresulting program actually slower.\n\nBackground\nBefore sending the trace to the backend to produce actual machine code, it is\noptimized.  The optimizer applies a number of techniques to remove or reduce\nthe number of operations: most of these are well known compiler optimization\ntechniques, with the difference that it is easier to apply them in a tracing\nJIT because it only has to deal with linear traces.  Among the techniques:\n\nconstant folding\ncommon subexpression elimination\nallocation removal, as described in the paper that I recently presented at\nPEPM\nstore/load propagation\nloop invariant code motion\n\nIn some places it turns out that if the interpreter author rewrites some parts\nof the interpreter with these optimizations in mind the traces that are produced\nby the optimizer can be vastly improved.\nIn this post I will describe two hints that allow the interpreter author to\nincrease the optimization opportunities for constant folding. For constant\nfolding to work, two conditions need\nto be met:\n\nthe arguments of an operation actually need to all be constant,\ni.e. statically known by the optimizer\nthe operation needs to be pure, i.e. always yield the same result given\nthe same arguments.\n\nThe PyPy JIT generator automatically detects the majority of these conditions.\nHowever, for the cases in which the automatic detection does not work, the\ninterpreter author can apply hints to improve the optimization\nopportunities. There is one kind of hint for both of the conditions above.\nNote: These hints are written by an interpreter developer and applied to the\nRPython source of the interpreter. Normal Python users will never see them.\n\n\nWhere Do All the Constants Come From\nIt is worth clarifying what is a \"constant\" in this context.  A variable of\nthe trace is said to be constant if its value is statically known by the\noptimizer.\nThe simplest example of constants are literal values.  For example, if in the\nRPython source code we have a line like y = x + 1, the second operand will\nbe a constant in the trace.\nHowever, the optimizer can statically know the value of a variable even if it\nis not a constant in the original source code. For example, consider the\nfollowing fragment of RPython code:\nif x == 4:\n    y = y + x\n\nIf the fragment is traced with x being 4, the following trace is\nproduced:\n\nguard(x == 4)\ny = y + x\n\nIn the trace above, the value of x is statically known thanks to the\nguard. Remember that a guard is a runtime check. The above trace will run to\ncompletion when x == 4. If the check fails, execution of the trace is\nstopped and the interpreter continues to run.\nThere are cases in which it is useful to turn an arbitrary variable\ninto a constant value. This process is called promotion and it is an old idea\nin partial evaluation (it's called \"the trick\" there). Promotion is also heavily\nused by Psyco and by all older versions of PyPy's JIT. Promotion is a technique\nthat only works well in JIT compilers, in\nstatic compilers it is significantly less applicable.\nPromotion is essentially a tool for trace specialization. In some places in the\ninterpreter it would be very useful if a variable were constant, even though it\ncould have different values in practice. In such a place, promotion is used. The\ntypical reason to do that is if there is\na lot of computation depending on the value of that variable.\nLet's make this more concrete. If we trace a call to the following function:\ndef f1(x, y):\n    z = x * 2 + 1\n    return z + y\n\nWe get a trace that looks like this:\n\nv1 = x * 2\nz = v1 + 1\nv2 = z + y\nreturn(v2)\n\nObserve how the first two operations could be constant-folded if the value of\nx were known. Let's assume that the value of x can vary, but does so\nrarely, i.e. only takes a few different values at runtime. If this is the\ncase, we can add a hint to promote x, like this:\ndef f2(x, y):\n    x = hint(x, promote=True)\n    z = x * 2 + 1\n    return z + y\n\nThe meaning of this hint is that the tracer should pretend that x is a\nconstant\nin the code that follows. When just running the code, the function has no\neffect, as it simply returns its first argument. When tracing, some extra work\nis done. Let's assume that this changed function is traced with\nthe arguments 4 and 8. The trace will be the same, except for one\noperation at the beginning:\n\nguard(x == 4)\nv1 = x * 2\nz = v1 + 1\nv2 = z + y\nreturn(v2)\n\nThe promotion is turned into a guard operation in the trace. The guard\ncaptures the value of x as it was at runtime. From the point of view of the\noptimizer, this guard is not any different than the one produced by the if\nstatement in the example above. After the guard, the rest of the trace can\nassume that x is equal to 4, meaning that the optimizer will turn this\ntrace into:\n\nguard(x == 4)\nv2 = 9 + y\nreturn(v2)\n\nNotice how the first two arithmetic operations were constant folded. The hope is\nthat the guard is executed quicker than the multiplication and the addition that\nwas now optimized away.\nIf this trace is executed with values of x other than 4, the guard will\nfail, and execution will continue in the interpreter. If the guard fails often\nenough, a new trace will be started from the guard. This other trace will\ncapture a different value of x. If it is e.g. 2, then the optimized\ntrace looks like this:\n\nguard(x == 2)\nv2 = 5 + y\nreturn(v2)\n\nThis new trace will be attached to the guard instruction of the first trace. If\nx takes on even more values, a new trace will eventually be made for all of them,\nlinking them into a chain. This is clearly not desirable, so we should promote\nonly variables that don't vary much. However, adding a promotion hint will never produce wrong\nresults. It might just lead to too much assembler code.\nPromoting integers, as in the examples above, is not used that often.\nHowever, the internals of dynamic language interpreters often\nhave values that are variable but vary little in the context of parts of a user\nprogram. An example would be the types of variables in a user function. Even\nthough in principle the argument to a Python function could be any Python type,\nin practise the argument types tend to not vary much. Therefore it is possible to\npromote the types. In the next blog post I will give a complete example for how\nthis works.\n\n\nDeclaring New Pure Operations\nIn the last section we saw a way to turn arbitrary variables into constants. All\npure operations on these constants can be constant-folded. This works great for\nconstant folding of simple types, e.g. integers. Unfortunately, in the context of an\ninterpreter for a dynamic\nlanguage, most operations actually manipulate objects, not simple types. The\noperations on objects are often not pure and might even have side-effects. If\none reads a field out of a constant reference to an object this cannot\nnecessarily be folded away because the object can be mutated. Therefore, another\nhint is needed.\nAs an example, take the following class:\nclass A(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def f(self, val):\n        self.y = self.compute() + val\n\n    def compute(self):\n        return self.x * 2 + 1\n\nTracing the call a.f(10) of some instance of A yields the following\ntrace (note how the call to compute is inlined):\n\nx = a.x\nv1 = x * 2\nv2 = v1 + 1\nv3 = v2 + val\na.y = v3\n\nIn this case, adding a promote of self in the f method to get rid of the\ncomputation of the first few operations does not help. Even if a is a\nconstant reference to an object, reading the x field does not necessarily\nalways yield the same value. To solve this problem, there is another annotation,\nwhich lets the interpreter author communicate invariants to the optimizer. In\nthis case, she could decide that the x field of instances of A is\nimmutable, and therefore compute\nis a pure function. To communicate this, there is a purefunction decorator.\nIf the code in compute should be constant-folded away, we would change the\nclass as follows:\nclass A(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def f(self, val):\n        self = hint(self, promote=True)\n        self.y = self.compute() + val\n\n    @purefunction\n    def compute(self):\n        return self.x * 2 + 1\n\nNow the trace will look like this:\n\nguard(a == 0xb73984a8)\nv1 = compute(a)\nv2 = v1 + val\na.y = v2\n\nHere, 0xb73984a8 is the address of the instance of A that was used\nduring tracing. The call to compute is not inlined, so that the optimizer\nhas a chance to see it. Since compute function is marked as pure, and its\nargument\nis a constant reference, the call will be removed by the optimizer. The final\ntrace looks like this:\n\nguard(a == 0xb73984a8)\nv2 = 9 + val\na.y = v2\n\n(assuming that the x field's value is 4).\nOn the one hand, the purefunction annotation is very powerful. It can be\nused to constant-fold arbitrary parts of the computation in the interpreter.\nHowever, the annotation also gives you ample opportunity to mess things up. If a\nfunction is annotated to be pure, but is not really, the optimizer can produce\nsubtly wrong code. Therefore, a lot of care has to be taken when using this\nannotation.\n\nObservably Pure Functions\nWhy can't we simply write an analysis to find out that the x fields of the\nA instances is immutable and deduce that compute is a pure function,\nsince it only reads the x field and does not have side effects? This might\nbe possible in this particular case, but in practice the functions that are\nannotate with the purefunction decorator are usually more complex.\nThe easiest example for this is that of a function that uses memoization to\ncache its results. If you analyze this function, it looks like the function has\nside effects, because it changes the memoizing dictionary. However, because this side\neffect is not externally visible, the function from the outside is pure. This is\na property that is not easily detectable by analysis. Therefore, the purity\nof this function needs to be annotated.\n\n\nImmutable Fields\nOne of the most common cases of pure functions is reading immutable\nvalues out of objects. Since this is so common, we have special syntactic sugar\nfor it. A RPython class can have a class attribute _immutable_fields_ set to\na list of strings, listing the fields that cannot be changed. This is equivalent\nto using getters and annotating them with purefunction.\n\n\n\nConclusion\nIn this blog post I explained two more hints that can be used in the source code\nof the interpreter. They are used to influence what the optimizer does with the\ntrace. I realize the examples given here are a bit too small, in the next\ninstallment I will give a worked-out example that puts all the pieces together.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with_15-3281215865169782921.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 1: Controlling the Extent of Tracing",
      "text": "The question I was asked most often during my recent US trip was how exactly\nthe hints work that interpreter authors can use to improve the execution speed\nof the programs running on their interpreters. Since those hints are not really\ndocumented all that well, I decided to write blog posts about them. This is the\nfirst one.\n\nBackground\nFirst, let's recap some basics: PyPy's approach to implementing dynamic\nlanguages is to write an interpreter for\nthe language in RPython. This interpreter can be translated to C and then\nfurther to machine code. The interpreter consists of code in the form of a\nlarge number of generated C functions and some data. Similarly, the user\nprogram consists of functions in the language the interpreter executes.\nAs was explained in a blog post and a paper two years ago, PyPy's JIT is a\nmeta-tracer. Since we want to re-use our tracer for a variety of languages, we\ndon't trace the execution of the user program, but instead trace the execution\nof the interpreter that is running the program. This means that the traces\ndon't contain the bytecodes of the language in question, but RPython-level\noperations that the interpreter did to execute the program.\nOn the other hand, the loops that are traced by the tracer are the loops in the\nuser program. This means that the tracer stops tracing after one iteration of\nthe loop in the user function that is being considered. At this point, it can\nhave traced many iterations of the interpreter main loop.\nHere's a diagram of this process:\n\n\n\nOn the left you see the levels of execution. The CPU executes the binary of\nPyPy's Python interpreter, which consists of RPython functions that have been\ncompiled first to C, then to machine code. Some of these functions contain\nloops, others don't. The interpreter runs a Python program written by a\nprogrammer (the user). If the tracer is used, it traces operations on the level\nof the interpreter. However, the extent of the trace is determined by the loops\nin the user program.\n\n\nHow Far Should Tracing Go\nWhen the tracer encounters a function call at the interpreter level, e.g. the\ninterpreter main loop calling a helper function, it can do one of two things:\n\nit can trace into the helper function, effectively inlining it into the trace.\nit can not trace into the function and instead record a call to that function\nas an operation in the trace. Such a call operation in the trace is sometimes\ncalled residual call.\n\nAs a default, the tracer will try to trace into the helper because that will\ngive more information to the optimizer, allowing it to do a better job. This is\nparticularly important for the allocation removal optimization, because if a\nfreshly allocated object is passed as an argument to a residual call, its\nallocation cannot be optimized away.\nThere is a problem however if the helper function itself contains a loop. The\ntracer records the linear sequence of operations that are being executed. Thus\nwhen it encounters a loop on the interpreter level it records all the\noperations of every iteration of the loop itself, with the net effect of\nunrolling it. The only places where the tracer stops and tries to close the\ntrace is in the main loop of the interpreter. When the tracer encounters the\nmain loop, it also checks whether the original user loop has been closed, and\nthus whether it can stop tracing.\nFor most helper functions in the interpreter that contain loops, fully\nunrolling does not make sense. If a loop is unrolled, the trace is specific to\nthe number of iteration that was seen during tracing. If the trace is later\nexecuted with a different number of iterations, the trace will be left via a\nguard failure, which is inefficient. Therefore the default behaviour of the\ntracer is to never trace into a function on the interpreter level that contains\na loop, but to trace into all non-looping helper functions.\nThis default behaviour is essentially a heuristic, but one that usually makes\nsense. We want to produce just enough traces to make the resulting code\nefficient, but not more. Therefore we trace as much as possible (everything by\ndefault) except the functions which loops where tracing would produce code that\nis less general than it could be.\nAs an example for a helper with a loop, take string concatenation. It loops over\nthe characters of both arguments and copies them over into the result string. It\ndoes not make sense to unroll the loops in this function. If we do that,\nthe resulting trace can only be used for strings of the length that was seen\nduring tracing. In practise, the string lengths are usually different each run,\nmeaning that the trace with unrolling is not run to completion in most cases.\n\n\nInfluencing the Default Behaviour\nSometimes the default behaviour is not actually what is wanted. This is\nsomething the interpreter author has to decide, usually by looking at the traces\nthat are produced and deciding that they should be improved. There are two ways\nin which the default is wrong:\n\nfalse negatives: if a helper function that does contain a loop should\nbe traced into, unrolling the loop.\nfalse positives: if a helper function that does not contain a loop is\ninlined into the trace, but the interpreter author decides that this is not\nhelpful.\n\nIf the interpreter author finds false negatives or false positives, she can fix\nthat by applying a hint to the tracer. These hints take the form of function\ndecorators (which both live in the pypy.rlib.jit module). In the next two\nsubsections I will describe these two function decorators and their use.\n\nUnrolling Functions With Loops\nThe first decorator, used to fix false negatives, is the unroll_safe\ndecorator. It is used to tell the tracer to always trace into a function that\nhas a loop, effectively unrolling the loop. This decorator should be used only\nif the loop in the helper function is expected to always run for the same number\nof iterations. This sounds like a strong restriction, in practise this is less\nsevere: The number of iterations needs to only be the same in the context where\nthe helper functions is traced from.\nIt is easiest to understand this condition via an example. Let's look at the\nBUILD_TUPLE bytecode in Python. It takes one argument, the length n of\nthe tuple being built. The bytecode pops n arguments from the stack, turns\nthem into a tuple and pushes that tuple on the stack. Thus the function that\nimplements BUILD_TUPLE in PyPy's Python interpreter calls a helper\npopvalues which pops n values from the stack and returns them in a list.\nThis helper is implemented with a loop and would thus not be traced into by\ndefault.  The loop in the helper can run for very different numbers of\niterations, because it is used in a variety of places. However, for every\nconcrete BUILD_TUPLE bytecode, the argument will be constant. Therefore it\nis safe (and even necessary) to annotate popvalues with the unroll_safe\ndecorator.\nA different example is the implementation of the isinstance builtin. It is\nused to check whether an object a is an instance of a class B like\nthis: isinstance(a, B). The second argument of the function can also be a\ntuple of classes to check whether an object is an instance of one of a number of\nclasses: isinstance(a, (A, B, C, D)). To implement this second case, the\nimplementation of isinstance contains a loop iterating over the elements of\nthe tuple. The number of loop iterations can vary, but is usually fixed for each\nindividual call site which typically just lists a few classes in the source\ncode. Therefore it is also safe to annotate the implementation of isinstance\nwith the unroll_safe decorator.\n\n\nPreventing the Tracing of Functions\nThe second decorator dont_look_inside is used to fix false positives. It\ntells the JIT to never trace into the decorated function and just always produce\na residual call instead. This decorator is in many ways less important than the\nunrolling one (except for a special situation that I will describe in a\nfollow-up post). It is used if tracing into a function is not expected to yield\nany speed benefits, because the optimizer will not be able to improve it much.\nThis is often the case if the called helper function does not contain any\n\"dynamic\" behaviour. In such a situation it is better to just leave the function\ncall in the trace, because that produces less code.\nAn example would be the import mechanism in Python. It's very unlikely that any\nperformance improvement can be had by turning part of it into assembler.\nTherefore we hide it from the tracer by annotating them with\ndont_look_inside.\n\n\n\nConclusion\nIn this post we discussed two hints that can be used to control precisely which\nparts of the interpreter should be meta-traced. If these hints are used\ncarefully, this can go a long way to making the interpreter produce traces that\ncontain exactly the interesting part of the execution, and will contain calls to\nthe functions that can not be optimized by tracing techniques.\nIn the next part of this series I will discuss a different set of hints that can\nbe used to strongly optimize traces.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with-871085470935630424.html"
    },
    {
      "title": "Bay Area 2011 Tour Summary",
      "text": "We spent the week in the San Francisco Bay Area showing off PyPy.\nHere are notes and photos of the tour.\n\nDay 1: Google SF\nGoogle has offices in downtown San Francisco.  They are at a beautiful\nplace and the views are spectacular.  We thank Wesley Chun and Guido van\nRossum for organizing this meeting.  Between 25 and 30 engineers showed\nup.  Some of them were Python programmers, but others were C++\nprogrammers; and they all seem to have real problems that they want to\nsolve with PyPy.  We didn't have prepared slides so far, so we mostly\nran demos and talked.  As predicted, Google would love SWIG support.\nThey suggested that we rename the translation toolchain (as we vaguely\nthought too) to separate it more from PyPy's Python interpreter; up\nuntil today, many had no idea that they could use PyPy for other\nlanguages.  All in all, it was very positive and people looked forward\nto meeting up at PyCon.\n\n\nDay 2: Stanford\n\n\n\n\nThis was the most academically-oriented talk.  You can find the\nabstract, the slides (PgUp/PgDown to navigate) and the video here.\nThere were around 35 people in the audience, and maybe 1000 real-time\nvideo watchers (who didn't get to ask questions).  The live audience\nseemed to be a mixture of students, professors, and people from the\nlocal industry.  We thank David Allison and Andy Freeman for organizing\nit.  It has been two or three years since they invited me (Armin) and I\nfinally managed to get here :-)\nThe slides are longer than the talk; we focused on the JIT because that\nwas what the audience was most interested in.  They were really\nimpressed at the stability, the tests, and that we don't have lots of\nbugs reported in the JIT of our latest public release.  We later found\nout that many who came to the talk believed that they were going to get\na talk about how we jitted a subset of python because real python is too\nhard -- impossible to do.  They came to heckle with examples of how\npython was impossible.  So they were amazed when the first slide of\nArmin's presentation was \"Python is complicated\", and the next slide\n\"Python is messy\".  It was a positive outcome.  We made new fans :-)\n\n\nDay 3: Yelp\n\n\n\n\n\nAs you can see in the image, tons of people showed up -- ~140.  Thanks\nto Grace Law, who is the coordinator for the SF Python Meet-up, and to\nJimmy Retzlaff and Ashley King-Bishof from Yelp.  Yelp is also located\nin downtown San Francisco.  This looks like the place to be if you are a\nstart-up in California (and not in Silicon Valley): lots of enthusiastic\nyoung people are here, and they are hiring.  Yelp has an enormous open\nspace, suitable for huge parties, and the coolest beer dispensers on the\nplanet, made as a hack-a-thon project by three Yelp engineers (pictured\nbelow):\n\n\n\n\n\n\n\n\n\nBy the way, their management structure seems to be flat.  There are\nalmost no line managers, i.e. managers for the engineering staff;\ninstead they self-organize into teams.  This is not what you expect\nfor the USA; things appear to have changed a lot.\nThe talk was in two sections, \"PyPy from the user's point of view\" and\n\"How the JIT works\".  Good feedback; impressed that we support all of\nPython 2.7 (including all the modules that are in C in the stdlib), and\nimpressed that the Python 3.0 conversion is not considered a big deal by\nus, although we have no precise date yet.  The plan is, of course, just\nto tweak the interpreter until it supports both (by adding the necessary\nconditions); the other aspects like GC and the JIT will not be affected\nat all.\n\n\nDay 4: Dropbox\n\n\n\n\n\n\n\nThis was another place full of excited, successful young people.  The\nCTO looks like he turned 30 last week, and he's been CTO for 4 years\nnow.  The three of us were quite obviously the oldest people there.  We\nfelt old.  They have another great big open barn complex. It's\nloud. Very loud.  Loud refrigerators, loud street noise, loud machinery\nin the walls doing who knows what, loudly.\nThis was the first tech talk at dropbox.  Thanks to Rian Hunter for\norganizing it.  They have a big kitchen, and we held the talk in there.\nThere was a skylight, which made the room too bright, so harder to read\nthe slides than would otherwise be the case.  They were jazzed about our\nvisit, and wanted copies of all the pictures Jacob took before he left.\nThey seemed familiar with Google V8, and thought that how long it took\nto build PyPy was a great incentive for us to make PyPy faster.  They\nare very interested in fast ctypes, fast SWIG, fast Cython.  They were\npleased and surprised that we don't have too much JIT bloat (typically\n~10% of the total RAM usage).\nThe mobile developers want a smaller Python more than a faster one.\nPython takes too much memory given the tiny amount available on a lot of\ncell phones.  Not that we have an answer to this problem now.\nThey were pleased to learn that we will soon be able to JIT ctypes code.\nAnd the fact that Armin knows many ways to segfault CPython was a bit of\na shock.  We talked for an hour after the presentation.  Again, a very\npositive outcome.\n\n\nDays 5 and 6: Noisebridge sprint\n\n\n\nAbout six people showed up for the sprint.  (Late.  Californians really\ndo start the day at 11.)  Noisebridge is a very eclectic place; people\nshow up to do pretty much everything from sewing to breaking apart\nequipment to making robots and beer.  It's donation-driven.  Thanks to\nJim Stockford for volunteering the space and arranging this and helping\nus set up for the sprint.\nDuring the sprint, we did a little bit of everything; there was no clear\npattern.  Ademan worked on sqlite, Greg Price looked to see if his\nsoftware could run on PyPy, Will worked on the documentation, and a few\nof us fixed some more 2.7 tests.  Alex Gaynor and Fijal joined us, too.\n\n\nDay 7: Google Mountain View and Mozilla\nWe gave two talks on the 7th day of our trip so we were already quite\nexhausted. Fortunately new people joined, so the talks were actually split\nbetween multiple people. We would like to thank Peter Norvig and Ben Bayer\nfor inviting us to Google and Andreas Gal, Brendan Eich and Dave Herman\nfor inviting us to Mozilla. Both talks should hopefully appear online\nat some point soon, but as of now we don't have a link.\nIt was pretty incredible to find ourselves at Mozilla talking with at\nleast 15 people who deeply understood the ideas of tracing JITs and\nalso understood why we undertook the decision to generate our JIT\ninstead of writing it. They suffered from having to write JavaScript\nJIT (even multiple ones) by hand, as Armin did with Psyco.  He deeply\nsympathizes. The discussion afterwards was very successful and we're\nlooking forward to cooperating with them.  Many exciting things were\ndiscussed as possibilities.\nNext day we went to Pycon, which is ongoing and a topic for yet another\nblog post.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/bay-area-2011-tour-summary-9117372109664978472.html"
    },
    {
      "title": "US Trip Report: POPL, Microsoft, IBM",
      "text": "Some notes from my recent trip (from 23rd of January to 17th of February) to the\nUS where, I presented PyPy at various scientifically oriented places. In\nsummary, there seems to be quite a bit of interest in PyPy within the research\ncommunity, details below.\n\nPEPM/POPL/STOP\nFrom the 24th to the 29th of January I was in Austin, Texas at the POPL\nconference, where I gave a talk at one of the workshops, PEPM (Partial\nEvaluation and Program Manipulation). The title of our paper is\n\"Allocation Removal by Partial Evaluation in a Tracing JIT\", the abstract is:\n\nThe performance of many dynamic language implementations suffers from high\nallocation rates and runtime type checks. This makes dynamic languages less\napplicable to purely algorithmic problems, despite their growing\npopularity. In this paper we present a simple compiler optimization based\non online partial evaluation to remove object allocations and runtime type\nchecks in the context of a tracing JIT. We evaluate the optimization using\na Python VM and find that it gives good results for all our (real-life)\nbenchmarks.\nThe talk (slides) seemed to be well-received and there was\na good discussion afterwards. PEPM in general was a very enjoyable workshop\nwith many interesting talks on partial evaluation (which I am very interested\nin) and a great keynote by Olivier Danvy about \"A Walk in the Semantic Park\".\nPOPL itself was a bit outside of the area I am most knowledgeable in, most of\nthe talks being on formal topics. Some of the talks that stuck to my mind:\n\n\"The Design of Kodu: A Tiny Visual Programming Language for Children on the\nXbox 360\", the keynote by Matthew MacLaurin from Microsoft Research. I didn't\nknow about Kodu before, and was very impressed by it.\n\n\n\"Automating String Processing in Spreadsheets using Input-Output Examples\"\n(paper) by Sumit Gulwani (also from MS Research) describes a plugin to Excel\nthat can automate many common string processing tasks by giving a couple of\nexamples, which are then abstracted into a generic string manipulation. Very\ncool.\n\n\n\"Dynamic Inference of Static Types for Ruby\" (paper) by   Michael Furr,\nJong-hoon (David) An, Jeffrey S. Foster and Michael Hicks describes an\napproach to type inference that works by observing the actual types seen\nduring unit-testing. Similar things have been done a few times before,\nhowever, the paper actually gives a correctness result.\n\n\n\"The Essence of Compiling with Traces\" (paper) by Shu-Yu Guo and Jens\nPalsberg describes a formalization of a simple imperative language and\nproves that executing it using trace compilation will do exactly the same\nthing than using an interpreter. It also looks at what conditions an\noptimization on traces must fulfill to still produce valid results.\n\nAfter the main conference, I took part in the STOP (Scripts to Programs)\nworkshop. It had a great keynote \"Scripting in a Concurrent World\" by John Field\nabout the Thorn language and a few interesting other talks.\n\n\nMicrosoft Research\nAfter POPL I went to Redmond to visit Microsoft Research for a week,\nspecifically the RiSE group. This is the group that did the SPUR project,\na meta-tracing JIT for C# applied to a JavaScript interpreter in C#. I compared\nPyPy to SPUR last year. I am very grateful for Microsoft for inviting me\nthere.\nAt Microsoft I gave a talk about \"PyPy's Approach to Implementing Dynamic\nLanguages Using a Tracing JIT Compiler\", the slides of which can be found\nhere. The talk was filmed and is online. People seemed to be impressed\nwith the \"product qualities\" of PyPy, e.g. the buildbot infrastructure and\nspeed tracking website.\nThe rest of the time I discussed with various researchers in the RiSE group,\nparticularly with Nikolai Tillmann. We talked a lot about similarities and\ndifferences between SPUR and PyPy and tried to understand our respective projects\nbetter. SPUR is a really great project and I learned a lot in the discussions,\nfor example about the optimizations and heuristics their trace compiler uses.\nAnother very cool project done by the RiSE group that I learned more about is\nPEX. PEX is a unit test generator for C# that tries to produce unit tests for\nso-far untested execution paths within methods. There is an online puzzle\nversion of it, if you want to get an impression of the technology (including a\nvery impressive C# IDE in the browser).\n\n\nIBM\nFor the last part of the trip I stayed in New York City for two weeks,\nmostly as a vacation. However, I also visited IBM Watson Research Center for\ntwo days, to which I had been invited by David Edelsohn.\nThe first day I gave the same presentation I had given at Microsoft (with some\nimprovements to the slides), again it was quite well received. The rest of\nthe time I spent in (very fruitful) discussions with various people and teams,\namong them the Liquid Metal team and the Thorn team.\nThe second day I met with members of the FIORANO group, who are working on\ndynamic compilation for dynamic languages and Java. They explored various ways\nto speed up Python, both by improving the CPython interpreter as well as with\nJIT compilation techniques.\nAnother of their projects is to add a trace compiler to IBM's J9 JVM, about\nwhich the paper \"A Trace-based Java JIT Compiler Retrofitted from a\nMethod-based Compiler\" is going to appear at CGO. I discussed tracing JITs with\nPeng Wu, one of the authors of that paper. Peng tries to systematically look at\nthe various heuristics found in the different VMs that use tracing JITs. This\nis a very different perspective from the one I usually have, focusing on how to\nimprove PyPy's specific heuristics. Therefore that discussion helped me thinking\nabout the issues more generally.\nAnother goal of the group is to try to find benchmarks that are representative\nfor typical Python workloads, which is something that has been done very\ncarefully for Java e.g. when developing the DaCapo benchmark suite. The\nbenchmarks that the Python community uses have not been selected in such a\ncareful and measured way, so I think that trying to be more systematic there is\na very worthwhile endeavour.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/us-trip-report-popl-microsoft-ibm-3874568000250679204.html"
    },
    {
      "title": "PyPy Winter Sprint Report",
      "text": "A few weeks ago I had the great fortune to attend the PyPy winter sprint in Leysin Switzerland. I've wanted to contribute to PyPy for a long time and I thought diving into a sprint might be a good way to get familiar with some of the code. What I wasn't expecting was to be using RPython to implement new methods on built-in Python objects on the first day. The main thing I took away from the sprint was just how easy it is to get involved in developing PyPy (well, some bits of it at least and being surrounded by core developers helps). I wrote up a very short description of how to get started here, but I'll do a longer blog post with examples on my own blog soon(ish).\n\nThe sprint was kicked off by Armin merging the \"fast-forward\" branch of PyPy onto trunk. \"fast-forward\" brings PyPy from Python 2.5 compatibility to Python 2.7. Along with this it brought a large number of test failures, as the sterling work done by Benjamin Peterson and Amaury Forgeot d'Arc was not complete. This immediately set the primary sprint goal to reduce the number of test failures.\n\nWe made a great deal of progress on this front, and you can see how close PyPy is now from the buildbots.\n\nJacob Hall\u00e9n and I started working through the list of tests with failures alphabetically. We made short work of test_asyncore and moved onto test_bytes where I was stuck for the rest of the sprint. I spent much of the remaining days working with Laura Creighton on the pypy bytearray implementation to make it more compatible with Python 2.7. This meant adding new methods, changing some of the Python protocol method implementations and even changing the way that bytearray is constructed. All in all great fun and a great introduction to working with RPython.\n\nA big part of the compatibility with Python 2.7 work was done by Laura and Armin who basically rewrote the math module from scratch. This was needed to incorporate all the improvements made (mostly by Mark Dickinson) in CPython in 2.7. That involved a lot of head-scratching about such subtleties as whether -0.0 should be considered almost equal to 0.0 and other fun problems.\n\n\n\n\nThe first meal together, before everyone had arrived\n\nIf you add on top of this the wonderful people, the beautiful scenery, the Swiss cheese fondues, managing to not kill myself with a days skiing and traditional pypy card games, I can heartily recommend pypy sprints as a close approximation of geek nirvana.\n\n\n\nView of the mountains from the sprint\n\n\nWorking on 2.7 compatibility wasn't the only work that happened during the sprint. Other activities included:\n\nAntonio Cuni worked on the \"jittypes\" branch. This is a reimplementation of the core of the PyPy ctypes code to make it jittable. The goal is that for common cases the jit should be able to turn ctypes calls from Python into direct C level calls. This work was not completed but very close and is great for the future of integrating C libraries with PyPy. As ctypes is also available in CPython and IronPython, and hopefully will be available in Jython soon, integrating C code with Python through ctypes is the most \"implementation portable\" technique.\nDavid Schneider continued his work on the JIT backend for ARM. PyPy has been cross-compilable to ARM for a long time, but bringing the JIT to ARM will provide a *fast* PyPy for ARM, which includes platforms like Android. Again David didn't complete this work but did complete the float support.\nH\u00e5kan Ardo was present for two days and continued his crazy-clever work on JIT optimisations, some of which are described in the Loop invariant code motion blog entry.\nHolger Krekel worked on updating the PyPy test suite to the latest version of py.test and also worked with me on the interminable bytearray changes for part of the sprint.\nNo one was sure what \u00a0Maciej Fija\u0142kowski worked on but he seemed to be quite busy.\n\nI think that was most of the work done during the actual sprint. There was also a great deal of healthy discussion about the future of PyPy. Expect lots more interesting and exciting developments over the coming year.",
      "tags": "sprint",
      "url": "https://www.pypy.org/posts/2011/02/pypy-winter-sprint-report-4155886720346408516.html"
    },
    {
      "title": "The PyPy San Franciso Bay Area Tour 2011",
      "text": "PyPy is coming to the San Francisco Bay Area in the beginning of March with\na series of talks and a mini sprint.\n\n\nWednesday March 2, 4:15 p.m.  Armin Rigo gives\na\ntalk at Stanford.  open to the public.\n\nThursday March 3, 6:00 p.m.  General talk at Yelp, 706 Mission St 9th Floor,\n  San Francisco CA 94103 open to the public.\n\nSaturday and Sunday March 5 and 6.\n  PyPy mini sprint at noisebridge.\n  2169 Mission street between 17th and 18th in San Francisco.  Open to the public.\n\nMonday March 7th, 11:30 a.m.  Google Tech talk in Mountain View at the\n  Googleplex.  Not open to the public (but the video should be available\n  later).\n\nMonday March 7th, 2:30 p.m.  Talk at Mozilla in Mountain View.  Not\n  open to the public (but Mozilla developers can videoconference).\n\n\nFrom the PyPy project team we will have Armin Rigo, Maciej Fija\u0142kowski\n(from 6th March), Laura Creighton and Jacob Hall\u00e9n and possibly\nChristian Tismer attending.\n\nMost of the talks will focus on (some of) the highlights and the\nstatus of pypy:\n\n\nmost Python benchmarks run much faster than with CPython or Psyco\nthe real-world PyPy compiler toolchain itself (200 KLocs) runs twice as fast\nsupports x86 32 and 64bit and is in the process of supporting ARM\nfull compatibility with CPython (more than Jython/IronPython)\nfull (and JIT-ed) ctypes support to call C libraries from Python\nsupports Stackless Python (in-progress)\nnew \"cpyext\" layer which integrates existing CPython C extensions\nan experimental super-fast JIT-compilation of calls to C++ libraries\n\n\nAs is usual for us, there is vastly more material that is available for\nus to cover than time, especially when it comes to possible future\ndirections for PyPy.  We want to reserve a certain amount of time at\neach talk purely to discuss things that are of interest to audience\nmembers.  However, if you already know what you wish we would discuss,\nand are attending a talk (or even if you aren't), please let us know.\nYou can either reply to this blog post, or mail Laura directly at\nlac at openend.se .\n\nApart from getting more technical and project insight, our travel is\nalso a good possibility for companies in the SF area to talk to us\nregarding contracting.  In September 2011 our current \"Eurostars\" research\nproject ends and some of us are looking for ways to continue working on\nPyPy through consulting, subcontracting or hiring.  The two companies,\nOpen End and merlinux, have successfully done a number of such contracts\nand projects in the past.  If you want to talk business or get together for\nlunch or dinner, let us know! If you would like us to come to your company\nand make a presentation, let us know!  If you have any ideas about what\nwe should discuss in a presentation so that you could use it to convince\nthe powers-that-be at your place of employment that investing time and\nmoney in PyPy would be a good idea, let us know!\n\nOn Tuesday March 8th we will be heading for Atlanta for the Python VM\nand Language Summits before attending PyCon.  Maciej Fija\u0142kowski and\nAlex Gaynor will be giving a talk entitled\nWhy is\nPython slow and how can PyPy help?\nMaciej will also be giving the talk\nRunning\nultra large telescopes in Python which is\npartially about his experiences using PyPy in the  Square Kilometer Array\nproject in South Africa.  There will be a PyPy Sprint March 14-17.\nAll are welcome.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/02/pypy-san-franciso-bay-area-tour-2011-6179180737090334330.html"
    },
    {
      "title": "PyPy faster than C on a carefully crafted example",
      "text": "Good day everyone.\nRecent round of optimizations, especially loop invariant code motion\nhas been very good for small to medium examples. There is work ongoing to\nmake them scale to larger ones, however there are few examples worth showing\nhow well they perform. This one following example, besides getting benefits\nfrom loop invariants, also shows a difference between static and dynamic\ncompilation. In fact, after applying all the optimizations C does, only a\nJIT can use the extra bit of runtime information to run even faster.\nThe example is as follows. First Python. I create two files, x.py:\n\ndef add(a, b):\n  return a + b\n\nAnd y.py:\n\nfrom x import add\n\ndef main():\n    i = 0\n    a = 0.0\n    while i < 1000000000:\n        a += 1.0\n        add(a, a)\n        i += 1\n\nmain()\n\nFor C, x.c:\n\ndouble add(double a, double b)\n{\n  return a + b;\n}\n\nand y.c:\n\ndouble add(double a, double b);\n\nint main()\n{\n  int i = 0;\n  double a = 0;\n  while (i < 1000000000) {\n    a += 1.0;\n    add(a, a);\n    i++;\n  }\n}\n\nResults?\n\n1.97s - PyPy\n3.07s - C\n\nCompilation options:\n\nPyPy trunk (386ed41eae0c), running pypy-c y.py\nC - gcc -O3 (GCC 4.4.5 shipped with Ubuntu Maverick)\n\nHence, PyPy 50% faster than C on this carefully crafted example. The reason\nis obvious - static compiler can't inline across file boundaries. In C,\nyou can somehow circumvent that, however, it wouldn't anyway work\nwith shared libraries. In Python however, even when the whole import system\nis completely dynamic, the JIT can dynamically find out what can be inlined.\nThat example would work equally well for Java and other decent JITs, it's\nhowever good to see we work in the same space :-)\nCheers,\nfijal\nEDIT: Updated GCC version",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/02/pypy-faster-than-c-on-carefully-crafted-5614784244310486765.html"
    },
    {
      "title": "A JIT Backend for ARM Processors",
      "text": "In the past few months, I have been developing as a part of my master thesis\nthe ARM backend for the the PyPy JIT, in the arm-backend branch. Currently, it is still work in progress: all integer and object operations are working and\nthe support for floating point is also under development.\nARM processors are very widely used, beeing deployed in servers, some netbooks\nand mainly mobile devices such as phones and tablets. One of our goals is to be\nable to run PyPy on phones, specially on Android. Currently is not yet possible\nto translate and compile PyPy for Android automatically, but there has been\nsome work  on using Android's NDK to compile PyPy's generated C code.\nThe JIT Backend targets the application profile of the ARMv7 instruction set\narchitecture which is found for example in the Cortex-A8 processors used in many Android powered devices and in Apple's A4 processors built into the latest iOS devices. To develop and\ntest the backend we are using a BeagleBoard-xM which has a 1 GHz ARM\nCortex-A8 and 512 MB of RAM running the ARM port of Ubuntu 10.10.\nCurrently on Linux it is possible to translate and cross-compile PyPy's Python\ninterpreter as well as other interpreters with the ARM JIT backend enabled\nusing Scratchbox 2 to provide a build environment and the GNU ARM cross\ncompilation toolchain. So far the backend only supports the Boehm garbage\ncollector which does not produce the best results combined with the JIT, but we\nplan to add support for the other GCs in the future, doing so should increase\nthe performance of PyPy on ARM.\nWhile still debugging the last issues with the backend we already can run some\nsimple benchmarks on Pyrolog, a prolog interpreter written in RPython.\nEven using Boehm as the GC the results look very promising. In the benchmarks\nwe compare Pyrolog to SWI-Prolog, a prolog interpreter written in C, which\nis available from the package repositories for Ubuntu's ARM port.\nThe benchmarks can be found in the pyrolog-bench repository.\n\nBenchmarkSWI-Prolog in ms.Pyrolog in ms.Speedup\n\niterate60.06.010.0\niterate_assert130.06.021.67\niterate_call3310.05.0662.0\niterate_cut60.0359.00.16713\niterate_exception4950.0346.014.306\niterate_failure400.0127.03.1496\niterate_findall740.0No res.\niterate_if140.06.023.333\n\nThe iterate_call benchmark, which constructs a predicate and calls it at\nruntime, with a speedup of 662 times over SWI-Prolog is an example where the\nJIT can show its strength. The Pyrolog interpreter and the JIT treat\ndynamically defined predicates as static ones and can generate optimezed code\nin both cases. Whereas SWI only compiles statically defined rules and has to\nfall back to interpretation on dynamic ones.\nFor simple benchmarks running on PyPy's Python intepreter we see some speedups\nover CPython, but we still need to debug the backend bit more before we can\nshow numbers on more complex benchmarks. So, stay tuned.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/01/jit-backend-for-arm-processors-5994810755839586463.html"
    },
    {
      "title": "PyPy wants you!",
      "text": "If you ever considered contributing to PyPy, but never did so far, this is a\ngood moment to start! :-)\nRecently, we merged the fast-forward branch which brings Python 2.7\ncompatibility, with the plan of releasing a new version of PyPy as soon as all\ntests pass.\nHowever, at the moment there are still quite a few of failing tests because\nof new 2.7 features that have not been implemented yet: many of them are easy\nto fix, and doing it represents a good way to get confidence with the code\nbase, for those who are interested in it. Michael Foord wrote a little howto\nexplaining the workflow for running lib-python tests.\nThus, if you are willing to join us in the effort of having a PyPy compatible\nwith Python 2.7, probably the most sensible option is to come on the #PyPy IRC\nchannel on Freenode, so we can coordinate each other not to fix the same test\ntwice.\nMoreover, if you are a student and are considering participating in the next\nGoogle Summer of Code this is a good time to get into pypy. You have the\nopportunity to get a good understanding of pypy for when you decide what you\nwould like to work on over the summer.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/01/pypy-wants-you-4543209863582915733.html"
    },
    {
      "title": "Loop invariant code motion",
      "text": "Recently, the jit-unroll-loops branch was merged. It implements the\nidea described in \nUsing Escape Analysis Across Loop Boundaries for Specialization.\nThat post does only talk about virtuals, but the idea turned out\nto be more far reaching. After the metainterpreter produces a trace,\nseveral optimizations are applied to the trace before it is turned\ninto binary code. Removing allocations is only one of them. There are also\nfor instance\n\n Heap optimizations that removes memory accesses by reusing results\n  previously read from or written to the same location.\n Reusing of the results of pure operations if the same pure\n  operation is executed twice.\n Removal of redundant guards.\n ...\n\nA lot of these optimizations are in one way or another removing\noperations form the trace and/or reusing previous results. All of these\noptimizations could benefit from being able to operate across loop\nboundaries. Not only in the sense that operations operating on loop\ninvariants could be moved out of the loop entirely. But also that\nresults produced at the end of an iteration could be reused at the\nbeginning of the next even if there are no loop invariants involved.\n\n\n\nThis is achieved by unrolling the trace into two iterations, and\nletting the optimizer work on this two-iteration-trace.\nThe optimizer will now be able to optimize the second iteration more than the\nfirst since it can reuse results from the first iteration. The\noptimized version of the first iteration we call the preamble and the\noptimized version of the second iteration we call the loop. The\npreamble will end with a jump to the loop, while the loop will end\nwith a jump to itself. This means that the preamble will be executed\nonce for the first iteration, the loop will be executed for all following\niterations.\n \n\nSqrt example\nHere is an example of a Python implementation of sqrt using a fairly\nsimple algorithm\n\n\n\n  \n\ndef sqrt(y, n=10000):\n    x = y / 2\n    while n > 0:\n        n -= 1\n        x = (x + y/x) / 2\n    return x\n\n\n\nIf it is called with sqrt(1234.0),  \na fairly long trace is produced. From this trace\nthe optimizer creates\nthe\nfollowing preamble (Loop 1) and loop (Loop 0) \n\n\n\n\n\n\n\nLooking at the preamble, it starts by making sure that it is not \ncurrently being profiled, the guard\non i5, and that the function object have not been changed\nsince the trace was made, the guard on p3. Somewhat\nintermixed with that, the\ninteger variable n is unboxed, by making sure p11\npoints to an integer object and reading out the integer value from\nthat object. \nThese operations are not needed in the\nloop (and have been removed from it) as emitting the same guards again\nwould be redundant and n becomes a virtual before the\nend of the preamble.\n\n        guard_value(i5, 0, descr=<Guard6>) \n        guard_nonnull_class(p11, ConstClass(W_IntObject), descr=<Guard7>) \n        guard_value(p3, ConstPtr(ptr15), descr=<Guard8>) \n        i16 = getfield_gc_pure(p11, descr=<W_IntObject.inst_intval>)\n\n\nNext comes a test and a guard implementing the while statement\nfollowed by the decrementing of n. These operation appear\nboth in the preamble and in the loop\n\n        i18 = int_gt(i16, 0)\n        guard_true(i18, descr=<Guard9>) \n        i20 = int_sub(i16, 1)\n\n\nAfter that the two floating point variables x and y\nare unboxed. Again this is only needed in the preamble. Note how the\nunboxed value of y, called f23, is passed unchanged\nfrom the preamble to the loop in arguments of the jump \nto allow it to be reused. It will not become a virtual\nsince it is never changed within the loop.\n\n        guard_nonnull_class(p12, 17652552, descr=<Guard10>) \n        guard_nonnull_class(p10, 17652552, descr=<Guard11>) \n        f23 = getfield_gc_pure(p10, descr=<W_FloatObject.inst_floatval>)\n        f24 = getfield_gc_pure(p12, descr=<W_FloatObject.inst_floatval>)\n\n\nFollowing that is the actual calculations performed in the loop in\nform of floating point operations (since the function was called with\na float argument). These appear in both the loop\nand the preamble.\n\n        i26 = float_eq(f24, 0.000000)\n        guard_false(i26, descr=<Guard12>) \n        f27 = float_truediv(f23, f24)\n        f28 = float_add(f24, f27)\n        f30 = float_truediv(f28, 2.000000)\n\n\nFinally there are some tests checking if a signal was received\n(such as when the user presses ctrl-C) and thus should execute some\nsignal handler or if we need to hand over to another thread. This is\nimplemented with a counter that is decreased once every iteration. It\nwill go below zero after some specific number of iterations, tunable by\nsys.setcheckinterval. The counter is read from and written to\nsome global location where it also can be made negative by a C-level\nsignal handler. \n\n        i32 = getfield_raw(32479328, descr=<pypysig_long_struct.c_value>)\n        i34 = int_sub(i32, 2)\n        setfield_raw(32479328, i34, descr=<pypysig_long_struct.c_value>)\n        i36 = int_lt(i34, 0)\n        guard_false(i36, descr=<Guard13>) \n        jump(p0, p1, p2, p4, p10, i20, f30, f23, descr=<Loop0>)\n\n\n\nBridges\n\nWhen a guard fails often enough, the meta-interpreter is started again\nto produce a new trace starting at the failing guard. The tracing is\ncontinued until a previously compiled loop is entered. This could\neither be the the same loop that contains the failing guard\nor some completely different loop. If it is the same loop, executing\nthe preamble again maybe be unnecessary.\nIt is preferable to end the bridge with a jump directly to \nthe loop. To achieve this the optimizer tries to produce short\n  preambles that are inlined at the end of bridges allowing\nthem to jump directly to the loop. Inlining is better than jumping to\na common preamble because most of the inlined short preamble can\ntypically be removed again by the optimizer.\nCreating such a short\npreamble is however not always possible. Bridges jumping to loops for which\nno short preamble can be generated have to end with a jump to the\nfull preamble instead.\n\n\n\nThe short preamble is created by comparing the operations in the\npreamble with the operations in the loop. The\noperations that are in the preamble but not in the loop \nare moved to the short preamble whenever it is safe to move them to\nthe front of the operations remaining. In other words, the full preamble\nis equivalent to the short preamble followed by one iteration of the\nloop. \n\n\n\nThis much has currently been implemented. To give the full picture\nhere, there are two more features that \nhopefully will be implemented in the near future.\nThe first is to replace the full preamble, used by the interpreter\nwhen it reaches a compiled loop, with the short preamble.\nThis is currently not done and is probably not as straight forward as\nit might first seem. The problem is where to resume interpreting on a\nguard failure. However, implementing that should save some\nmemory. Not only \nbecause the preamble will become smaller, but mainly because the\nguards will appear either in the loop or in the preamble, but not\nin both (as they do now). That means there will only be a single bridge and \nnot potentially two copies once the guards are traced.\n\n\n\nThe sqrt example above would with a short preamble result in a trace\nlike this\n\n\n\n\n\nIf it is executed long enough, the last guard will be traced to form a\nbridge. The trace will inherit the virtuals from its parent. This can\nbe used to optimize away the part of the inlined short preamble\nthat deals with virtuals. The resulting bridge should look\nsomething like\n\n\n    [p0, p1, p2, p3, p4, f5, i6]\n    i7 = force_token()\n    setfield_gc(p1, i7, descr=<PyFrame.vable_token>)\n    call_may_force(ConstClass(action_dispatcher), p0, p1, descr=<VoidCallDescr>)\n    guard_not_forced(, descr=<Guard19>) \n    guard_no_exception(, descr=<Guard20>) \n\n    guard_nonnull_class(p4, 17674024, descr=<Guard21>) \n    f52 = getfield_gc_pure(p4, descr=<W_FloatObject.inst_floatval>)\n    jump(p1, p0, p2, p3, p4, i38, f53, f52, descr=<Loop0>)\n\n\nHere the first paragraph comes from the traced bridge and the second\nis what remains of the short preamble after optimization. The\nbox p4 is \nnot a virtual (it contains a pointer to y which is never\nchanged), and it is only virtuals \nthat the bridge inherit from it's parents. This is why the last two\noperations currently cannot be removed.\n\n\n\n\nEach time the short preamble is inlined, a new copy of each of the\nguards in it is generated. Typically the short preamble is inlined in\nseveral places and thus there will be several copies of each of those\nguards. \nIf they fail often enough bridges\nfrom them will be traced (as with all guards). But since there\ntypically are several copies of each guard the same bridge\nwill be generated in \nseveral places. To prevent this, mini-bridges from the inlined guards\nare produced already during the inlining. These mini-bridges contain\nnothing but a jump to the preamble.\n\n\nThe mini-bridges needs the arguments of the preamble to be able\nto jump to it. These arguments contain among other things, boxed\nversions of the \nvariables x and y. Those variables are virtuals in\nthe loop, and have to be allocated. Currently those allocations\nare placed in front of the inlined guard. Moving those allocations into\nthe mini-bridges is the  second feature that \nhopefully will be implemented in the near future. \n\nAfter this feature is\nimplemented, the result should look something like\n\n\n\n\n\nMultiple specialized versions\n\nFloating point operations were generated in the trace above\nbecause sqrt was called with a float argument. If it is\ninstead called with an int argument, integer operations will be generated. The\nsomewhat more complex situations is when both int's and float's are\nused as arguments. Then the jit need to generate multiple versions of\nthe same loop, specialized in different ways. The details, given\nbelow, on how this is achieved is somewhat involved. For the casual\nreader it would make perfect sense to skip to the next section here.\n\n\n\nConsider the case when sqrt is first called with a float\nargument (but with n small enough not to generate the\nbridge). Then the trace shown above will be\ngenerated. If sqrt is now called with an int argument, the\nguard in the preamble testing that the type of the input object is float\nwill fail:\n\n        guard_nonnull_class(p12, 17652552, descr=<Guard10>) \n\nIt will fail every iteration, so soon enough a bridge will be\ngenerated from this guard in the preamble. This guard will end with a\njump to the same loop, and the optimizer will try to inline\nthe short preamble at the end of it. This will however fail\nsince now there are two guards on p12. One that makes sure it\nis an int and and one that makes sure it is a float. The optimizer\nwill detect that the second guard will always fail and mark the bridge\nas invalid. Invalid loops are not passed on to the backend for\ncompilation. \n\n\n\nIf a loop is detected to be invalid while inlining the short preamble,\nthe metainterpreter will continue to trace for yet another \niteration of the loop. This new trace can be compiled as above and\nwill produce a new loop with a new preamble that are now specialized\nfor int arguments instead of float arguments. The bridge that\npreviously became invalid will now be tried again. This time inlining\nthe short preamble of the new loop instead. This will produce a set of\ntraces connected like this\n\n\n\n\n(click for some hairy details)\n\n\nThe height of the boxes is this figure represents how many instructions\nthey contain (presuming the missing features from the previous section\nare implemented). Loop 0 is specialized for floats and it's preamble have\nbeen split into two boxes at the failing guard. Loop 2 is specialized\nfor ints and is larger than Loop 0. This is mainly because the integer\ndivision in python does not map to the integer division of the\nmachine, but have to be implemented with several instructions (integer\ndivision in python truncates its result towards minus\ninfinity, while the the machine integer division truncates towards\n0). Also the height of the bridge is about the same as the height of\nLoop 2. This is because it contains a full iteration of the loop.\n\n\n\nA More Advanced Example\n\nLet's conclude with an example that is a bit more advanced, where this unrolling\napproach actually outperforms the previous approach. Consider\nmaking a\nfixed-point\nimplementation of the square root using 16 bit's of decimals. This can be\ndone using the same implementation\nof sqrt but calling it with an object of a class representing\nsuch fixed-point real numbers:\n\n\nclass Fix16(object):\n    def __init__(self, val, scale=True):\n        if isinstance(val, Fix16):\n            self.val = val.val\n        else:\n            if scale:\n                self.val = int(val * 2**16)\n            else:\n                self.val = val\n\n    def __add__(self, other):\n        return  Fix16(self.val + Fix16(other).val, False)\n\n    def __sub__(self, other):\n        return  Fix16(self.val - Fix16(other).val, False)\n\n    def __mul__(self, other):\n        return  Fix16((self.val >> 8) * (Fix16(other).val >> 8), False)\n\n    def __div__(self, other):\n        return  Fix16((self.val << 16) / Fix16(other).val, False)\n\n\n\n\nBelow is a table comparing the runtime of the sqrt function above with\ndifferent argument types on different python interpreters. Pypy 1.4.1\nwas released before the optimizations described in this post were in place\nwhile they are in place in the \nnightly\n  build from January 5, \ndenoted pypy in the table. There are also the running time for the same\nalgorithms implemented in C and compiled with \"gcc -O3\n-march=native\". Tests were executed on a 2.53GHz Intel Core2\nprocessor with n=100000000 iterations.\nComparing the integer versions with C may be considered a\nbit unfair because of the more advanced integer division operator in\npython. The left part of this table shows runtimes of sqrt in\na program containing a single call to sqrt (i.e. only a single\nspecialized version of the loop is needed). The right part shows the\nruntime of sqrt when it has been called with a different\ntype of argument before.\n\n\n\n\n  First callSecond call\n  floatintFix16\u00a0\u00a0\n               floatintFix16\n  cpython\n     28.18 s\n     22.13 s\n     779.04 s\n    \n     28.07 s\n     22.21 s\n     767.03 s    \n  \n  pypy 1.4.1\n     1.20 s\n     6.49 s\n     11.31 s\n    \n     1.20 s\n     6.54 s\n     11.23 s\n  \n  pypy\n     1.20 s\n     6.44 s\n     6.78 s\n    \n     1.19 s\n     6.26 s\n     6.79 s\n  \n  gcc\n     1.15 s\n     1.82 s\n     1.89 s\n    \n     1.15 s\n     1.82 s\n     1.89 s\n  \n\n\n\n\nFor this to work in the last case, when Fix16 is the argument type in\nthe second type, \nthe trace_limit had to be increased from its default value to prevent\nthe metainterpreter from aborting while tracing the second version of\nthe loop. Also sys.setcheckinterval(1000000) were used to prevent the\nbridge from being generated. With the bridge the performance of the\nlast case is significantly worse. Maybe because the optimizer currently\nfails to generate a short preamble for it. But the slowdown\nseems too big for that to be the only explanation. Below are the runtimes\nnumbers with checkinterval set to its default value of 100:\n\n\n  First callSecond call\n  floatintFix16\u00a0\u00a0\n               floatintFix16\n  cpython\n     28.71 s\n     22.09 s\n     781.86 s\n    \n     28.28 s\n     21.92 s\n     761.59 s\n  \n  pypy 1.4.1\n     1.21 s\n     6.48 s\n     11.22 s\n    \n     1.72 s\n     7.58 s\n     12.18 s\n  \n  pypy\n     1.21 s\n     6.27 s\n     7.22 s\n    \n     1.20 s\n     6.29 s\n     90.47 s\n  \n\n\n\nConclusions\nEven though we are seeing speedups in a variety of different small\nbenchmarks, more complicated examples are not affected much by these\noptimizations. It might partly be because larger examples have longer\nand more complicated loops, and thus allowing optimizations to operate\nacross loop boundary will have a smaller relative effect. Another problem is\nthat with more complicated examples there will be more bridges, and bridges\nare currently not handled very well (most of the time all virtuals are\nforced at the end of the bridge as explained above). But moving those\nforcings into the mini bridges should fix that.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/01/loop-invariant-code-motion-1998392217676829154.html"
    },
    {
      "title": "PyPy 1.4.1",
      "text": "Here is PyPy 1.4.1 :-)\n\nUpdate: Win32 binaries available.\n\nEnjoy!\n\nRelease announcement\n\nWe're pleased to announce\nthe 1.4.1 release of PyPy.\nThis release consolidates all the bug fixes that occurred since the\nprevious release.  To everyone that took the trouble to report\nthem, we want to say thank you.\n\nWhat is PyPy\n\nPyPy is a very compliant Python interpreter, almost a drop-in\nreplacement for CPython.  Note that it still only emulates Python\n2.5 by default; the fast-forward branch with Python 2.7\nsupport is slowly getting ready but will only be integrated in\nthe next release.\n\nIn two words, the advantage of trying out PyPy instead of CPython\n(the default implementation of Python) is, for now, the\nperformance.  Not all programs are faster in PyPy, but we are\nconfident that any CPU-intensive task will be much faster, at\nleast if it runs for long enough (the JIT has a slow warm-up\nphase, which can take several seconds or even one minute on the\nlargest programs).\n\nNote again that we do support compiling and using C extension\nmodules from CPython (pypy setup.py install).  However, this\nis still an alpha feature, and the most complex modules typically\nfail for various reasons; others work (e.g. PIL) but take a\nserious performance hit.  Also, for Mac OS X see below.\n\nPlease note also that PyPy's performance was optimized almost\nexclusively on Linux.  It seems from some reports that on Windows\nas well as Mac OS X (probably for different reasons) the\nperformance might be lower.  We did not investigate much so far.\n\nMore highlights\n\n\n\nWe migrated to Mercurial (thanks to Ronny Pfannschmidt and\n  Antonio Cuni) for the effort) and moved to bitbucket.  The new\n  command to check out a copy of PyPy is:\n  hg clone https://bitbucket.org/pypy/pypy\n\nIn long-running processes, the assembler generated by old\n  JIT-compilations is now freed.  There should be no more leak,\n  however long the process runs.\n\nImprove a lot the performance of the binascii module, and\n  of hashlib.md5 and hashlib.sha.\n\nMade sys.setrecursionlimit() a no-op.  Instead, we rely purely\n  on the built-in stack overflow detection mechanism, which also\n  gives you a RuntimeError -- just not at some exact recursion\n  level.\n\nFix argument processing (now e.g. pypy -OScpass works like\n  it does on CPython --- if you have a clue what it does there\n  :-) )\n\ncpyext on Mac OS X: it still does not seem to work.  I get\n  systematically a segfault in dlopen().  Contributions welcome.\n\nFix two corner cases in the GC (one in minimark, one in\n  asmgcc+JIT).  This notably prevented pypy translate.py -Ojit\n  from working on Windows, leading to crashes.\n\nFixed a corner case in the JIT's optimizer, leading to Fatal\n  RPython error: AssertionError.\n\nAdded some missing built-in functions into the 'os' module.\n\nFix ctypes (it was not propagating keepalive information from\n  c_void_p).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/pypy-141-7283625923182122073.html"
    },
    {
      "title": "PyPy migrates to Mercurial",
      "text": "The assiduous readers of this blog surely remember that during the last\nD\u00fcsseldorf sprint in October, we started the process for migrating our main\ndevelopment repository from Subversion to Mercurial.  Today, after more than\ntwo months, the process has finally been completed :-).\nThe new official PyPy repository is hosted on BitBucket.\nThe migration has been painful because the SVN history of PyPy was a mess and\nnone of the existing conversion tools could handle it correctly.  This was\npartly because PyPy started when subversion was still at version 0.9 when some\nbest-practices were still to be established, and partly because we probably\nmanaged to invent all the possible ways to do branches (and even some of the\nimpossible ones: there is at least one commit which you cannot do with the\nplain SVN client but you have to speak to the server by yourself :-)).\nThe actual conversion was possible thanks to the enormous work done by Ronny\nPfannschmidt and his hackbeil tool. I would like to personally thank Ronny\nfor his patience to handle all the various requests we asked for.\nWe hope that PyPy development becomes even more approachable now, at least from\na version control point of view.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/pypy-migrates-to-mercurial-3308736161543832134.html"
    },
    {
      "title": "Oh, and btw: PyPy gets funding through \"Eurostars\"",
      "text": "There is a supporting reason why we made so many advances in the last year:\nfunding through Eurostars, a European research funding program.\nThe title of our proposal (accepted in 2009) is: \"PYJIT - a fast\nand flexible toolkit for dynamic programming languages based on PyPy\".\nAnd the participants are Open End AB, the Heinrich-Heine-Universit\u00e4t\nD\u00fcsseldorf (HHU), and merlinux GmbH.\nIt's not hard to guess what PYJIT is actually about, is it?\nQuoting: \"The PYJIT project will deliver a fast and flexible\nJust-In-Time Compiler toolkit based on PyPy to the market of dynamic\nlanguages.  Our main aim is to showcase our project's results for the\nOpen Source language Python, providing unprecedented levels of\nflexibility and with speed hitherto only available using statically\ntyped languages.\" (Details in German or in Swedish :-)\nA subgoal is to improve our development and testing infrastructure,\nmainly showcased by Holger's recent py.test releases, the testing tool\nused by PyPy for its 16K tests and the speed.pypy.org infrastructure\n(web app programmed by Miquel Torres on his own time).\nThe overall scope of this project is smaller than that of the previous EU project\nfrom 2004 to 2007.  The persons that are (or were) getting money to work\non PyPy are Samuele Pedroni (at Open End), Maciej Fijalkowski (as a\nsubcontractor), Carl Friedrich Bolz, Armin Rigo, Antonio Cuni (all at\nHHU), and Holger Krekel (at merlinux) as well as Ronny Pfannschmidt (as\na subcontractor).\nThe Eurostars funding lasts until August 2011.  What comes afterwards?\nWell, for one, many of the currently funded people have done work without\ngetting funding in previous years.  This will probably continue.\nWe also have non-funded people in the core group right now and we'll\nhope to enlarge it further.  But of course there are still large tasks\nahead which may greatly benefit from funding.  We have setup a\ndonation infrastructure and maybe we can win one or more larger\norganisations to provide higher or regular sums of money to fund future\ndevelopment work.   Another possibility for companies is to pay\nPyPy developers to help and improve PyPy for their particular use cases.\nAnd finally, your help, donations and suggestions are always\nwelcome and overall we hope to convince more and more people it's\nworthwhile to invest into PyPy's future.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/oh-and-btw-pypy-gets-funding-through-3568486750776147382.html"
    },
    {
      "title": "Leysin Winter sprint",
      "text": "Hi all,\n\n\n\n\n\nThe next sprint will be in Leysin, Switzerland, during the week of the 16th-22nd of January 2011.\n\nNow that we have released 1.4, and plan to release 1.4.1 soon, the sprint is going to be mainly working on fixing issues reported by various users.  Of course this does not prevent people from showing up with a more precise interest in mind.\n\nAs usual, the break day on the sprint will likely be a day of skiing :-)\n\nHoping to see you there.\n\n\n\n\n\n\n\n\nUpdate: there are actually a number of branches that we want to polish and merge into trunk: at least fast-forward, jit-unroll-loops, arm-backend and jitypes2.  For more details, see the announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/leysin-winter-sprint-8115212435349091722.html"
    },
    {
      "title": "PyPy 1.4 release aftermath",
      "text": "A couple days have passed since the announcement of the 1.4 release, and this\nis a short summary of what happened afterwards. Let's start with\nnumbers:\n\n16k visits to the release announcement on our blog\nwe don't have download statistics unfortunately\n10k visits to speed center\nmost traffic comes from referring sites, reddit alone creating above a third\nof our traffic\n\nNot too bad for a project that doesn't have a well-established user base.\nLessons learned:\n\nReleases are very important. They're still the major way projects communicate\nwith community, even if we have nightly builds that are mostly stable.\nNo segfaults were reported, no incompatibilities between JIT and normal\ninterpretation. We think that proves (or at least provides a lot of\nexperimental evidence) that our write-once-and-then-transform method is\neffective.\nA lot of people complained about their favorite module in C not working, we\nshould have made it clearer that CPyExt is in alpha state.  Indeed, we\nwould like to know which C extension modules do work :-).\nSome people reported massive speedups, other reported slowdowns compared\nto CPython. Most of those slowdowns relate to modules being inefficient\n(or doing happy nonsense), like ctypes. This is expected, given that\nnot all modules are even jitted (although having them jitted is usually\na matter of a couple of minutes).\nNobody complained about a lack of some stdlib module. We implemented the ones\nwhich are used more often, but this makes us wonder if less used stdlib modules\nhave any users at all.\n\nIn general feedback has been overwhelmingly positive and we would like to\nthank everyone trying (and especially those reporting problems)\nCheers,\nfijal",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/12/pypy-14-release-aftermath-2979780282210978576.html"
    },
    {
      "title": "We are not heroes, just very patient",
      "text": "Inspired by some of the comments to the release that said \"You are heroes\", I though a bit about the longish history of PyPy and hunted around for some of the mailing list posts that started the project. Then I put all this information together into the following timeline:\n\n   timeline \n\nThere is also a larger version of the timeline. Try to click on some of the events, the links usually go to the sprint descriptions. I also tried to find pictures for the sprints but succeeded for only half of them, if anybody still has some, I would be interested. It's kind of fun to browse around in some of the old sprint descriptions to see how PyPy evolved. Some of the current ideas have been around for a long time, some are new. In the description of the releases I put estimates for the speed of the release.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/we-are-not-heroes-just-very-patient-7114408885070101720.html"
    },
    {
      "title": "PyPy 1.4: Ouroboros in practice",
      "text": "We're pleased to announce the 1.4 release of PyPy. This is a major breakthrough\nin our long journey, as PyPy 1.4 is the first PyPy release that can translate\nitself faster than CPython.  Starting today, we are using PyPy more for\nour every-day development.  So may you :) You can download it here:\nhttps://pypy.org/download.html\n\nWhat is PyPy\nPyPy is a very compliant Python interpreter, almost a drop-in replacement\nfor CPython. It is fast (pypy 1.4 and cpython 2.6 comparison).\nNew Features\nAmong its new features, this release includes numerous performance improvements\n(which made fast self-hosting possible), a 64-bit JIT backend, as well\nas serious stabilization. As of now, we can consider the 32-bit and 64-bit\nlinux versions of PyPy stable enough to run in production.\nNumerous speed achievements are described on our blog. Normalized speed\ncharts comparing pypy 1.4 and pypy 1.3 as well as pypy 1.4 and cpython 2.6\nare available on the benchmark website. For the impatient: yes, we got a lot faster!\n\n\nMore highlights\n\nPyPy's built-in Just-in-Time compiler is fully transparent and\nautomatically generated; it now also has very reasonable memory\nrequirements.  The total memory used by a very complex and\nlong-running process (translating PyPy itself) is within 1.5x to\nat most 2x the memory needed by CPython, for a speed-up of 2x.\nMore compact instances.  All instances are as compact as if\nthey had __slots__.  This can give programs a big gain in\nmemory.  (In the example of translation above, we already have\ncarefully placed __slots__, so there is no extra win.)\nVirtualenv support: now PyPy is fully compatible with virtualenv: note that\nto use it, you need a recent version of virtualenv (>= 1.5).\nFaster (and JITted) regular expressions - huge boost in speeding up\nthe re module.\nOther speed improvements, like JITted calls to functions like map().\n\nCheers,\nCarl Friedrich Bolz, Antonio Cuni, Maciej Fijalkowski,\nAmaury Forgeot d'Arc, Armin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/pypy-14-ouroboros-in-practice-5437628000869417542.html"
    },
    {
      "title": "Improving Memory Behaviour to Make Self-Hosted PyPy Translations Practical",
      "text": "In our previous blog post, we talked about how fast PyPy can translate\nitself compared to CPython.  However, the price to pay for the 2x speedup was\nan huge amount of memory: actually, it was so huge that a standard -Ojit\ncompilation could not be completed on 32-bit because it required more than the\n4 GB of RAM that are addressable on that platform.  On 64-bit, it consumed\n8.3 GB of RAM instead of the 2.3 GB needed by CPython.\nThis behavior was mainly caused by the JIT, because at the time we wrote the\nblog post the generated assembler was kept alive forever, together with some\nbig data structure needed to execute it.\nIn the past two weeks Anto and Armin attacked the issue in the jit-free\nbranch, which has been recently merged to trunk.  The branch solves several\nissues. The main idea of the branch is that if a\nloop has not been executed for a certain amount of time (controlled by the new\nloop_longevity JIT parameter) we consider it \"old\" and no longer needed,\nthus we deallocate it.\n(In the process of doing this, we also discovered and fixed an\noversight in the implementation of generators, which led to generators being\nfreed only very slowly.)\nTo understand the freeing of loops some more, let's look at how many loops are\nactually created during a translation.\nThe purple line in the following graph shows how many loops (and bridges) are\nalive at any point in time with an infinite longevity, which is equivalent to\nthe situation we had before the jit-free branch.  By contrast, the blue\nline shows the number of loops that you get in the current trunk: the\ndifference is evident, as now we never have more than 10000 loops alive, while\npreviously we got up to about 37000 ones.  The time on the X axis is expressed\nin \"Giga Ticks\", where a tick is the value read out of the Time Stamp Counter\nof the CPU.\n\n\n\nThe grey vertical bars represent the beginning of each phase of the\ntranslation:\n\nannotate performs control flow graph construction and type inference.\nrtype lowers the abstraction level of the control flow graphs with types to that of C.\npyjitpl constructs the JIT.\nbackendopt optimizes the control flow graphs.\nstackcheckinsertion finds the places in the call graph that can overflow the C stack and inserts checks that raise an exception instead.\ndatabase_c produces a database of all the objects the C code will have to know about.\nsource_c produces the C source code.\ncompile_c calls the compiler to produce the executable.\n\nYou can nicely see, how the number of alive graphs drops shortly after the\nbeginning of a new phase.\nThose two fixes, freeing loops and generators, improve the memory usage greatly:\nnow, translating PyPy\non PyPy on 32-bit consumes 2 GB of RAM, while on CPython it consumes 1.1 GB.\nThis result can even be improved somewhat, because we are not actually freeing\nthe assembler code itself, but\nonly the large data structures around it; we can consider it as a residual\nmemory leak of around 150 MB in this case.  This will be fixed in the\njit-free-asm branch.\nThe following graph shows the memory usage in more detail:\n\n\nthe blue line (cpython-scaled) shows the total amount of RAM that the\nOS allocates for CPython.  Note that the X axis (the time) has been\nscaled down so that it spans as much as the PyPy one, to ease the\ncomparison. Actually, CPython took more than twice as much time as PyPy to\ncomplete the translation\nthe red line (VmRss) shows the total amount of RAM that the\nOS allocates for PyPy: it includes both the memory directly handled by\nour GC and the \"raw memory\" that we need to allocate for other tasks, such\nas the assembly code generated by the JIT\nthe brown line (gc-before) shows how much memory is used by the GC\nbefore each major collection\nthe yellow line (gc-after) shows how much memory is used by the GC\nafter each major collection: this represent the amount of memory which is\nactually needed to hold our Python objects.  The difference between\ngc-before and gc-after (the GC delta) is the amout of memory that the GC\nuses before triggering a new major collection\n\n\n\n\n\nBy comparing gc-after and cpython-scaled, we can see that PyPy\nuses mostly the same amount of memory as CPython for storing the application\nobjects (due to reference counting the memory usage in CPython is always very\nclose to the actually necessary memory).  The extra memory\nused by PyPy is due to the GC delta, to the machine code generated by the JIT\nand probably to some other external effect (such as e.g. Memory\nFragmentation).\nNote that the GC delta can be set arbitrarly low (another recent addition --\nthe default value depends on the actual RAM on your computer; it probably\nworks to translate if your computer has precisely 2 GB, because in this\ncase the GC delta and thus the total memory usage will be somewhat\nlower than reported here), but the cost is to have more\nfrequent major collections and thus a higher run-time overhead.  The same is\ntrue for the memory needed by the JIT, which can be reduced by telling the JIT\nto compile less often or to discard old loops more frequently.  As often\nhappens in computer science, there is a trade-off between space and time, and\ncurrently for this particular example PyPy runs twice as fast as CPython by\ndoubling the memory usage. We hope to improve even more on this trade-off.\nOn 64-bit, things are even better as shown by the the following graph:\n\n\n\nThe general shape of the lines is similar to the 32-bit graph. However, the\nrelative difference to CPython is much better: we need about 3 GB of RAM, just\n24% more than the 2.4 GB needed by CPython.  And we are still more than 2x\nfaster!\nThe memory saving is due (partly?) to the vtable ptr optimization, which is\nenabled by default on 64-bit because it has no speed penalty (see\nUnifying the vtable ptr with the GC header).\nThe net result of our work is that now translating PyPy on PyPy is practical\nand takes less than 30 minutes.  It's impressive how quickly you get used to\ntranslation taking half the time -- now we cannot use CPython any more for that\nbecause it feels too slow :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/improving-memory-behaviour-to-make-self-856966667913962461.html"
    },
    {
      "title": "Running large radio telescope software on top of PyPy and twisted",
      "text": "Hello.\nAs some of you already know, I've recently started working on a\nvery large radio telescope at SKA South Africa. This telescope's\noperating software runs almost exclusively on Python (several high throughput\npieces are in C or CUDA or directly executed by FPGAs). Some cool telescope pictures:\n\n\n\n\n\n\n\n(photos courtesy of SKA South Africa)\nMost of the operation software is using the KatCP protocol to talk between devices.\nThe currently used implementation is Open Source software with a custom home built\nserver and client. As part of the experiments, I've implemented a Twisted based\nversion and run in on top of CPython and PyPy for both the default\nimplementation and the one based on Twisted to see how those perform.\nThere are two testing scenarios: the first one is trying to saturate the connection\nby setting up multiple sensors that report state every 10ms, the second one\nis measuring a round-trip between sending a request and receiving the response.\nBoth numbers are measuring the number of requests per 0.2s, so the more the better. On X axis there is a number of simultanously connected clients.\nAll benchmark code is available in the KatCP repository.\nThe results are as follows:\n\n\n\n\n\n\nAs you can see, in general Twisted has larger overhead for a single client\nand scales better as the number of clients increases. That's I think expected,\nsince Twisted has extra layers of indirection. The round trip degradation of\nTwisted has to be investigated, but for us scenario1 is by far more important.\nAll across the board PyPy performs much better than CPython for both\nTwisted and a home-made solution, which I think is a pretty good result.\nNote: we didn't roll this set up into production yet, but there are high\nchances for both twisted and PyPy to be used in some near future.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/running-large-radio-telescope-software-7600337209616168504.html"
    },
    {
      "title": "Efficiently Implementing Python Objects With Maps",
      "text": "As could be foreseen by my Call for Memory Benchmarks post a while ago, I am\ncurrently working on improving the memory behaviour of PyPy's Python\ninterpreter. In this blog post I want to describe the various data a Python\ninstance can store. Then I want to describe how a branch that I did and that was\nrecently merged implements the various features of instances in a very\nmemory-efficient way.\n\nPython's Object Model\nAll \"normal\" new-style Python instances (i.e. instances of subclasses of object\nwithout added declarations) store two (or possibly three) kinds of information.\n\nStoring the Class\nEvery instance knows which class it belongs to. This information is accessible\nvia the .__class__ attribute. It can also be changed to other (compatible\nenough) classes by writing to that attribute.\n\n\nInstance Variables\nEvery instance also has an arbitrary number of attributes stored (also called\ninstance variables). The instance variables used can vary per instance, which is\nnot the case in most other class-based languages: traditionally (e.g. in\nSmalltalk or Java) the class describes the shape of its instances,\nwhich means that the\nset of admissible instance variable names is the same for all instances of a\nclass.\nIn Python on the other hand, it is possible to add arbitrary attributes to an\ninstance at any point. The instance behaves like a dictionary mapping attribute\nnames (as strings) to the attribute values.\nThis is actually how CPython implements instances. Every instance has a\nreference to a dictionary that stores all the attributes of the instance. This\ndictionary can be reached via the .__dict__ attribute. To make things more\nfun, the dictionary can also be changed by writing to that attribute.\n\n\nExample\nAs an example, consider the following code:\nclass A(object):\n    pass\n\ninstance1 = A()\ninstance1.x = 4\ninstance1.y = 6\ninstance1.z = -1\n\ninstance2 = A()\ninstance2.x = 1\ninstance2.y = 2\ninstance2.z = 3\n\nThese two instances would look something like this in memory:\n\n(The picture glosses over a number of details, but it still shows the essential\nissues.)\nThis way of storing things is simple, but unfortunately rather inefficient. Most\ninstances of the same class have the same shape, i.e. the same set of instance\nattribute names. That means that the key part of all the dictionaries is\nidentical (shown grey here). Therefore storing that part repeatedly in all\ninstances is a waste. In addition, dictionaries are themselves rather large.\nSince they are typically implemented as hashmaps, which must not be too full to\nbe efficient, a dictionary will use something like 6 words on average per key.\n\n\nSlots\nSince normal instances are rather large, CPython 2.2 introduced slots, to make\ninstances consume less memory. Slots are a way to fix the set of attributes an\ninstance can have. This is achieved by adding a declaration to a class, like\nthis:\nclass B(object):\n    __slots__ = [\"x\", \"y\", \"z\"]\n\nNow the instances of B can only have x, y and z as attributes\nand don't have a dictionary at all. Instead, the instances of B get\nallocated with enough size to hold exactly the number of instance variables that\nthe class permits. This clearly saves a lot of memory over the dictionary\napproach, but has a number of disadvantages. It is obviously less flexible, as\nyou cannot add additional instance variables to an instance if you happen to\nneed to do that. It also introduces a set of rules and corner-cases that can\nbe surprising sometimes (e.g. instances of a subclass of a class with slots that\ndoesn't have a slots declaration will have a dict).\n\n\n\nUsing Maps for Memory-Efficient Instances\nAs we have seen in the diagram above, the dictionaries of instances of the same\nclass tend to look very similar and share all the keys. The central idea to use\nless memory is to \"factor out\" the common parts of the instance dictionaries\ninto a new object, called a \"map\" (because it is a guide to the landscape of the\nobject, or something). After that factoring out, the representation of the\ninstances above looks something like this:\n\nEvery instance now has a reference to its map, which describes what the instance\nlooks like. The actual instance variables are stored in an array (called\nstorage in the diagram). In the example here, the map describes that the\ninstances have three attributes x, y and z. The numbers after the\nattributes are indexes into the storage array.\nIf somebody adds a new attribute to one of the instances, the map for that\ninstance will be changed to another map that also contains the new attribute,\nand the storage will have to grow a field with the new attribute. The maps are\nimmutable, immortal and reused as much as possible. This means, that two\ninstances of the same class with the same set of attributes will have the same\nmap. This also means that the memory the map itself uses is not too important,\nbecause it will potentially be amortized over many instances.\nNote that using maps makes instances nearly as small as if the correct slots had\nbeen declared in the class. The only overhead needed is the indirection to the\nstorage array, because you can get new instance variables, but not new slots.\nThe concept of a \"map\" that describes instances is kind of old and comes from\nthe virtual machine for the Self programming language. The optimization was\nfirst described in 1989 in a paper by Chambers, Ungar and Lee with the title An\nEfficient Implementation of Self, a Dynamically-Typed Object-Oriented Language\nBased on Prototypes. A similar technique is used in Google's V8 JavaScript\nengine, where the maps are called hidden classes and in the Rhino\nJavaScript engine.\nThe rest of the post describes a number of further details that occur if\ninstances are implemented using maps.\n\nSupporting Dictionaries with Maps\nThe default instance representation with maps as shown above works without\nactually having a dictionary as part of each instance. If a dictionary is\nactually requested, by accessing the .__dict__ attribute, it needs to be\ncreated and cached. The dictionary is not a normal Python dictionary, but a thin\nwrapper around the object that forwards all operations to it. From the user's\npoint of view it behaves like a normal dictionary though (it even has the\ncorrect type).\nThe dictionary needs to be cached, because accessing .__dict__ several times\nshould always return the same dictionary. The caching happens by using a\ndifferent map that knows about the dictionary and putting the dictionary into\nthe storage array:\n\nThings become really complex if the fake dict is used in strange ways. As long\nas the keys are strings, everything is fine. If somebody adds other keys to the\ndict, they cannot be represented by the map any more (which supports only\nattributes, i.e. string keys in the __dict__). If that happens, all the\ninformation of the instance will move into the fake dictionary, like this:\n\nIn this picture, the key -1 was added to the instance's dictionary. Since\nusing the dictionary in arbitrary ways should be rare, we are fine with the\nadditional time and memory that the approach takes.\n\n\nSlots and Maps\nMaps work perfectly together with slots, because the slots can just be stored\ninto the storage array used by the maps as well (in practise there are some\nrefinements to that scheme).  This means that putting a __slots__ on a\nclass has mostly no effect, because the instance only stores the values of the\nattributes (and not the names), which is equivalent to the way slots are stored\nin CPython.\n\n\n\nImplementation Details\nIn the diagrams above, I represented the maps as flat objects. In practise this\nis a bit more complex, because it needs to be efficient to go from one map to\nthe next when new attributes are added. Thus the maps are organized in a tree.\nThe instances with their maps from above look a bit more like this in practise:\n\nEvery map just describes one attribute of the object, with a name and a an\nindex. Every map also has a back field, that points to another map\ndescribing what the rest of the object looks like. This chain ends with a\nterminator, which also stores the class of the object.\nThe maps also contain the information necessary for making a new object of\nclass A. Immediately after the new object has been created, its map is the\nterminator. If the x attribute is added, its maps is changed to the\nsecond-lowest map, and so on. The blue arrows show the sequence of maps that\nthe new object goes through when the attributes x, y, z are added.\nThis representation of maps as chains of objects sounds very inefficient if an\nobject has many attributes. The whole chain has to be walked to find the index.\nThis is true to some extent. The problem goes away in the presence of the JIT,\nwhich knows that the chain of maps is an immutable structure, and will thus\noptimize away all the chain-walking. If the JIT is not used, there are a few\ncaches that try to speed up the walking of this chain (similar to the method\ncache in CPython and PyPy).\n\n\nResults\nIt's hard to compare the improvements of this optimization in a fair way, as\nthe trade-offs are just very different. Just to give an impression, a million\nobjects of the same class with three fields on a 32bit system takes:\nwithout slots:\n\n182 MiB memory in CPython\n177 MiB memory in PyPy without maps\n40 MiB memory in PyPy with maps\n\nwith slots:\n\n45 MiB memory in CPython\n50 MiB memory in PyPy without maps\n40 MiB memory in PyPy with maps\n\nNote how maps make the objects a bit more efficient like CPython using slots.\nAlso, using slots has no additional effect in PyPy.\n\n\nConclusion\nMaps are a powerful approach to shrinking the memory used by many similar\ninstances. I think they can be pushed even further (e.g. by adding information\nabout the types of the attributes) and plan to do so in the following months.\nDetails will be forthcoming.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/efficiently-implementing-python-objects-3838329944323946932.html"
    },
    {
      "title": "Speeding up PyPy by donations",
      "text": "PyPy joins the Software Freedom Conservancy\n\nGood news. PyPy is now a member of the Software Freedom Conservancy (SFC),\nsee the SFC blog post.  This allows us to manage non-profit monetary aspects of\nthe project independently from a company or particular persons.   So we\ncan now officially receive donations both from people prefering right or\nleft sides, see the Donate buttons on our home page and our blog.\nAnd you can use PayPal or Google Checkout, Donations are tax-exempt in the\nUSA and hopefully soon in Europe as well.\nWhat's it going to get used for?  For the immediate future we intend to use\nthe donations for funding travels of core contributors to PyPy sprints\nwho otherwise can't afford to come.  So if you have no time but some\nmoney you can help to encourage coding contributors to care for PyPy.\nIf we end up with bigger sums we'll see and take suggestions.  Money\nspending decisions will be done by core PyPy people according to\nnon-profit guidelines.  And we'll post information from time to time\nabout how much we got and where the money went.\nIf you have any questions regarding the SFC membership or donations\nyou may send email to sfc at pypy.org  which will be observed\nby Carl Friedrich Bolz, Jacob Hallen and Holger Krekel - the initial\nPyPy SFC representatives on behalf of the PyPy team.  Many thanks go\nout to Bradley M. Kuhn for helping to implement the PyPy SFC membership.\ncheers,\nHolger & Carl Friedrich",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/speeding-up-pypy-by-donations-6035529829962326007.html"
    },
    {
      "title": "A snake which bites its tail: PyPy JITting itself",
      "text": "We have to admit: even if we have been writing for years about the fantastic\nspeedups that the PyPy JIT gives, we, the PyPy developers, still don't use it\nfor our daily routine.  Until today :-).\nReaders brave enough to run translate.py to translate PyPy by themselves\nsurely know that the process takes quite a long time to complete, about a hour\non super-fast hardware and even more on average computers.  Unfortunately, it\nhappened that translate.py was a bad match for our JIT and thus ran much\nslower on PyPy than on CPython.\nOne of the main reasons is that the PyPy translation toolchain makes heavy use\nof custom metaclasses, and until few weeks ago metaclasses disabled some of\nthe central optimizations which make PyPy so fast.  During the recent\nD\u00fcsseldorf sprint, Armin and Carl Friedrich fixed this problem and\nre-enabled all the optimizations even in presence of metaclasses.\nSo, today we decided that it was time to benchmark again PyPy against itself.\nFirst, we tried to translate PyPy using CPython as usual, with the following\ncommand line (on a machine with an \"Intel(R) Xeon(R) CPU W3580 @ 3.33GHz\" and\n12 GB of RAM, running a 32-bit Ubuntu):\n\n$ python ./translate.py -Ojit targetpypystandalone --no-allworkingmodules\n\n... lots of output, fractals included ...\n\n[Timer] Timings:\n[Timer] annotate                       ---  252.0 s\n[Timer] rtype_lltype                   ---  199.3 s\n[Timer] pyjitpl_lltype                 ---  565.2 s\n[Timer] backendopt_lltype              ---  217.4 s\n[Timer] stackcheckinsertion_lltype     ---   26.8 s\n[Timer] database_c                     ---  234.4 s\n[Timer] source_c                       ---  480.7 s\n[Timer] compile_c                      ---  258.4 s\n[Timer] ===========================================\n[Timer] Total:                         --- 2234.2 s\n\nThen, we tried the same command line with PyPy (SVN revision 78903, x86-32 JIT\nbackend, downloaded from the nightly build page):\n\n$ pypy-c-78903 ./translate.py -Ojit targetpypystandalone --no-allworkingmodules\n\n... lots of output, fractals included ...\n\n[Timer] Timings:\n[Timer] annotate                       ---  165.3 s\n[Timer] rtype_lltype                   ---  121.9 s\n[Timer] pyjitpl_lltype                 ---  224.0 s\n[Timer] backendopt_lltype              ---   72.1 s\n[Timer] stackcheckinsertion_lltype     ---    7.0 s\n[Timer] database_c                     ---  104.4 s\n[Timer] source_c                       ---  167.9 s\n[Timer] compile_c                      ---  320.3 s\n[Timer] ===========================================\n[Timer] Total:                         --- 1182.8 s\n\nYes, it's not a typo: PyPy is almost two times faster than CPython!\nMoreover, we can see that PyPy is faster in each of the individual steps apart\ncompile_c, which consists in just a call to make to invoke gcc.\nThe slowdown comes from the fact that the Makefile also contains a lot of\ncalls to the trackgcroot.py script, which happens to perform badly on PyPy\nbut we did not investigate why yet.\nHowever, there is also a drawback: on this specific benchmark, PyPy consumes\nmuch more memory than CPython.  The reason why the command line above contains\n--no-allworkingmodules is that if we include all the modules the\ntranslation crashes when it's complete at 99% because it consumes all the 4GB\nof memory which is addressable by a 32-bit process.\nA partial explanation if that so far the assembler generated by the PyPy JIT\nis immortal, and the memory allocated for it is never reclaimed.  This is\nclearly bad for a program like translate.py which is divided into several\nindependent steps, and for which most of the code generated in each step could\nbe safely be thrown away when it's completed.\nIf we switch to 64-bit we can address the whole 12 GB of RAM that we have, and\nthus translating with all working modules is no longer an issue.  This is the\ntime taken with CPython (note that it does not make sense to compare with the\n32-bit CPython translation above, because that one does not include all the\nmodules):\n\n$ python ./translate.py -Ojit\n\n[Timer] Timings:\n[Timer] annotate                       ---  782.7 s\n[Timer] rtype_lltype                   ---  445.2 s\n[Timer] pyjitpl_lltype                 ---  955.8 s\n[Timer] backendopt_lltype              ---  457.0 s\n[Timer] stackcheckinsertion_lltype     ---   63.0 s\n[Timer] database_c                     ---  505.0 s\n[Timer] source_c                       ---  939.4 s\n[Timer] compile_c                      ---  465.1 s\n[Timer] ===========================================\n[Timer] Total:                         --- 4613.2 s\n\nAnd this is for PyPy:\n\n$ pypy-c-78924-64 ./translate.py -Ojit\n\n[Timer] Timings:\n[Timer] annotate                       ---  505.8 s\n[Timer] rtype_lltype                   ---  279.4 s\n[Timer] pyjitpl_lltype                 ---  338.2 s\n[Timer] backendopt_lltype              ---  125.1 s\n[Timer] stackcheckinsertion_lltype     ---   21.7 s\n[Timer] database_c                     ---  187.9 s\n[Timer] source_c                       ---  298.8 s\n[Timer] compile_c                      ---  650.7 s\n[Timer] ===========================================\n[Timer] Total:                         --- 2407.6 s\n\nThe results are comparable with the 32-bit case: PyPy is still almost 2 times\nfaster than CPython.  And it also shows that our 64-bit JIT backend is as good\nas the 32-bit one.  Again, the drawback is in the consumed memory: CPython\nused 2.3 GB while PyPy took 8.3 GB.\nOverall, the results are impressive: we knew that PyPy can be good at\noptimizing small benchmarks and even middle-sized programs, but as far as we\nknow this is the first example in which it heavily optimizes a huge, real world\napplication.  And, believe us, the PyPy translation toolchain is complex\nenough to contains all kinds of dirty tricks and black magic that make Python\nlovable and hard to optimize :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/snake-which-bites-its-tail-pypy-jitting-5161284681004717142.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report 2010",
      "text": "This years installment of the yearly PyPy D\u00fcsseldorf Sprint is drawing to a\nclose. As usual, we worked in the seminar room of the programming language\ngroup at the University of D\u00fcsseldorf. The sprint was different from previous\nones in that we had fewer people than usual and many actually live in\nD\u00fcsseldorf all the time.\nDavid spent the sprint working on the arm-backend branch, which is adding an\nARM backend to the JIT. With the help of Armin he added support for bridges in\nthe JIT and generally implemented missing operations, mostly for handling integers so far.\nRonny and Anto worked the whole week trying to come up with a scheme for\nimporting PyPy's SVN history into a mercurial repository without loosing too\nmuch information. This is a non-trivial task, because PyPy's history is gnarly.\nWe are nearly at revision 79000 and when we started using it, Subversion was at\nversion 0.1. All possible and impossible ways to mangle and mistreat a\nSubversion repository have been applied to PyPy's repo, so most of the\nimporting tools just give up. Ronny and Anto came up with a new plan and new\nhelper scripts every day, only to then discover another corner case that they\nhadn't thought of. Now they might actually have a final plan (but they said\nthat every day, so who knows?).The branch history of PyPy's repository (every box is a branch)Carl Friedrich and Lukas started working in earnest on memory benchmarks to\nunderstand the memory behaviour of Python code better. They have now\nimplemented a generic memory benchmark runner and a simple analysis that walks\nall objects and collects size information about them. They also added some\nbenchmarks that were proposed in the comments of the recent call for\nbenchmarks. As soon as some results from that work are there, we will post\nabout them.\nThere were also some minor tasks performed during the sprint. Armin implemented\nthe _bisect module and the dict.popitem method in RPython. Armin and\nCarl Friedrich made the new memory-saving mapdict implementation more suitable\nto use without the JIT (blog post should come about that too, at some point).\nThey also made classes with custom metaclasses a lot faster when the JIT is\nused.\nThe last three days of the sprint were spent working on H\u00e5kan's\njit-unroll-loops branch.  The branch is meant to move loop invariants out of\nthe loop, using techniques very similar to what is described in the recent post\non escape analysis across loop boundaries (see? it will soon stop being\nscience-fiction). Some of the ideas of this approach also come from LuaJIT\nwhich also uses very aggressive loop invariant code motion in its optimizers.\nMoving loop invariants outside of the loop is very useful, because many of the\nlookups that Python programs do in loops are loop invariants. An example is if\nyou call a function in a loop: The global lookup can often be done only once.\nThis branch fundamentally changes some of the core assumptions of the JIT, so\nit is a huge amount of work to make it fit with all the other parts and to\nadapt all tests. That work is now nearly done, some failing tests remain. The\nnext steps are to fix them and then do additional tests with the translated\nexecutable and look at the benchmarks.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/dusseldorf-sprint-report-2010-371223200425847723.html"
    },
    {
      "title": "The peace of green",
      "text": "No, we are not going to talk about the environment (i.e., the set of variables\nas printed by /usr/bin/env. What else? :-)).\nAfter months in which we had a couple of tests failing every day, we finally\nmanaged to turn (almost) everything green today, at least on Linux.  Enjoy\nthis screenshoot taken from the nightly build page:\n\n\n\n\nAs usual, the full buildbot results can be seen from the summary page.\ncheers,\nAnto",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/peace-of-green-4230271053903469504.html"
    },
    {
      "title": "PhD Thesis about PyPy's CLI JIT Backend",
      "text": "Hi all,\nfew months ago I finished the PhD studies and now my thesis is available,\njust in case someone does not have anything better to do than read it :-).\nThe title of the thesis is High performance implementation of Python for\nCLI/.NET with JIT compiler generation for dynamic languages, and its mainly\nbased on my work on the CLI backend for the PyPy JIT (note that the CLI JIT\nbackend is currently broken on trunk, but it's still working in the cli-jit\nbranch).\nThe thesis might be useful also for people that are not directly interested in\nthe CLI JIT backend, as it also contains general information about the inner\nworkings of PyPy which are independent from the backend: in particular,\nchapters 5 and 6 explain how the JIT frontend works.\n\nHere is the summary of chapters:\n\nIntroduction\nThe problem\nEnter PyPy\nCharacterization of the target platform\nTracing JITs in a nutshell\nThe PyPy JIT compiler generator\nThe CLI JIT backend\nBenchmarks\nConclusion and Future Work\n\n\n\ncheers,\nAnto",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/phd-thesis-about-pypys-cli-jit-backend-969267841095296323.html"
    },
    {
      "title": "Next PyPy sprint",
      "text": "Hi all,\n\nThe next PyPy sprint is scheduled for the end of the month, from the 25th to the 31st of October 2010.  It will be done at the university of D\u00fcsseldorf, Germany, where three of us are working.\n\nPlease see this link for more information.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/next-pypy-sprint-4850394963147107623.html"
    },
    {
      "title": "PyPy in Google's Summer of Code 2010",
      "text": "Hello.\nThis year we had a record of two and a half applications (one was on a cross\nsection of PyPy and numpy) accepted for the Google\nSoC program. Since it ended a couple of weeks ago, we wanted to present the results that\nwere achieved. All three projects were completed successfully, although the rate\nof success varied quite a bit.\nThe Numpy proposal progress significantly on making numpy compatible with\nPyPy's CPython's extension module support, but failed to bring PyPy's numpy\nimplementation into a usable shape (which is a somewhat ambitious goal, one\nmight argue). The experiments done during the projects are living on the\nmicronumpy branch.\nThe Fast ctypes proposal did some useful experiments on how to JIT external\ncalls from PyPy to C, however, the actual code as of now is not very\ninteresting and it's quite far from providing a full ctypes replacement (or\nequivalent).\nDefinitely the most successful proposal was a 64bit (x86_64) backend for PyPy's\nJIT. It not only includes working 64bit JIT (merged into PyPy trunk), but also\na working asmgcc for x86_64 linux platform, that makes it possible to run the JIT\non this architecture with our advanced garbage collectors. One can say that\nx64_64 is now no longer a second-class citizen for PyPy, although it definitely\ndidn't receive as much testing as the x86 platform. Expect this to be a major\nselling point for the next PyPy release :-)\nCheers,\nfijal & the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/09/pypy-in-googles-summer-of-code-2010-1267220161643618015.html"
    },
    {
      "title": "Using Escape Analysis Across Loop Boundaries for Specialization",
      "text": "This blog post is a successor to the one about escape analysis in PyPy's\nJIT. The examples from there will be continued here. This post is a bit\nscience-fictiony. The algorithm that PyPy currently uses is significantly more\ncomplex and much harder than the one that is described here. The resulting\nbehaviour is very similar, however, so we will use the simpler version (and we\nmight switch to that at some point in the actual implementation).\nIn the last blog post we described how escape analysis can be used to remove\nmany of the allocations of short-lived objects and many of the type dispatches\nthat are present in a non-optimized trace. In this post we will improve the\noptimization to also handle more cases.\nTo understand some more what the optimization described in the last blog post\ncan achieve, look at the following figure:\n\n\n\nThe figure shows a trace before optimization, together with the lifetime of\nvarious kinds of objects created in the trace. It is executed from top to\nbottom. At the bottom, a jump is used to execute the same loop another time.\nFor clarity, the figure shows two iterations of the loop.\nThe loop is executed until one of the guards in the trace fails, and the\nexecution is aborted.\nSome of the operations within this trace are new operations, which each create a\nnew instance of some class. These instances are used for a while, e.g. by\ncalling methods on them, reading and writing their fields. Some of these\ninstances escape, which means that they are stored in some globally accessible\nplace or are passed into a function.\nTogether with the new operations, the figure shows the lifetimes of the\ncreated objects. Objects in category 1 live for a while, and are then just not\nused any more. The creation of these objects is removed by the\noptimization described in the last blog post.\nObjects in category 2 live for a while and then escape. The optimization of the\nlast post deals with them too: the new that creates them and\nthe field accesses are deferred, until the point where the object escapes.\nThe objects in category 3 and 4 are in principle like the objects in category 1\nand 2. They are created, live for a while, but are then passed as an argument\nto the jump operation. In the next iteration they can either die (category\n3) or escape (category 4).\nThe optimization of the last post considered the passing of an object along a\njump to be equivalent to escaping. It was thus treating objects in category 3\nand 4 like those in category 2.\nThe improved optimization described in this post will make it possible to deal\nbetter with objects in category 3 and 4. This will have two consequences: on\nthe one hand, more allocations are removed from the trace (which is clearly\ngood). As a side-effect of this, the traces will also be type-specialized.\n\nOptimizing Across the Jump\nLet's look at the final trace obtained in the last post for the example loop.\nThe final trace was much better than the original one, because many allocations\nwere removed from it. However, it also still contained allocations:\n\n\n\nThe two new BoxedIntegers stored in p15 and p10 are passed into\nthe next iteration of the loop. The next iteration will check that they are\nindeed BoxedIntegers, read their intval fields and then not use them\nany more. Thus those instances are in category 3.\nIn its current state the loop\nallocates two BoxedIntegers at the end of every iteration, that then die\nvery quickly in the next iteration. In addition, the type checks at the start\nof the loop are superfluous, at least after the first iteration.\nThe reason why we cannot optimize the remaining allocations away is because\ntheir lifetime crosses the jump. To improve the situation, a little trick is\nneeded. The trace above represents a loop, i.e. the jump at the end jumps to\nthe beginning. Where in the loop the jump occurs is arbitrary, since the loop\ncan only be left via failing guards anyway. Therefore it does not change the\nsemantics of the loop to put the jump at another point into the trace and we\ncan move the jump operation just above the allocation of the objects that\nappear in the current jump. This needs some care, because the arguments to\njump are all currently live variables, thus they need to be adapted.\nIf we do that for our example trace above, the trace looks like this:\n\n\n\nNow the lifetime of the remaining allocations no longer crosses the jump, and\nwe can run our escape analysis a second time, to get the following trace:\n\n\n\nThis result is now really good. The code performs the same operations than\nthe original code, but using direct CPU arithmetic and no boxing, as opposed to\nthe original version which used dynamic dispatching and boxing.\nLooking at the final trace it is also completely clear that specialization has\nhappened. The trace corresponds to the situation in which the trace was\noriginally recorded, which happened to be a loop where BoxedIntegers were\nused. The now resulting loop does not refer to the BoxedInteger class at\nall any more, but it still has the same behaviour. If the original loop had\nused BoxedFloats, the final loop would use float_* operations\neverywhere instead (or even be very different, if the object model had\nuser-defined classes).\n\n\nEntering the Loop\nThe approach of placing the jump at some other point in the loop leads to\none additional complication that we glossed over so far. The beginning of the\noriginal loop corresponds to a point in the original program, namely the\nwhile loop in the function f from the last post.\nNow recall that in a VM that uses a tracing JIT, all programs start by being\ninterpreted. This means that when f is executed by the interpreter, it is\neasy to go from the interpreter to the first version of the compiled loop.\nAfter the jump is moved and the escape analysis optimization is applied a\nsecond time, this is no longer easily possible.  In particular, the new loop\nexpects two integers as input arguments, while the old one expected two\ninstances.\nTo make it possible to enter the loop directly from the intepreter, there\nneeds to be some additional code that enters the loop by taking as input\narguments what is available to the interpreter, i.e. two instances. This\nadditional code corresponds to one iteration of the loop, which is thus\npeeled off:\n\n\n\n\n\nSummary\nThe optimization described in this post can be used to optimize away\nallocations in category 3 and improve allocations in category 4, by deferring\nthem until they are no longer avoidable. A side-effect of these optimizations\nis also that the optimized loops are specialized for the types of the variables\nthat are used inside them.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/09/using-escape-analysis-across-loop-2887031293132023676.html"
    },
    {
      "title": "Escape Analysis in PyPy's JIT",
      "text": "The goal of a just-in-time compiler for a dynamic language is obviously to\nimprove the speed of the language over an implementation of the language that\nuses interpretation. The first goal of a JIT is thus to remove the\ninterpretation overhead, i.e. the overhead of bytecode (or AST) dispatch and the\noverhead of the interpreter's data structures, such as operand stack etc. The\nsecond important problem that any JIT for a dynamic language needs to solve is\nhow to deal with the overhead of boxing of primitive types and of type\ndispatching. Those are problems that are usually not present in statically typed\nlanguages.\nBoxing of primitive types means that dynamic languages need to be able to handle\nall objects, even integers, floats, etc. in the same way as user-defined\ninstances. Thus those primitive types are usually boxed, i.e. a small\nheap-structure is allocated for them, that contains the actual value.\nType dispatching is the process of finding the concrete implementation that is\napplicable to the objects at hand when doing a generic operation at hand. An\nexample would be the addition of two objects: The addition needs to check what\nthe concrete objects are that should be added are, and choose the implementation\nthat is fitting for them.\nLast year, we wrote a blog post  and a paper about how PyPy's meta-JIT\napproach works. These explain how the meta-tracing JIT can remove the overhead\nof bytecode dispatch. In this post (and probably a followup) we want to explain\nhow the traces that are produced by our meta-tracing JIT are then optimized to\nalso remove some of the overhead more closely associated to dynamic languages,\nsuch as boxing overhead and type dispatching. The most important technique to\nachieve this is a form of escape analysis that we call virtual objects.\nThis is best explained via an example.\n\nRunning Example\nFor the purpose of this blog post, we are going to use a very simple object\nmodel, that just supports an integer and a float type. The objects support only\ntwo operations, add, which adds two objects (promoting ints to floats in a\nmixed addition) and is_positive, which returns whether the number is greater\nthan zero. The implementation of add uses classical Smalltalk-like\ndouble-dispatching. These classes could be part of the implementation of a very\nsimple interpreter written in RPython.\nclass Base(object):\n    def add(self, other):\n        \"\"\" add self to other \"\"\"\n        raise NotImplementedError(\"abstract base\")\n    def add__int(self, intother):\n        \"\"\" add intother to self, where intother is a Python integer \"\"\"\n        raise NotImplementedError(\"abstract base\")\n    def add__float(self, floatother):\n        \"\"\" add floatother to self, where floatother is a Python float \"\"\"\n        raise NotImplementedError(\"abstract base\")\n    def is_positive(self):\n        \"\"\" returns whether self is positive \"\"\"\n        raise NotImplementedError(\"abstract base\")\n\nclass BoxedInteger(Base):\n    def __init__(self, intval):\n        self.intval = intval\n    def add(self, other):\n        return other.add__int(self.intval)\n    def add__int(self, intother):\n        return BoxedInteger(intother + self.intval)\n    def add__float(self, floatother):\n        return BoxedFloat(floatother + float(self.intval))\n    def is_positive(self):\n        return self.intval > 0\n\nclass BoxedFloat(Base):\n    def __init__(self, floatval):\n        self.floatval = floatval\n    def add(self, other):\n        return other.add__float(self.floatval)\n    def add__int(self, intother):\n        return BoxedFloat(float(intother) + self.floatval)\n    def add__float(self, floatother):\n        return BoxedFloat(floatother + self.floatval)\n    def is_positive(self):\n        return self.floatval > 0.0\n\nUsing these classes to implement arithmetic shows the basic problem that a\ndynamic language implementation has. All the numbers are instances of either\nBoxedInteger or BoxedFloat, thus they consume space on the heap. Performing many\narithmetic operations produces lots of garbage quickly, thus putting pressure on\nthe garbage collector. Using double dispatching to implement the numeric tower\nneeds two method calls per arithmetic operation, which is costly due to the\nmethod dispatch.\nTo understand the problems more directly, let us consider a simple function that\nuses the object model:\ndef f(y):\n    res = BoxedInteger(0)\n    while y.is_positive():\n        res = res.add(y).add(BoxedInteger(-100))\n        y = y.add(BoxedInteger(-1))\n    return res\n\nThe loop iterates y times, and computes something in the process. To\nunderstand the reason why executing this function is slow, here is the trace\nthat is produced by the tracing JIT when executing the function with y\nbeing a BoxedInteger:\n\n# arguments to the trace: p0, p1\n# inside f: res.add(y)\nguard_class(p1, BoxedInteger)\n    # inside BoxedInteger.add\n    i2 = getfield_gc(p1, intval)\n    guard_class(p0, BoxedInteger)\n        # inside BoxedInteger.add__int\n        i3 = getfield_gc(p0, intval)\n        i4 = int_add(i2, i3)\n        p5 = new(BoxedInteger)\n            # inside BoxedInteger.__init__\n            setfield_gc(p5, i4, intval)\n# inside f: BoxedInteger(-100)\np6 = new(BoxedInteger)\n    # inside BoxedInteger.__init__\n    setfield_gc(p6, -100, intval)\n\n# inside f: .add(BoxedInteger(-100))\nguard_class(p5, BoxedInteger)\n    # inside BoxedInteger.add\n    i7 = getfield_gc(p5, intval)\n    guard_class(p6, BoxedInteger)\n        # inside BoxedInteger.add__int\n        i8 = getfield_gc(p6, intval)\n        i9 = int_add(i7, i8)\n        p10 = new(BoxedInteger)\n            # inside BoxedInteger.__init__\n            setfield_gc(p10, i9, intval)\n\n# inside f: BoxedInteger(-1)\np11 = new(BoxedInteger)\n    # inside BoxedInteger.__init__\n    setfield_gc(p11, -1, intval)\n\n# inside f: y.add(BoxedInteger(-1))\nguard_class(p0, BoxedInteger)\n    # inside BoxedInteger.add\n    i12 = getfield_gc(p0, intval)\n    guard_class(p11, BoxedInteger)\n        # inside BoxedInteger.add__int\n        i13 = getfield_gc(p11, intval)\n        i14 = int_add(i12, i13)\n        p15 = new(BoxedInteger)\n            # inside BoxedInteger.__init__\n            setfield_gc(p15, i14, intval)\n\n# inside f: y.is_positive()\nguard_class(p15, BoxedInteger)\n    # inside BoxedInteger.is_positive\n    i16 = getfield_gc(p15, intval)\n    i17 = int_gt(i16, 0)\n# inside f\nguard_true(i17)\njump(p15, p10)\n\n(indentation corresponds to the stack level of the traced functions).\nThe trace is inefficient for a couple of reasons. One problem is that it checks\nrepeatedly and redundantly for the class of the objects around, using a\nguard_class instruction. In addition, some new BoxedInteger instances are\nconstructed using the new operation, only to be used once and then forgotten\na bit later. In the next section, we will see how this can be improved upon,\nusing escape analysis.\n\n\nVirtual Objects\nThe main insight to improve the code shown in the last section is that some of\nthe objects created in the trace using a new operation don't survive very\nlong and are collected by the garbage collector soon after their allocation.\nMoreover, they are used only inside the loop, thus we can easily prove that\nnobody else in the program stores a reference to them. The\nidea for improving the code is thus to analyze which objects never escape the\nloop and may thus not be allocated at all.\nThis process is called escape analysis. The escape analysis of\nour tracing JIT works by using virtual objects: The trace is walked from\nbeginning to end and whenever a new operation is seen, the operation is\nremoved and a virtual object is constructed. The virtual object summarizes the\nshape of the object that is allocated at this position in the original trace,\nand is used by the escape analysis to improve the trace. The shape describes\nwhere the values that would be stored in the fields of the allocated objects\ncome from. Whenever the optimizer sees a setfield that writes into a virtual\nobject, that shape summary is thus updated and the operation can be removed.\nWhen the optimizer encounters a getfield from a virtual, the result is read\nfrom the virtual object, and the operation is also removed.\nIn the example from last section, the following operations would produce two\nvirtual objects, and be completely removed from the optimized trace:\n\np5 = new(BoxedInteger)\nsetfield_gc(p5, i4, intval)\np6 = new(BoxedInteger)\nsetfield_gc(p6, -100, intval)\n\nThe virtual object stored in p5 would know that it is an BoxedInteger, and that\nthe intval field contains i4, the one stored in p6 would know that\nits intval field contains the constant -100.\nThe following operations, that use p5 and p6 could then be\noptimized using that knowledge:\n\nguard_class(p5, BoxedInteger)\ni7 = getfield_gc(p5, intval)\n# inside BoxedInteger.add\nguard_class(p6, BoxedInteger)\n# inside BoxedInteger.add__int\ni8 = getfield_gc(p6, intval)\ni9 = int_add(i7, i8)\n\nThe guard_class operations can be removed, because the classes of p5 and\np6 are known to be BoxedInteger. The getfield_gc operations can be removed\nand i7 and i8 are just replaced by i4 and -100. Thus the only\nremaining operation in the optimized trace would be:\n\ni9 = int_add(i4, -100)\n\nThe rest of the trace is optimized similarly.\nSo far we have only described what happens when virtual objects are used in\noperations that read and write their fields. When the virtual object is used in\nany other operation, it cannot stay virtual. For example, when a virtual object\nis stored in a globally accessible place, the object needs to actually be\nallocated, as it will live longer than one iteration of the loop.\nThis is what happens at the end of the trace above, when the jump operation\nis hit. The arguments of the jump are at this point virtual objects. Before the\njump is emitted, they are forced. This means that the optimizers produces code\nthat allocates a new object of the right type and sets its fields to the field\nvalues that the virtual object has. This means that instead of the jump, the\nfollowing operations are emitted:\n\np15 = new(BoxedInteger)\nsetfield_gc(p15, i14, intval)\np10 = new(BoxedInteger)\nsetfield_gc(p10, i9, intval)\njump(p15, p10)\n\nNote how the operations for creating these two instances has been moved down the\ntrace. It looks like for these operations we actually didn't win much, because\nthe objects are still allocated at the end. However, the optimization was still\nworthwhile even in this case, because some operations that have been performed\non the forced virtual objects have been removed (some getfield_gc operations\nand guard_class operations).\nThe final optimized trace of the example looks like this:\n\n# arguments to the trace: p0, p1\nguard_class(p1, BoxedInteger)\ni2 = getfield_gc(p1, intval)\nguard_class(p0, BoxedInteger)\ni3 = getfield_gc(p0, intval)\ni4 = int_add(i2, i3)\ni9 = int_add(i4, -100)\n\nguard_class(p0, BoxedInteger)\ni12 = getfield_gc(p0, intval)\ni14 = int_add(i12, -1)\n\ni17 = int_gt(i14, 0)\nguard_true(i17)\np15 = new(BoxedInteger)\nsetfield_gc(p15, i14, intval)\np10 = new(BoxedInteger)\nsetfield_gc(p10, i9, intval)\njump(p15, p10)\n\nThe optimized trace contains only two allocations, instead of the original five,\nand only three guard_class operations, from the original seven.\n\n\nSummary\nIn this blog post we described how simple escape analysis within the scope of\none loop works. This optimizations reduces the allocation of many intermediate\ndata structures that become garbage quickly in an interpreter. It also removes a\nlot of the type dispatching overhead. In a later post, we will explain how this\noptimization can be improved further.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/09/escape-analysis-in-pypys-jit-1780048403046080197.html"
    },
    {
      "title": "EuroPython 2010 Videos available",
      "text": "Hi all,\nthe videos of the talks from EuroPython 2010 are now available on\nblip.tv: in particular, there are the three videos of the PyPy talk.\nPart 1: What's news in PyPy 1.2 and 1.3 (by Antonio Cuni)\nPart 2: Just in Time compilation (by Armin Rigo)\nPart 3: cpyext (by Amaury Forgeot d'Arc)\nMoreover, here is Mark Shannon's talk which compares HotPy, Unladen Swallow\nand PyPy:",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/europython-2010-videos-available-8446190660370796142.html"
    },
    {
      "title": "Call for Benchmarks",
      "text": "As you know, a lot of PyPy's recent development effort has gone into speeding up\nexecution of Python programs. However, an additional good property of PyPy's\nPython interpreter is that most objects are represented in a much more compact\nway than in CPython. We would like to investigate some more advanced techniques\nto reduce the memory usage of Python programs further.\nTo do this it is necessary to investigate the memory behaviour of real programs\nwith large heaps. For speed measurements there are standard benchmarks, but for\nmemory improvements there is nothing comparable, the memory behaviour of large\nprograms is not that well understood. Therefore we are looking for programs that we\ncan study and use as benchmarks.\nSpecifically we are looking for Python programs with the following properties:\n\nlarge heaps of about 10MB-1GB\nshould have non-trivial runtime as well (in the range of a few seconds), to\njudge the speed impact of optimizations\nideally pure-Python programs that don't use extension modules so that they run\nunder both CPython and PyPy (this is optional, but makes my life much easier).\n\nWe are also rather interested in programs that do a lot of string/unicode\nprocessing.\nWe would be grateful for all ideas. Telling us about a program also has the\nadvantage that we will work on optimizing PyPy for it :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/call-for-benchmarks-2605012131351543912.html"
    },
    {
      "title": "PyOhio",
      "text": "This weekend I delivered a talk at PyOhio (an annual conference in Columbus, OH, USA) on PyPy and Unladen Swallow.  The talk covered reasons that Python, the language, is hard to optimize, why CPython is slow, and a few optimizations that PyPy and Unladen Swallow have implemented.  The slides from my talk are online, and the talk was recorded so a video will follow.  I gave a similar talk to ChiPy (the Chicago Python user group), which was also recorded and the video is available.  Both audiences were excited about the futures for PyPy and Unladen Swallow, and for the future of a faster Python.\nAlex",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/pyohio-2568618480482575546.html"
    },
    {
      "title": "Using virtualenv with PyPy",
      "text": "Thanks to the work that was recently done on the sys-prefix branch, it is\nnow possible to use virtualenv with PyPy.\nTo try it, you need:\n\n\na recent version of PyPy: PyPy 1.3 does not contain the necessary logic to\nwork with virtualenv, so you need a more recent PyPy from subversion\ntrunk. You can either build it by yourself or download one of our\nprecompiled nightly builds\na copy of virtualenv-pypy: this is a fork of virtualenv that contains\nall the patches needed to work with PyPy, and hopefully will be merged\nback at some point.  It should be totally compatible with the official\nversion of virtualenv, so it is safe to use it even to create non-PyPy\nenvironments.  If you notice some weird behavior that does not happen with\nthe standard virtualenv, please let us know.\n\n\nThe directory layout has been redesigned in a way that it is possible to use\nvirtualenv to install a PyPy both from a precompiled tarball or from an svn\ncheckout:\n\n# from a tarball\n$ virtualenv -p /opt/pypy-c-jit-76426-linux/bin/pypy my-pypy-env\n\n# from the svn checkout\n$ virtualenv -p /path/to/pypy-trunk/pypy/translator/goal/pypy-c my-pypy-env\n\nOnce the environment has been created, you can enter it as usual. Note that\nbin/python is now a symlink to bin/pypy.\nEnjoy it :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/using-virtualenv-with-pypy-7238942727709530503.html"
    },
    {
      "title": "A Play on Regular Expression",
      "text": "The paper where the algorithms we described in the recent blog posts come from is now available. It  is written as a play in three Acts with a cast of three and is very readable and funny. The Haskell code is at Sebastian Fischer's github pages.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/play-on-regular-expression-9014941705636345998.html"
    },
    {
      "title": "EuroPython 2010 report",
      "text": "So, EuroPython 2010 is over, I am flying home and it's time to write a report\nabout the conference from the PyPy point of view.\nAs usual, the conference was very interesting and went very well. The quality\nof the talks I attended to was high on average and most importantly I could\nmeet a lot of interesting people to discuss various things.\nOn the first day, Armin, Amaury and I presented the usual PyPy status talk\n(here are the slides):\nthe talk is an extended version of the one that I and Armin presented at\nPycon Italia in May and is divided in three parts: first I talked about the\ncurrent status of the project, what is the content of the recent 1.2 and 1.3\nreleases and showed a demo of a simple Django application that renders a\nMandelbrot fractal and is measurably faster on PyPy than on CPython.  In the\nsecond part of the talk, Armin gave an introduction about the ideas that stand\nbehind the JIT.  Finally, in the third part Amaury explained how the new\ncpyext module lets PyPy to compile and load existing CPython extensions\nwritten in C.\nI think that the talk was well received: the only drawback is that there was\nno time to answer questions at the end of the presentation.  However, we\nreceived a lot of \"offline\" questions after the talk finished and thorough the\nwhole conference: it is always great to see that people are interested in our\nwork, and I'd like to thank everybody for the feedback that they gave to us.\nPyPy was also mentioned in the interesting Mark Shannon's talk, where he\ncompared the optimization techniques used by PyPy, Unladen Swallow and\nHotPy, which is Mark's own PhD project.  Moreover, Henrik Vendelbo\ngave a talk about how to tweak PyPy to produce a standalone\nexecutable which embeds a whole python application to make deployment easier,\nwhile Andrew Francis explained his implementation of the Go select\nstatement based on the stackless.py module implemented in PyPy.  Personally,\nI am glad to see that people start to think of PyPy as a useful starting\npoint to experiment with new features and use cases that we did not think\nabout: after all, one of PyPy explicit goals is to be \"flexible and easy to\nexperiment with\".\nAfter the conference there were the usual post EuroPython sprints: this\nyear we had not planned a PyPy sprint, but some people showed interest\nin it and since Armin and I happened to be still around the day after the\nconference, we decided to do a mini 1-day sprint, with 6 or 7 people\npresent. Since there were only two core developers it was impossible to use\nour usual pairing scheme, in which every newcomer pairs with someone who is\nexperienced with the source code to gain knowledge of it.  However, I think it\nwas still a successful day of work, and we managed to fix a couple of bugs\nthat was standing in our issue tracker.  Again, I'd like to thank all the\npeople that came and worked with us during the sprint.\nIn conclusion I really enjoyed the EuroPython 2010 experience: the fact that I\nmanaged to find a place in Birmingham where to eat a good Italian-style \"gelato\"\nhelped a lot :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/europython-2010-report-7803731360759120212.html"
    },
    {
      "title": "CERN Sprint Report \u2013 Wrapping C++ Libraries",
      "text": "The last five days we have been sprinting in a meeting room in the Computing\nCenter at CERN in Gen\u00e8ve, Switzerland. Present are Armin Rigo, Antonio Cuni,\nCarl Friedrich Bolz and Wim Lavrijsen (LBL). The goal of the sprint was to use\nsome of the C++ technology developed at CERN to make it possible to use C++\nlibraries from PyPy's Python interpreter. For this we used the Reflex\nlibrary, which provides reflection information for C++ classes. We discussed\nusing Reflex in PyPy during the D\u00fcsseldorf sprint of 2008, please read\nthat blog post if you want some more details on how Reflex works. There is\nsupport for this sort of C++/Python integration also for CPython, using the\nPyROOT module.\nThe sprint was very successful. On Monday we had a few discussion about how\nReflex could best be integrated with PyPy. One of the goals of the sprint was to\nmake the approach JIT-friendly from the start, so that calls to C++ libraries\ncan be reasonably fast. After the discussion we started coding on the\nreflex-support branch. This branch adds a new cppyy builtin module to\nPyPy's Python interpreter (why we chose that name is left as an exercise to the\nreader). This module can be used to load C++ classes, construct instances and\ncall static and instance methods on them.\nThe work has just started, as of now, the argument and return types of the\nmethods are restricted to some simple C types, such as int, double and\nchar* and pointers to class instances. Most of the work necessary to\nproperly resolve overloaded methods is done, but default arguments are not.\nAs an example, suppose there is a C++ class like this:\nclass example01 {\nprivate:\n    static int count;\n    int somedata;\npublic:\n\n    example01(int a) : somedata(a) {\n        count++;\n    }\n    ~example01() {\n        count--;\n    }\n    static int getCount() {\n        return count;\n    }\n\n    int addDataToInt(int a) {\n        return somedata + a;\n    }\n};\nint example01::count = 0;\n\nYou can now use it from PyPy's Python interpreter in the following way, after\nyou have used Reflex to generate reflection information for the class:\nimport cppyy\ncppyy.load_lib(\"example01Dict.so\") # contains the Reflex information\nexample01_class = cppyy.gbl.example01\ninstance = example01_class(7)\nassert example01_class.getCount() == 1\nres = instance.addDataToInt(4)\nassert res == 11\nres = instance.addDataToInt(-4)\nassert res == 3\ninstance.destruct() # so far explicit destruction needed\nassert example01_class.getCount() == 0\n\nWe also did some very early JIT work and some early performance measurements.\nThe rough figures are that cppyy is two times faster at calling a simple C++\nmethod from Python than PyROOT. To get a feeling for how fast things could\ngo in the end, we also implemented a proof-of-concept for some more advanced JIT\ntechnology (which requires a patch for Reflex and uses a GCC extension). With\nthis, the speedup over PyROOT is a factor of 20. Of course, this is still a\nlot slower than a C++ to C++ method call (probably by at least an order of\nmagnitude).\nThe sprint was very productive because we managed to get the right people into\nthe same room working together. Wim has a lot of experience with C++ and Reflex,\nand is the author of PyROOT, and of course the others know a lot about PyPy\n(at the end of the sprint, Anto was very glad that he stopped using C++ a long\ntime ago). Also, working at CERN was very cool. The atmosphere is amazing, and\nwe got to visit the ATLAS control room. Extremely advanced technology, and\nalso research on a completely different scale than what we are used to.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/cern-sprint-report-wrapping-c-libraries-6547377950791793143.html"
    },
    {
      "title": "Comparing SPUR to PyPy",
      "text": "Recently, I've become aware of the SPUR project of Microsoft Research and\nread some of their papers (the tech report \"SPUR: A Trace-Based JIT Compiler\nfor CIL\" is very cool). I found the project to be very interesting and since\ntheir approach is in many ways related to what PyPy is doing, I now want to\ncompare and contrast the two projects.\n\nA Tracing JIT for .NET\nSPUR consist of two parts: On the one hand it is a VM for CIL, the\nbytecode of the .NET VM. This VM uses a tracing JIT compiler to compile the\nprograms it is running to machine code. As opposed to most existing VMs that\nhave a tracing JIT it does not use an interpreter at all. Instead it\ncontains various variants of a JIT compiler that produce different versions of\neach method. Those are:\n\na profiling JIT which produces code that does lightweight profiling when\nrunning the compiled method\na tracing JIT which produces code that produces a trace when running the\ncompiled method\na transfer-tail JIT which is used to produce code which is run to get from a\nfailing guard back to the normal profiling version of a method\nan optimizing JIT that actually optimizes traces and turns them into machine code\n\n\nOptimizations Done by the Optimizing JIT\nSPUR's optimizing JIT does a number of powerful optimizations on the traces before it\nturns them into machine code. Among them are usual compiler optimizations such\nas register allocation, common subexpression elimination, loop invariant code\nmotion, etc.\nIt also performs some optimizations that are specific to the tracing context and\nare thus not commonly found in \"normal\" compilers:\n\nguard implication: if a guard is implied by an earlier guard, it is removed\nguard strengthening: if there is a sequence of guards that become stronger\nand stronger (i.e. each guard implies the previous one), the first guard in\nthe sequence is replaced by the last one, and all others are removed. This can\ngreatly reduce the number of guards and is generally safe. It can shift a\nguard failure to an earlier point in the trace, but the failure would have\noccurred at some point in the trace anyway.\nload/store optimizations: this is an optimization for memory reads/writes.\nIf several loads from the same memory location occur without writes in\nbetween, all but the first one are removed. Similarly, if a write to a memory\nlocation is performed, this write is delayed as much as possible. If there is\na write to the same location soon afterwards, the first write can be removed.\nescape analysis: for allocations that occur in a loop, the optimizer checks\nwhether the resulting object escapes the loop. If not, the allocation is moved\nbefore the loop, so that only one object needs to be allocated, instead of one\nevery loop iteration.\nuser-controlled loop unrolling: not exactly an optimization, but an\ninteresting feature anyway. It is possible to annotate a CIL method with a\nspecial decorator [TraceUnfold] and then the tracing JIT will fully unroll\nthe loops it contains. This can be useful for loops than are known to run a\nsmall and fixed number of iterations for each call-site.\nuser controlled tracing: The user can also control tracing up to a point.\nMethods can be annotated with [NativeCall] to tell the tracer to never\ntrace their execution. Instead they appear as a direct call in the trace.\n\n\n\n\nA JavaScript Implementation\nIn addition to the tracing JIT I just described, SPUR also contains a JavaScript\nimplementation for .NET. The approach of this implementation is to translate\nJavaScript to CIL bytecode, doing some amount of type inference to detect\nvariables that have fixed types. All operations where no precise type could be\ndetermined are implemented with calls to a JavaScript runtime system, which does\nthe necessary type dispatching. The JavaScript runtime is implemented in C#.\nThe JavaScript implementation and the CLI VM with a tracing JIT sound quite\nunrelated at first, but together they amplify each other. The tracing JIT traces\nthe JavaScript functions that have been translated to CLI bytecode. Since the\nJavaScript runtime is in C#, it exists as CLI bytecode too. Thus it can be\ninlined into the JavaScript functions by the tracer. This is highly beneficial,\nsince it exposes the runtime type dispatching of the JavaScript operations to\nthe optimizations of the tracing JIT. Particularly the common expression\nelimination helps the JavaScript code. If a series of operations is performed on\nthe same object, the operations will all do the same type checks. All but the\ntype checks of the first operation can be removed by the optimizer.\n\nPerformance Results\nThe speed results of the combined JavaScript implementation and tracing JIT are\nquite impressive. It beats TraceMonkey for most benchmarks in SunSpider (apart\nfrom some string-heavy benchmarks that are quite slow) and can compete with V8\nin many of them. However, all this is steady-state performance and it seems\nSPUR's compile time is rather bad currently.\n\n\nFurther Possibilities\nA further (so far still hypothetical) advantage of SPUR is that the approach can\noptimize cases where execution crosses the border of two different systems. If\nsomebody wrote an HTML layout engine and a DOM in C# to get a web browser and\nintegrated it with the JavaScript implementation described above, the tracing\nJIT could optimize DOM manipulations performed by JavaScript code as well as\ncallbacks from the browser into JavaScript code.\nOf course the approach SPUR takes to implement JavaScript is completely\ngeneralizable. It should be possible to implement other dynamic languages in the\nsame way as JavaScript using SPUR. One would have to write a runtime system for\nthe language in C#, as well as a compiler from the language into CIL bytecode.\nGiven these two elements, SPUR's tracing JIT compiler would probably do a\nreasonable job at optimizing this other language (of course in practise, the\nlanguage implementation would need some tweaking and annotations to make it\nreally fast).\n\n\n\nComparison With PyPy\nThe goals of PyPy and SPUR are very similar. Both projects want to implement\ndynamic languages in an efficient way by using a tracing JIT. Both apply the\ntracing JIT \"one level down\", i.e. the runtime system of the dynamic language is\nvisible to the tracing JIT. This is the crucial point of the approach of both\nprojects. Since the runtime system of the dynamic language is visible to the\ntracing JIT, the JIT can optimize programs in that dynamic language. It does not\nitself need to know about the semantics of the dynamic language. This makes the\ntracing JIT usable for a variety of dynamic languages. It also means that the\ntwo halves can be implemented and debugged independently.\nIn SPUR, C# (or another language that is compilable to CIL) plays the role of\nRPython, and CIL is equivalent to the intermediate format that PyPy's\ntranslation toolchain uses. Both formats operate on a similar abstraction level,\nthey are quite close to C, but still have support for the object system of their\nrespective language and are garbage-collected.\nSPUR supports only a JavaScript implementation so far, which could maybe change in\nthe future. Thus JavaScript in SPUR corresponds to Python in PyPy, which was the\nfirst dynamic language implemented in PyPy (and is also the reason for PyPy's\nexistence).\nThere are obviously also differences between the two projects, although many of\nthem are only skin-deep. The largest difference is the reliance of SPUR on\ncompilers on all levels. PyPy takes the opposite approach of using interpreters\nalmost everywhere. The parts of PyPy that correspond to SPUR's compilers are (I\nwill use the Python implementation of PyPy as an example):\n\nthe JavaScript-to-CIL compiler corresponds to the Python interpreter of PyPy\nthe profiling JIT corresponds to a part of PyPy's translation toolchain\nwhich adds some profiling support in the process of turning RPython code into\nC code,\nthe tracing JIT corresponds to a special interpreter in the PyPy JIT which\nexecutes an RPython program and produces a trace of the execution\nthe transfer-tail JIT corresponds to PyPy's blackhole interpreter, also\ncalled fallback interpreter\nthe optimizing JIT corresponds to the optimizers and backends of PyPy's JIT\n\n\nPyPy's Optimizations\nComparing the optimizations that the two projects perform, the biggest\ndifference is that PyPy does \"trace stitching\" instead of fully supporting trace\ntrees. The difference between the two concerns what happens when a new trace\ngets added to an existing loop. The new trace starts from a guard in the\nexisting loop that was observed to fail often. Trace stitching means that the\nloop is just patched with a jump to the new trace. SPUR instead recompiles the\nwhole trace tree, which gives the optimizers more opportunities, but also makes\ncompilation a lot slower. Another difference is that PyPy does not perform\nloop-invariant code motion yet.\nMany of the remaining optimizations are very similar. PyPy supports guard\nimplication as well as guard strengthening. It has some load/store\noptimizations, but PyPy's alias analysis is quite rudimentary. On the other\nhand, PyPy's escape analysis is very powerful. PyPy also has support for the\nannotations that SPUR supports, using some decorators in the pypy.rlib.jit\nmodule. User-controlled loop unrolling is performed using the unroll_safe\ndecorator, tracing of a function can be disabled with the dont_look_inside\ndecorator.\nPyPy has a few more annotations that were not mentioned in the SPUR tech report.\nMost importantly, it is possible to declare a function as pure, using the\npurefunction decorator. PyPy's optimizers will remove calls to a function\ndecorated that way if the arguments to the call are all constant. In addition it\nis possible to declare instances of classes to be immutable, which means that\nfield accesses on constant instances can be folded away. Furthermore there is\nthe promote hint, which is spelled x = hint(x, promote=True). This will\nproduce a guard in the trace, to turn x into a constant after the guard.\n\n\n\nSummary\nGiven the similarity between the projects' goals, it is perhaps not so\nsurprising to see that PyPy and SPUR have co-evolved and reached many similar\ndesign decisions. It is still very good to see another project that does many\nthings in the same way as PyPy.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/comparing-spur-to-pypy-8835011873209414462.html"
    },
    {
      "title": "\"Blackhole\" interpreter",
      "text": "Hi all,\n\nHere are a few words about the JIT's \"great speedup in compiling\ntime\" advertized on the PyPy 1.3 release (see the\n\nprevious blog post).\nThe exact meaning behind these words needs a fair bit of\nexplanation, so here it is in case you are interested.\n\nIf you download a version of PyPy 1.3 that includes a JIT\ncompiler, you get an executable that could be qualified as rather\nfat: it actually contains three interpreters.  You have on the\none hand the regular Python interpreter.  It is here because it's\nnot possible to JIT-compile every single piece of Python code you\ntry to run; only the most executed loops are JIT-compiled.  They\nare JIT-compiled with a tracing interpreter that operates one\nlevel down.  This is the second interpreter.  This tracing step\nis quite slow, but it's all right because it's only invoked on\nthe most executed loops (on the order of 100 to 1000 times in\ntotal in a run of a Python script that takes anyway seconds or\nminutes to run).\n\nSo apart from the JIT compilation itself, we have two worlds in\nwhich the execution proceeds: either by regular interpretation,\nor by the execution of assembler code generated by the JIT\ncompiler.  And of course, we need to be able to switch from one\nworld to the other quickly: during regular interpretation we have\nto detect if we already have generated assembler for this piece\nof code and if so, jump to it; and during execution of the\nassembler, when a \"guard\" fails, i.e. when we meet a path of\nexecution for which we did not produce assembler, then we need to\nswitch back to regular interpretation (or occasionally invoke the\nJIT compiler again).\n\nLet us consider the cost of switching from one world to another.\nDuring regular interpretation, if we detect that we already have\nassembler corresponding to this Python loop, then we just jump to\nit instead of interpreting the Python loop.  This is fairly\ncheap, as it involves just one fast extra check per Python loop.\nThe reverse is harder because \"guard\" failures can occur at any\npoint in time: it is possible that the bit of assembler that we\nalready executed so far corresponds to running the first 4 Python\nopcodes of the loop and a half.  The guard that failed just now\nis somewhere in the middle of interpreting that opcode -- say,\nmultiplying these two Python objects.\n\nIt's almost impossible to just \"jump\" at the right place in the\ncode of the regular interpreter -- how do you jump inside a\nregular function compiled in C, itself in a call chain, resuming\nexecution of the function from somewhere in the middle?\n\nSo here is the important new bit in PyPy 1.3.  Previously, what\nwe would do is invoke the JIT compiler again in order to follow\nwhat needs to happen between the guard failure and the real end\nof the Python opcode.  We would then throw away the trace\ngenerated, as the only purpose was to finish running the current\nopcode.  We call this \"blackhole interpretation\".  After the end\nof the Python opcode, we can jump to the regular interpreter\neasily.\n\nDoing so was straightforward, but slow, in case it needs to be\ndone very often (as in the case in some examples, but not all).\nIn PyPy 1.3, this blackhole interpretation step has been\nredesigned as a time-critical component, and that's where the\nthird interpreter comes from.  It is an interpreter that works\nlike the JIT compiler, but without the overhead of tracing (e.g.\nit does not need to box all values).  It was designed from the\nground up for the sole purpose of finishing the execution of the\ncurrent Python opcode.  The bytecode format that it interprets is\nalso new, designed for that purpose, and the JIT compiler itself\n(the second interpreter) was adapted to it.\nThe old bytecode format in PyPy 1.2 is gone\n(it was more suited for the JIT compiler, but less for blackhole\ninterpretation).\n\nIn summary, it was a lot of changes in the most front-end-ish\nparts of the JIT compiler, even though it was mostly hidden\nchanges.  I hope that this longish blog post helped bring it a\nbit more to the light :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/06/blackhole-interpreter-2752965445510091289.html"
    },
    {
      "title": "PyPy 1.3 released",
      "text": "Hello.\nWe're please to announce the release of PyPy 1.3. This release has two major\nimprovements. First of all, we stabilized the JIT compiler since 1.2 release,\nanswered user issues, fixed bugs, and generally improved speed.\nWe're also pleased to announce alpha support for loading CPython extension\nmodules written in C. While the main purpose of this release is increased\nstability, this feature is in alpha stage and it is not yet suited for\nproduction environments.\n\nHighlights of this release\n\nWe introduced support for CPython extension modules written in C. As of now,\nthis support is in alpha, and it's very unlikely unaltered C extensions will\nwork out of the box, due to missing functions or refcounting details. The\nsupport is disabled by default, so you have to do:\n\nimport cpyext\n\nbefore trying to import any .so file. Also, libraries are source-compatible\nand not binary-compatible. That means you need to recompile binaries, using\nfor example:\n\npypy setup.py build\n\nDetails may vary, depending on your build system. Make sure you include\nthe above line at the beginning of setup.py or put it in your PYTHONSTARTUP.\nThis is alpha feature. It'll likely segfault. You have been warned!\n\nJIT bugfixes. A lot of bugs reported for the JIT have been fixed, and its\nstability greatly improved since 1.2 release.\n\nVarious small improvements have been added to the JIT code, as well as a great\nspeedup of compiling time.\n\n\n\n\nCheers,\nMaciej Fijalkowski, Armin Rigo, Alex Gaynor, Amaury Forgeot d'Arc and the PyPy team\n\n\nUpdate:The correct command to build extension is \"pypy setup.py build\", not \"python setup.py build\" as it was stated before.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/06/pypy-13-released-8546085566902489304.html"
    },
    {
      "title": "A JIT for Regular Expression Matching",
      "text": "This is part 2 of a series, see Part 1 for an introduction. In this post\nI want to describe how the JIT generator of the PyPy project can be used to turn\nthe elegant but not particularly fast regular expression matcher from the first\npart into a rather fast implementation. In addition, I will show some speed\nmeasurements against various regular expression implementations.\nAgain, note the disclaimer: This technology could not easily be used\nto implement Python's re-module.\n\nExample Expression and First Numbers\nThe regular expression I will use as an example in the rest of this paper is\nthe expression (a|b)*a(a|b){20}a(a|b)*. It matches all strings that have two\na with exactly 20 characters between them. This regular expression has\nthe property that the corresponding DFA needs 2**(n+1) different states. As\nan input string, we use a random string (of varying lengths) that does not\nmatch the regular expression. I will give all results as number of chars matched\nper second. While this is not a particularly typical regular expression, it\nshould still be possible to get some ballpark numbers for the speeds of various\nimplementations \u2013 as we will see, the differences between implementations are\nhuge anyway.\nAll the benchmarks were performed on my laptop, which has an Intel Core2 Duo\nP8400 processor with 2.26 GHz and 3072 KB of cache on a machine with 3GB RAM\nrunning Ubuntu Linux 10.04.\nTo get a feeling for the orders of magnitude involved, the CPython re module\n(which is implemented in C and quite optimized) can match 2'500'000 chars/s.\nGoogle's new re2 implementation still matches 550'000 chars/s. Google's\nimplementation is slower, but their algorithm gives complexity and space\nguarantees similar to our implementation in the last blog post.\nOn the other end of the performance scale is the pure-Python code from the last\nblog post running on CPython. It can match only 12'200 chars/s and is thus 200\ntimes slower than the re module.\n\n\nTranslating the Matcher\nThe code described in the last blog post is not only normal Python code, but\nalso perfectly valid RPython code. Nothing particularly dynamic is going on in\nthe code, thus it can be translated with PyPy's translation toolchain to C code.\nThe resulting binary is considerably faster and can match 720'000 chars/s, 60\ntimes faster than the untranslated version.\nAnother approach is to write equivalent versions of the algorithms in lower\nlevel languages. This has been done for C++ by Sebastian Fischer and for Java by\nBaltasar Tranc\u00f3n y Widemann. The algorithm is object-oriented enough to be\nmapped very closely to the respective languages. The C++ version is\na little bit faster than the RPython version translated to C, at 750'000 chars/s. That's\nnot very surprising, given their similarity. The Java version is more than twice\nas fast, with 1'920'000 chars/s. Apparently the Java JIT compiler is a lot\nbetter at optimizing the method calls in the algorithm or does some other\noptimizations. One reason for this could be that the Java JIT can assume that\nthe classes it sees are all there are (and it will invalidate the generated\nmachine code if more classes are loaded), whereas the C++ compiler needs to\ngenerate code that works even in the presence of more regular expression\nclasses.\n\n\nGenerating a JIT\nTo get even more performance out of the RPython code, it is possible to generate\na JIT for it with the help of the PyPy translation toolchain. To do this, the\nmatching code needs to be extended somewhat by some hints that tell PyPy's JIT\ngenerator how this is to be done. The JIT generator can automatically produce a\nJIT compiler from an RPython interpreter of the source language. In our case,\nwe view the regular expression matcher as an interpreter for regular\nexpressions. Then the match function corresponds to the\ndispatch loop of a traditional interpreter.\nOur regular expression matcher is a very peculiar interpreter. The matcher\nworks by running exactly one loop (the one in match) as many times as the\ninput string is long, irrespective of the \"program\", i.e. the particular\nregular expressions. In addition, within the loop there are no conditions (e.g.\nif statements) at all, it is just linear code. This makes it almost perfectly\nsuited\nto the JIT generator, which produces a tracing JIT. A tracing JIT compiles the\nhot loops of a program (i.e. regular expression) and has to do extra work if\nthere are conditions in the loop. In our case, there is exactly one loop per\nregular expression, without any condition.\n\nJIT Hints\nThe hints that are needed for the match function of the last blog post can\nbe seen here (the function is slightly rewritten, e.g. the JIT does only\nproperly support a while loop as the main dispatch loop):\njitdriver = jit.JitDriver(reds=[\"i\", \"result\", \"s\"], greens=[\"re\"])\n\ndef match(re, s):\n    if not s:\n        return re.empty\n    # shift a mark in from the left\n    result = re.shift(s[0], 1)\n    i = 1\n    while i < len(s):\n        jitdriver.can_enter_jit(i=i, result=result, s=s, re=re)\n        jitdriver.jit_merge_point(i=i, result=result, s=s, re=re)\n        # shift the internal marks around\n        result = re.shift(s[i], 0)\n        i += 1\n    re.reset()\n    return result\n\nThe jitdriver is an instance describing the data of the interpreter we are\ndealing with. The arguments to the constructor need to list all local variables\nof the dispatch loop. The local variables are classified into two classes, red\nones and green ones. The green ones hold the objects that make up the program\nthat the interpreter currently runs and which position in the program is\ncurrently being executed. In a typical bytecode interpreter, the bytecode object\nand the program counter would be green. In our case, the regular expression is\nthe program, so it is green. The rest of the variables are red.\nThe green variables are treated specially by the JIT generator. At runtime, for\na given value of the green variables, one piece of machine code will be\ngenerated. This piece of machine code can therefore assume that the value of\nthe green variable is constant.\nThere are two additional hints, which are method calls on the\njitdriver instance. The jit_merge_point method marks the beginning of\nthe main interpreter loop. The can_enter_jit function marks the point where\na loop in the user program can be closed, which in our case is trivial, it's\njust at the end of the interpreter loop (for technical reasons it is put at the beginning, because nothing must happen between the can_enter_jit and jit_merge_point invocations).\nThose are the hints that the JIT generator needs to function at all. We added\nsome additional hints, that give the JIT generator more information to work\nwith. Those hints are immutability information, which means that certain\ninstance fields can not be changed after the object has been constructed. Apart\nfrom the marked field, none of the fields of any of the Regex subclasses\ncan change. For example for the Char class this is expressed in the\nfollowing way:\nclass Char(Regex):\n    _immutable_fields_ = [\"c\"]\n    def __init__(self, c):\n        ...\n\nThese hints allow the generated JIT to constant-fold reads out of the immutable\nfields in some situations.\n\n\nAdaptions to the Original Code\nIn the introduction above I wrote that the code within the loop in match\nuses no conditions. It is indeed true that none of the _shift methods\nhave an if statement or similar. However, there are some hidden conditions\ndue to the fact that the and and or boolean operators are used, which\nare short-circuiting. Therefore the JIT-version of the code needs to be adapted\nto use the non-short-circuiting operators & and |.\n\n\nJIT Example\nTo get an impression of how the generated machine code looks like, consider the\nregular expression (a|b)*. As regular expression objects this would be\nRepetition(Alternative(Char('a'), Char('b'))). The machine code in its intermediate,\nmachine-independent form looks as follows (I have slightly cleaned it up and\nadded comments for clarity):\n# arguments of the loop\n# i0 is i in the match function\n# result0 is result in the match function\n# s0 is s in the match function\n[i0, result0, s0] # those are the arguments to the machine code\nchar = s0[i0] # read the character\n# read the current mark:\ni5 = ConstPtr(ptr_repetition).marked\ni7 = char == 'a' # is the character equal to 'a'\ni8 = i5 & i7\ni10 = char == 'b' # is the character equal to 'b'\ni11 = i5 & i10\n# write new mark\nConstPtr(ptr_chara).marked = i8\ni13 = i8 | i11\n# write new mark\nConstPtr(ptr_charb).marked = i11\n# write new mark\nConstPtr(ptr_alternative).marked = i13\n# increment the index\ni17 = i0 + 1\ni18 = len(s0)\n# write new mark\nConstPtr(ptr_repetition).marked = i13\n# check that index is smaller than the length of the string\ni19 = i17 < i18\nif not i19:\n    go back to normally running match\njump(i17, i13, s0) # start from the top again\n\nThe various ConstPtr(ptr_*) denote constant addresses of parts of the regular\nexpression tree:\n\nptr_repetition is the Repetition\nptr_chara is Char('a')\nptr_charb is Char('b')\nptr_alternative is the Alternative\n\nEssentially the machine code reads the next char out of the string, the current\nmark out of the Repetition and then performs some boolean operations on\nthose, writing back the new marks. Note in particular how the generated\nmachine code does not need to do any method calls to shift and _shift and\nthat most field reads out of the regular expression classes have been optimized\naway, because the fields are immutable. Therefore the machine code does not\nneed to deconstruct the tree of regular expression objects at all, it just\nknows where in memory the various parts of it are, and encodes that directly\ninto the code.\n\n\nPerformance Results With JIT\nWith the regular expression matcher translated to C and with a generated JIT,\nthe regular expression performance increases significantly. Our running example\ncan match 16'500'000 chars/s, which is more than six times faster than the\nre module. This is not an entirely fair comparison, because the re\nmodule can give more information than just \"matches\" or \"doesn't match\", but\nit's still interesting to see. A more relevant comparison is that between the\nprogram with and without a JIT: Generating a JIT speeds the matcher up by more\nthan 20 times.\n\n\n\nConclusion\nSo, what have we actually won? We translated the relatively simple and very slow\nregular expression matching algorithm from the last post to C and were thus able\nto speed it up significantly. The real win is gained by also generating a JIT\nfor the matcher, which can be regarded as a simple interpreter. The resulting\nmatcher is rather fast.\nThe lesson from these posts is not that you can or should write a practical\nand general regular expression module in this way \u2013 indeed, enhancing the\nalgorithm to support more features of the re module would be a lot of work\nand it is also unclear what the speed results for more realistic regular\nexpressions would be. However, it makes for a great case study of the JIT\ngenerator. It was relatively straightforward to generate a JIT for the regex\nmatcher, and the speed results were great (Admittedly I know rather a lot about\nPyPy's JIT though). This approach is generalizable to many programs that are\nsufficiently \"interpreter-like\" (whatever that exactly means).\nAll the results that appeared at various points in this blog post can be seen\nhere:\n\n\n\n\n\n\n\nImplementation\nchars/s\nspeedup over pure Python\n\nPure Python code\n12'200\n1\n\nPython re module\n2'500'000\n205\n\nGoogle's re2 implementation\n550'000\n45\n\nRPython implementation translated to C\n720'000\n59\n\nC++ implementation\n750'000\n61\n\nJava implementation\n1'920'000\n157\n\nRPython implementation with JIT\n16'500'000\n1352\n\n\n\n\nSources\nAll the source code can be found in my Subversion user directory on Codespeak.\n\nEdit: Armin is right (see first comment). I fixed the problem.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/06/jit-for-regular-expression-matching-3877859053629057968.html"
    },
    {
      "title": "PyPy in Google's Summer of Code 2010",
      "text": "Good news everyone.\nThis year, thanks to google generosity and PSF support, we got two and a\nhalf of students for PyPy's summer of code. We didn't cut any students, but one\nof the projects is a joint project of PyPy and numpy. Hereby I present\ndescriptions, in my own words with my own opinions and in arbitrary order.  For\nmore details please follow links to particular blogs.\n\nJason Creighton: 64bit JIT backend for PyPy\nIntel 64bit (and I mean x86_64) compatibility for JIT has been one of the top\nrequested features (along with GIL removal). While GIL removal is not really an\neasy task, having our JIT emit 64bit assembler is sort of easy, thanks to our\nJIT backend abstraction. It will likely be faster, thanks to abundance of\nregisters.\n\n\nBartosz Skowron: Fast ctypes for PyPy\nHistorically weak point of PyPy was compatibility with extension modules.  We\nhave progressed quite a bit in recent years, first introducing ctypes for\npypy then progressing towards CPython extension modules. However, ctypes is\nwell known to be slow (and it's even slower on PyPy) and writing CPython\nextension modules is ugly, and it's going to be only with compatibility layer\nthat'll keep this slow. What happens if we try to employ JIT technology to\nctypes? Maybe we can compile calls to C code from Python as a direct calls in\ncompiled assembler? Why not?\nThis project will look how the JIT technology can be employed to do some\nsort of FFI. There is no guarantee we'll get super-fast ctypes as a result,\nbut it's good to see progress in that area.\n\n\nDan Roberts: Numpy in PyPy\nThis is a joint project of numpy and PyPy. The main objective is to bring\nnumpy to PyPy, possibly fast. The official mentor for this project is\nStefan van der Walt from numpy community. During initial meeting it was\nagreed that probably the best way to go would be to support original numpy\nwith CPython extension compatibility and then provide a minimal native numpy\nframework for pypy. The former would retain full compatibility, while the\nlatter would have JIT integration, with line of our previous\nnumeric experiments. There would be an explicit interface from converting\none array to another for convinience.\n\nOverall, I'm very happy to see so much support for PyPy from SoC. I hope all\nthree proposals will be successful!\nCheers,\nfijal & pypy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/05/pypy-in-googles-summer-of-code-2010-5321939902318322352.html"
    },
    {
      "title": "An Efficient and Elegant Regular Expression Matcher in Python",
      "text": "Two weeks ago, I was at the Workshop Programmiersprachen und Rechenkonzepte,\na yearly meeting of German programming language researchers. At the workshop,\nFrank Huch and Sebastian Fischer gave a really excellent talk about an\nelegant regular expression matcher written in Haskell. One design goal of the\nmatcher was to run in time linear to the length of the input string (i.e.\nwithout backtracking) and linear in the size of the regular expression. The\nmemory use should also only be linear in the regular expression.\nDuring the workshop, some of the Haskell people and me then implemented the\nalgorithm in (R)Python. Involved were Frank, Sebastian, Baltasar Tranc\u00f3n y\nWidemann, Bernd Bra\u00dfel and Fabian Reck.\nIn this blog post I want to describe this implementation and show the code of\nit, because it is quite simple. In a later post I will show what optimizations\nPyPy can perform on this matcher and also do some benchmarks.\nA Note on terminology: In the rest of the post \"regular expression\" is meant\nin the Computer Science sense, not in the POSIX sense. Most importantly, that\nmeans that back-references are not allowed.\nAnother note: This algorithm could not be used to implement PyPy's re\nmodule! So it won't help to speed up this currently rather slow implementation.\n\nImplementing Regular Expression Matchers\nThere are two typical approaches to implement regular expression. A naive one is\nto use a back-tracking implementation, which can lead to exponential matching\ntimes given a sufficiently evil regular expression.\nThe other, more complex one, is to transform the regular expression into a\nnon-deterministic finite automaton (NFA) and then transform the NFA into a\ndeterministic finite automaton (DFA). A DFA can be used to efficiently match\na string, the problem of this approach is that turning an NFA into a DFA can\nlead to exponentially large automatons.\nGiven this problem of potential memory explosion, a more sophisticated approach\nto matching is to not construct the DFA fully, but instead use the NFA for\nmatching. This requires some care, because it is necessary to keep track of\nwhich set of states the automaton is in (it is not just one state, because the\nautomaton is non-deterministic).\nThe algorithm described here is essentially equivalent to this approach, however\nit does not need an intermediate NFA and represents a state of a corresponding\nDFA as marked regular expression (represented as a tree of nodes). For many\ndetails about an alternative approach to implement regular expressions\nefficiently, see Russ Cox excellent article collection.\n\n\nThe Algorithm\nIn the algorithm the regular expression is represented as a tree of nodes. The\nleaves of the nodes can match exactly one character (or the epsilon node, which\nmatches the empty string). The inner nodes of the tree combine other nodes in\nvarious ways, like alternative, sequence or repetition. Every node in the tree\ncan potentially have a mark. The meaning of the mark is that a node is marked,\nif that sub-expression matches the string seen so far.\nThe basic approach of the algorithm is that for every character of the input\nstring the regular expression tree is walked and a number of the nodes in the\nregular expression are marked. At the end of the string, if the top-level node\nis marked, the string matches, otherwise it does not. At the beginning of the\nstring, one mark gets shifted into the regular expression from the top, and then\nthe marks that are in the regex already are shifted around for every additional\ncharacter.\nLet's start looking at some code, and an example to make this clearer. The base\nclass of all regular expression nodes is this:\nclass Regex(object):\n    def __init__(self, empty):\n        # empty denotes whether the regular expression\n        # can match the empty string\n        self.empty = empty\n        # mark that is shifted through the regex\n        self.marked = False\n\n    def reset(self):\n        \"\"\" reset all marks in the regular expression \"\"\"\n        self.marked = False\n\n    def shift(self, c, mark):\n        \"\"\" shift the mark from left to right, matching character c.\"\"\"\n        # _shift is implemented in the concrete classes\n        marked = self._shift(c, mark)\n        self.marked = marked\n        return marked\n\nThe match function which checks whether a string matches a regex is:\ndef match(re, s):\n    if not s:\n        return re.empty\n    # shift a mark in from the left\n    result = re.shift(s[0], True)\n    for c in s[1:]:\n        # shift the internal marks around\n        result = re.shift(c, False)\n    re.reset()\n    return result\n\nThe most important subclass of Regex is Char, which matches one\nconcrete character:\nclass Char(Regex):\n    def __init__(self, c):\n        Regex.__init__(self, False)\n        self.c = c\n\n    def _shift(self, c, mark):\n        return mark and c == self.c\n\nShifting the mark through Char is easy: a Char instance retains a mark\nthat is shifted in when the current character is the same as that in the\ninstance.\nAnother easy case is that of the empty regular expression Epsilon:\nclass Epsilon(Regex):\n    def __init__(self):\n        Regex.__init__(self, empty=True)\n\n    def _shift(self, c, mark):\n        return False\n\nEpsilons never get a mark, but they can match the empty string.\n\nAlternative\nNow the more interesting cases remain. First we define an abstract base class\nBinary for the case of composite regular expressions with two children, and\nthen the first subclass Alternative which matches if either of two regular\nexpressions matches the string (usual regular expressions syntax a|b).\nclass Binary(Regex):\n    def __init__(self, left, right, empty):\n        Regex.__init__(self, empty)\n        self.left = left\n        self.right = right\n\n    def reset(self):\n        self.left.reset()\n        self.right.reset()\n        Regex.reset(self)\n\nclass Alternative(Binary):\n    def __init__(self, left, right):\n        empty = left.empty or right.empty\n        Binary.__init__(self, left, right, empty)\n\n    def _shift(self, c, mark):\n        marked_left  = self.left.shift(c, mark)\n        marked_right = self.right.shift(c, mark)\n        return marked_left or marked_right\n\nAn Alternative can match the empty string, if either of its children can.\nSimilarly, shifting a mark into an Alternative shifts it into both its\nchildren. If either of the children are marked afterwards, the Alternative\nis marked too.\nAs an example, consider the regular expression a|b|c, which would be\nrepresented by the objects Alternative(Alternative(Char('a'), Char('b')), Char('c')).\nMatching the string \"a\" would lead to the following marks in\nthe regular expression objects (green nodes are marked, white ones are\nunmarked):\n\n\nAt the start of the process, no node is marked. Then the first char is matched,\nwhich adds a mark to the Char('a') node, and the mark will propagate up the\ntwo Alternative nodes.\n\n\nRepetition\nThe two remaining classes are slightly trickier. Repetition is used to match\na regular expression any number of times (usual regular expressions syntax\na*):\nclass Repetition(Regex):\n    def __init__(self, re):\n        Regex.__init__(self, True)\n        self.re = re\n\n    def _shift(self, c, mark):\n        return self.re.shift(c, mark or self.marked)\n\n    def reset(self):\n        self.re.reset()\n        Regex.reset(self)\n\nA Repetition can always match the empty string. The mark is shifted into the\nchild, but if the Repetition is already marked, this will be shifted into\nthe child as well, because the Repetition could match a second time.\nAs an example, consider the regular expression (a|b|c)* matching the string\nabcbac:\n\nFor every character, one of the alternatives matches, thus the repetition matches\nas well.\n\n\nSequence\nThe only missing class is that for sequences of expressions, Sequence (usual\nregular expressions syntax ab):\nclass Sequence(Binary):\n    def __init__(self, left, right):\n        empty = left.empty and right.empty\n        Binary.__init__(self, left, right, empty)\n\n    def _shift(self, c, mark):\n        old_marked_left = self.left.marked\n        marked_left = self.left.shift(c, mark)\n        marked_right = self.right.shift(\n            c, old_marked_left or (mark and self.left.empty))\n        return (marked_left and self.right.empty) or marked_right\n\nA Sequence can be empty only if both its children are empty. The mark\nhandling is a bit delicate. If a mark is shifted in, it will be shifted to the\nleft child regular expression. If that left child is already marked before the\nshift, that mark is shifted to the right child. If the left child can match the\nempty string, the right child gets the mark shifted in as well.\nThe whole sequence matches (i.e. is marked), if the left child is marked after\nthe shift and if the right child can match the empty string, or if the right\nchild is marked.\nConsider the regular expression abc matching the string abcd. For the\nfirst three characters, the marks wander from left to right, when the d is\nreached, the matching fails.\n\n\n\nMore Complex Example\nAs a more complex example, consider the expression ((abc)*|(abcd))(d|e)\nmatching the string abcabcabcd.\n\nNote how the two branches of the first alternative match the first abc in\nparallel, until it becomes clear that only the left alternative (abc)* can\nwork.\n\n\nComplexity\nThe match function above loops over the entire string without going back and\nforth. Each iteration goes over the whole tree every time. Thus the complexity\nof the algorithm is O(m*n) where m is the size of the regular expression\nand n is the length of the string.\n\n\n\nSummary & Outlook\nSo, what have we achieved now? The code shown here can match regular expressions\nwith the desired complexity. It is also not much code. By itself, the Python\ncode shown above is not terribly efficient. In the next post I will show how the\nJIT generator can be used to make the simple matcher shown above really fast.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/05/efficient-and-elegant-regular-2727904462179540436.html"
    },
    {
      "title": "Running wxPython on top of pypy",
      "text": "Hello,\nThese last three weeks we have been busy working on the cpyext subsystem, which\nallows pypy to execute extension modules written with the Python C API.\nToday we hacked enough to have wxPython compile, and run its wonderful demo.\nThis:\n\ncannot be distinguished from the same run with a\nstandard python interpreter, but this:\n\nshows an exception that\nCPython never produces.\nwxPython is a big extension module: it has more than 500 classes and 7500\nfunctions, most of the code is automatically generated by swig.  It uses\nadvanced techniques, like \"Original Object Return\" and cross-platform\npolymorphism, that effectively allows the developer to seamlessly subclass C++\nobjects in Python and write GUI applications efficiently.\nThe demo application runs reasonably fast, it feels slower than with CPython,\nbut I did not activate the JIT option of pypy.  It still crashes in some places\n(the demo is very comprehensive and covers all the aspects of wxPython), and\nthreads are expected to not work at the moment.\nWe had to modify a little the code of wxPython, mainly because it often stores\nborrowed references into C++ objects.  This does not work well in pypy, where\nall other counted references can disappear, and allows the address of the object\nto change.  The solution is to use weak references instead.  The patch is here,\nit will eventually be merged into the upstream wxPython version.\nThis first real test proves that CPython extensions can be migrated to pypy\nwithout much pain.  It also points some places which can be improved, like\nbetter diagnostics in crashes, better support of distutils...\nAmaury Forgeot d'Arc",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/05/running-wxpython-on-top-of-pypy-52246787415886751.html"
    },
    {
      "title": "Using CPython extension modules with PyPy natively, or: PyPy can load .pyd files with CPyExt!",
      "text": "PyPy is now able to load\nand run CPython extension modules (i.e. .pyd and .so files) natively by using the new CPyExt\nsubsystem.\nUnlike the solution presented in another blog post (where extension modules like\nnumpy etc. were run on CPython and proxied through TCP), this solution does not require\na running CPython anymore. We do not achieve full binary compatiblity\nyet (like Ironclad), but recompiling the extension is generally enough.\nThe only prerequisite is that the necessary functions of the C API of CPython are already\nimplemented in PyPy. If you are a user or an author of a module and miss certain functions\nin PyPy, we invite you to implement them. Up until now, a lot of people (including a lot of\nnew committers) have stepped up and implemented a few functions to get their favorite module\nrunning. See the end of this post for a list of names.\nRegarding speed, we tried the following: even though there is a bit of overhead when running\nthese modules, we could run the regular expression engine of CPython (_sre.so) and execute\nthe spambayes benchmark of the Unladen Swallow benchmark suite (cf. speed.pypy.org) and\nexperience a speedup:\nIt became two times faster on pypy-c than with the built-in regular\nexpression engine of PyPy. From Amdahl's Law it follows that the _sre.so must run several\ntimes faster than the built-in engine.\nCurrently pursued modules include PIL and others. Distutils support is nearly ready.\nIf you would like to participate or want information on how to use this new feature, come and join\nour IRC channel #pypy on freenode.\nAmaury Forgeot d'Arc and Alexander Schremmer\nFurther CPyExt Contributors:\nAlex Gaynor\nBenjamin Peterson\nJean-Paul Calderone\nMaciej Fijalkowski\nJan de Mooij\nLucian Branescu Mihaila\nAndreas St\u00fchrk\nZooko Wilcox-O Hearn",
      "tags": "cpyext,CPython,extension modules,speed",
      "url": "https://www.pypy.org/posts/2010/04/using-cpython-extension-modules-with-5864754772659599217.html"
    },
    {
      "title": "PyPy on google open source blog",
      "text": "Hello\nBea D\u00fcring, from the PyPy team, wrote a post for google open source blog covering PyPy's 1.2 release. It's also the first public mention of the fact that google provided financial support for PyPy's 2.5 compatibility. Thanks!\nCheers\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/04/pypy-on-google-open-source-blog-1192495586835103069.html"
    },
    {
      "title": "Introducing nightly builds and ubuntu PPA",
      "text": "Hello.\n\nWe're pleased to announce two things that we were constantly asked for: Nightly builds and Ubuntu PPA for 1.2 release made by Bartosz Skowron. There are no nightly build ubuntu packages (yet).\n\n\nNightly builds are what they are - pure pypy executables with JIT compiled in (for linux only now). They require either a pypy checkout or a release download. The main difference is that by default display more debugging information than release builds and that they contain recent bugfixes and improvements of course :-)\n\nCheers\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/introducing-nightly-builds-and-ubuntu-3346203966988761264.html"
    },
    {
      "title": "Blog coverage of speed.pypy.org",
      "text": "If you want to read a detailed analysis about why speed.pypy.org is cool, head over to Saveen Reddy's blog at the MSDN.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/blog-coverage-of-speedpypyorg-2291955489972824511.html"
    },
    {
      "title": "Heroes of the 1.2 Release",
      "text": "Now that the release is done I wanted to list and to thank some people that\nwere essential in the process of getting it out of the door, particularly\nbecause the work of some of them is not very visible usually.\nArmin Rigo and Maciej Fija\u0142kowski tirelessly worked on most aspects of\nthe release, be it fixing the last known bugs and performance problems,\npackaging or general wizardry.\nAmaury Forgeot d'Arc made sure that PyPy 1.2 actually supports Windows as a\nplatform properly and compiled the Windows binaries.\nMiquel Torres designed and implemented our new speed overview page,\nhttps://speed.pypy.org which is a great tool for us to spot performance\nregressions and to showcase our improvements to the general public.\ntav designed the new user-oriented web page, https://pypy.org which is a lot\nnicer for people that only want to use PyPy as a Python implementation (and not\nbe confused by how PyPy is actually made).\nHolger Krekel fixed our main development server codespeak.net, even while\nbeing on vacation and not really having online connectivity. Without that, we\ncouldn't actually have released anything.\nBartosz Skowron worked a lot on making Ubuntu packages for PyPy, which is\nreally cool. Even though he didn't quite finish in time for the release, we will\nhopefully get them soon.\nThanks to all you guys!",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/03/heroes-of-12-release-7211722984024027191.html"
    },
    {
      "title": "Introducing the PyPy 1.2 release",
      "text": "We are pleased to announce PyPy's 1.2 release.\nThis version 1.2 is a major milestone and it is the first release to ship\na Just-in-Time compiler that is known to be faster than CPython\n(and unladen swallow) on some real-world applications (or the best benchmarks\nwe could get for them). The main theme for the 1.2 release is speed.\nThe JIT is stable and we don't observe crashes. Nevertheless we would\nrecommend you to treat it as beta software and as a way to try out the JIT\nto see how it works for you.\nHighlights:\n\nThe JIT compiler.\nVarious interpreter optimizations that improve performance as well as help\nsave memory. Read our various blog posts about achievements.\nIntroducing a new PyPy website at pypy.org made by tav and improved\nby the PyPy team.\nIntroducing speed.pypy.org made by Miquel Torres, a new service that monitors our performance\nnightly.\nThere will be ubuntu packages on PyPy's PPA made by Bartosz Skowron,\nhowever various troubles prevented us from having them as of now.\n\nKnown JIT problems (or why you should consider this beta software) are:\n\nThe only supported platform is 32bit x86 for now, we're looking for help with\nother platforms.\nIt is still memory-hungry.  There is no limit on the amount of RAM that\nthe assembler can consume; it is thus possible (although unlikely) that\nthe assembler ends up using unreasonable amounts of memory.\n\nIf you want to try PyPy, go to the download page on our excellent new site\nand find the binary for your platform. If the binary does not work (e.g. on\nLinux, because of different versions of external .so dependencies), or if\nyour platform is not supported, you can try building from the source.\nThe PyPy release team,\nArmin Rigo, Maciej Fijalkowski and Amaury Forgeot d'Arc\nTogether with\nAntonio Cuni, Carl Friedrich Bolz, Holger Krekel, Samuele Pedroni and many others.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/03/introducing-pypy-12-release-2791388655442447862.html"
    },
    {
      "title": "State of PyPy talk from Pycon",
      "text": "Hello.\n\nThe last PyPy video from pycon has been uploaded. It's a very short (less than 10 minutes) \"keynote\" talk about state of PyPy.\n\nEnjoy!\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/state-of-pypy-talk-from-pycon-6748503931490058986.html"
    },
    {
      "title": "Introducing speed.pypy.org",
      "text": "Hello.\nSome time ago, we introduced our nightly performance graphs. This was a quick\nhack to allow us to see performance regressions. Thanks to Miquel Torres,\nwe can now introduce https://speed.pypy.org, which is a Django-powered web\napp sporting a more polished visualisation of our nightly performance runs.\nWhile this website is not finished yet, it's already far better than our previous\napproach :-)\nDetails about announcement on pypy-dev are found here.\nIf you're are interested in having something similar for other benchmark runs, contact Miquel (tobami at gmail).\nQuoting Miquel: \"I would also like to note, that if other performance-oriented\nopensource projects are interested, I would be willing to see if we can set-up\nsuch a Speed Center for them. There are already people interested in\ncontributing to make it into a framework to be plugged into buildbots, software\nforges and the like. Stay tuned!\"",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/introducing-speedpypyorg-1822874891591164256.html"
    },
    {
      "title": "Benchmarking twisted",
      "text": "Hello.\nI recently did some benchmarking of twisted on top of PyPy. For the very\nimpatient: PyPy is up to 285% faster than CPython. For more patient people,\nthere is a full explanation of what I did and how I performed measurments,\nso they can judge themselves.\nThe benchmarks are living in twisted-benchmarks and were mostly written\nby Jean Paul Calderone. Even though he called them \"initial exploratory\ninvestigation into a potential direction for future development resulting\nin performance oriented metrics guiding the process of optimization and\navoidance of complexity regressions\", they're still much much better than\naverage benchmarks found out there.\nThe methodology was to run each benchmark for\nquite some time (about 1 minute), measuring number of requests each 5s.\nThen I looked at dump of data and substracted some time it took\nfor JIT-capable interpreters to warm up (up to 15s), averaging\neverything after that. Averages of requests per second are in the table below (the higher the better):\n\n\n\n\n\n\n\n\nbenchname\nCPython\nUnladen swallow\nPyPy\n\nnames\n10930\n11940 (9% faster)\n15429 (40% faster)\n\npb\n1705\n2280 (34% faster)\n3029 (78% faster)\n\niterations\n75569\n94554 (25% faster)\n291066 (285% faster)\n\naccept\n2176\n2166 (same speed)\n2290 (5% faster)\n\nweb\n879\n854 (3% slower)\n1040 (18% faster)\n\ntcp\n105M\n119M (7% faster)\n60M (46% slower)\n\n\n\nTo reproduce, run each benchmark with:\n\nbenchname.py -n 12 -d 5\nWARNING: running tcp-based benchmarks that open new connection for each\nrequest (web & accept) can exhaust number of some kernel structures,\nlimit n or wait until next run if you see drops in request per second.\nThe first obvious thing is that various benchmarks are more or less amenable\nto speedups by JIT compilation. Accept and tcp getting smallest speedups, if at\nall. This is understandable, since JIT is mostly about reducing interpretation\nand frame overhead, which is probably not large when it comes to accepting\nconnections. However, if you actually loop around, doing something, JIT\ncan give you a lot of speedup.\nThe other obvious thing is that PyPy is the fastest python interpreter\nhere, almost across-the board (Jython and IronPython won't run twisted),\nexcept for raw tcp throughput. However, speedups can vary and I expect\nthis to improve after the release, as there are points, where PyPy can\nbe improved. Regarding raw tcp throughput - this can be a problem for\nsome applications and we're looking forward to improve this particular\nbit.\nThe main reason to use twisted for this comparison is a lot of support from\ntwisted team and JP Calderone in particular, especially when it comes to\nproviding benchmarks. If some open source project wants to be looked at\nby PyPy team, please provide a reasonable set of benchmarks and infrastructure.\nIf, however, you're a closed source project fighting with performance problems\nof Python, we're providing contracting for investigating opportunities, how\nPyPy and not only PyPy, can speed up your project.\nCheers,\nfijal\n\nBenchmark descriptions:\n\nnames - simple DNS server\nweb - simple http hello world server\npb - perspective broker, RPC mechanism for twisted\niterations - empty twisted loop\naccept - number of tcp connections accepted per second\ntcp - raw socket transfer throughput\n\nUsed interpreters:\n\nCPython 2.6.2 - as packaged by ubuntu\nUnladen swallow svn trunk, revision 1109\nPyPy svn trunk, revision 71439\n\nTwisted version used: svn trunk, revision 28580\nMachine: unfortunately 32bit virtual-machine under qemu, running ubuntu karmic,\non top of Quad core intel Q9550 with 6M cache. Courtesy of Michael Schneider.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2010/03/hello-5058108566628405592.html"
    },
    {
      "title": "Pycon 2010 report",
      "text": "Hello.\nGreetings to everybody from Pycon 2010 Atlanta. Right now I'm sitting in\na sprint room with people sprinting on various projects, like CPython,\ntwisted etc. The conference was really great, and I've seen some good talks,\nalthough I've been too exhausted from my own talks to go to too many.\nProbably I should stay away from proposing that many talks to next pycon :-)\nThe highlight of sprints was that we got a common mercurial repository at python.org for python benchmarks. We might be able to come up with\n\"the python benchmark suite\" which will mostly consist \nof simple benchmarks using large python libraries, rather than microbenchmarks.\nThe repository was started by the Unladen Swallow people and we already\nhave common commit access among PyPy, CPython, Unladen Swallow, Jython\nand Iron Python. We don't have yet a common place to run benchmarks,\nbut we should be able to fix that soon.\nRegarding the talks, there are online videos for\nHow to write cross-interpreter python programs and Speed of PyPy talks,\namong other talks from Pycon.\nThere should be a video for my short keynote shortly.\nThe talks were well received as there is interest in PyPy's progress.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/02/pycon-2010-report-6986911457623699520.html"
    },
    {
      "title": "Nightly graphs of PyPy's performance",
      "text": "Hello.\nIn the past few months, we made tremendous progress on the JIT front.\nTo monitor the progress daily, we introduced recently some cool graphs\nthat plot revision vs performance. They are based on unladen swallow\nbenchmark runner and they're written entirely in JavaScript, using canvas\nvia the JQuery and Flot libraries.\nIt's amazing what you can do in JavaScript these days... They are also\ntested via the very good oejskit plugin, that integrates py.test\nwith JavaScript testing, driven by the command line.\nAs you can probably see, we're very good on some benchmarks and not that\ngreat on others. Some of the bad results come from the fact that while we\ndid a lot of JIT-related work, other PyPy parts did not see that much\nlove. Some of our algorithms on the builtin data types are inferior to those\nof CPython. This is going to be an ongoing focus for a while.\nWe want to first improve on the benchmarks for a couple\nof weeks before doing a release to gather further feedback.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/01/nightly-graphs-of-pypys-performance-8360469412941669946.html"
    },
    {
      "title": "Accelerating PyPy development by funding",
      "text": "PyPy has recently made some great speed and memory progress towards providing the most efficient Python interpreter out there.  We also just announced\nour plans for the pypy-1.2 release.  Much of this is driven by personal\ncommitment, by individuals and companies investing time and money.\nNow we'd appreciate some feedback and help regarding getting money\ninto the PyPy project to help its core members (between\n5 and 15 people depending how you count) to sustain themselves.  We see\nseveral options:\n\nuse a foundation structure and ask for tax-exempt donations to the\nproject, its developers and infrastructure.  We just got\na letter from the Software Freedom Conservancy that they view\nour application favourably so this option becomes practical hopefully\nsoon.\noffer to implement certain features like a 64bit JIT-backend,\nNumpy for PyPy or a streamlined installation in exchange for money,\ncontributed in small portions/donations.  Do you imagine you or your\ncompany would sponsor PyPy on a small scale for efforts like this?\nAny other bits you'd like to see?\noffer to implement larger scale tasks by contracting PyPy related companies,\nnamely Open End and merlinux who have successfully done such\ncontracts in the past.  Please don't hesitate to contact\nholger@merlinux.eu and bea@openend.se if you want to start a\nconversation on this.\napply for public/state funding - in fact we are likely to get some\nfunding through Eurostars, more on that separately.  Such funding\nis usually only a 50-60% percentage of actual employment and\nproject costs, and is tied to research questions rather than\nto make PyPy a production-useable interpreter, though.\n\nAnything else we should look out for?\ncheers & thanks for any feedback,\nMaciej and Holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/12/accelerating-pypy-development-by-8973749020516679741.html"
    },
    {
      "title": "Planning a next release of PyPy",
      "text": "The PyPy core team is planning to make a new release before the next Pycon US.\nThe main target of the 1.2 release is packaging the good results\nwe have achieved applying our current JIT compiler generator to our\nPython interpreter. Some of that progress has been chronicled in\nrecent posts on the status blog. By releasing them in a\nrelatively stable prototype we want to encourage people to try them with their\nown code and to gather feedback in this way. By construction the JIT compiler\nshould support all Python features, what may vary are the speedups\nachieved (in some cases the JIT may produce worse results than the PyPy\ninterpreter which we would like to know) and the extra memory required\nby it.\nFor the 1.2 release we will focus on the JIT stability first, less on\nimproving non-strictly JIT areas. The JIT should be good at many things\nas shown by previous blog postings. We want the JIT compiler in the\nrelease to work well on Intel 32 bits on Linux, with Mac OS X and\nWindows being secondary targets.  Which compilation targets work will\ndepend a bit on contributions.\nIn order to finalize the release we intend to have a concentrated\neffort (\"virtual sprint\") from the 22nd to the 29th of\nJanuary. Coordination will happen as usual through the #pypy irc\nchannel on freenode. Samuele Pedroni will take the role of release\nmanager as he already did in the past.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2009/12/planning-next-release-of-pypy-4193252449406707091.html"
    },
    {
      "title": "Leysin Winter Sprint: reported",
      "text": "Update: the sprint has been reported to some later date.\n\nThe next PyPy sprint will probably still be in Leysin, Switzerland, for the\nseventh time.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/12/leysin-winter-sprint-23-30th-january-7768876505015446348.html"
    },
    {
      "title": "Using CPython extension modules with PyPy, or: PyQt on PyPy",
      "text": "If you have ever wanted to use CPython extension modules on PyPy,\nwe want to announce that there is a solution that should be compatible\nto quite a bit of the available modules. It is neither new nor written\nby us, but works nevertheless great with PyPy.\nThe trick is to use RPyC, a transparent, symmetric remote procedure\ncall library written in Python. The idea is to start a\nCPython process that hosts the PyQt libraries\nand connect to it via TCP to send RPC commands to it.\nI tried to run PyQt applications\nusing it on PyPy and could get quite a bit of the functionality of these\nworking. Remaining problems include regular segfaults of CPython\nbecause of PyQt-induced memory corruption and bugs because classes\nlike StandardButtons behave incorrectly when it comes to arithmetical operations.\nChanges to RPyC needed to be done to support remote unbound __init__ methods,\nshallow call by value for list and dict types (PyQt4 methods want real lists and dicts\nas parameters), and callbacks to methods (all remote method objects are wrapped into\nsmall lambda functions to ease the call for PyQt4).\nIf you want to try RPyC to run the PyQt application of your choice, you just\nneed to follow these steps. Please report your experience here in the blog\ncomments or on our mailing list.\n\n\nDownload RPyC from the RPyC download page.\nDownload this patch and apply it to RPyC by running\npatch -p1 < rpyc-3.0.7-pyqt4-compat.patch in the RPyC directory.\nInstall RPyc by running python setup.py install as root.\nRun the file rpyc/servers/classic_server.py using CPython.\nExecute your PyQt application on PyPy.\n\n\nPyPy will automatically connect to CPython and use its PyQt libraries.\nNote that this scheme works with nearly every extension library. Look\nat pypy/lib/sip.py on how to add new libraries (you need to create\nsuch a file for every proxied extension module).\nHave fun with PyQt\nAlexander Schremmer",
      "tags": "CPython,extension modules,PyQt4,RPyC",
      "url": "https://www.pypy.org/posts/2009/11/using-cpython-extension-modules-with-4951018896657992031.html"
    },
    {
      "title": "Some benchmarking",
      "text": "Hello.\n\nRecently, thanks to the surprisingly helpful Unhelpful, also known as Andrew Mahone,\nwe have a decent, if slightly arbitrary, set of performances graphs.\nIt contains a couple of benchmarks already\nseen on this blog as well as some taken from The Great Computer\nLanguage Benchmarks Game. These benchmarks don't even try to represent \"real applications\"\nas they're mostly small algorithmic benchmarks. Interpreters used:\n\n\n\nPyPy trunk, revision 69331 with --translation-backendopt-storesink, which is\nnow on by default\n\n\nUnladen swallow trunk, r900\n\nCPython 2.6.2 release\n\n\nHere are the graphs; the benchmarks and the runner script are available\n\n\n\n\nAnd zoomed in for all benchmarks except binary-trees and fannkuch.\n\n\n\nAs we can see, PyPy is generally somewhere between the same speed\nas CPython to 50x faster (f1int). The places where we're the same\nspeed as CPython are places where we know we have problems - for example generators are\nnot sped up by the JIT and they require some work (although not as much by far\nas generators & Psyco :-). The glaring inefficiency is in the regex-dna benchmark.\nThis one clearly demonstrates that our regular expression engine is really,\nreally, bad and urgently requires attention.\n\n\nThe cool thing here is, that although these benchmarks might not represent\ntypical python applications, they're not uninteresting. They show\nthat algorithmic code does not need to be far slower in Python than in C,\nso using PyPy one need not worry about algorithmic code being dramatically\nslow. As many readers would agree, that kills yet another usage of C in our\nlives :-)\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/11/some-benchmarking-9211261260383281459.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report",
      "text": "While the D\u00fcsseldorf is dwindling off, we put our minds to the task of retelling\nour accomplishments. The sprint was mostly about improving the JIT and we         \nmanaged to stick to that task (as much as we managed to stick to anything). The   \nsprint was mostly filled with doing many small things.                        \n                                               \nInlining                                                                 \nCarl Friedrich and Samuele started the sprint trying to tame the JIT's inlining.\nUntil now, the JIT would try to inline everything in a loop (except other loops)  \nwhich is what most tracing JITs actually do. This works great if the resulting    \ntrace is of reasonable length, but if not it would result in excessive memory     \nconsumption and code cache problems in the CPU. So far we just had a limit on     \nthe trace size, and we would abort tracing when the limit was reached. This       \nwould happen again and again for the same loop, which is not useful at all. The   \nnew approach introduced is to be more clever when tracing is aborted by marking   \nthe function with the largest contribution to the trace size as non-inlinable. The\nnext time this loop is traced, it usually then gives a reasonably sized trace.\nThis gives a problem because now some functions that don't contain loops are not\ninlined, which means they never get assembler code for them generated. To remedy  \nthis problem we also make it possible to trace functions from their start (as     \nopposed to just tracing loops). We do that only for functions that can not be     \ninlinined (either because they contain loops or they were marked as               \nnon-inlinable as described above).                                            \nThe result of this is that the Python version telco decimal benchmark runs                                 \nto completion without having to arbitrarily increase the trace length limit.                    \nIt's also about 40% faster than running it on CPython. This is one of the first                 \nnon-tiny programs that we speed up.                                                         \n                                                                                          \n                                                 \nReducing GC Pressure                                                                   \nArmin and Anto used some GC instrumentation to find places in pypy-c-jit                     \nthat allocate a lot of memory. This is an endlessly surprising exercise, as                     \nusually we don't care too much about allocations of short-lived objects when                    \nwriting RPython, as our GCs usually deal well with those. They found a few                      \nplaces where they could remove allocations, most importantly by making one of                   \nthe classes that make up traces smaller.                                                    \n                                                                                          \n                                          \nOptimizing Chains of Guards                                                            \nCarl Friedrich and Samuele started a simple optimization on the trace level that             \nremoves superfluous guards. A common pattern in a trace is to have stronger                     \nand stronger guards about the same object. As an example, often there is first a                \nguard that an object is not None, later followed by a guard that it is exactly                  \nof a given class and then even later that it is a precise instance of that                      \nclass. This is inefficient, as we can just check the most precise thing in the                  \nplace of the first guard, saving us guards (which take memory, as they need resume data).       \nMaciek, Armin and Anto later improved on that by introducing a new guard that                   \nchecks for non-nullity and a specific class in one guard, which allows us to                    \ncollapse more chains.                                                                       \n                                                                                          \n                                         \nImproving JIT and Exceptions                                                           \nArmin and Maciek went on a multi-day quest to make the JIT and Python-level                  \nexceptions like each other more. So far, raising and catching exceptions would                  \nmake the JIT generate code that has a certain amusement value, but is not really                \nfast in any way. To improve the situation, they had to dig into the exception                   \nsupport in the Python interpreter, where they found various inefficiencies. They                \nalso had to rewrite the exceptions module to be in RPython (as opposed to                                                             \njust pure Python + an old hack). Another problems is that tracebacks give you                   \naccess to interpreter frames. This forces the JIT to deoptimize things, as                      \nthe JIT keeps some of the frame's content in CPU registers or on the CPU stack,                 \nwhich reflective access to frames prevents.                                                     \nCurrently we try to improve the simple cases where the traceback is never                       \nactually accessed. This work is not completely finished, but some cases are                     \nalready significantly faster.                                                               \n                                                                                          \n                                       \nMoving PyPy to use py.test 1.1                                                         \nHolger worked on porting PyPy to use the newly released py.test 1.1. PyPy                   \nstill uses some very old support code in its testing infrastructure, which makes                \nthis task a bit annoying. He also gave the other PyPy developers a demo of some                 \nof the newer py.test features and we discussed which of them we want to start                   \nusing to improve our tests to make them shorter and clearer. One of the things                  \nwe want to do eventually is to have less skipped tests than now.                            \n                                                                                          \n                           \nUsing a Simple Effect Analysis for the JIT                                             \nOne of the optimization the JIT does is caching fields that are read out of                  \nstructures on the heap. This cache needs to be invalidated at some points, for\nexample when such a field is written to (as we don't track aliasing much).\nAnother case is a call in the assembler, as the target function could\narbitrarily change the heap. This of course is imprecise, since most functions\ndon't actually change the whole heap, and we have an analysis that finds out\nwhich sorts of types of structs and arrays a function can mutate. During the\nsprint Carl Friedrich and Samuele integrated this analysis with the JIT, to help\nit invalidate caches less aggressively. Later Anto and Carl Friedrich also\nported this support to the CLI version of the JIT.\n\n\nMiscellaneous\nSamuele (with some assistance of Carl Friedrich) set up a buildbot slave on a\nMac Mini at the University. This should let us stabilize on the Max OS X. So far\nwe still have a number of failing tests, but now we are in a situation to\nsanely approach fixing them.\nAnto improved the CLI backend to support the infrastructure for producing the\nprofiling graphs Armin introduced.\nThe guinea-pigs that were put into Carl Friedrich's care have been fed (which\nwas the most important sprint task anyway).\n Samuele & Carl Friedrich",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/11/dusseldorf-sprint-report-2505348213879053352.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Started",
      "text": "The D\u00fcsseldorf sprint starts today. Only Samuele and me are there so far, but that should change over the course of the day. We will mostly work on the JIT during this sprint, trying to make it a lot more practical. For that we need to decrease its memory requirements some more and to make it use less aggressive inlining. We will post more as the sprint progresses.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/11/dusseldorf-sprint-started-7608527610228870250.html"
    },
    {
      "title": "PyPy on RuPy 2009",
      "text": "Hello.\n\nIt's maybe a bit late to announce, but there will be PyPy talk\nat Rupy conference this weekend in\nPoznan. Precisely, I'll be talking mostly about PyPy's JIT and\nhow to use it. Unfortunately the talk is on Saturday, at 8:30 in the morning.\n\n\nEDIT: Talk is online, together with examples\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/11/pypy-on-rupy-2009-5675275348619189353.html"
    },
    {
      "title": "Logging and nice graphs",
      "text": "Hi all,\n\nThis week I worked on improving the system we use for logging.  Well, it was not really a \"system\" but rather a pile of hacks to measure in custom ways timings and counts and display them.  So now, we have a system :-)\n\nThe system in question was integrated in the code for the GC and the JIT, which are two independent components as far as the source is concerned.  However, we can now display a unified view.  Here is for example pypy-c-jit running pystone for (only) 5000 iterations:\n\n\n\nThe top long bar represents time.  The bottom shows two summaries of the total time taken by the various components, and also plays the role of a legend to understand the colors at the top.  Shades of red are the GC, shades of green are the JIT.\n\nHere is another picture, this time on pypy-c-jit running 10 iterations of richards:\n\n\n\nWe have to look more closely at various examples, but a few things immediately show up.  One thing is that the GC is put under large pressure by the jit-tracing, jit-optimize and (to a lesser extent) the jit-backend components.  So large in fact that the GC takes at least 60-70% of the time there.  We will have to do something about it at some point.  The other thing is that on richards (and it's likely generally the case), the jit-blackhole component takes a lot of time.  \"Blackholing\" is the operation of recovering from a guard failure in the generated assembler, and falling back to the interpreter.  So this is also something we will need to improve.\n\nThat's it!  The images were generated with the following commands:\n\nPYPYLOG=/tmp/log pypy-c-jit richards.py\npython pypy/tool/logparser.py draw-time /tmp/log --mainwidth=8000 --output=filename.png\n\nEDIT: nowadays the command-line has changed to:python rpython/tool/logparser.py draw-time /tmp/log --mainwidth=8000 filename.png",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/11/hi-all-this-week-i-worked-on-improving-6515977421244851229.html"
    },
    {
      "title": "GC improvements",
      "text": "In the last week, I (Armin) have been taking some time off the\nJIT work to improve our GCs.  More precisely, our GCs now take\none or two words less for every object.  This further reduce the\nmemory usage of PyPy, as we will show at the end.\n\nBackground information: RPython object model\n\nWe first need to understand the RPython object model as\nimplemented by our GCs and our C backend.  (Note that the\nobject model of the Python interpreter is built on top of\nthat, but is more complicated -- e.g. Python-level objects\nare much more flexible than RPython objects.)\n\nConsider these two RPython classes:\n    \n\nclass A:\n    def __init__(self, x):\n        self.x = x\n    def f(self):\n        return self.x * 42\n\nclass B(A):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def f(self):\n        return self.x + self.y\n\n\nThe instances of A and B look like this in memory (all cells\nare one word):\n\n\nGC header\nvtable ptr of A\nhash\nx\n\n\n\nGC header\nvtable ptr of B\nhash\nx\ny\n\n\nThe first word, the GC header, describes the layout.  It\nencodes on half a word the shape of the object, including where it\ncontains further pointers, so that the GC can trace it.  The\nother half contains GC flags (e.g. the mark bit of a\nmark-and-sweep GC).\n\nThe second word is used for method dispatch.  It is similar to a\nC++ vtable pointer.  It points to static data that is mostly a\ntable of methods (as function pointers), containing e.g. the method f\nof the example.\n\nThe hash field is not necessarily there; it is only present in classes\nwhose hash is ever taken in the RPython program (which includes being\nkeys in a dictionary).  It is an \"identity hash\": it works like\nobject.__hash__() in Python, but it cannot just be the address of\nthe object in case of a GC that moves objects around.\n\nFinally, the x and y fields are, obviously, used to store the value\nof the fields.  Note that instances of B can be used in places that\nexpect a pointer to an instance of A.\n\nUnifying the vtable ptr with the GC header\n\nThe first idea of saving a word in every object is the observation\nthat both the vtable ptr and the GC header store information about\nthe class of the object.  Therefore it is natural to try to only have\none of them.  The problem is that we still need bits for the GC flags,\nso the field that we have to remove is the vtable pointer.\n\nThis means that method dispatch needs to be more clever: it\ncannot directly read the vtable ptr, but needs to compute it\nfrom the half-word of the GC header.  Fortunately, this can be\ndone with no extra instruction on the assembler level.  Here is\nhow things will look like in the end, assuming a 32-bit x86\nmachine (but note that as usual we just generate portable C).\n\nThe trick for achieving efficiency is that we store all\nvtables together in memory, and make sure that they don't take\nmore than 256 KB in total (16 bits, plus 2 bits of alignment).\nHere is how the assembler code (produced by the normal C\ncompiler, e.g. gcc) for calling a method looks like.  Before\nthe change:\n\n\nMOV EDX, [EAX + 4]               # load the vtable ptr from object EAX\nMOV EDX, [EDX + method_offset]   # load the function pointer from the vtable\nCALL EDX\n\n\nInstead, we now have:\n\n\nMOVZX EDX, [EAX]     # load the 16-bit part of the GC header from EAX\nMOV EDX, [vtable_start + 4*EDX + method_offset]\nCALL EDX\n\n\nNote that the complex addressing scheme done by the second MOV\nis still just one instruction: the vtable_start and\nmethod_offset are constants, so they are combined.  And as the\nvtables are anyway aligned at a word boundary, we can use\n4*EDX to address them, giving us 256 KB instead of just 64 KB\nof vtables.\n\nOptimizing the hash field\n\nIn PyPy's Python interpreter, all application-level objects\nare represented as an instance of some subclass of W_Root.\nSince all of these objects could potentially be stored in a\ndictionary by the application Python program, all these\nobjects need a hash field.  Of course, in practice, only a\nfraction of all objects in a Python program end up having\ntheir hash ever taken.  Thus this field of W_Root is wasted\nmemory most of the time.\n\n(Up to now, we had a hack in place to save the hash field\non a few classes like W_IntegerObject, but that meant that\nthe Python expression ``object.__hash__(42)'' would raise\na TypeError in PyPy.)\n\nThe solution we implemented now (done by some Java GCs, among\nothers) is to add a hash field to an object when the\n(identity) hash of that object is actually taken.  This means\nthat we had to enhance our GCs to support this.  When objects\nare allocated, we don't reserve any space for the hash:\n\nobject at 0x74B028\n\n...00...\nx\ny\n\n    \nWhen the hash of an object is taken, we use its current memory\naddress, and set a flag in the GC header saying that this\nparticular object needs a hash:\n\nobject at 0x74B028\n\n...01...\nx\ny\n\n\nIf the GC needs to move the object to another memory location,\nit will make the new version of the object bigger, i.e. it\nwill also allocate space for the hash field:\n\nobject at 0x825F60\n\n...11...\nx\ny\n0x74B028\n\n\nThis hash field is immediately initialized with the old memory\naddress, which is the hash value that we gave so far for the\nobject.  To not disturb the layout of the object, we always\nput the extra hash field at the end.  Of course, once set,\nthe hash value does not change even if the object needs to\nmove again.\n\nResults\n\nRunning the following program on PyPy's Python interpreter\nwith n=4000000:\n\n\ndef make_linked_list(n):\n    a = None\n    i = 0\n    while i < n:\n        b = X()\n        b.next = a\n        a = b\n        i += 1\n\n\nthe two optimizations together save 32 MB of RAM (i.e. 8 bytes\nper object).  The version of PyPy we measured this with was built\nas follows:\n\n\n./translate.py --gcremovetypeptr targetpypystandalone --objspace-std-withsharingdict\n\n\nThe total amount of RAM used on a 32-bit Linux is 247 MB,\ncompleting in 10.3 seconds.  On CPython, it consumes 684 MB\nand takes 89 seconds to complete...  This nicely shows that\nour GCs are much faster at allocating objects, and that our\nobjects can be much smaller than CPython's.\n\nArmin Rigo & Carl Friedrich Bolz",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/10/gc-improvements-6174120095428192954.html"
    },
    {
      "title": "First pypy-cli-jit benchmarks",
      "text": "As the readers of this blog already know, I've been working on porting the\nJIT to CLI/.NET for the last months.  Now that it's finally possible to get a\nworking pypy-cli-jit, it's time to do some benchmarks.\nWarning: as usual, all of this has to be considered to be a alpha version:\ndon't be surprised if you get a crash when trying to run pypy-cli-jit.  Of\ncourse, things are improving very quickly so it should become more and more\nstable as days pass.\nFor this time, I decided to run four benchmarks. Note that for all of them we\nrun the main function once in advance, to let the JIT recoginizing the hot\nloops and emitting the corresponding code.  Thus, the results reported do\nnot include the time spent by the JIT compiler itself, but give a good\nmeasure of how good is the code generated by the JIT.  At this point in time,\nI know that the CLI JIT backend spends way too much time compiling stuff, but\nthis issue will be fixed soon.\n\n\nf1.py: this is the classic PyPy JIT benchmark. It is just a function\nthat does some computational intensive work with integers.\nfloatdemo.py: this is the same benchmark involving floating point\nnumbers that have already been described in a previous blog post.\noodemo.py: this is just a microbenchmark doing object oriented stuff\nsuch as method calls and attribute access.\nrichards2.py: a modified version of the classic richards.py, with a\nwarmup call before starting the real benchmark.\n\n\nThe benchmarks were run on a Windows machine with an Intel Pentium Dual Core\nE5200 2.5GHz and 2GB RAM, both with .NET (CLR 2.0) and Mono 2.4.2.3.\nBecause of a known mono bug, if you use a version older than 2.1 you need\nto pass the option -O=-branch to mono when running pypy-cli-jit, else it\nwill just loop forever.\nFor comparison, we also run the same benchmarks with IronPython 2.0.1 and\nIronPython 2.6rc1.  Note that IronPython 2.6rc1 does not work with mono.\nSo, here are the results (expressed in seconds) with Microsoft CLR:\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\npypy-cli-jit\nipy 2.0.1\nipy 2.6\nipy2.01/ pypy\nipy2.6/ pypy\n\n\n\nf1\n0.028\n0.145\n0.136\n5.18x\n4.85x\n\nfloatdemo\n0.671\n0.765\n0.812\n1.14x\n1.21x\n\noodemo\n1.25\n4.278\n3.816\n3.42x\n3.05x\n\nrichards2\n1228\n442\n670\n0.36x\n0.54x\n\n\n\n\nAnd with Mono:\n\n\n\n\n\n\n\n\n\nBenchmark\npypy-cli-jit\nipy 2.0.1\nipy2.01/ pypy\n\n\n\nf1\n0.042\n0.695\n16.54x\n\nfloatdemo\n0.781\n1.218\n1.55x\n\noodemo\n1.703\n9.501\n5.31x\n\nrichards2\n720\n862\n1.20x\n\n\n\n\nThese results are very interesting: under the CLR, we are between 5x faster\nand 3x slower than IronPython 2.0.1, and between 4.8x faster and 1.8x slower\nthan IronPython 2.6.  On the other hand, on mono we are consistently faster\nthan IronPython, up to 16x.  Also, it is also interesting to note that\npypy-cli runs faster on CLR than mono for all benchmarks except richards2.\nI've not investigated yet, but I think that the culprit is the terrible\nbehaviour of tail calls on CLR: as I already wrote in another blog post,\ntail calls are ~10x slower than normal calls on CLR, while being only ~2x\nslower than normal calls on mono.  richads2 is probably the benchmark that\nmakes most use of tail calls, thus explaining why we have a much better result\non mono than CLR.\nThe next step is probably to find an alternative implementation that does not\nuse tail calls: this probably will also improve the time spent by the JIT\ncompiler itself, which is not reported in the numbers above but that so far it\nis surely too high to be acceptable. Stay tuned.",
      "tags": "cli,jit,pypy",
      "url": "https://www.pypy.org/posts/2009/10/first-pypy-cli-jit-benchmarks-6698484455072589492.html"
    },
    {
      "title": "PyPy's JIT now supports floats",
      "text": "Hello.\n\n\n\nWe've just merged branch which adds float support to x86 backend.\nThis means that floating point operations are now super fast\nin PyPy's JIT. Let's have a look at example, provided by \nAlex Gaynor\nand stolen from Factor blog.\n\n\n\nThe original version of the benchmark, was definitely tuned for the performance needs of CPython.\n\nFor running this on PyPy, I changed to a bit simpler version of the program,\nand I'll explain a few changes that I did, which the reflect current\nlimitations of PyPy's JIT. They're not very deep and they might be\nalready gone while you're reading it:\n\n\n\nUsage of __slots__. This is a bit ridiculous, but we spend quite a bit\n  of time to speed up normal instances of new-style classes which are\n  very fast, yet ones with __slots__ are slower. To be fixed soon.\n\nUsage of reduce. This one is even more obscure, but reduce is not\n  perceived as a thing producing loops in a program. Moving to\n  a pure-Python version of reduce fixes the problem.\n\nUsing x ** 2 vs x * x. In PyPy, reading a local variable is a\n  no-op when JITted (the same as reading local variable in C). However\n  multiplication is simpler operation that power operation.\n\n\n\nI also included the original Java benchmark. Please\nnote that original java version is similar to my modified one\n(not the one specifically tuned for CPython)\n\n\nThe performance figures below (for n = 1 000 000), average of 10 runs:\n\n\nCPython 2.6: 7.56s\nCPython & psyco 2.6: 4.44s\nPyPy: 1.63s\nJava (JVM 1.6, client mode): 0.77s\n\n\n\nand while JVM is much faster, it's very good that we can even compare :-)\n\n\nCheers\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/10/pypys-jit-now-supports-floats-7003493323596806737.html"
    },
    {
      "title": "First results of the JIT",
      "text": "Hi all,\n\nJust a quick note to tell you that we are progressing on the\nJIT front.  Here are the running times of the richards\nbenchmark on my laptop:\n\n8.18 seconds with CPython 2.5.2;\n\n2.61 seconds with pypy-c-jit (3x faster than CPython);\n\n1.04 seconds if you ignore the time spent making assembler (8x faster than CPython);\n\n1.59 seconds on Psyco, for reference (5x faster that CPython).\n\nYes, as this table shows, we are spending 1.57 seconds in the JIT\nsupport code.  That's too much -- even ridiculously so -- for anything but a\nlong-running process.  We are working on that :-)\n\nIf you want to build your own pypy-c-jit (for x86-32 only for now):\n\nyou need a Subversion checkout of trunk;\n\nrun pypy/translator/goal/translate.py with the -Ojit\n  option;\n\nas usual, wait a long time (and be sure you have more than 1GB of RAM).\n\nFor now pypy-c-jit spews a lot of debugging output and\nthere are a few known\nexamples where it crashes.  As we like to repeat, however, it's a complete JIT:\napart from the crashes (the bugs are probably in the JIT support code), it supports the whole Python language from the start -- in the sense of doing correct things.  Future work include\nPython-specific improvements by e.g. tweaking the data structures used to store Python objects so that they are more JIT-friendly.\n\nEDIT: Oh yes, fijal reminds me that CPython 2.6 is 30% faster than CPython 2.5 on this benchmark (which is mostly my \"fault\", as I extracted a small part of PyPy and submitted it as a patch to CPython that works particularly well for examples like richards).  It does not fundamentally change the fact that we are way faster though.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/09/first-results-of-jit-6674537807334018925.html"
    },
    {
      "title": "PyPy sprint in D\u00fcsseldorf, 6 Nov - 13 Nov",
      "text": "The next PyPy sprint will be held in the Computer Science department of\nHeinrich-Heine Universit\u00e4t D\u00fcsseldorf from the 6th to the 13th of\nNovember 2009. This is a fully public sprint, everyone is welcome to\njoin us.\n\nTopics and goals\nAt the sprint we intend to work on the JIT generator in PyPy and on\napplying it to PyPy Python interpreter.\nThe precise work that will be done is not fixed, as we don't know in\nwhich state the JIT will be in November.  However, possible areas of\nwork might include:\n\ntweaking the interpreter/objspace to be more JIT-friendly, e.g.\ninstance implementation code, call code\nif there is interest starting non x86-32 JIT backends\ntrying out existing software to find features where the optimizations\nof the JIT could be improved\nimproving our benchmarking infrastructure\n\nWe will give special priority to topics that \"non-core\" people find\ninteresting (as long as they are somehow JIT-related).\nFor an introduction of how our JIT-generation process works, please\nrefer to our blog:\nhttps://morepypy.blogspot.com/2009/03/jit-bit-of-look-inside.html\nThere is also a more dense academic paper about the subject:\nhttps://codespeak.net/svn/pypy/extradoc/talk/icooolps2009/bolz-tracing-jit-final.pdf\n\n\nLocation\nThe sprint will take place in a seminar room of the computer science\ndepartment.  It is in the building 25.12 of the university campus. For\ntravel instructions see\n\nhttps://stups.cs.uni-duesseldorf.de/anreise/esbahn.php\n\n\nRegistration\nIf you'd like to come, please subscribe to the pypy-sprint mailing\nlist and drop a note about your interests and post any questions.\nMore organisational information will be send to that list.  We'll keep a\nlist of people which we'll update (which you can do so yourself if\nyou have codespeak commit rights).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/09/pypy-sprint-in-dusseldorf-6-nov-13-nov-8153983964308175836.html"
    },
    {
      "title": "PyPy gets a new compiler",
      "text": "Today, I merged the parser-compiler branch, which I have been working on over the summer. It contained a total rewrite of both PyPy's Python parser and AST compiler. PyPy's old parser was (in)famous internally for being complicated and slow (with many algorithmic complexities greater than O(n)). The new parser is a simple as I could make it LL(1) parser like CPython (though it doesn't share the hacks of CPython's parser).\n\nThe new compiler is based on the Abstract Syntax Trees (AST) that CPython 2.5 introduced instead of PyPy's old AST based on the compiler package's. This means that Python code running on PyPy will be able to use the same _ast interface as CPython. PyPy's _ast implementation supports AST features that CPython 2.6 added, including compiling modified AST to bytecode and executing it. In this rewrite, some more obscure compiler features were added, too. For example, jumps in bytecode can now be greater than 65535 bytes! (That's like an if statement with 7000 lines of code in the body.)\n\nWhile the PyPy translation toolchain still has many obscure details and hacks, this merge completes the process of making the actual Python interpreter very clean. Hopefully, this will make adding new features much easier and make PyPy less frustrating to maintain as well as providing application level code with an improved AST interface!",
      "tags": "compiler,parser,speed",
      "url": "https://www.pypy.org/posts/2009/08/pypy-gets-new-compiler_25-6401910947439531107.html"
    },
    {
      "title": "Gothenburg JIT sprint report",
      "text": "Finally, we managed to squeeze in some time to write a report about what\nhas been going on the mysterious JIT sprint in Gothenburg, Sweden.\nThe main goals of the sprint were to lay down the groundwork for getting\nmore JIT work going in the next months and get more of PyPy developers\nup to speed with the current state of the JIT. One of the elements was\nto get better stability of the JIT, moving it slowly from being a prototype to\nactually work nicely on larger programs.\n\nThe secret goal of the sprint was to seek more speed, which Anto and\nCarl Friedrich did even during the break day:\n\n\nWe spent the first two days improving test coverage of the x86 backend\nand the optimizer. Now we have 100% coverage with unittests\n(modulo figleaf bugs), which does not mean anything, but it's better\nthan before.\n\nThen we spent quite some time improving the optimizer passes, so\nnow we generate far less code than before the sprint, because a lot of\nit is optimized away. On the interpreter side, we marked more objects\n(like code objects) as immutable, so that reading fields from them\ncan be constant-folded.\nAnother important optimization that we did is to remove consecutive\nreading of the same fields from the same structure, if no code in between\ncan change it.\nOur JIT is a hybrid environment, where only hot loops of code are jitted\nand the rest stays being interpreted. We found out that the performance\nof the non-jitted part was suboptimal, because all accesses to python\nframes went through an extra layer of indirection. We removed this layer\nof indirection, in the case where the jit and the interpreter cannot\naccess the same frame (which is the common case).\nWe also spent some time improving the performance of our x86 backend,\nby making it use more registers and by doing more advanced variable\nrenaming at the end of loops. It seems that using more registerd is not as\nmuch of a win as we hoped, because modern day processors are much\nsmarter than we thought.\nThe most mind bending part was finding why we loose performance by\nmaking the JIT see more of the interpreter. It took us two very frustrating\ndays and 36 gray hairs to find out that from the JIT we call a different malloc\nfunction in the Boehm GC, which is by far slower than the version that\nwe use from the interpreter. This meant that the more we jitted, the\nslower our code got, purely because of the mallocs.\nNow that this is fixed, the world makes much more sense again.\nA lot of the sprint's work is not directly measurable in the performance\nfigures, but we did a lot of work that is necessary for performance to\nimprove in the next weeks. After we have done a bit more work, we should\nbe able to provide some performance figures for programs that are\nmore realistic than just loops that count to ten millions (which are\nvery fast already :).\nNow we're going to enjoy a couple of days off to recover from the sprint.\nB\u00e4sta h\u00e4lsningar,\nCarl Friedrich, fijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/08/gothenburg-jit-sprint-report-3309138497953458138.html"
    },
    {
      "title": "PyPy numeric experiments",
      "text": "Because PyPy will be presenting at the upcoming euroscipy conference, I have been playing recently with the idea of NumPy and PyPy integration. My idea is to integrate PyPy's JIT with NumPy or at least a very basic subset of it.  Time constraints make it impossible to hand write a JIT compiler that understands NumPy. But given PyPy's architecture we actually have a JIT generator, so we don't need to write one :-)\n\n\n\nOur JIT has shown that it can speed up small arithmetic examples significantly. What happens with something like NumPy?\n\n\nI wrote a very minimal subset of NumPy in RPython, called micronumpy (only single-dimension int arrays that can only get and set items), and a benchmark against it. The point of this benchmark is to compare the performance of a builtin function (numpy.minimum) against the equivalent hand-written function, written in pure Python and compiled by our JIT.\n\n\nThe goal is to prove that it is possible to write algorithms in Python instead of C without loss of efficiency. Sure, we can write some functions (like minimum in the following example), but there is a whole universe of other ufuncs which would be cool to have in Python instead, assuming this could be done without a huge loss in efficiency.\n\n\nHere are the results. This is comparing PyPy svn revision 66303 in the pyjitpl5 branch against python 2.6 with NumPy 1.2.1. The builtin numpy.minimum in PyPy is just a naive implementation in RPython, which is comparable to the speed of a naive implementation written in C (and thus a bit slower than the optimized\nversion in NumPy):\n\n\n\nNumPy (builtin function)0.12s\nPyPy's micronumpy (builtin function)0.28s\nCPython (pure Python)11s\nPyPy with JIT (pure Python)0.91s\n\n\nAs we can see, PyPy's JIT is slower than the optmized NumPy's C version, but still much faster than CPython (12x).\n\n\nWhy is it slower? When you actually look at assembler, it's pretty obvious that it's atrocious. There's a lot of speedup to be gained out of just doing simple optimizations on resulting assembler. There are also pretty obvious limitations, like x86 backend not being able to emit opcodes for floats or x86_64 not being there. Those limitations are not fundamental in any sense and can be relatively straightforward to overcome. Therefore it seems we can get C-level speeds for pure Python implementations of numeric algorithms using NumPy arrays in PyPy. I think it's an interesting perspective that Python has the potential of becoming less of a glue language and more of a real implementation language in the scientific field.\n\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2009/07/pypy-numeric-experiments-2221073696038673235.html"
    },
    {
      "title": "ECOOP 2009",
      "text": "Last week (from 6th to 10th of July) Anto, Armin and me (Carl Friedrich) were in\nthe magnificent city of Genova, Italy at the ECOOP conference. In this blog\npost I want to give a (necessarily personal) account of what we did there.\n\nWorkshop days: ICOOOLPS\nThe first two days of the conference were the workshop days. On Monday we\nattended the ICOOOLPS workshop, (see the programme of the workshop). We\nhad gotten two papers accepted at the workshop (one about layering PyPy's JIT\non top of the CLR and one about the basic idea of PyPy's tracing JIT) and\nthus gave two presentations at the workshop, one was given by Anto, the other\nby me. Both went reasonably well, we got some positive feedback.\nNearly all the other talks were rather interesting as well. I particularly liked\nthe one by Hans Schippers, who presented a machine model built on delegation\ncalled delMDSOC.  The model is meant implement most features that a language\nwould need that makes it possible to separate cross-cutting concerns. In the\ntalk at ICOOOLPS he presented an extension to the model that adds concurrency\nsupport, using a combination of actors and coroutines. He then showed that the\nconcurrency mechanisms of Java, Salsa (and extension of Java adding actors) and\nIo can be mapped to this model.\nFurthermore there were two interesting invited talks, one by Andreas Gal\n(Mozilla), and one by Cliff Click (Azul Systems). Andreas explained how\nTraceMonkey works. This was very useful for me, because his talk was just before\nmine and I could thus kill most of my introduction about tracing JIT compilers\nand have more time for the really interesting stuff :-).  Cliff talked about\nimplementing other languages on top of the JVM and some of the pitfalls in\ngetting them perform well.\nAll in all, ICOOOLPS was a very enjoyable workshop, also with many interesting\ndiscussions.\nOn Tuesday there were more workshops, but also the PyPy tutorial, so I only went\nto a few talks of the COP workshop and spent the rest of the morning\npreparing the tutorial (see next section).\n\n\nTutorial\nOn Tuesday afternoon we gave a PyPy Tutorial, as part of the ECOOP summer\nschool. The first lesson we learned was that (as opposed to a community\nconference) people don't necessarily want to actually take their laptop out and\ntry stuff. We gave a slow walk-through about the full life-cycle of development\nof a dynamic language interpreter using PyPy's tool-chain: Starting from writing\nyour interpreter in RPython, testing it on top of CPython to translating it to\nC, .NET or Java to actually adding hints to get a JIT inserted.\nThere were about seven people attending the tutorial, a couple of which were\nvery interested and were asking questions and discussing. Some of the\ndiscussions were even very technical, e.g. one about the details of our\ntype-inference algorithm for RPython and why we cannot do a bottom-up analysis\nbut have to use forward-propagation instead.\nJan Vitek of Purdue University told of some of the problems of the OVM\nproject, which is (among other things) a Java implementation in Java (OVM also\nwants to support implementing VMs for other languages with it, if I understood\ncorrectly). He said that the project has\nessentially gotten too large and complicated, which means that it is very hard\nfor new people to get into the project. While PyPy doesn't have some of the\nproblems of a full Java implementation (e.g. right now our concurrency support\nis minimal) I definitely think that some of these risks apply to PyPy as well\nand we should find ways to improve the situation in this regard. Channeling\nSamuele: Somewhere inside the large lumbering blob of PyPy there is an elegant\ncore trying to get out.\n\n\nMain Conference\nFrom Wednesday till Friday the main conference was happening. Many of the\ntalks were not all that interesting for me, being quite Java centric. One talk\nthat I liked a lot was \"Making Sense of Large Heaps\", which was presented by\nNick Mitchell (IBM). He presented a tool called \"Yeti\" that can be used to\nanalyze large heaps of Java programs. The tool uses some clever algorithms and\nheuristics to summarize the heap usage of data structures in intelligent ways to\nmake it easier to find possible memory-wasters in a program. Nick also gave Anto\nand me a demo of the tool, where we tried to apply it to pypy-jvm (we found\nout that a fifth of the static data in there belongs to the parser/compiler :-(\n).\nOn each of the days of the conference there was a keynote. I missed the one by\nSimon Peyton-Jones on Wednesday about type classes in Haskell. On Thursday,\nDavid Ungar was awarded the Dahl-Nygaard-Prize for his work on the Self\nprogramming language. Subsequently he gave a really inspiring keynote with the\ntitle \"Self and Self: Whys and Wherefores\" where he recollected Self's history,\nboth on a technical as well as on a social level. Parts of the talk were\nsnippets from the movies Self: The Movie and Alternate Reality Kit, both\nof which I highly recommend.\nThe keynote on Friday was by Cliff Click with the title \"Java on 1000 Cores:\nTales of Hardware/Software Co-design\". He described the custom CPU architecture\nthat Azul Systems has developed to run Java server applications on hundreds of\ncores. The talk mostly talked about the hardware, which I found very interesting\n(but some people didn't care for too much). Azul's CPU is essentially 54 in-order\nRISC cores in a single processor. The cores have a lot of extensions that make\nit easier to run Java on them, e.g. hardware read- and write-barriers,\nhardware-transactional-memory and hardware escape-detection (!).\nIn addition to the talks, there is of course always the hallway track (or coffee\ntrack) which is the track where you stand in the hallway and discuss with\npeople. As usual, this was the most interesting part of the conference. One of\nthose talks was Anto and me giving a PyPy demo to David Ungar. We had a very\ninteresting discussion about VM implementation in general and the sort of\ndebugging tools you need to write in particular. He liked PyPy a lot, which\nmakes me very happy. He also liked the fact that I have actually read most Self\npapers :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/07/ecoop-2009-8415055006373020774.html"
    },
    {
      "title": "EuroPython",
      "text": "EuroPython is coming.  We have two 30-minutes talks that we will present.  In addition, the sprint takes place the 29th of June (there will be no-one from the team on the 28th of June), as well as on the 3rd and 4th of July.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/06/europython-8318355560715932819.html"
    },
    {
      "title": "JIT progress",
      "text": "In the last days I finally understood how to do virtualizables.  Now the frame overhead is gone. This was done with the help of discussion with Samuele, porting ideas from PyPy's first JIT attempt.\n\n\nThis is of course work in progress, but it works in PyPy (modulo a few XXXs, but no bugs so far).  The performance of the resulting code is quite good: even with Boehm (the GC that is easy to compile to but gives a slowish pypy-c), a long-running loop typically runs 50% faster than CPython.  That's \"baseline\" speed, moreover: we will get better speed-ups by applying optimizations on the generated code.  Doing so is in progress, but it suddenly became easier because that optimization phase no longer has to consider virtualizables -- they are now handled earlier.\n\nUpdate:Virtualizables is basically a way to avoid frame overhead. The frame object\nis allocated and has a pointer, but the JIT is free to unpack it's fields (for example python\nlevel locals) and store them somewhere else (stack or registers). Each external (out of jit) access\nto frame managed by jit, needs to go via special accessors that can ask jit where those variables\nare.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/06/jit-progress-7289127796450840053.html"
    },
    {
      "title": "News from the jit front",
      "text": "As usual, progress is going slower then predicted,\nbut nevertheless, we're working hard to make some progress.\n\n\nWe recently managed to make our nice GCs cooperate with our JIT. This is\none point from our detailed plan. As of now, we have a JIT with GCs and\nno optimizations. It already speeds up some things, while slowing down\nothers. The main reason for this is that the JIT generates assembler which is kind\nof ok, but it does not do the same level of optimizations gcc would do.\n\n\nSo the current status of the JIT is that it can produce assembler out\nof executed python code (or any interpreter written in RPython actually),\nbut the results are not high quality enough since we're missing optimizations.\n\n\nThe current plan, as of now, looks as follows:\n\n\nImprove the handling of GCs in JIT with inlining of malloc-fast\n  paths, that should speed up things by a constant, not too big factor.\n\n\nWrite a simplified python interpreter, which will be a base for experiments\n  and to make sure that our JIT does correct things with regard to\n  optimizations. That would work as mid-level integration test.\n\n\nThink about ways to inline loop-less python functions into their parent's loop.\n\n\nGet rid of frame overhead (by virtualizables)\n\n\nMeasure, write benchmarks, publish\n\n\nProfit\n\n\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/06/news-from-jit-front-367552118380842303.html"
    },
    {
      "title": "ICOOOLPS Submissions",
      "text": "Both of the papers that people from the PyPy team submitted to ICOOOLPS have\nbeen accepted. They are:\n\n\n\"Faster than C#: efficient implementation of dynamic languages on .NET\"\n(pdf1) by Armin, Anto and Davide Ancona, who is Anto's Ph.D. advisor\n\"Tracing the Meta-Level: PyPy\u2019s Tracing JIT Compiler\" (pdf2) by Carl\nFriedrich, Armin, Anto and Maciek\n\n\n(the pdfs are obviously the submitted versions, not the final ones).\nThis year ICOOOLPS (Implementation, Compilation, Optimization of\nObject-Oriented Languages, Programs and Systems) is being held on July the 6th\nat ECOOP 2009 in Genova, Italy.  Other than these two papers, Anto and Carl\nFriedrich will also present a PyPy tutorial, on July the 7th.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/05/icooolps-submissions-6705901656116873587.html"
    },
    {
      "title": "4 weeks of GDB",
      "text": "Hello.\n\nSo, according to our jit\nplan we're mostly done with point 1, that is to provide a JIT that compiles\npython code to assembler in the most horrible manner possible but doesn't\nbreak. That meant mostly 4 weeks of glaring at GDB and megabytess of assembler\ngenerated by C code generated from python code. The figure of 4 weeks proves\nthat our approach is by far superior to the one of psyco, since Armin says it's\n\"only 4 weeks\" :-)\n\n\nRight now, pypy compiled with JIT can run the whole CPython test suite\nwithout crashing, which means we're done with obvious bugs and the only\nones waiting for us are really horrible.  (Or they really don't exist.\nAt least they should never be about obscure Python corner cases: they can\nonly be in the 10'000 lines of relatively clear code that is our JIT\ngenerator.)\n\n\nBut... the fun thing is that we can actually concentrate on optimizations!\nSo the next step is to provide a JIT that is correct *and* actually speeds\nup python. Stay tuned for more :-)\n\nCheers,\nfijal, armin & benjamin\n\nUPDATE: for those of you blessed with no knowledge of C, gdb stands for GNU debugger, a classic debugger for C. (It's also much more powerful than python debugger, pdb, which is kind of surprising).",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/04/4-weeks-of-gdb-522864241041643529.html"
    },
    {
      "title": "1.1 final released",
      "text": "We just released PyPy 1.1 final. Not much changed since the beta, apart\nfrom some more fixed bugs. Have fun with it!",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2009/04/11-final-released-225813777919757859.html"
    },
    {
      "title": "Roadmap for JIT",
      "text": "Hello.\n\n\nFirst a disclaimer. This post is more about plans for future than current\nstatus. We usually try to write about things that we have done, because\nit's much much easier to promise things than to actually make it happen,\nbut I think it's important enough to have some sort of roadmap.\n\n\nIn recent months we came to the point where the 5th generation of\nJIT prototype was working as nice\nor even a bit nicer than 1st one back in 2007. Someone might ask \"so why\ndid you spend all this time without going forward?\". And indeed, we spend\na lot of time moving sideways, but as posted, we also spent a lot of time\ndoing some other things, which are important as well.\nThe main advantage of current JIT incarnation is much much simpler than\nthe first one. Even I can comprehend it, which is much of an improvement :-)\n\n\nSo, the prototype is working and gives very nice speedups in range of 20-30x\nover CPython. We're pretty confident this prototype will work and will\nproduce fast python interpreter eventually. So we decided that now we'll\nwork towards changing prototype into something stable and solid. This\nmight sound easy, but in fact it's not. Having stable assembler backend\nand optimizations that keep semantics is not as easy as it might sound.\n\n\nThe current roadmap, as I see it, looks like as following:\n\n\n Provide a JIT that does not speedup things, but produce assembler without\n  optimizations turned on, that is correct and able to run CPython's library\n  tests on a nightly basis.\n\n\n Introduce simple optimizations, that should make above JIT a bit faster than\n  CPython. With optimizations disabled JIT is producing incredibly dumb\n  assembler, which is slower than correspoding C code, even with removal\n  of interpretation overhead (which is not very surprising).\n\n\n Backport optimizations from JIT prototype, one by one, keeping an eye\n  on how they perform and making sure they don't break anything.\n\n\n Create new optimizations, like speeding up attribute access.\n\n\n Profit.\n\n\n\nThis way, we can hopefully provide a working JIT, which gives fast python\ninterpreter, which is a bit harder than just a nice prototype.\n\n\nTell us what you think about this plan.\n\nCheers,\nfijal & others.",
      "tags": "jit,pypy,roadmap,speed",
      "url": "https://www.pypy.org/posts/2009/04/roadmap-for-jit-377358891902851723.html"
    },
    {
      "title": "Leysin Sprint Report",
      "text": "The Leysin sprint is nearing its end, as usual here is an attempt at a summary\nof what we did.\nRelease Work\nLarge parts of the sprint were dedicated to fixing bugs. Since the easy bugs\nseem to have been fixed long ago, those were mostly very annoying and hard bugs.\nThis work was supported by our buildbots, which we tried to get free of\ntest-failures. This was worked on by nearly all participants of the sprint\n(Samuele, Armin, Anto, Niko, Anders, Christian, Carl Friedrich). One\nparticularly annoying bug was the differences in the tracing events that PyPy\nproduces (fixed by Anders, Samuele and Christian). Some details about larger\ntasks are in the sections below.\nThe work culminated in the beta released on Sunday.\n\nStackless\nA large number of problems came from our stackless features, which do some\nadvanced things and thus seem to contain advanced bugs. Samuele and Carl\nFriedrich spent some time fixing tasklet pickling and unpickling. This was\nachieved by supporting the (un)pickling of builtin code objects. In addition\nthey fixed some bugs in the finalization of tasklets. This needs some care\nbecause the __del__ of a tasklet cannot run at arbitrary points in time, but\nonly at safe points. This problem was a bit subtle to get right, and popped up\nnearly every morning of the sprint in form of a test failure.\nArmin and Niko added a way to restrict the stack depth of the RPython-level\nstack. This can useful when using stackless, because if this is not there it is\npossible that you fill your whole heap with stack frames in the case of an\ninfinite recursion. Then they went on to make stackless not segfault when\nthreads are used at the same time, or if a callback from C library code is in\nprogress. Instead you get a RuntimeError now, which is not good but better\nthan a segfault.\n\n\n\nKilling Features\nDuring the sprint we discussed the fate of the LLVM and the JS backends. Both\nhave not really been maintained for some time, and even partially untested\n(their tests were skipped). Also their usefulness appears to be limited. The JS\nbackend is cool in principle, but has some serious limitations due to the fact\nthat JavaScript is really a dynamic language, while RPython is rather static.\nThis made it hard to use some features of JS from RPython, e.g. RPython does not\nsupport closures of any kind.\nThe LLVM backend had its own set of problems. For\na long time it produced the fastest form of PyPy's Python interpreter, by first\nusing the LLVM backend, applying the LLVM optimizations to the result, then\nusing LLVM's C backend to produce C code, then apply GCC to the result :-).\nHowever, it is not clear that it is still useful to directly produce LLVM\nbitcode, since LLVM has rather good C frontends nowadays, with llvm-gcc and\nclang. It is likely that we will use LLVM in the future in our JIT (but that's\nanother story, based on different code).\nTherefore we decided to remove these two backends from SVN, which Samuele and\nCarl Friedrich did. They are not dead, only resting until somebody who is\ninterested in maintaining them steps up.\n\n\nWindows\nOne goal of the release is good Windows-support. Anders and Samuele set up a new\nwindows buildbot which revealed a number of failures. Those were attacked by\nAnders, Samuele and Christian as well as by Amaury (who was not at the sprint,\nbut thankfully did a lot of Windows work in the last months).\n\n\nOS X\nChristian with some help by Samuele tried to get translation working again under\nMac OS X. This was a large mess, because of different behaviours of some POSIX\nfunctionality in Leopard. It is still possible to get the old behaviour back,\nbut whether that was enabled or not depended on a number of factors such as\nwhich Python is used. Eventually they managed to successfully navigate that maze\nand produce something that almost works (there is still a problem remaining\nabout OpenSSL).\n\n\nDocumentation\nThe Friday of the sprint was declared to be a documentation day, where (nearly)\nno coding was allowed. This resulted in a newly structured and improved getting\nstarted document (done by Carl Friedrich, Samuele and some help of Niko) and\na new document describing differences to CPython (Armin, Carl Friedrich) as\nwell as various improvements to existing documents (everybody else). Armin\nundertook the Sisyphean task of listing all talks, paper and related stuff\nof the PyPy project.\n\n\n\nVarious Stuff\n\nJava Backend Work\nNiko and Anto worked on the JVM backend for a while. First they had to fix\ntranslation of the Python interpreter to Java. Then they tried to improve the\nperformance of the Python interpreter when translated to Java. Mostly they did a\nlot of profiling to find performance bottlenecks. They managed to improve\nperformance by 40% by overriding fillInStackTrace of the generated exception\nclasses. Apart from that they found no simple-to-fix performance problems.\n\n\nJIT Work\nArmin gave a presentation about the current state of the JIT to the sprinters as\nwell as Adrian Kuhn, Toon Verwaest and Camillo Bruni of the University of Bern\nwho came to visit for one day. There was a bit of work on the JIT going on too;\nArmin and Anto tried to get closer to having a working JIT on top of the CLI.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/04/leysin-sprint-report-1416905818217912359.html"
    },
    {
      "title": "Beta for 1.1.0 released",
      "text": "Today we are releasing a beta of the upcoming PyPy 1.1 release. There\nare some Windows and OS X issues left that we would like to address\nbetween now and the final release but apart from this things should be\nworking. We would appreciate feedback.\nThe PyPy development team.\n\nPyPy 1.1: Compatibility & Consolidation\nWelcome to the PyPy 1.1 release - the first release after the end of EU\nfunding. This release focuses on making PyPy's Python interpreter more\ncompatible with CPython (currently CPython 2.5) and on making the\ninterpreter more stable and bug-free.\nPyPy's Getting Started lives at:\n\nhttps://codespeak.net/pypy/dist/pypy/doc/getting-started.html\n\nHighlights of This Release\n\n\nMore of CPython's standard library extension modules are supported,\namong them ctypes, sqlite3, csv, and many more. Most of these extension modules\nare fully supported under Windows as well.\nhttps://codespeak.net/pypy/dist/pypy/doc/cpython_differences.html\nhttps://morepypy.blogspot.com/2008/06/pypy-improvements.html\n\nThrough a large number of tweaks, performance has been improved by\n10%-50% since the 1.0 release. The Python interpreter is now between\n0.8-2x (and in some corner case 3-4x) slower than CPython. A large\npart of these speed-ups come from our new generational garbage\ncollectors.\nhttps://codespeak.net/pypy/dist/pypy/doc/garbage_collection.html\n\nOur Python interpreter now supports distutils as well as\neasy_install for pure-Python modules.\n\nWe have tested PyPy with a number of third-party libraries. PyPy can\nrun now: Django, Pylons, BitTorrent, Twisted, SymPy, Pyglet, Nevow,\nPinax:\nhttps://morepypy.blogspot.com/2008/08/pypy-runs-unmodified-django-10-beta.html\nhttps://morepypy.blogspot.com/2008/07/pypys-python-runs-pinax-django.html\nhttps://morepypy.blogspot.com/2008/06/running-nevow-on-top-of-pypy.html\n\nA buildbot was set up to run the various tests that PyPy is using\nnightly on Windows and Linux machines:\nhttps://codespeak.net:8099/\n\nSandboxing support: It is possible to translate the Python\ninterpreter in a special way so that the result is fully sandboxed.\nhttps://codespeak.net/pypy/dist/pypy/doc/sandbox.html\nhttps://blog.sandbox.lt/en/WSGI%20and%20PyPy%20sandbox\n\n\n\n\n\nOther Changes\n\n\nThe clr module was greatly improved. This module is used to\ninterface with .NET libraries when translating the Python\ninterpreter to the CLI.\nhttps://codespeak.net/pypy/dist/pypy/doc/clr-module.html\nhttps://morepypy.blogspot.com/2008/01/pypynet-goes-windows-forms.html\nhttps://morepypy.blogspot.com/2008/01/improve-net-integration.html\n\nStackless improvements: PyPy's stackless module is now more\ncomplete. We added channel preferences which change details of the\nscheduling semantics. In addition, the pickling of tasklets has been\nimproved to work in more cases.\n\nClassic classes are enabled by default now. In addition, they have\nbeen greatly optimized and debugged:\nhttps://morepypy.blogspot.com/2007/12/faster-implementation-of-classic.html\n\nPyPy's Python interpreter can be translated to Java bytecode now to\nproduce a pypy-jvm. At the moment there is no integration with\nJava libraries yet, so this is not really useful.\n\nWe added cross-compilation machinery to our translation toolchain to\nmake it possible to cross-compile our Python interpreter to Nokia's\nMaemo platform:\nhttps://codespeak.net/pypy/dist/pypy/doc/maemo.html\n\nSome effort was spent to make the Python interpreter more\nmemory-efficient. This includes the implementation of a mark-compact\nGC which uses less memory than other GCs during collection.\nAdditionally there were various optimizations that make Python\nobjects smaller, e.g. class instances are often only 50% of the size\nof CPython.\nhttps://morepypy.blogspot.com/2008/10/dsseldorf-sprint-report-days-1-3.html\n\nThe support for the trace hook in the Python interpreter was\nimproved to be able to trace the execution of builtin functions and\nmethods. With this, we implemented the _lsprof module, which is\nthe core of the cProfile module.\n\nA number of rarely used features of PyPy were removed since the previous\nrelease because they were unmaintained and/or buggy. Those are: The\nLLVM and the JS backends, the aspect-oriented programming features,\nthe logic object space, the extension compiler and the first\nincarnation of the JIT generator. The new JIT generator is in active\ndevelopment, but not included in the release.\nhttps://codespeak.net/pipermail/pypy-dev/2009q2/005143.html\nhttps://morepypy.blogspot.com/2009/03/good-news-everyone.html\nhttps://morepypy.blogspot.com/2009/03/jit-bit-of-look-inside.html\n\n\n\n\n\nWhat is PyPy?\nTechnically, PyPy is both a Python interpreter implementation and an\nadvanced compiler, or more precisely a framework for implementing dynamic\nlanguages and generating virtual machines for them.\nThe framework allows for alternative frontends and for alternative\nbackends, currently C, Java and .NET.  For our main target \"C\", we can\n\"mix in\" different garbage collectors and threading models,\nincluding micro-threads aka \"Stackless\".  The inherent complexity that\narises from this ambitious approach is mostly kept away from the Python\ninterpreter implementation, our main frontend.\nSocially, PyPy is a collaborative effort of many individuals working\ntogether in a distributed and sprint-driven way since 2003.  PyPy would\nnot have gotten as far as it has without the coding, feedback and\ngeneral support from numerous people.\nHave fun,\n\nthe PyPy release team, [in alphabetical order]\nAmaury Forgeot d'Arc, Anders Hammerquist, Antonio Cuni, Armin Rigo,\nCarl Friedrich Bolz, Christian Tismer, Holger Krekel,\nMaciek Fijalkowski, Samuele Pedroni\nand many others:\nhttps://codespeak.net/pypy/dist/pypy/doc/contributor.html",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2009/04/beta-for-110-released-4604559533184706699.html"
    },
    {
      "title": "Leysin Sprint Started",
      "text": "The Leysin Sprint started today. The weather is great and the view is wonderful, as usual. Technically we are working on the remaining test failures of the nightly test runs and are generally trying to fix various long-postponed bugs. I will try to give more detailed reports as the sprint progresses.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/04/leysin-sprint-started-4551365436232104640.html"
    },
    {
      "title": "Pycon videos are online",
      "text": "Hi.\n\nWe didn't yet write full pycon summary, but both of our talks are now online: PyPy status talk and python in a sandbox.\nUpdate:\nSlides are also available: PyPy status talk and Python in a sandbox.\n\n\nEnjoy!\nfijal & holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/04/pycon-videos-are-online-909873128878039557.html"
    },
    {
      "title": "VM summit: nice to see friendly competition",
      "text": "So Google has launched the unladen swallow project\nwith this first goal: \n\n\n    Produce a version of Python at least 5x faster than CPython.\n\n\nWe discussed some details with Collin Winter,  Jeffrey Yasskin and Thomas Wouters\nduring the VM summit yesterday. We were a bit confused about usage\nof the term JIT, because as far as we understood, it's going to be upfront\ncompilation into LLVM.  In the past we have looked into LLVM\n \u2013  at one point PyPy extensively use it but it\nwasn't clear how we could make good use to it. \nThey also consider changing to something else than LLVM.  It's gonna be \ninteresting to see how this works out. \n\n\nIt's good to see friendly competition, and we think that should take up\nthe challenge and see if we can produce faster pickling, run 2to3 and \nDjango faster than what they can come up with.  We also talked \nto IronPython and Jython developers and all agreed that some\ncommon benchmarks would be good.  And maybe do weekly\npress releases about small speed increases? :) \n\n\nThe idea of the VM summit here in Chicago was to bring together implementors\nof various virtual machine languages.  There were members of the communities of\nIronPython, CPython, GemStone's MagLev, Rubinius, Mozilla's TraceMonkey, Parrot, \nSun's Da Vinci Machine, Microsoft's DLR, Jython and JRuby.\nEverybody got to talk 5-10 minutes on their current status and \nchallenges.  It is clear that you cannot begin to cover the \ncomplexities and architectures of the involved projects. \nBut that wasn't too much of a problem because the rest of\nthe day everybody freely and dynamically grouped on their\nissues of choice.  We established some more personal contacts,\nwas great to chat with people like Andreas Gal from the University of \nCalifornia, Irvine, who have a very similar idea about the JIT\nthat we have.  Actually, we could probably haved mixed our\ntwo presentations and nobody would have actually noticed :-).\n\n\nAt the end of the presentation part, John Rose presented his\nslides. John is a Hotspot developer, and while not precisely a dynamic\nlanguage implementor, he has a lot of experience in virtual\nmachine implementation. It's very good to see the JVM being extended towards\nsupporting dynamic-language specific things, in order to be something\nmore than just a good platform for Java.  We'll probably have \nsome extra meetup with him the next days. \n\ncheers, \nholger and fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/vm-summit-nice-to-see-friendly-8755773725359396485.html"
    },
    {
      "title": "PyPy talk at OpenBossa 09",
      "text": "Yesterday i gave my PyPy status/mobile perspectives at OpenBossa, Nokia's developer conference for embedded platforms in Brazil.  Found it a bit of a tough task to do that in 50 minutes.  I had some 50, later more developers attending the talk and was happy with the questions and the feedback.  Guess it's a good sign if the number of people grows during a talk :)  It was the first time i tried to work more with pictures and actually used some devianart photos from Marikaz to mark section transitions.  I summarize/highlight some key points here in the post.\nAfter intro and 2.5 compatibility status, i talked about our measurements of PyPy's Python on Nokia's N810 internet tablet. The best bit is that for almost all Python data structures PyPy has smaller memory representations than CPython.  Particularly good are class instances which often score at 50% of CPython's sizes.  Startup time is also often better and can be improved.  On the bad side, PyPy's quite large base interpreter size and its bytecode execution is often worse. In the talk i also outline ideas for \"perfect PYC files\" for minimizing module import times and maximizing sharing across interpreter processes. I also briefly discussed the PyPy situation with extension modules and regarding C++ libs.  Most of these ideas arose from sprint discussions last year.  In the morning i also had some good talk with Stefan Seefeld about Boost Python and the new QT4 bindings.   Maybe to use Boost Python is also a good opportunity - but PyPy does not currently have a C-level or C++ level API.\nIn subsequent lunch discussions people agreed that PyPy has three main interesting areas currently:\n\nthe Python Just-In-Time Compiler\na virtualized, sandboxed Python interpreter\nan efficient Python interpreter for small devices\n\nI think our upcoming 1.1 release will be a good point in time for many people to look some more into PyPy.  I hope we are crossing the chasm soon.  It's been a while since the project started :)  Getting some more sponsoring to sustain and increase our current efforts probably wouldn't hurt.\nNow i am off to spend my last day in Recife / Brazil, fly back to Germany in the evening and then spend time on preparing for Pycon 2009. And I guess i am going to enjoy some naturally cold air - at least my two jogging sessions at Brazillian beaches, at a sustained 30 degrees celsius, were tough.  I guess i shouldn't complain, though :)\nWas great meeting all the brazillian guys and the few women - just had breakfeast with Kate Alhola, kernel hacker and working on the new \"Freemantle\" graphical platform.  Many thanks go to Marcio Marcedo and the Python team at INDT who invited me here.  Hope to come again next year and eventually talk more about the Zone VM :)\nIf you are interested in some more not so pypy-specific bits about the conference and what i experienced, you might head over to my tetamap blog.\nholger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/pypy-talk-at-openbossa-09-5135830287297423499.html"
    },
    {
      "title": "Good news everyone!",
      "text": "A quick update from the JIT front. As of yesterday, we're now able to translate\na highly-experimental Python interpreter that contains JIT. It mostly crashes\nimmediately, mostly due to some unsupported operations in the assembler backend,\nbut for a carefully crafted program, we're able to get massive speedups.\nFor something as complex as:\n\n\n  i = 0\n  while i < 10000000:\n   i = i + 1\n\n\nour JIT is about 20x faster than CPython. That's still about 3x slower than\nPsyco, but looking at assembler code it's obvious that we can speed it up\na lot. These are very good news, since we don't encode python semantics at\nall in the JIT. The JIT is automatically generated from the Python interpreter\nsource code. This means we should be able to expand it to handle more complex\npython programs relatively quickly (interested assembler experts needed!).\n\n\nThis is actually the fifth incarnation of JIT that happened over the last\ntwo years. It's by far simpler and more promising than any of the previous\napproaches. Expect more details soon!\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/03/good-news-everyone-421421336094214242.html"
    },
    {
      "title": "JIT - a bit of look inside",
      "text": "The previous post about our JIT explained a bit from the 1000 km\nperspective how the tracing JIT would approach a language like Python.\n\n\nI would like to step a bit inside and give a zoom to some of its features that\nare already working.\nWhile probably not the most innovative, I think it's very nice to look\nat the way we work with the JIT and what tools we use.\n\n\nThe main cool thing is that you can work on and try the JIT (including trying\nit on the Python interpreter!) without even generating a single bit of\nassembler. How? Let's start with something very simple. Let's take\na simple interpreter for language X.\n\n\nLanguage X has 3 opcodes: CO_INCREASE, CO_DECREASE and CO_JUMP_BACK_3.\nCO_INCREASE increase the accumulator by one, CO_DECREASE decrease\nit by one, CO_JUMP_BACK_3 jump 3 opcodes back, if the accumulator is smaller\nthan 100 (this is only to maintain some halting conditions possible).\nThe interpreter for language X looks like this::\n\n\n    jitdriver = JitDriver(greens = ['i'], reds = ['res', 'a'])\n    code = [CO_INCREASE, CO_INCREASE, CO_INCREASE,\n            CO_JUMP_BACK_3, CO_INCREASE, CO_DECREASE]\n            \n    def add(res, a):\n        return res + a\n\n    def sub(res, a):\n        return res - a\n\n    def main_interpreter_loop(a):\n        i = 0\n        res = 0\n        c = len(code)\n        while i < c:\n            jitdriver.jit_merge_point(res=res, i=i, a=a)\n            elem = code[i]\n            if elem == CO_INCREASE:\n                res = add(res, a)\n            elif elem == CO_DECREASE:\n                res = sub(res, a)\n            else:\n                if res > 100:\n                    pass\n                else:\n                    i = i - 3\n                    jitdriver.can_enter_jit(res=res, i=i, a=a)\n                    continue\n            i = i + 1\n        return res\n\n\nAll very simple code, expect the jitdriver hints, which instruct JIT how to\nbehave (they are the equivalent of the ``add_to_position_key`` of last the blog\npost).\n\n\nLet's look how this code is processed. This will also give a glance\nat how we work in this code. This particular piece can be found\non a branch in pypy/jit/metainterp/test/test_loop.py\nand can be run with ./test_all.py jit/metainterp/test/test_loop.py -k test_example -s --view from pypy directory. The -s option lets you see the debugging output, while\n--view will show you some graphs. So, let's look at graphs in order:\n\n\n\nAnd the same picture with a bit of zoom for the first block:\n\n\n\n\nThis is the call graph of an interpreter loop, nothing magic so far. This is an\nintermediate representation of translation toolchain input. If you look around\nyou can follow how the opcodes are dispatched (with a chain of ifs) and helpers\ncalled. Next graph is very boring, because it's a bit lower level representation\nof the same thing (you exit with q or escape btw :).\n\n\nWhen we exit the graph viewer, we can see the trace generated by interpreting\nthis graph with a given bytecode (variable code in paste above). It's something\nlike:\n\n\n\n        [compiler] ENTER\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 0] -> 0\n        [runner:cpu]    int_eq [0, 0] -> True\n        [runner:cpu]    int_add [9, 1] -> 10\n        [runner:cpu]    int_add [0, 1] -> 1\n        [runner:cpu]    int_lt [1, 6] -> True\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 1] -> 0\n        [runner:cpu]    int_eq [0, 0] -> True\n        [runner:cpu]    int_add [10, 1] -> 11\n        [runner:cpu]    int_add [1, 1] -> 2\n        [runner:cpu]    int_lt [2, 6] -> True\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 2] -> 0\n        [runner:cpu]    int_eq [0, 0] -> True\n        [runner:cpu]    int_add [11, 1] -> 12\n        [runner:cpu]    int_add [2, 1] -> 3\n        [runner:cpu]    int_lt [3, 6] -> True\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 3] -> 1\n        [runner:cpu]    int_eq [1, 0] -> False\n        [runner:cpu]    int_eq [1, 2] -> False\n        [runner:cpu]    int_gt [12, 100] -> False\n        [runner:cpu]    int_sub [3, 3] -> 0\n        [compiler] LEAVE\n\n\nIt's entering JIT, doing some primitive operations for bytecode dispatching\nand repeating the loop. Note that at the end of the interpreted loop\n(not to be confused with the interpreter loop), we see int_sub [3, 3]\nwhich resets the bytecode position to the beginning. At this time JIT\n(instructed by can_enter_jit hint) notices that all green variables\nare the same (here only i),\nhence we can compile the efficient loop from this point.\n\n\n\n\nThe loop contains 3 additions and a check (for i < 100), exactly\nthe same as our interpreted program would do, but completely without\ninterpretation overhead!\n\n\nAs you might have noticed, there is no assembler involved so far. All of this\ninstruction execution is done directly, in pure python. In fact, the\ncode for executing instructions is located in jit/backend/llgraph\nwhich directly interprets instructions. This is by far simpler (and easier\nto debug) than x86 assembler.\n\n\nAnd this is basically it: the very simple interpreter and a jit for it.\nOf course we actually can generate assembler for that. Also the missing\npiece is optimizing the generated graphs. While for this example,\nby removing the interpretetation overhead, we're done, with more complex\nexamples it's important to further optimize traces. Hopefully this and\nhow we actually generate assembler will be topics for next blog posts.\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/03/jit-bit-of-look-inside-7472130507462677287.html"
    },
    {
      "title": "PyPy on Mobiles, at OpenBossa",
      "text": "Next week i am going to give a talk on PyPy at OpenBossa, a developer conference on embedded platforms.  I've written up a bit more of my background and why i find it very interesting to go there on my blog.  Probably will mostly follow up there or on twitter and not much here on the PyPy blog because it's not all about PyPy.  To summarize how i see it: i think there is great potential for Python and PyPy on mobiles and am thrilled to hear about what's going on currently and to discuss opportunities.\ncheers, holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/pypy-on-mobiles-at-openbossa-845760004725129519.html"
    },
    {
      "title": "Applying a Tracing JIT to an Interpreter",
      "text": "After I had failed once more to explain to someone on IRC what the idea behind\nthe current JIT generator work of PyPy, I decided to just write a blog post to\nexplain it. Here it is :-). The post turned out to be a bit long, so please bear\nwith me.\nThe goal of the post is to give an understanding of how PyPy's JIT generator is\ngoing to work. To do this, I will look at what happens when you write an\ninterpreter in Java and apply a completely normal tracing JIT to it (for this\nreason all the code examples will be in some sort of pseudo-Java). The\nresulting generated machine code is bad, so I will explain a way to fix the\noccurring problem.\nThe techniques I describe here are conceptually similar to what we are doing in\nPyPy. The details (as usual) are different. The reasons why I am trying to\nexplain things in this way is that I can start from tracing JITs, which are a\nknown existing technique.\nTo understand the following, it is helpful to already know a bit how a normal\ntracing JIT works. I will give a reminder of how it is working, but there also\nexist a couple of more thorough introductions on the web already.\nI also will leave out a lot of details about the more detailed workings of\ntracing JITs and only explain the things that are relevant to what I am trying\nto get to here.\nTracing JITs\nTracing JITs are an idea explored by the Dynamo project in the context of\ndynamic optimization of machine code at runtime. The techniques were then\nsuccessfully applied to Java VMs and are now being used by Mozilla's\nTraceMonkey JavaScript VM. They are built on some basic assumptions:\n\n\nprograms spend most of their runtime in loops\nseveral iterations of the same loop are likely to take similar code paths\nthe best way to gain information about the behaviour of a program is to\nobserve it\n\n\nThe basic approach of a tracing JIT is to only generate machine code for\ncommonly executed loops and to interpret the rest of the program. The code for\nthose common loops however should be highly optimized, including aggressive\ninlining.\nThe generation of loops works as follows: At first, everything is interpreted.\nThe interpreter does a bit of lightweight profiling to figure out which loops\nare run often. When a common loop is identified, the interpreter enters a\nspecial mode (called tracing mode). When in tracing mode, the interpreter\nrecords a history (the trace) of all the operations it executes, in addition\nto actually performing the operations. During tracing, the trace is repeatedly\nchecked whether the interpreter is at a position in the program that it had seen\nearlier in the trace. If this happens, the trace recorded corresponds to a loop\nin the program that the tracing interpreter is running. At this point, this loop\nis turned into machine code by taking the trace and making machine code versions\nof all the operations in it.\nThis process assumes that the path through the loop that was traced is a\n\"typical\" example of possible paths (which is statistically likely). Of course\nit is possible that later another path through the loop is taken, therefore the\nmachine code will contain guards, which check that the path is still the same.\nIf during execution of the machine code a guard fails, the machine code is left\nand execution falls back to using interpretation (there are more complex\nmechanisms in place to still produce more code for the cases of guard failures,\nbut they are of no importance for this post).\nIt is important to understand when the tracer considers a loop in the trace to\nbe closed. This happens when the position key is the same as at an earlier\npoint. The position key describes the position of the execution of the program,\ne.g. usually contains things like the function currently being executed and the\nprogram counter position of the tracing interpreter.\nLet's look at a small example. Take the following code:\n\nint sum_1_to_n(int n) {\n    int result = 0;\n    while (n >= 0) {\n        result += n;\n        n -= 1;\n    }\n    return result;\n}\n\nThe tracing JIT will at one point trace the execution of the while loop in\nsum_1_to_n. The trace might look as follows:\n\nguard_true(n >= 0);\nresult += n;\nn -= 1;\n<loop_back>\n\nThis trace will then be turned into machine code. Note that the machine code\nloop is by itself infinite and can only be left via a guard failure.\nA slightly more complex example:\n\nint f(int a, int b) {\n    if (b % 46 == 41)\n        return a - b;\n    else\n        return a + b;\n}\n\nint strange_sum(int n) {\n    int result = 0;\n    while (n >= 0) {\n        result = f(result, n);\n        n -= 1;\n    }\n    return result;\n}\n\nThe trace of the loop in strange_sum would maybe look like this:\n\nguard_true(n >= 0);\na = result;\nb = n;\nguard_false(b % 46 == 41);\nresult = a + b;\nn -= 1;\n<loop_back>\n\nThis would then be turned into machine code. Note how f was inlined into the\nloop and how the common else case was turned into machine code, while the\nother one is implemented via a guard failure.\nApplying a Tracing JIT to an Interpreter\nIn the rest of the post we will explore what happens when the program that is\nbeing executed/compiled by the tracing JIT is itself a (bytecode) interpreter\nfor another language.\nA stylized bytecode interpreter for a simple programming language could look as\nfollows:\n\nW_Object interpret(String bytecode, ...) {\n    Stack<W_Object> stack = new Stack<W_Object>();\n    int pc = 0;\n    while (true) { // bytecode dispatch loop\n        char instruction = bytecode.charAt(pc);\n        pc += 1;\n        switch (instruction) {\n            case ADD:\n                W_Object arg2 = stack.pop();\n                W_Object arg1 = stack.pop();\n                stack.push(do_addition(arg1, arg2));\n                break;\n            case SUB:\n                W_Object arg2 = stack.pop();\n                W_Object arg1 = stack.pop();\n                stack.push(do_substraction(arg1, arg2));\n                break;\n            case RETURN:\n                return stack.pop();\n            case JUMP_BACKWARD:\n                pc -= (int)bytecode.charAt(pc);\n                break;\n            case LOAD_INTEGER:\n                int value = (int)bytecode.charAt(pc);\n                pc += 1;\n                stack.push(new W_Integer(value));\n                break;\n            case PRINT:\n                do_print(stack.pop());\n                break;\n            case DUP:\n                stack.push(stack.peek());\n                break;\n            case JUMP_IF_TRUE:\n                ...\n            ...\n        }\n    }\n\nIf we apply a tracing JIT to this function, it will trace and compile the\nexecution of one bytecode, because after one bytecode the bytecode dispatch loop\nis closed. E.g. it might trace and produce machine code for the execution of a\nSUB. (Sidenote: this interpret function is an example where one of the\nassumptions of a tracing JIT break down: two iterations of the bytecode dispatch\nloop are rarely going to follow the same code path, because usually two\nconsecutive bytecodes encode different instructions).\nThe important bit to remember here is that the tracing JIT will produce a\nmachine code loop that corresponds to the bytecode dispatch loop in the\ninterpret function. Let's see how we can change that.\nImproving the Generated Code\nIf we want to make use of the fact that the program that is being jitted is\nitself an interpreter, we need to change the tracing JIT a bit. To be more\nprecise we add a way for the user of the tracing JIT to add information to the\nposition key that the tracing JIT uses to decide when a loop is closed. This is\ndone by a call to a magic function add_to_position_key. This allows the\nprogram writer to influence the tracing JIT's behaviour.\nThe semantics of add_to_position_key is as follows: The method itself does\nnot do anything. It has an effect only when it is seen during tracing. If it is\nseen during tracing, the tracer adds the argument of the call to the position\nkey that the tracer is using to find out whether a loop was closed or not.\nIn the example of the interpret function above, we would add a call to this\nfunction into the while loop as follows:\n\nW_Object interpret(String bytecode, ...) {\n    Stack stack = new Stack();\n    int pc = 0;\n    while (true) { // bytecode dispatch loop\n        add_to_position_key(pc);\n        add_to_position_key(bytecode);\n        char instruction = bytecode.charAt(pc);\n        pc += 1;\n        switch (instruction) {\n            case ADD:\n    ...\n\nWhen the modified tracing JIT traces now the interpret function executing a\nSUB, something interesting happens. When the bytecode loop is closed, the\nmodified tracing JIT does not consider the trace to be a loop, because the value of\npc has been increased by one, so the position key differs. Instead it\ncontinues to trace, effectively unrolling the bytecode dispatch loop of\ninterpret.\nThe only way for a loop to be considered closed is if the pc variable has\nthe same value a second time.  This can only happen after a JUMP_BACKWARD\ninstruction has been executed.  A JUMP_BACKWARD instruction will only be in\nthe bytecode when the bytecode represents a loop. This means that the modified\ntracing JIT will trace the interpret function and will only consider that\nthe trace represents a loop when the bytecode itself represents a loop! Thus, a\nmachine code loop will eventually be created that corresponds to the loop in the\nbytecode.\nLet's look at at example. If we have a bytecode that corresponds to the\nfollowing instructions:\n\npc |   instruction\n---+---------------------\n0  |  LOAD_INTEGER 0\n2  |  DUP\n3  |  PRINT\n4  |  LOAD_INTEGER 1\n6  |  ADD\n7  |  JUMP_BACKWARD 6\n\nThis loop will print integers starting from 0 and going on from there. The\nmodified tracing JIT will unroll the bytecode dispatch until it sees the\nJUMP_BACKWARD bytecode. After that bytecode the pc will be 2 again. Thus\nthe earlier position key is repeated, which means that the loop will be closed.\nThe produced machine code will do the equivalent of the following Java code:\n\n...\nguard_true(pc == 2)\nguard_true(bytecode == \"... correct bytecode string ...\")\nwhile (true) {\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == DUP);\n    stack.push(stack.peek());\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == PRINT);\n    do_print(stack.pop());\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == LOAD_INTEGER)\n    value = (int)bytecode.charAt(pc);\n    pc += 1\n    stack.push(W_Integer(value))\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == ADD)\n    arg2 = stack.pop()\n    arg1 = stack.pop()\n    stack.push(do_addition(arg1, arg2))\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == JUMP_BACKWARD)\n    pc -= (int)bytecode.charAt(pc);\n}\n\nThis is machine code that essentially does what the bytecode above did. Of\ncourse the code still remains some remnants of the interpreter (like the program\ncounter manipulations, the stack handling, etc), which would have to be removed\nby some clever enough optimization step. If this were done, result would look a\nlot more natural.\nSummary\nIf a tracing JIT is enhanced by a way to influence its loop-closing behaviour we\ncan significantly improve its performance when the jitted program is itself an\ninterpreter. The result is that in such a case the produced machine code\nwill correspond to the functions that are being interpreted, not to the code of\nthe interpreter itself.\nNow, what does all this have to do with PyPy? What we are working on since a\nwhile is a sort of tracing JIT for RPython which allows to be customized with a\nfunction very similar to the add_to_position_key described above. This will\nmake it possible to make the tracing JIT generate code that corresponds to the\ncode that the interpreter interprets. For example, we would add a call to\nadd_to_position_key to SPy, PyPy's Smalltalk VM. Then the tracing JIT will\nproduce machine code for Smalltalk-level loops, with all the usual benefits of a\ntracing JIT (like inlining of intermediate methods, constant-folding, ...).\nThis JIT differs from normal tracing JITs in that it also supports very powerful\nconstant-folding and allocation-removal optimizations. Those optimizations will\n(hopefully) be the content of a later blog post.\nThe basics of this process have been working fine since quite a while. What the\nwork currently focuses on is to improve the optimizers to remove not only the\nbytecode manipulation code, but also the stack handling, and a large number of\nother inefficiencies.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/03/applying-tracing-jit-to-interpreter-3287844903778799266.html"
    },
    {
      "title": "The next Leysin Winter Sprint",
      "text": "PyPy Leysin Winter Sprint (14-21th April 2009)\n\nThe next PyPy sprint will be in Leysin, Switzerland, for the\nsixth time.  This sprint will take place immediately after\nEaster.  This is a fully public sprint: newcomers and topics\nother than those proposed below are welcome.\n\n\n\n\n\n\n\nThe overall idea of the sprint is to continue working on making PyPy ready\nfor general use.  There are a few tasks left in there.  In parallel, we\nwill continue the work on the JIT, if there is general interest.  And as\nusual, we are ready to add any other task -- please mention on the mailing\nlist what you would like to work on; the list of task is not really fixed.\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski until Sunday, the 19th; afterwards, the\ninstallations close.  (There was quite a lot of snow this winter, so\nthere should be some left even though it's relatively late in the season.)\n\n\n\n\n\nFor more information see the announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/next-leysin-winter-sprint-1791506307881043273.html"
    },
    {
      "title": "Wroclaw 2009 sprint progress report",
      "text": "Hello.\n\nWe have just finished probably the smallest sprint ever\nin PyPy history. For most of the time it was just me\nand Armin pairing together.\n\nWe also had a chance to work a bit with people from\nthe University, but there were definitely not enough\ncore developers to organize the work in a reasonable\nmanner. At some point we ended up having two pairs containing\nfour people each.\n\nJakub and Bartosz (who were our gentle hosts) worked\non getting PyPy's sandbox integrated with django.\nIt's still just an example what you can do (ie you\ncan do much more), but it's already interesting to look\nat. The code can be found in user dir. This server (not yet online anywhere, sorry)\nis able to run untrusted python code provided by user inside\na fully configurable sandbox.\n\nWe also implemented missing peepholer optimizations from\nCPython, finding out that some peepholer tests were failing,\njust because PyPy is optimizing better :-)\n\nThe main part of the sprint was work on JIT (most notable the fifth\ngeneration of the JIT), which was moved\nfrom the obscure directory in Carl's user in svn (which contains\nbranches these days!) into a PyPy branch. It's still very much\nwork in progress and a lot of pen and paper or handwaving was\ninvolved, but we were able to implement a lot of basics in record time.\n\nRight now we need a lot of rest after the exhaustive sprint,\nbut after that, stay tuned for more information about\nprogressing JIT!\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/02/wroclaw-2009-sprint-progress-report-2510073170049635489.html"
    },
    {
      "title": "Wroclaw 2009 PyPy sprint and talk",
      "text": "The next PyPy sprint will be held in Wroc\u0142aw, Poland 7-14th February 2009. This is fully public\nsprint and all newcomers are welcomed. Preceeding the sprint there\nwill be a talk at University of Technology in Wroc\u0142aw held at 22nd of January.\n\nFor detailed info about the sprint, look here.\n\nThe talk will be a general, high-level overview about PyPy project. There is a very nice poster, made by Jakub Gustak and Bartosz Skowron (in polish):\n\n\n\nTalk details:\n\nLocation: Politechnika Wroc\u0142awska, budynek C-13, sala 0.31\nDate: 22nd January 2009, 19:00\nLanguage: very likely polish, although talk can be as well in english if some non-polish native would show up.\n\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/01/wroclaw-2009-pypy-sprint-and-talk-8240928228677982487.html"
    },
    {
      "title": "Pycon 2009",
      "text": "Hello.\n\nBoth of our PyPy talks has been accepted for Pycon US 2009. Although both\nare somehow related to PyPy, they're vastly different in\ntopics, attitude and target audience.\n\nThe first one is a classic PyPy status talk - we'll mostly talk about\nour achievements from the last year (readers of this blog are aware of most,\nbut not all :) as well as some general introduction and plans for the future.\n\n\nThe second one is about PyPy's sandboxing features. This is in my opinion\na very underestimated feature, also by us, because it's not really well\nadvertised or documented. The main purpose of the talk is to present\nto the general public how this works and how to use it.  Hopefully we will\nget to work and publish about this a bit more ahead of Pycon already. \nUnlike Zope's Restricted Python, it provides you with the full python\nlanguage, inside a fully\nvirtualized sandbox, controlled from an external process by a custom\nsecurity policy. Stay tuned for more :-)\n\n\nSee you at Pycon 2009!\n\n\nCheers,\nfijal and holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/12/pycon-2009-9090464449197911432.html"
    },
    {
      "title": "Porting the JIT to CLI (part 3)",
      "text": "In my two previous posts, we talked about the PyPy JIT generator, seeing\nthat it can produce huge speedups and how its backend-independent frontend\nworks.\nIn this post, we will look closer at the internals of the CLI JIT backend; in\nparticular, we will see how we work around some serious limitations of the\nplatform, and why these workarounds didn't have any serious impact on the\nperformances of our toy virtual machine.\n\nGraphs, blocks, links\n\n\n\n\nOne of the core aspect of PyPy translator is the concept of flow graph: a\nflow graph is a data structure that represents the code we are operating on.\nIt is composed by a set of basic blocks, each block containing a sequence\nof operations; blocks are connected together by links, and each link can\ncarry a variable number of arguments whose value is passed to the target\nblock.  In case a block contains more than one outgoing links, the one to\nfollow is selected by looking at the value of a designated variable (the\nexitswitch), thus making possible to implement conditional jumps.  To have\na more complete description of the flow graphs model, check the documentation.\n\nAs we saw in the previous post, the generated JIT compiler makes heavy use of\nflexswitches to generate efficient code, continuously intermixing\nJIT-compile time and runtime.\nIn terms of graphs, we can think of a flexswitch as a special block whose\nlinks change over time.  In particular, adding a new case to the flexswitch is\nequivalent to create a link whose target is a new block where the just\ngenerated code starts.  Thus, the graphs grows over the time, as showed by\nthe following images:\n\n\n\n\n\n\nIn the images above, the block containing the flexswitch is colored in\ncyan. In the first picture, there is only one block connected to the\nflexswitch: this block contains the code to restart the JIT compilation.  The\nsecond picture shows the graph after the first case has been added: you can\nclearly see that a new block has been created and attached to the flexswitch.\nFinally, the third picture shows the graph after a while, with a lot of new\nblocks attached.\n\n\nTranslate graphs to CLI\nConceptually, the goal of the CLI JIT backend is to express these graphs in\nterms of CLI bytecode.\nTranslating the single block is easy, as it is just a list of sequential\noperation, and it's straightforward to map each operation to the equivalent\nCLI opcode or to a call to a helper method.  Moreover, we need a way to\nexpress links between the various basic blocks: if the links are known in\nadvance, render them is as easy as emitting a (potentially conditional) jump to\nthe target block.  Thus, we won't discuss this part in detail, as it is quite\nstraightforward.\nThe hard part is how to implement flexswitches: at the time when we are\nemitting the code, some of the blocks of this growable graph don't even exist:\nhow can we make a jump to a non existent block of code?  For backends that\nemit assembly code, it is rather easy: when they need to add a new case to the\nflexswitch, they can just patch the existing code to insert a jump to a\nnewly allocated area of the memory, where the new code is being generated in.\nFor CLI this approach is not feasible, as the VM will never allow us to modify\nexisting code. Thus, we need to think of a different approach.\n\n\nGraphs and methods\nIn .NET, the basic unit of compilation is the method: the only way to\nexecute some bytecode is to wrap it into a method.  Moreover, it is not\npossible to execute a method until it has been completed, and after this point\nit is no longer possible to add new code.\nBecause of all these constraints we cannot simply map each graph to its own\nmethod, since we saw that our graphs can grow after they have already been\nexecuted few times.\nHence, we need to distinguish between the two concepts:\n\n\na graph is the logical unit of code as seen by the JIT compiler:\nconcretely, the CLI JIT backend renders it as one or more methods;\na method is a collection of basic blocks; each method has the so\ncalled parent graph, i.e. the graph its blocks logically belongs to.\n\n\nThe first method of a graph is called main method (which has\nnothing to do with the Main static methods found in .exe files); other\nmethods are called children methods.\nWhen we want to add a new case to the flexswitch, we create a method\ncontaining all the new code; then we wrap the method inside a delegate (the\n.NET equivalent of a function pointer) and pass it to the flexswitch, so that\nit can later invoke it.\n\n\nThe hard bit: non-local links\nUsing this approach, after a while the blocks of our original graph are\nscattered over a lot of different methods; however, there are no constraints\nabout how these blocks can be linked together, so it happens to have links\nbetween blocks which are not in the same method. In the following, we will\nrefer to them as non-local links.\nIf the non-local block we want to jump to happens to be at the beginning of\nits containing method, it is enough to invoke the method; but, what if we want\nto jump somewhere in the middle?  What we really want is to produce a method\nwhich has multiple entry-points; again, doing it in assembly would be\ntrivial, but the virtual machine does not provide any support for it, so we\nneed a work around.\nEach method in a graph is assigned an unique 16 bit method id; each block in\na method is assigned a progressive 16 bit block number.  From this two\nnumbers, we can compute the block id as an unsigned integer, by storing\nthe method id in the first 16 bits and the block number in the second 16 bits.\nBy construction, the block id is guaranteed to be unique in the graph.\nThe following picture shows a graph composed of three methods; the id of each\nmethod is shown in red, while the block ids are shown in red (for the method\nid part) and black (for the block number part).  The graph contains three\nnon-local links; in particular, note the link between blocks 0x00020001\nand 0x00010001 which connects two block that resides in different methods.\n\n\n\nEvery method contains a special dispatch block, (not shown in the picture above) whose goal is to jump to\nthe specified block number inside the method itself.  The first argument of a\nchild method is always a block id; when the method starts, it immediately\njumps to the dispatch block, and thus to the desired block.For example, suppose to have a method which contains 3 blocks numbered 0, 1,\n2; here is how its dispatch blocks looks like; for simplicity it is shown as\nC# code, but it is actually generated as IL bytecode:\n\n// dispatch block\nint methodid = (blockid & 0xFFFF0000) >> 16); // take the first 16 bits\nint blocknum = blockid && 0x0000FFFF;         // take the second 16 bits\n\nif (methodid != MY_METHOD_ID) {\n// jump_to_unknown block\n...\n}\n\nswitch(blocknum) {\ncase 0:\ngoto block0;\ncase 1:\ngoto block1;\ncase 2:\ngoto block2;\ndefault:\nthrow new Exception(\"Invalid block id\");\n}\n\nWhenever we want to jump to a non-local block, it is enough to store the block\nid in the appropriate variable and jump to the dispatch block.  If the block\nresides in a different method, the jump_to_unknown block is entered; this\nspecial block is implemented differently by the main method and the child\nmethods, as we will see soon.\nEach time a new method is added to the graph, we build a delegate\nfor it, and store it in a special array\ncalled method_map; since we assign the method id sequentially starting\nfrom 0, we are sure that to fetch the method whose id is n we can simply\nload the n-th element of the array.\nThe jump_to_unknown block of the main method uses this array to select the\nright method, and calls it (FlexSwitchCase is the type of delegates for\nall children methods):\n\n// jump_to_unknown block of the main method\nFlexSwitchCase meth = method_map[methodid];\nblockid = meth(blockid, ...); // execute the method\ngoto dispatch_block;\n\nEach child method returns a block id specifying the next block to jump to;\nafter its execution, we assign the return value to the blockid variable,\nand jump again to the dispatch block, which will jump again to the appropriate\nblock.\nKeeping this in mind, it is straightforward to implement the\njump_to_unknown block of children methods: it is enough to return the\ntarget block id to the caller, and let its dispatch loop do the right thing.\nIf the caller is also a child method, it will return it again, until we reach\nthe dispatch loop of the main method, which will finally do the jump.  In\ntheory, we could implement things differently and jumping directly from a\nchild method to another one, but in that case the call stack could grows\nindefinitely in case of a tight loop between two blocks residing in different\nmethods.\nTo implement the dispatch block we can exploit the switch opcode of the\nCLI; if the .NET JIT is smart enough, it can render it using an indirect jump;\noverall, jumping to a non-local block consists of an indirect function call\n(by invoking the delegate) plus an indirect jump (by executing the switch\nopcode); even if this is more costly than a simple direct jump, we will see in\nthe next section that this not the main source of overhead when following a\nnon-local link.\nObviously, the slow dispatching logic is needed only when we want to jump to a\nnon-local block; if the target block happens to reside in the same method as\nthe current one, we can directly jump to it, completely removing the overhead.\nMoreover, the dispatch blocks are emitted only if needed, i.e. if the parent\ngraph contains at least one flexswitch; graphs without flexswitches are\nrendered in the obvious way, by making one method per graph.\n\n\nThe slow bit: passing arguments\nJumping to the correct block is not enough to follow a link: as we said\nbefore, each link carries a set of arguments to be passed from the source to\nthe target block.  As usual, passing arguments across local links is easy, as\nwe can just use local variables to hold their values; on the other hand,\nnon-local links make things more complex.\nThe only way to jump to a block is to invoke its containing method, so the\nfirst solution that comes to mind is to specify its input arguments as\nparameter of the method; however, each block has potentially a different\nnumber (and different types) of input arguments than every other block, so we\nneed to think of something else.\nAn alternative solution could be to compute the union of the sets of input\narguments of all the blocks in the method, and use this set as a signature\nfor the method; this way, there would be enough space to specify the input\narguments for every block we might want to jump to, each block ignoring the\nexceeding unused parameters.\nUnfortunately, all the children methods must have the very same signature,\nas they are all called from the same calling site in the dispatch block of the\nmain method.  Since the union of the set of input arguments (and hence the\ncomputed signature) varies from method to method, this solution cannot work.\nWe might think to determine the signature by computing the union of input\narguments of all blocks in the graph; this way, all the children methods\nwould have the same signature.  But as we said above, the graph grows new\nblocks at runtime, so we cannot determine in advance which set of input\narguments we will need.\nTo solve the problem we need a way to pass a variable number of arguments\nwithout knowing in advance neither their number nor their types.  Thus, we use\nan instance of this class:\n\npublic class InputArgs {\npublic int[] ints;\npublic float[] floats;\npublic object[] objs;\n...\n}\n\nSince the fields are arrays, they can grow as needed to contain any number of\narguments; arguments whose type is primitive are stored in the ints or\nfloats array, depending on their type; arguments whose type is a reference\ntype are stored in the objs array: it's up to each block to cast each\nargument back to the needed type.\nThis solution impose a huge overhead on both writing and reading arguments:\n\n\nwhen writing, we need to make sure that the arrays are big enough to\ncontains all the arguments we need; if not, we need to allocate a bigger\narray.  Moreover, for each argument we store into the array the virtual\nmachine performs a bound-check, even if we know the index will never be\nout of bounds (because we checked the size of the array in advance);\nwhen reading, the same bound-check is performed for each argument read;\nmoreover, for each value read from the objs array we need to insert a\ndowncast.\n\n\nTo mitigate the performance drop, we avoid to allocate a new InputArgs\nobject each time we do a non-local jump; instead, we preallocate one at the\nbeginning of the main method, and reuse it all the time.\nOur benchmarks show that passing arguments in arrays is about 10 times slower\nthan passing them as real parameter of a method.  Unfortunately, we couldn't\ncome up with anything better.\n\n\nImplement flexswitches\nNow, we can exploit all this machinery to implement flexswitches, as this is\nour ultimate goal.  As described above, the point is to be able to add new\ncases at runtime, each case represented as a delegate.  Here is an excerpt\nof the C# class that implements a flexswitch that switches over an integer\nvalue:\n\npublic class IntLowLevelFlexSwitch:\n{\npublic uint default_blockid = 0xFFFFFFFF;\npublic int numcases = 0;\npublic int[] values = new int[4];\npublic FlexSwitchCase[] cases = new FlexSwitchCase[4];\n\npublic void add_case(int value, FlexSwitchCase c)\n{\n...\n}\n\npublic uint execute(int value, InputArgs args)\n{\nfor(int i=0; i<numcases; i++)\nif (values[i] == value) {\n return cases[i](0, args);\n}\nreturn default_blockid;\n}\n}\n\nFor each case, we store both the triggering value and the corresponding\ndelegate; the add_case method takes care to append value and c to\nthe values and cases arrays, respectively (and resize them if\nnecessary).  The interesting bit is the execute method: it takes a value\nand a set of input arguments to be passed across the link and jumps to the\nright block by performing a linear search in the values array.\nAs shown by previous sections, the first argument of a FlexSwitchCase is\nthe block id to jump to; since when we go through a flexswitch we always want\nto jump to the first block of the method, we pass the special value 0 as a\nblock id, which precisely means jump to the first block.  This little\noptimization let us not to have to explicitly store the block id for the first\nblock of all the cases.\nThe value returned by execute is the next block id to jump to; if the\nvalue is not found in the values array, we return the default_blockid,\nwhose value has been set before by the JIT compiler; default_blockid\nusually points to a block containing code to restart the JIT compiler again;\nwhen the JIT compiler restarts, it emits more code for the missing case, then\ncalls add_case on the flexswitch; from now on, the new blocks are wired\ninto the existing graph, and we finally managed to implement growable\ngraphs.\n\n\nPerformances\nAs we saw, implementing growable graphs for CLI is a pain, as the virtual machine\noffers very little support, so we need an incredible amount of workarounds.\nMoreover, the code generated is much worse than what an assembly backend could\nproduce, and the cost of following a non-local link is very high compared to\nlocal links.\nHowever, our first blog post showed that we still get very good\nperformances; how is it possible?\nAs usual in computer science, most of the time of a running program in\nspent in a tiny fraction of the code; our benchmark is no exception, and the\nvast majority of the time is spent in the inner loop that multiplies numbers;\nthe graph is built in such a way that all the blocks that are part of the\ninner loop reside in the same method, so that all links inside are local (and\nfast).\nFlexswitches and non-local links play a key role to select the right\nspecialized implementation of the inner loop, but once it is selected they are\nnot executed anymore until we have finished the computation.\nIt is still unclear how things will look like when we will compile the full\nPython language instead of a toy one; depending on the code, it could be\npossible to have non-local links inside the inner loop, thus making\nperformance much worse.\n\n\nAlternative implementations\nBefore implementing the solution described here, we carefully studied a lot of\npossible alternatives, but all of them either didn't work because of a\nlimitation of the virtual machine or they could work but with terrible\nperformances.\nIn particular, in theory it is possible to implement non-local links using\ntail calls, by putting each block in its own method and doing a tail call\ninstead of a jump; this would also solve the problem of how to pass arguments,\nas each method could have its own signature matching the input args of the\nblock.  I would like to explain this solution in a more detailed way as I\nthink it's really elegant and nice, but since this post is already too long,\nI'll stop here :-).\nIn theory, if the .NET JIT were smart enough it could inline and optimize away\nthe tail calls (or at least many of those) and give us very efficient code.\nHowever, one benchmark I wrote shows that tail calls are up to 10 times\nslower (!!!) than normal calls, thus making impractical to use them for our\npurposes.\n\n\nConclusion\nDespite the complexity of the implementation, our result are extremely good;\nthe speedup we got is impressive, and it proves that PyPy's approach to JIT\ncompiler can work well also on top of object oriented virtual machines like\n.NET or the JVM.\nGenerating bytecode for those machine at runtime is not a new idea; Jython,\nIronPython, JRuby and other languages have been doing this for years.\nHowever, Jython and IronPython do only a simple \"static\" translation, which\ndoesn't take advantage of the informations gathered at runtime to generate\nbetter, faster and specialized code.  Recently, JRuby grew a new strategy to\nJIT-compile only hotspots, taking advantage of some informations gathered\nwhile interpreting the code; this is still a \"one-shot\" compilation, where the\ncompiled code does not change over time.\nTo my knowledge, PyPy brings the first example of a\nlanguage which implements a truly JIT compiler on top of the underlying JIT\ncompiler of the virtual machine, emitting bytecode that changes and adapts\nover the time.  If someone knows other languages doing that, I would really\nlike to know more.\nBeing so innovative, the problem of this approach is that the current virtual\nmachines are not designed to support it in a native way, and this forces us to\nput a lot of workarounds that slow down the generated code.  The hope is that\nin the future the virtual machines will grow features that help us to generate\nsuch kind of code.  The experimental Da Vinci VM seems to go in the right\ndirection, so it is possible that in the future I will try to write a JIT\nbackend for it.\nAt the moment, the CLI JIT backend is almost complete, and all the hardest\nproblems seems to be solved; the next step is to fix all the remaining bugs\nand implement some minor feature that it's still missing, then try to apply it\nto the full Python language and see what is the outcome.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/12/porting-jit-to-cli-part-3-3519327524638923621.html"
    },
    {
      "title": "Porting the JIT to CLI (part 2)",
      "text": "In my previous post, we saw that PyPy JIT generator can produce huge\nspeedups when applied to the tlc toy language.\nIn this post we will dive a bit into the internals of PyPy JIT, to see how it\nmanages to do so. Note that this is a very high level overview of how the\nJIT works, and applies to all backends.  Then, in the third post of this\nseries, we will look closer at the CLI JIT backend, seeing how it works around\nsome .NET limitations and how the generated code looks like.\n\nPyPy JIT for dummies\nAs you surely know, the key idea of PyPy is that we are too lazy to write a\nJIT of our own: so, instead of passing nights writing a JIT, we pass years\ncoding a JIT generator that writes the JIT for us :-).\nI'm not going to explain how the JIT generator does its job, (perhaps this\nwill be the subject of another blog post), but how the generated JIT\nworks.\nThere are values that, if known at compile-time (i.e., when the JIT compiler\nruns), let the JIT to produce very efficient code.  In a dynamic language,\ntypes are the primary example: for instance, suppose you are a compiler and\nyou have to compile to following Python function:\n\ndef mysum(a):\n  return a + 1\n\nAt compile time, you don't have any knowledge about the type of the parameter:\nit could be integer, float, an user defined object, etc.  In this situation,\nthe only safe choice is to emit code which does the usual, slow, full lookup\nto know how to perform the operations.\nOn the other hand, suppose that you knew in advance that the parameter is an\ninteger: this time, you could emit code that exploits this extra\nknowledge, by performing directly a fast integer addition.\nThe idea behind PyPy JIT is that if you don't have enough knowledge to\ngenerate efficient code, you stop compiling and wait until you know\nexactly what you need.  Concretely, you emit code that runs until the point\nwhere you stopped the compilation, then it triggers a special procedure that\nrestarts the compiler.  This time the JIT compiler knows everything\nyou need, because you can inspect the state of the running program.\nLet's see an example: the first time the JIT compiles mysum, it produces\nsomething like this pseudo-code:\n\nPyObject mysum_compiled(PyObject a)\n{\n  Type a_type = a.GetType();\n  switch(a_type) {\n      default: continue_compilation(a_type, <position>);\n  }\n}\n\nIf you call mysum(41), the execution goes in the default branch of the\nswitch, thus calling continue_compilation: its job is to restart the JIT\ncompiler, which now can emit fast code because it knows the exact type of\na; then, it modifies the original mysum_compiled function, in\norder to make it executing the newly generated code the next time it\nencounters an integer at that point:\n\nPyObject mysum_compiled(PyObject a)\n{\n  Type a_type = a.GetType();\n  switch(a_type) {\n      PyInteger: return new PyInteger(a.value+1); // fast path!\n      default: continue_compilation(a_type, <position>);\n  }\n}\n\nFrom now on, every time we call mysum with an integer argument, the JIT\ncompiler is not called anymore and the fast path is directly executed; if we\nhappen to call mysum with a float arguments, the switch goes again in the\ndefault branch, and the JIT compiler is started once more to produce fast\ncode also for this case.  What happens in practice is that compile-time and\nruntime are continuously intermixed, until the switches are stable enough and\nthe compiler is not needed anymore.\nIn PyPy jargon, this kind of \"growable switch\" is called flexswitch, and\nit's one of the most important concept of our JIT generator.\n\nPromotion\nHow can the JIT generator know which values are useful to know to generate\nefficient code and which aren't?  Unfortunately it can't, or at least our JIT\ngenerator is not smart enough at the moment.\nTo get the best from it, the developers of the VM need to instruct it by\nannotating the variables on which we want the JIT to stop until it knows the\nactual values; this is done by using particular hints, called promote\nand promote_class; variables annotated with such hints are said to be\npromoted. If something is promoted, a flexswitch is used to gain\ninformation about it, as seen in the last section.\nFor an example, let's look at an excerpt from main dispatch loop of the tlc\nvirtual machine:\n\nelif opcode == ADD:\n  a, b = stack.pop(), stack.pop()\n  hint(a, promote_class=True)\n  hint(b, promote_class=True)\n  stack.append(b.add(a))\n\nThis the implementation of the ADD opcode: first, it pops two values from\nthe stack; then, it computes the result; finally, it push the result to the\nstack again.  In between, both the classes of a and b have been\npromoted: this means that when the JIT emits the code for b.add(a), it\nknows exactly what is happening: if it sees that both are instances of the\nIntObj class, it inlines the method call and emits a fast integer addition\ninstead.\n\nVirtuals\nThe other important concept of the JIT is the presence of virtual\nstructures, virtual lists, and virtual dictionaries.  Again, I'm not\ngoing to explain in depth how they work, but only why they are so important for\ngenerating highly efficient code.\nThe essence of virtuals is that you don't allocate objects until you really\nneed to do it, e.g. because they are being passed as an argument to some\nexternal function.  Instead, we store all the informations we need as local\nvariables; e.g., in the case of a virtual structure, we create as many local\nvariables as the number of its fields: if the structure escapes the local\nscope, we force it to a real object, by allocating memory on the heap and\ninitializing it after the current value of the local variables.\nThis technique allows the JIT to avoid the allocation of many temporary\nobjects that hold intermediate results; consider for example the following\nPython loop:\n\nresult = 0\nfor i in range(N):\n  result += i\nreturn result\n\nWithout the JIT, at each iteration, a new int object is created and bound\nto the result variable, while the previous one is discarded and not needed\nanymore.  By combining virtuals and promotion, the JIT can emit code that does\nthe whole computation locally, and allocates a real object only at the end,\nwhen it escapes from the local scope because it is returned from the\nfunction.\n\nPutting it all together\nThis is, essentially, how PyPy's generated JITs work.  To summarize, our JITs\nemit multiple versions of each chunk of code: each version is specialized\nand optimized for one particular case.\nThe cost of selecting the right specialization to use (through flexswitches)\nis almost always negligible compared to how much time you save by running the\nfast version instead of the more-general-but-slow one.  Moreover, each\nspecialized version knows the exact shape of the objects it's dealing with, so\nthey can be virtualized to make the generated code even more efficient.\nAt the end, the actual code generation is done by one of the JIT backends:\nthe backends exploit all the knowledge gathered by the previous steps to\nproduce highly efficient code, but this will be the subject of the next blog\npost.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/11/porting-jit-to-cli-part-2-2456826431882963884.html"
    },
    {
      "title": "Porting the JIT to CLI (part 1)",
      "text": "As the readers of this blog already know, I have been working on the CLI\nJIT backend for some months: last Friday, it reached an important milestone,\nas it is now able to produce huge speedups for a little dynamic language.  To\nknow how huge the speedup is, read on :-).\nThe goal of PyPy JIT generator is to take an interpreter and, with the help of\nfew annotations, automatically generate a JIT compiler for it.  In this post,\nwe will talk about the tlc virtual machine: while tlc it is just a toy\nlanguage, it contains some features that make it an interesting target for our\nJIT generator.\n\nThe tlc virtual machine\ntlc is executed by a stack based, dynamically typed virtual machine (for\nthose who knows a bit about the Python VM: does it sound familiar? :-)).\nThere are three types of objects: integers, nil, and cons cells (i.e.\nlisp-like pairs of objects).\nAs the VM is very simple, it provides only few opcodes:\n\n\nopcodes to manipulate the stack, like PUSH, POP, etc.\ninteger operations, like ADD, MUL, all the comparisons, etc.:\nthese operations can only be applied to integers;\nlist operations, like CONS, CAR, CDR: these operations can\nonly be applied to lists;\nother operations, including jumps and conditional jumps.\n\n\nThe VM is interesting for our purposes because it has a lot of similarities\nwith Python (though on a smaller scale, of course):\n\n\nit has to do type-checks at runtime before doing most of the operations;\nevery time you do an arithmetic operation, it has to unbox the operand,\ndo the computation, and the box the result again.\n\n\nThis means that even if you have a program which only uses integers, you are\npaying a lot of overhead.\nTo know more about this toy VM, look at its source code: the interesting\nbits are the classes used to represent objects, and the interp_eval\nfunction, which contains the main loop of the virtual machine.  As you can\nsee, the implementation is quite straightforward; all the hint calls you\nsee are the special annotations needed by the JIT generator to produce better\ncode.\n\n\nLet's JIT it!\nSo, the whole point is to generate a JIT compiler from it, isn't it?\nFirst, checkout a fresh copy of the oo-jit branch:\n\n$ svn co https://codespeak.net/svn/pypy/branch/oo-jit\n\nThen, go to the oo-jit/pypy/jit/tl directory, and compile the tlc VM\nwith the CLI backend and JIT enabled:\n\n$ cd oo-jit/pypy/jit/tl/\n$ ../../translator/goal/translate.py -b cli --jit --batch targettlc\n...\nlot of texts\n...\n\nIf everything went OK, you now have a targettlc-cli executable, which\naccepts two arguments: the name of the file containing the tlc program we\nwant to run, and an integer to be passed to it.\nLuckily, in the same directory we have a factorial.tlc file that contains\nthe bytecode for a function that -- guess? -- computes the factorial of a\ngiven integer; let's try it:\n\n$ ./targettlc-cli factorial.tlc 5\nNon jitted:    120 (0.009371 seconds)\nWarmup jitted: 120 (0.208954 seconds)\nWarmed jitted: 120 (0.000323999999999991 seconds)\n\nCool, it seems that the result was computed correcly :-). As you can see from\nthe output, we ran the program three times:\n\n\nby plain interpretation, without any jitting;\nwith the jit enabled: this run includes the time spent by doing the\ncompilation itself, plus the time spent by running the produced code;\nagain with the jit enabled, but this time the compilation has already\nbeen done, so we are actually measuring how good is the code we produced.\n\n\nSo, it's time to run a benchmark: let's try to compute the factorial of a very\nbig number; the result will be 0, because obviously after a while we overflow,\nbut after all we are interested in the time spent, not in the result:\n\n$ ./targettlc-cli factorial.tlc 5000000\nNon jitted:    0 (19.93247 seconds)\nWarmup jitted: 0 (0.293229999999998 seconds)\nWarmed jitted: 0 (0.0494239999999984 seconds)\n\n$ python -c 'print 19.93247/0.0494239999999984'\n403.295362577\n\nAnd no, I didn't make any mistake in copying&pasting: the jitted version is\nreally 400 times faster that the non jitted one!\nWarning: my laptop seems to be not very well suited for benchmarks, as the\nresults vary a lot from run to run; I've run the benchmarks a lot of times,\nand I got speedup factors up to 500 times, so your results may be different.\n\n\nMore benchmarks\nIt's also interesting to compare the result with a manual written C#\nversion of the factorial, to see how good is code we produced; to get\nreasonable results, we need to compute a larger factorial, to let to code to\nrun a bit more:\n\n$ ./targettlc-cli --onlyjit factorial.tlc 100000000\nWarmup jitted: 0 (0.980856 seconds)\nWarmed jitted: 0 (0.769716 seconds)\n\n$ mono factorial.exe 100000000\nC#:            0 (0.153777 seconds)\n\n$ python -c 'print 0.769716/0.153777'\n5.00540392907\n\nWe know that the generated code is far from being optimal, but probably the\nfactor of five is at least partially due to the fact that Mono's own JIT is optimized for\nC#-like code, and our code has a completely different shape.\nAll the benchmarks above were run under Linux, with Mono 1.9.1.  Here are the\nresults for the same benchmarks, but run with Microsoft CLR (on a different\nmachine, so the absolute values are not comparable):\n\n$ ./targettlc-cli factorial.tlc 5000000\nNon jitted:    0 (15,640625 seconds)\nWarmup jitted: 0 (0,4375 seconds)\nWarmed jitted: 0 (0,03125 seconds)\n\n$ python -c 'print 15.640625/0.03125'\n500.5\n\n$ ./targettlc-cli --onlyjit factorial.tlc 100000000\nWarmup jitted: 0 (0,90625 seconds)\nWarmed jitted: 0 (0,515625 seconds)\n\n$ ./factorial.exe 100000000\nC#:            0 (0,34375 seconds)\n\n$ python -c 'print 0.515625/0.34375'\n1.5\n\nThe results are even better than before; this is probably thanks to CLR's JIT,\nthat does a better job than Mono when faced to something which is different\nthan the usual C#-like code.\n\n\nConclusions (for now)\nThis is a very important result, because it proves that PyPy's approach to JIT\ncompilers can be applied effectively also to OO virtual machines; the result\nis even better than what I expected, because when generating code for .NET we\nhave much less freedom than when generating assembly code, and I had to play\nsome tricks to work around some .NET limitations.\nMoreover, it worked at the first try :-). I tried to compile the tlc\nvirtual machine as soon as all the related JIT tests were passing, and\nsurprisingly everything worked just fine, even if it was the very first time I\nwas trying to apply some features of the JIT to something bigger than a test:\nI think this is yet another prove that Test Driven Development just works!\nEven if this is a major milestone, the CLI JIT backend is not yet completed:\nas a consequence it can't still be used for the full PyPy, but all the\nhardest problems should have been solved now.\nSince a lot of readers asked for more technical details, especially about the\nJIT, I will try to soon write a second blog post explaining how the CLI backend works\ninternally, with a brief look to the generated code to see how it looks like.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/11/porting-jit-to-cli-part-1-8712941279840156635.html"
    },
    {
      "title": "One year PyPy Blog",
      "text": "Last Friday the PyPy Status Blog had its first anniversary. Yay! After not\nreally buying into any of this new-fangled \"blog\" stuff for a long time we just\nbit the bullet and got started. Totally surprisingly it even worked. We posted\n76 post in the last year, more than one per week. By now we have more than 800\nsubscribers (according to feedburner), which is quite cool for a rather niche\nblog.\nTo make our blog even more interesting, I would like to ask for some feedback\nvia the comments:\n\n\nWhich posts did you like in particular?\nWhat sort of posts would you be interested in getting more of?\nAny other improvements we could make?",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/11/one-year-pypy-blog-3267056180369310162.html"
    },
    {
      "title": "Sprint Discussions: JIT Generator Planning",
      "text": "Background\nFinally, the JIT post :-). First some background: Despite our plans at the end\nof the EU period, PyPy's Python interpreter didn't get a good and widely\napplicable JIT in the last year. The reason for that was that we discovered that\nalthough the basic idea to generate JIT compilers is good, the concrete\nprototype made during the EU period is basically flawed. It could have\nbeen pushed a bit farther, but would have run into deep troubles eventually. One\nof the problems would have been performance instability: change a seemingly\nunrelated bit in your source program, and the performance changes in unexpected\nways, which is clearly not desirable. Another problem with that old approach is\nthat too much assembler code is generated, leading to memory problems, and also\nthat the generated assembler is bad in various ways, e.g. it is hard in that\napproach to do proper register allocation.\nTherefore we decided that it would be worthless to pursue this direction much\nfurther. Instead we tried to research approaches to fixing the inherent\nproblems. This research was largely done in Prolog and I eventually wrote my\nMaster's thesis about it. From the Prolog work we got some good insights into\nwhat needs to be done and what sorts of techniques are needed. Also, it inspired\nArmin to do some more exploration on a small Python prototype which used the\nlessons learned from Prolog and also some additional ideas from tracing JITs. So\nfar, however, the prototype is neither in RPython, nor much integrated with\nPyPy.\nThis research is not the only thing happening in the JIT-area. During the last\nyear, Antonio Cuni was working on bringing the JIT to pypy-cli. This\nconsisted mostly of writing a .NET backend for the old JIT-generator. Some\nfurther work is being done since August by John Witulski, who is writing an\nAMD64 backend for the JIT-generator for his Bachelor's thesis.\n\nWhere to go from thereDuring the sprint we discussed in which directions we should continue now. We\nplan to work quite a bit on the JIT in the coming months. Both Armin and Anto\nare in D\u00fcsseldorf for four months, and them and me plan to mostly work on the\nJIT (as well as giving a lecture on \"Dynamic Programming Languages\", trying to\nensnare some more students).\nThe first step will be to experiment a bit more with Armin's prototype. So far\nit looks rather promising, but there are some unsolved issues that we need to\nlook into first. The first issue is to think a bit about how to efficiently do\nprofiling to compile only important code paths. The other large issue are\nso-called \"virtualizables\". Roughly speaking, they are the frame objects of the\ninterpreter from which the JIT is generated. They need special treatment,\nbecause on the one hand it is important that they get optimized away to make the\ncode fast, since the frames are accessed all the time for the local variables;\non the other hand they should still be usable for introspection if code is\naround that is trying to look into them.\nWhen this is done, the prototype needs to be ported to RPython, which is a\nnon-trivial task, since it is rather dynamic so far (it is rather important that\nthe unresolved issues are done before the porting, because once the prototype is\nin RPython, experimentation will be harder). The porting has the potential to be\ntedious, but in a sense it is \"just work\", as opposed to unclear research.\nAt this point it will become important to think about the backend interface. The\ninterface that the old frontend used to produce assembler code won't be usable\nfor the new approach, so things need to be rearranged slightly. Afterwards the\nbackends will have more information and be invoked at a slightly higher level,\nwhich should allow them to produce better code.\nWhen all this is done, the JIT generator will be in a rather good state and it\nshould become possible (modulo a lot of details, of course), to use it on the\nPython interpreter.\nConclusion\nI am intentionally not attaching any time estimates to this blog post. So far\nour time estimates have not been very accurate when it comes to the JIT, which\nonly lead to disappointment when the JIT failed to materialize. We hope that we\nwill progress in interesting ways in the next four months, but who knows. Note\nthat we are really quite disappointed ourselves that it took so much longer than\nwe planned and hoped. The reason for this is mostly that this work really is\nresearch and sometimes it is just hard to predict what sort of problems turn up.\nPartial evaluation (the basis for our JIT generator) is a 30 years old technique\nthat was always just promising and never really successful, so the fact that we\nthink we can solve its problems in a few years is very much hubris anyway :-).\nOn the positive side, we think that we now know these problems much better than\never before and that we have a plan that has a chance to succeed.\nAlso we are still convinced that our approach has huge potential, despite the\ndifficulties. If we manage to pull it off, it should be significantly simpler to\nsupport new language features in the JIT and also to get speedups on some rather\ninteresting bits of the language. Some ideas we are having include generating a\nJIT for the regex engine or speed up ctypes-bindings to be nearly as fast as an\nextension module (or faster?). Also the JIT will be such that by construction\nthe JIT-generated code behaves identical to the original code, which isn't\nalways true for Psyco, for example.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/10/sprint-discussions-jit-generator-3301578822967655604.html"
    },
    {
      "title": "Sprint Discussions: C++ Library Bindings",
      "text": "At the beginning of this year, PyPy grew ctypes support, thanks to generous\nsupport by Google. This made it possible to interface with C libraries from\nour Python interpreter, something that was possible but rather tedious before.\nWhat we are lacking so far is a way to interface to large C++ libraries (like\nGUI libraries). During the sprint we had a brainstorming session about possible\napproaches for fixing this shortcoming.For CPython there are a number of approaches in common use:\n\n\nSIP, mainly used for PyQT\nSWIG\nBoost.Python\n\n\nThose all have the property that they produce some code that is then compiled\nwith a compiler to produce a CPython extension. The produced code also uses\nfunctions from CPython's C-API. This model is not simple to use for PyPy in its\ncurrent state. Since PyPy generates C code automatically, a fixed C-level API\ndoes not exist (it is not unlikely that at one point in the future we might have\nto provide one, but not yet). At the moment, PyPy very much has a \"Don't call\nus, we call you\"-approach.\nA very different approach is followed by the Reflex package, which is\ndeveloped at CERN (which has an incredible amount of C++ libraries). It is not\nmainly intended for writing Python bindings for C++ libraries but instead\nprovides reflection capabilities for C++. The idea is that for every C++ shared\nlibrary, an additional shared library is produced, which allows together with\nReflex to introspect properties of C++ classes, methods, etc. at runtime. These\nfacilities are then used for writing a small generic CPython extension module,\nthat allows CPython to use any C++ library for which this reflection information\nwas generated.\nThis approach is a bit similar to the ctypes module, apart from the fact\nthat ctypes does not use any reflection information, but the user has to\nspecify the data structures that occur in the C code herself. This makes it\nsometimes rather burdensome to write cross-platform library bindings.\nFor PyPy the approach seems rather fitting: We would need to implement only the\ngeneric extension module and could then use any number of C++ libraries. Of\ncourse some more evaluation is needed (e.g. to find out whether there are any\nrestrictions for the C++ code that the library can use and how bothersome it is\nto get this reflection information for a large library) but so far it seems\npromising.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/10/sprint-discussions-c-library-bindings-249141169883996521.html"
    },
    {
      "title": "Sprint Discussions: Release Planning",
      "text": "One of the discussions that happened during the sprint was about how to approach\nthe next PyPy release. There hasn't been a release since the end of the EU\nperiod, which is not an optimal situation. Therefore we plan to make a 1.1\nrelease at the beginning of next year, ideally before Pycon US. We'd also like\nto move towards time-based releases. This will be greatly helped by the\nnew buildbot infrastructure, which allows us to decide when the\nstate of the codebase is stable enough to release.\nAnother goal of the release is to involve more people from the wider PyPy\ncommunity by having bugdays and generally asking for more support. This will be\nparticularly useful for bugs on platforms that no one of the core developers\ngroup is using.\nFeature-wise the release will mostly contain CPython 2.5 language support,\nincluding some new extension modules (like ctypes, expat, sqlite).\nIn addition we plan to make it easier to actually install and use the PyPy\nPython interpreter, which means some sort of proper installation procedure and\nsupporting distutils on top of PyPy. Another part of the release will be\nsupport for fully sand-boxing an interpreter.\nAdditionally there were also a large number of improvements on several levels\nsince the last release, like optimizations, faster oldstyle-classes, better\nGCs, correct finalization behaviour, lots and lots of bugfixes, better\nthreading support (still with the GIL), some work on improving memory\nbehaviour,  ...\nIn contrast to our last release, we will focus mainly on PyPy's Python\nIntepreter and more particularly its C-version.   There are also various\nexperimental interpreters that PyPy contains, like for Prolog, Smalltalk,\nJavaScript and Scheme. We also don't intend to put the LLVM and Javascipt\nbackends in the release, since they are essentially unmaintained and at least\npartially broken.  If anybody is particularly interested in one of these\ncomponents, please feel free to step up and take responsibility for them.\nAnother thing that the release won't contain is a JIT.  I plan to make another\nblog-post about this soon, stay tuned.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2008/10/sprint-discussions-release-planning-7097053444808236145.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report Days 1-3",
      "text": "The D\u00fcsseldorf sprint is currently in full progress and this post will try to\nsummarize what progress has been made in the last days. We are (again) sprinting\nat the STUPS group of the D\u00fcsseldorf University. You can find the sprint\nannouncement and the daily planning file.\nHolger and Samuele put quite some effort over several days into setting up and\nimproving PyPy's testing infrastructure. PyPy has a variety of tests. On the one\nhand, there are of course our own tests. But then we also have the CPython tests\nthat should be run on top of pypy-c. Up to now we used a custom-made pile of\nhacks, held together by lots of duct-tape. It consisted of a variety of\ndifferent machines running different things with different reporting solutions.\nSome of the old test-results can still be found on wyvern. Now we are moving\nto a buildbot based solution together with a custom reporter to have a view\nsimilar to the old one. Some details are not quite finished yet, but most of the\nthings are already working rather well (currently all the results displayed\nare from the 2.5-merge branch).\nAnother large (and ongoing) topic of work is the 2.5 branch. It contains the\nwork done by our Summer-of-Code student, Bruno Gola, of adding CPython 2.5\nfeatures to PyPy's Python interpreter. While Bruno implemented most language\nfeatures and imported the 2.5 stdlib into PyPy, a lot of details were still\nmissing. In the last days nearly everybody worked on fixing small issues and\nfailing stdlib tests. While doing that we tried to categorize some CPython tests\nas implementation dependant so that we can skip them when running on PyPy.\n\nMemory Improvements\nOne goal of the sprint is to measure and to reduce the memory behaviour of our\nPython interpreter. The idea is to make pypy-c a realistic option for use on\nembedded devices. By memory behaviour we mean both the\ndynamic memory usage (how much bytes does a dict or an instance take) as well as\nthe size of the executable and details of the GC strategy.\nAlexander, Carl Friedrich and Antonio did some work on analyzing the static data\nthat a pypy-c executable contains. Our executables have the tendency to be\nrather large, both due to a lot of code and due to a large amount of static\ndata. The analysis didn't give any really surprising results, the problem is\nmostly that we have a lot of static data originating from a bit everywhere in\nour program. Two big offenders are the unicodedata-module with about 750 KB\nof static data and the multimethod-tables with about 150 KB of data.\nArmin, Iko, Anto and Maciek worked on a new approach to malloc-removal. This is\n(for PyPy) a crucial optimization of the translation toolchain that performs\nescape analysis to find out which objects don't outlive the frame they were\nallocated in. Since RPython is garbage-collected we usually have a lot of\nallocations, so it is important to statically get rid of many of them. To\nsuccessfully do that, some inlining is needed to give the analysis more context.\nThis leads to the fact that we have rather aggressive inlining-settings to allow\nas much malloc-removal as possible. The new approach tries to inline functions\nonly if this actually leads to the successful removal of a malloc operation. The\ncode is not finished quite yet, so it remains to be seen how successful it will\nbe.\nBefore the sprint Maciek had started to work on a mark-compact GC for PyPy. The\nidea is that it is better for memory-constrained-environments because it does\nnot double the memory-requirements during collections. During the sprint Armin\nand Maciek worked on cleaning up the code a bit and then merging the branch.\nAn interesting property of the mark-compact GC is that after a collection all\nthe memory that is not currently used by the program is returned to the\noperating system. Right now the GC is not as fast as our more mature ones, but\nit probably will be the basis for future tweaking.\nA small thing that was done by Alexander and Carl Friedrich to make objects smaller is\nto enable shared instance dictionaries also for instances of old-style\nclasses. Before it worked only for instances of new-style classes. Shared\ninstance dictionaries are a way to reduce the memory-usage of instances. In the\noptimal case, it gives the same memory-savings that __slots__ are giving,\nbut without any behavioural changes. Conceptually it is very similar e.g. to\nthe notion of \"map\" in the Self project, or the hidden classes that Google Chrome's V8\nis using (click on the link, there are nice graphics). The\ndifference is that for them it is mostly a way to get faster attribute access,\nand PyPy is so far only using it form memory savings (but that might change in\nthe future).\nIn parallel to all the other work, John Witulski worked tirelessly on advancing\nthe AMD64-JIT-backend. John has the implementation of this backend as the topic\nof his Bachelor's thesis. He is progressing quite well (especially also\nconsidering that this is his first sizeable Python project ever), just sometimes\nbeing impaired by such annoyances as errors in the official Intel documentation.\nBy now the backend is supporting many integer operations and control flow.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/10/dsseldorf-sprint-report-days-1-3-5256639868851086032.html"
    },
    {
      "title": "Prolog-JIT Master's-Thesis Finished",
      "text": "As we already blogged, in the last half-year or so, Michael Leuschel, Armin\nand me did a lot of JIT generator work on a Prolog prototype. The idea was to\nexperiment more quickly with some techniques than what would have been possible\nwith RPython. These experiments were quite successful in themselves. With very\nlittle code we managed to get a JIT that is not doing too badly when compared to\nexisting projects for Prolog.\nThis Prolog work was also the subject of my Master's thesis. I finished the\nthesis about two weeks ago (and since then have been mostly sleeping and then\nsprinting). The thesis should be self-contained when it comes to explaining the\nJIT concepts but needs knowledge of Prolog to be understandable.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2008/10/prolog-jit-masters-thesis-finished-5462132148241449867.html"
    },
    {
      "title": "PyPy/Python at the Maemo summit",
      "text": "Maciej and me visited the Maemo Summit in Berlin -\na community meetup around Nokia's Linux based\nmobile platform.  We spontaneously did a lightning\ntalk about a first running pypy-c on Maemo\nand got nice feedback.  \n\nWe also had a nice lunch with guys from the INDT in Brazil, including Marcio Marcedo and Marcelo Eduardo.  It turns out that Python is used a lot on Maemo, for example the nice Canola UI is done with it.  Will be interesting to see how this shapes up in relation to the iPhone and Android.\n\nA lot of Nokia engineers were around and they announced that from October on they are going for weekly new releases of their SDK for the new Fremantle (Maemo-5) debian-based platform until the SDK becomes final - if we got this right.  \n\nFunnily enough, we met Marius Gedminas from the Programmers of Vilnius - he gave a lightning talk on his impressions as a community member.  We think python programmers really should go much more to non-Python centric conferences.\n\nThe whole event took place at the C-Base - was a bit\ncrammed in some of the sessions with something like 200 people attending.\n\ncheers, Maciej and Holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/09/pypypython-at-maemo-summit-6115106472056714072.html"
    },
    {
      "title": "Pycon UK, Javascript and the GIL",
      "text": "Just got back from Pycon UK 2008 - here are some impressions. \nBoth the keynote speakers Mark Shuttleworth (Canonical) and \nTed Leung (Sun Microsystems) expressed their concerns about\nJavascript becoming so fast and prominent that it could displace\nPython in the future.  They also highlighted the fact that\nMulti-core systems get cheaper and more popular also on \ndesktop computers or notebooks.  They challenged the community\nto advance Python implementations to exploit it.  Question was up \nwhat PyPy can do here.  As it stands, PyPy still uses the good old\nGlobal Interpreter Lock (GIL) but our approaches should indeed \nlend itself well to do experimentation with free threading.  \n\nDuring the 2-day conference we met many interesting people, most \nnotably the guys from Resolver, among them William Reade who is working on\nIronClad -- which implements a fake python25.dll on top of\nIronPython.  He presented some good results for Numpy in his\nlightning talk.  This approach is surely something to follow\nclosely and potentially use for PyPy. \n\nWe also had lunch and a couple of chats with Jacob Kaplan-Moss from\nDjango fame - he is apparently up to try use PyPy's sandboxing features\nfor one of his projects, cool!\n\nConference itself was well organized for the 230 attending people - although\nthe venue might be a bit small for next year's EuroPython.  Ah, and\nwe gave three well attended talks, find the slides here:\n\n\nPyPy status and 1.1 plans\nPyPy JIT\npy.test tutorial\n\ncheers,\nHolger, Maciej, Anto (associated through merlinux, btw)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/09/pycon-uk-javascript-and-gil-8387247619202094916.html"
    },
    {
      "title": "D\u00fcsseldorf PyPy sprint 5-13th October, 2008",
      "text": "The PyPy team is happy to announce the next sprint, which will take place in\nthe Computer Science Department of the University of D\u00fcsseldorf, Germany.\nSprinting will start on the 6th of October and go on till the 12th. Please\narrive on the day before if you want to come.\nTopics of the sprint will be aiming at a 1.1 release and to work on integrating PyPy better \nwith small devices. Other topics are also welcome!\nWe will try to find a hotel with group rates, so if you are interested, please\nsign up soon! See the announcement for more details.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/09/dsseldorf-pypy-sprint-5-13th-october-8919978872121664955.html"
    },
    {
      "title": "pylib/py.test 0.9.2 released",
      "text": "PyPy and its 14638 automated tests use the py.test tool which is also used by many other projects.  PyPy developers have actually driven and contributed a lot to its development.  \n\nI just released version 0.9.2 of the py lib mainly fixing Windows issues and providing better packaging and integration with setuptools.  It's usable completely independently from PyPy - \"easy_install py\" gives you the py.test command line. Of course you can run py.test on top of a translated PyPy version as well. Here is a quick summary of what the py lib provides besides py.test:\n\npy.execnet: ad-hoc code distribution to SSH, Socket and local sub processes\npy.magic.greenlet: micro-threads on standard CPython (\"stackless-light\") and PyPy\npy.path: path abstractions over local and subversion files\npy.code: dynamic code compile and traceback printing support\ntested against Linux, Win32, OSX, works on python 2.3-2.6\n\nGood general entry points for installation and documentation:\n\nPypi pages\nDownload/Install\nDocumentation/API\n\nhave fun, holger krekel",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2008/08/pylibpytest-092-released-6233865913406513469.html"
    },
    {
      "title": "New translation option: --opt",
      "text": "Hi all,\n\nA few command-line options for translate.py have changed.\nMost interesting is that optimization levels are selected with\nthe option --opt, or -O for short.  This replaces --allopts,\nwhich was also called --faassen in reference to a person who\nis actually not involved in PyPy (so that was a bit of a\nstrange joke).  Also, --allworkingmodules is the default\nnowadays, and can be cancelled with --no-allworkingmodules.\nThreads are also included in --allworkingmodules now.\n\nExamples:\n\ntranslate.py (reasonable default, corresponds to --opt=2)\n    translate.py --opt=3 (best, maybe 10-20% faster)\n    translate.py --opt=1 (translation is faster and less RAM-hungry)\n\n\nFor more information, see:\n    \nGetting started\n    List of optimization levels",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/08/new-translation-option-opt-7737733390438084418.html"
    },
    {
      "title": "PyPy runs unmodified django 1.0 beta",
      "text": "This is just a quick update post to previous post - django folks commited all\noutstanding tickets and we are able to run unmodified django\non top of pypy-c. Instructions how to do it are well explained\non django wiki entry\n\nenjoy,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/08/pypy-runs-unmodified-django-10-beta-7105507436425430319.html"
    },
    {
      "title": "Europython 2008 PyPy talks and sprint sum up",
      "text": "The EuroPython 2008 conference and sprints have finished - it certainly was \na very eventful and successful conference for PyPy.  And many very interesting \nnon-PyPy talks as well.  PyPy presentations are available online: PyPy status talk\nPyPy for the rest of us, PyPy behind the scenes.  Armin and Maciej also did a well-attended \ntalk about PyPy's garbage collection, but that was quite interactive, no slides. \n\nThe talks were all well visited and we got good questions.  However, we still \nneed to work on sorting out the \"PyPy technology cloud\" and how to present\nit to different audiences.  Anyway, we are happy to hear feedback or questions\nabout the talks!\n\nAfter the conference there was a three-day PyPy sprint. Despite \nthe fact that most PyPy core developers were zombies, \nwe made good progress.  Particularly our newcomers did very well.  \nHere are some results: \n\n itertools rewritten in RPython for performance by Jakub\n  Gustak and Andrew Durdin \n\n a new ctypes based dbm and hashlib module, both by Gasper Zejn \n  with support from Henrik Vendelbo, they also got ctypes to nicely work on OSX. (sorry for lack of proper letters in names :)\n\n implement builtin function call profiling by Stephan Diehl, Antonio and Armin. \n\n running\n  Pinax on top of pypy-c, by Henrik, Holger, Gasper. \n\n Jim Baker started a _rawffi.py for Jython using JNA aiming\n  to provide support to run PyPy's ctypes on top of Jython. \n  When Jython gets this to run, PyPy's JVM backend should be \n  able to use it. Talk about Code Reuse :) \n\n oldstyle classes are now the default, this makes \n  PyPy mimick very closely cpython's 2.5 object model. \n\n Andrew started a port of the Malbolge \n  interpreter written in Python to RPython (obviously the only missing \n  link for PyPy to take over the world). \n\n various cleanups (a new option \"--lonepycfiles\" helps with\n  saner imports, remove int-float comparison shortcuts, ...) \n\nAt the end of the sprint we also discussed initial plans for a 1.1 release which we'd like to make happen this year.   So we are generally looking forward to a busy rest of 2008 and luckily this starts by many of us taking a good vacation first :) \n\nCheers,\nfijal & holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/europython-2008-pypy-talks-and-sprint-2255727845041197411.html"
    },
    {
      "title": "Finding Bugs in PyPy with a Fuzzer",
      "text": "Last week I played a bit with Fusil, which is a fuzzing framework.  The idea is\nto feed the interpreter code that calls the functions of a module with random values\nof various types as arguments in the hope that one hits an unchecked case. This is\ndone until a problem is hit , the most common problem being a segfault.  Victor Stinner,\nthe author of Fusil, is a regular in the PyPy IRC channel and thankfully helped me\ngetting started with Fusil. I used his project description for CPython as a starting\npoint and tweaked it a bit.  Reason is that PyPy is harder to segfault and so\nI tweaked Fusil to also count uncaught RPython-level exceptions as such a problem.\n(RPython has full exception support, and if an RPython-exception escapes to the top\nlevel, the Python interpreter aborts.  One should not be able to exploit this but\nbut for a user it is bad enough, because such exceptions cannot be caught from\nPython code.)\nUsing Fusil I found a number of cases where such exceptions happened (in some\npickle support-code, in the expat parser, in the os and in the termios\nmodule) and also one or two segfaults (in the parser module, of all places).\nI fixed all these problems so that by\nnow the fuzzer just runs for a very long time and only finds things that take\ntoo long (so they count as a way to do a DoS attack) like\npow(12355123123L, 12351512123121L) or round(1, 1000000000) (the latter\nshould probably be fixed). This probably just means that the fuzzer is not good\nenough, because there are certainly segfaults left in PyPy. However, the fact\nthat it is rather hard to find them validates our approach of using a\nhigh-level memory-managed language for our interpreter. Victor tells me that it\nis rather easy to find segfaults in CPython this way, he already found quite\nsome problems.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/finding-bugs-in-pypy-with-fuz-7503072572107631526.html"
    },
    {
      "title": "PyPy's Python runs Pinax / Django",
      "text": "During the EP2008 sprint we got Pinax running on top of PyPy. At our play1 server we have it running on top of pypy-c.  Not that you'll notice many differences to the original site but that's the point, isn't it? ... Well, in fact i am too lazy to customize our play1 version now - i rather spent a nice evening with the other sprint guys :) \n\nPinax integrates numerous reusable Django apps to take care of the things that many sites have in common. Many thanks particularly to Henrik Vendelbo who sorted out various Pinax and PyPy issues, and wrote up a nice DjangoAndPyPy wiki page describing the installation process.\n\ngreetings from Vilnius (Lithunia), Holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/pypys-python-runs-pinax-django-1265543049596913506.html"
    },
    {
      "title": "EP2008: PyPy meets Jython",
      "text": "One of the great events at EuroPython 2008 were our chats and meetings with the Jython and Sun people.  The Jython people recently are pushing into releasing Python version 2.5 and they currently pursue many interesting sub projects.  Coincidentally, PyPy also has tons of interesting areas and results :)  So we eventually got into brainstorming a number of possible technical collab ideas.  Further below is a first list as i wrote it down from our 10 people PyPy / Jython 30 minute close up meeting yesterday.\n\nIt felt great to be able to talk to the Jython people this way - kudos to Sun for their clear commitments and open ways to go about things!  I sense a genuine interest on fair collaboration with non-java developer communities.  Seems like they are serious about not focusing on \"Java this\", \"Java that\" anymore  but rather focus on the JVM platform.  Good!  And about language\nindependent interest in ambitious technology. Even Better!  I am tensed to see how things go from here.\n\nSo here the list of technical collab ideas:\nctypes - try to create _rawffi module in Java for Jython, which will enable Jython to reuse our existing ctypes implementation (and have PyPy use the Jython-rawffi for its own for PyPy.JVM) generally see to share work / (continue) collaborate regarding extension modulesJython/PyPy (and eventually IronPython): document known differences to CPython, maybe in a PEPPython Interpreter for Jython (in order to run CPython's .pyc files): re-use pypy's bytecode evaluator, implement a \"Jython object space\". re-use rpython-extension modules for jython (e.g. SRE), by compiling them to Java and reusing as a native library.collaborate on testing framework / benchmarking, have a common site to show test resultsmake py.test compatible with jythoncome up with a set of \"pure Python language\" tests, which would gather and refactor tests from CPython, PyPy and Jython. look into using java types / jython approaches for implementing free threading.share knowledge regarding JIT / psyco\nIf you have any more ideas, comments or would like to join efforts, let us know!\n\nCheers and thanks to Ted Leung, Frank Wierzbiki, Jim Baker and Tobias Ivarsson from Sun and Jython fame respectively,\n\nHolger",
      "tags": "ep2008,jython,pypy,sun",
      "url": "https://www.pypy.org/posts/2008/07/ep2008-pypy-meets-jython-1107070144380217881.html"
    },
    {
      "title": "PyPy at the EuroPython 2008",
      "text": "Greetings from Vilnius, Lithuania. There were already\ntwo pypy talks, one performed by Jacob Hallen\nPyPy for the rest of us and second\nby Maciej Fijalkowski PyPy status talk. The thing that\nwe forgotten to tell is that PyPy sandboxing feature\ncan also easily limit CPU and RAM usage as well as\nany other possible resource (like network transfer).\nFor anyone who would like to join, there is a PyPy\nsprint after the conference.\n\nCheers,\narigo & fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/pypy-at-europython-2008-1488914968455397674.html"
    },
    {
      "title": "JIT in Prolog",
      "text": "Hi all,\n\nSome news from the JIT front.  Progress on the JIT has been low-profile\nin the past few months.  No big results to announce yet, but we have\nplayed with some new ideas, and they are now documented as a draft\nresearch paper: Towards Just-In-Time Compilation and Specialisation of Prolog.\n\nProlog?  Yes.  To understand this slightly unusual choice of programming\nlanguage, here is first some background about our JIT.\n\nPyPy contains not a JIT but a JIT generator, which means that we\nonly write an interpreter for a language (say, the complete Python\nlanguage), and we get a JIT \"for free\".  More precisely, it's not for\nfree: we had to write the JIT generator, of course, as well as some\namount of subtle generic support code.  The JIT generator preprocesses\nthe (complete Python) interpreter that we wrote and links the result\nwith the generic support code; the result is a (complete Python) JIT.\n\nThe way that this works so far gives us a generated JIT that is very\nsimilar to Psyco in the way\nit works.\nBut Psyco has issues (and so the current PyPy JITs have the same issues):\nit can sometimes produce too much machine code,\ne.g. by failing to notice that two versions of the machine code are\nclose enough that they should really be one; and it can also sometimes\nfail in the opposite way, by making a single sub-efficient version of\nthe machine code instead of several efficient specialized versions.\n\nA few months ago we have chosen to experiment with improving this\ninstead of finishing and polishing what we had so far.  The choice was\nmostly because we were (and still are) busy finishing and polishing\neverything else in PyPy, so it was more fun to keep at least the JIT on\nthe experimental side.  Besides, PyPy is now getting to a rather good\nand complete state, and it is quite usable without the JIT already.\n\nAnyway, enough excuses.  Why is this about Prolog?\n\nIn PyPy, both the (complete Python) interpreter and the JIT support code\nare in RPython.  Now RPython is not\nan extremely complicated language, but still, it is far from the top on a\nminimalism scale.  In general, this is a good in practice (or at least I\nthink so): it gives\na reasonable balance because it is convenient to write interpreters\nin RPython, while not being so bloated that it makes our translation\ntoolchain horribly complicated (e.g. writing garbage collectors for\nRPython - or even JIT generators - is reasonable).  Still, it is not the\nbest choice for early research-level experimentation.\n\nSo what we did instead recently is hand-write, in Prolog, a JIT that\nlooks similar to what we would like to achieve for RPython with our JIT\ngenerator.  This gave much quicker turnaround times than we were used to\nwhen we played around directly with RPython.  We wrote tiny example\ninterpreters in Prolog (of course not a complete Python interpreter).\nSelf-inspection is trivial in Prolog, and generating Prolog code at\nruntime is very easy too.  Moreover, many other issues are also easier\nin Prolog: for example, all data structures are immutable \"terms\".\nOther languages than Prolog would have worked, too, but it happens to be\none that we (Carl Friderich, Michael Leuschel and myself) are familiar\nwith -- not to mention that it's basically a nice small dynamic\nlanguage.\n\nOf course, all this is closely related to what we want to do in PyPy.\nThe fundamental issues are the same.  Indeed, in PyPy, the major goals\nof the JIT are to remove, first, the overhead of allocating objects all\nthe time (e.g. integers), and second, the overhead of dynamic dispatch\n(e.g. finding out that it's integers we are adding).  The equivalent\ngoals in Prolog are, first, to avoid creating short-lived terms, and\nsecond, to remove the overhead of dispatch (typically, the dispatching\nto multiple clauses).  If you are familiar with Prolog you can find more\ndetails about this in the paper. So far we already played with many possible solutions\nin the Prolog JIT, and the paper describes the most mature one; we have\nmore experimentation in mind.  The main point here is that these are\nmostly language-independent techniques (anything that works both in\nProlog and in RPython has to be language-independent, right? :-)\n\nIn summary, besides the nice goal of speeding up Prolog, we are trying\nto focus our Prolog JIT on the issues and goals that have equivalents in\nthe PyPy JIT generator.  So in the end we are pretty convinced that it\nwill give us something that we can backport to PyPy -- good ideas about\nwhat works and what doesn't, as well as some concrete algorithms.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2008/06/hi-all-some-news-from-jit-front-7534695765973581706.html"
    },
    {
      "title": "PyPy code swarm",
      "text": "Following the great success of code_swarm, I recently produced a\nvideo that shows the commit history of the PyPy project.\nThe video shows the commits under the dist/ and branch/\ndirectories, which is where most of the development happens.\nIn the first part of the video, you can see clearly our sprint based\napproach: the video starts in February 2003, when the first PyPy\nsprint took place in Hildesheim: after a lot of initial activity, few\ncommits happened in the next two months, until the second PyPy sprint,\nwhich took place in Gothenburg in late May 2003; around the minute\n0:15, you can see the high commit rate due to the sprint.\nThe next two years follow more or less the same pattern: very high\nactivity during sprints, followed by long pauses between them; the\nmost interesting breaking point is located around the minute 01:55;\nit's January 2005, and when the EU project starts, the number of\ncommits just explodes, as well as the number of people involved.\nI also particularly appreciated minute 03:08 aka March 22, 2006: it's\nthe date of my first commit to dist/, and my nickname magically\nappears; but of course I'm biased :-).\nThe soundtrack is NIN - Ghosts IV - 34: thanks to xoraxax for\nhaving added the music and uploaded the video.\n                  PyPy Codeswarm from solse@trashymail.com on Vimeo.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/pypy-code-swarm-7038411918926116477.html"
    },
    {
      "title": "Funding of some recent progress by Google's Open Source  Programs",
      "text": "As readers of this blog already know, PyPy development has\nrecently focused on getting the code base to a more usable state. One\nof the most important parts of this work was creating an\nimplementation of the ctypes module for PyPy, which\nprovides a realistic way to interface with external libraries. The\nmodule is now fairly complete (if somewhat slow), and has generated a\ngreat deal of community interest.  One of the main reasons this work\nprogressed so well was that we received funding from Google's Open\nSource Programs Office.  This is\nreally fantastic for us, and we cannot thank Google and Guido enough for helping PyPy progress\nmore rapidly than we could have with volunteer-only time!\nThis funding opportunity arose from the PyPy US road trip at the end\nof last year, which included a visit to Google. You\ncan check out the video\nof the talk we gave during our visit.  We wrapped up our day with\ndiscussions about the possibility of Google funding some PyPy work and\nsoon after a we were at work on the proposal for improvements we'd\nsubmitted.\nOne nice side-effect of the funding is indeed that we can use some of\nthe money for funding travels of contributors to our sprint meetings.\nThe next scheduled Google funding proposal also aims at making our\nPython interpreter more usable and compliant with CPython. This will be done by trying to\nfully run Django on top of PyPy.  With\nmore efforts like this one we're hoping that PyPy can start to be used\nas a CPython replacement before the end of 2008.\nMany thanks to the teams at merlinux and Open End for making this development possible, including\nCarl Friedrich Bolz, Antonio Cuni, Holger Krekel, Maciek Fijalkowski\nat merlinux, Samuele Pedroni and yours truly at Open End.\nWe always love to hear feedback from the community, and you can get\nthe latest word on our development and let us know your thoughts here in the comments.\nBea D\u00fcring, Open End AB\n\nPS: Thanks Carl Friedrich Bolz for drafting this post.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/pypy-improvements-5272963843122158791.html"
    },
    {
      "title": "Pdb++ and rlcompleter_ng",
      "text": "When hacking on PyPy, I spend a lot of time inside pdb; thus, I tried\nto create a more comfortable environment where I can pass my nights\n:-).\nAs a result, I wrote two modules:\n\n\npdb.py, which extends the default behaviour of pdb, by adding\nsome commands and some fancy features such as syntax highlight and\npowerful tab completion; pdb.py is meant to be placed somewhere in\nyour PYTHONPATH, in order to override the default version of pdb.py\nshipped with the stdlib;\nrlcompleter_ng.py, whose most important feature is the ability\nto show coloured completions depending on the type of the objects.\n\n\nTo find more informations about those modules and how to install them,\nhave a look at their docstrings.\nIt's important to underline that these modules are not PyPy specific,\nand they work perfectly also on top of CPython.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/pdb-and-rlcompleterng-2414105295687348881.html"
    },
    {
      "title": "Running Nevow on top of PyPy",
      "text": "Another episode of the \"Running Real Application of top of PyPy\" series:\n\nToday's topic: Divmod's Nevow. Nevow (pronounced as the French \"nouveau\", or \"noo-voh\") is a web application construction kit written in Python.  Which means it's just another web framework, but this time built on top of Twisted.\nWhile, due to some small problems we're not yet able to pass full Twisted test suite on top of pypy-c, Nevow seems to be simple enough to work perfectly (959 out of 960 unit tests passing, with the last one recognized as pointless and about to be deleted).  Also, thanks to\nexarkun, Nevow now no longer relies on ugly details like refcounting.\n\nAs usual, translate pypy using:\n\ntranslate.py --gc=hybrid --thread targetpypystandalone --faassen --allworkingmodules --oldstyle\n\nOf course, obligatory to the series, screenshot:\n\n\nThis is Nevow's own test suite.\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/running-nevow-on-top-of-pypy-58891137802412513.html"
    },
    {
      "title": "Next sprint: Vilnius/Post EuroPython, 10-12th of July",
      "text": "As happened in the last years, there will be a PyPy sprint just after\nEuroPython.  The sprint will take place in the same hotel as the\nconference, from 10th to 12th of July.\nThis is a fully public sprint: newcomers are welcome, and on the first\nday we will have a tutorial session for those new to PyPy development.\nSome of the topics we would like to work on:\n\n\ntry out Python programs and fix them or fix PyPy or fix performance bottlenecks\nsome JIT improvement work\nport the stackless transform to ootypesystem\n\n\nOf course, other topics are also welcome.\nFor more information, see the full announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/next-sprint-vilniuspost-europython-10-3844544842675903586.html"
    },
    {
      "title": "German Introductory Podcast About Python and PyPy",
      "text": "During the Berlin Sprint Holger was interviewed by Tim Pritlove for Tim's\nPodcast \"Chaosradio Express\". The whole thing is in German, so only\ninteresting to German-speakers. The PyPy episode can be found here. The\ninterview is touching on a lot of topics, starting with a fairly general intro\nabout what Python is and why it is interesting and then moving to explaining and\ndiscussing PyPy. The bit about PyPy starts after about 45 minutes. There is also\na comment page about the episode.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/german-introductory-podcast-about-3836017753197345761.html"
    },
    {
      "title": "Running Pylons on top of PyPy",
      "text": "The next episode of the \"Running Real Applications on Top of PyPy\" series: \nYesterday, we spend some time with Philip Jenvey on tweaking Pylons and PyPy to cooperate with each other. While doing this we found some pretty obscure details, but in general things went well.\n\nAfter resolving some issues, we can now run all (72) Pylons tests on\ntop of pypy-c compiled with the following command:\n\n\ntranslate.py --gc=hybrid --thread targetpypystandalone --faassen --allworkingmodules --oldstyle\n\nand run some example application. Here is the obligatory screenshot (of course\nit might be fake, as usual with screenshots). Note: I broke application on purpose to showcase cool debugger, default screen is just boring:\n\nPlease note that we run example application without DB access, since\nwe need some more work to get SQLAlchemy run on top of pypy-c together with\npysqlite-ctypes. Just one example of an obscure details that sqlalchemy is\nrelying on in the test suite:\n\n\n  class A(object):\n  \u00a0\u00a0locals()[42] = 98\n\n\nUpdate:This is only about new-style classes.\n\nThis works on CPython and doesn't on PyPy.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/running-pylons-on-top-of-pypy-3234492105090025733.html"
    },
    {
      "title": "List comprehension implementation details",
      "text": "List comprehensions are a nice feature in Python. They are, however, just\nsyntactic sugar for for loops. E.g. the following list comprehension:\n\ndef f(l):\n    return [i ** 2 for i in l if i % 3 == 0]\n\nis sugar for the following for loop:\n\ndef f(l):\n    result = []\n    for i in l:\n        if i % 3 == 0:\n            result.append(i ** 2)\n    return result\n\nThe interesting bit about this is that list comprehensions are actually\nimplemented in almost exactly this way. If one disassembles the two functions\nabove one gets sort of similar bytecode for both (apart from some details, like\nthe fact that the append in the list comprehension is done with a special\nLIST_APPEND bytecode).\nNow, when doing this sort of expansion there are some classical problems: what\nname should the intermediate list get that is being built? (I said classical\nbecause this is indeed one of the problems of many macro systems). What CPython\ndoes is give the list the name _[1] (and _[2]... with nested list\ncomprehensions). You can observe this behaviour with the following code:\n\n$ python\nPython 2.5.2 (r252:60911, Apr 21 2008, 11:12:42)\n[GCC 4.2.3 (Ubuntu 4.2.3-2ubuntu7)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> [dir() for i in [0]][0]\n['_[1]', '__builtins__', '__doc__', '__name__', 'i']\n>>> [[dir() for i in [0]][0] for j in [0]][0]\n['_[1]', '_[2]', '__builtins__', '__doc__', '__name__', 'i', 'j']\n\nThat is a sort of nice decision, since you can not reach that name by any\n\"normal\" means. Of course you can confuse yourself in funny ways if you want:\n\n>>> [locals()['_[1]'].extend([i, i + 1]) for i in range(10)]\n[0, 1, None, 1, 2, None, 2, 3, None, 3, 4, None, 4, 5, None, 5, 6, None, 6, 7, None, 7, 8, None, 8, 9, None, 9, 10, None]\n\nNow to the real reason why I am writing this blog post. PyPy's Python\ninterpreter implements list comprehensions in more or less exactly the same way,\nwith on tiny difference: the name of the variable:\n\n$ pypy-c-53594-generation-allworking\nPython 2.4.1 (pypy 1.0.0 build 53594) on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n``the globe is our pony, the cosmos our real horse''\n>>>> [dir() for i in [0]][0]\n['$list0', '__builtins__', '__doc__', '__name__', 'i']\n\n\nNow, that shouldn't really matter for anybody, should it? Turns out it does. The\nfollowing way too clever code is apparently used a lot:\n\n__all__ = [__name for __name in locals().keys() if not __name.startswith('_') '\n               or __name == '_']\n\nIn PyPy this will give you a \"$list0\" in __all__, which will prevent the\nimport of that module :-(. I guess I need to change the name to match CPython's.\nLesson learned: no detail is obscure enough to not have some code depending\non it. Mostly problems on this level of obscurity are the things we are fixing\nin PyPy at the moment.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/list-comprehension-implementation-5289956690288817225.html"
    },
    {
      "title": "Better Profiling Support for PyPy",
      "text": "As PyPy is getting more and more usable, we need better tools to use to work on certain applications running on top of PyPy. Out of this interest, I spent some time implementing the _lsprof module, which is a part of the standard library since Python2.5. It is necessary for the cProfile module, which can profile Python programs with high accuracy and a lot less overhead than the older, pure-python profile module. Together with the excellent\nlsprofcalltree script, you can display this data using kcachegrind, which gives you great visualization possibilities for your profile data.\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/better-profiling-support-for-pypy-1848129914083462080.html"
    },
    {
      "title": "Threads and GCs",
      "text": "Hi all,\n\nWe can now compile a pypy-c that includes both thread support\nand one of our semi-advanced garbage collectors.  This means\nthat threaded Python programs can now run not only with a\nbetter performance, but without the annoyances of the Boehm\ngarbage collector.  (For example, Boehm doesn't like too much\nseeing large numbers of __del__(), and our implementation of\nctypes uses them everywhere.)\n\nMagic translation command (example):\n\n   translate.py --thread --gc=hybrid targetpypystandalone --faassen --allworkingmodules\n\nNote that multithreading in PyPy is based on a global\ninterpreter lock, as in CPython.  I imagine that we will get\nrid of the global interpreter lock at some point in the future\n-- I can certainly see how this might be done in PyPy, unlike\nin CPython -- but it will be a lot of work nevertheless.  Given\nour current priorities, it will probably not occur soon unless\nsomeone steps in.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/threads-and-gcs-1126087726480790112.html"
    },
    {
      "title": "Progresses on the CLI JIT backend front",
      "text": "In the last months, I've actively worked on the CLI backend for PyPy's\nJIT generator, whose goal is to automatically generate JIT compilers\nthat produces .NET bytecode on the fly.\nThe CLI JIT backend is far from be completed and there is still a lot\nof work to be done before it can handle the full PyPy's Python\ninterpreter; nevertheless, yesterday I finally got the first .NET\nexecutable that contains a JIT for a very simple toy language called\ntlr, which implements an interpreter for a minimal register based\nvirtual machine with only 8 operations.\nTo compile the tlr VM, follow these steps:\n\n\nget a fresh checkout of the oo-jit branch, i.e. the branch\nwhere the CLI JIT development goes on:\n\n$ svn co https://codespeak.net/svn/pypy/branch/oo-jit\n\n\ngo to the oo-jit/pypy/jit/tl directory, and compile the tlr VM\nwith the CLI backend and JIT enabled:\n\n$ cd oo-jit/pypy/jit/tl/\n$ ../../translator/goal/translate.py -b cli --jit --batch targettlr\n\n\n\n\nThe goal of our test program is to compute the square of a given\nnumber; since the only operations supported by the VM are addition and\nnegation, we compute the result by doing repetitive additions; I won't\ndescribe the exact meaning of all the tlr bytecodes here, as they are\nquite self-documenting:\n\nALLOCATE,    3,   # make space for three registers\nMOV_A_R,     0,   # i = a\nMOV_A_R,     1,   # copy of 'a'\n\nSET_A,       0,\nMOV_A_R,     2,   # res = 0\n\n# 10:\nSET_A,       1,\nNEG_A,\nADD_R_TO_A,  0,\nMOV_A_R,     0,   # i--\n\nMOV_R_A,     2,\nADD_R_TO_A,  1,\nMOV_A_R,     2,   # res += a\n\nMOV_R_A,     0,\nJUMP_IF_A,  10,   # if i!=0: goto 10\n\nMOV_R_A,     2,\nRETURN_A          # return res\n\nYou can find the program also at the end of the tlr module; to get an\nassembled version of the bytecode, ready to be interpreted, run this\ncommand:\n\n$ python tlr.py assemble > square.tlr\n\nNow, we are ready to execute the code through the tlr VM; if you are\nusing Linux/Mono, you can simply execute the targettlr-cli script\nthat has been created for you; however, if you use Windows, you have\nto manually fish the executable inside the targettlr-cli-data\ndirectory:\n\n# Linux\n$ ./targettlr-cli square.tlr 16\n256\n\n# Windows\n> targettlr-cli-data\\main.exe square.tlr 16\n256\n\nCool, our program computed the result correctly! But, how can we be\nsure that it really JIT compiled our code instead of interpreting it?\nTo inspect the code that it's generated by our JIT compiler, we simply\nset the PYPYJITLOG environment variable to a filename, so that the\nJIT will create a .NET assembly containing all the code that has been\ngenerated by the JIT:\n\n$ PYPYJITLOG=generated.dll ./targettlr-cli square.tlr 16\n256\n$ file generated.dll\ngenerated.dll: MS-DOS executable PE  for MS Windows (DLL) (console) Intel 80386 32-bit\n\nNow, we can inspect the DLL with any IL disassembler, such as\nilasm or monodis; here is an excerpt of the disassembled code,\nthat shows how our square.tlr bytecode has been compiled to .NET\nbytecode:\n\n.method public static  hidebysig default int32 invoke (object[] A_0, int32 A_1)  cil managed\n{\n    .maxstack 3\n    .locals init (int32 V_0, int32 V_1, int32 V_2, int32 V_3, int32 V_4, int32 V_5)\n\n    ldc.i4 -1\n    ldarg.1\n    add\n    stloc.1\n    ldc.i4 0\n    ldarg.1\n    add\n    stloc.2\n    IL_0010:  ldloc.1\n    ldc.i4.0\n    cgt.un\n    stloc.3\n    ldloc.3\n    brfalse IL_003b\n\n    ldc.i4 -1\n    ldloc.1\n    add\n    stloc.s 4\n    ldloc.2\n    ldarg.1\n    add\n    stloc.s 5\n    ldloc.s 5\n    stloc.2\n    ldloc.s 4\n    stloc.1\n    ldarg.1\n    starg 1\n\n    nop\n    nop\n    br IL_0010\n\n    IL_003b:  ldloc.2\n    stloc.0\n    br IL_0042\n\n    ldloc.0\n    ret\n}\n\nIf you know a bit IL, you can see that the code generated is not\noptimal, as there are some redundant operations like all those\nstloc/ldloc pairs; however, while not optimal, it is still quite good\ncode, not much different to what you would get by writing the square\nalgorithm directly in e.g. C#.\nAs I said before, all of this is still work in progress and there is\nstill much to be done. Stay tuned :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/progresses-on-cli-jit-backend-front-1021772190959551376.html"
    },
    {
      "title": "More windows support",
      "text": "Recently, thanks to Amaury Forgeot d'Arc and Michael Schneider, Windows became more of a first-class platform for PyPy's Python interpreter. Most RPython extension modules are now considered working (apart from some POSIX specific modules). Even CTypes now works on windows!\n\n\nNext step would be to have better buildbot support for all supported platforms (Windows, Linux and OS X), so we can control and react to regressions quickly. (Buildbot is maintained by JP Calderone)\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/more-windows-support-1747028151130099034.html"
    },
    {
      "title": "S3-Workshop Potsdam 2008 Writeup",
      "text": "Trying to give some notes about the S3 Workshop in Potsdam that several\nPyPyers and Spies (Armin, Carl Friedrich, Niko, Toon, Adrian) attended before\nthe Berlin sprint.  We presented a paper about SPy there. Below are some mostly\nrandom note about my (Carl Friedrich's) impressions of the conference and some\ntalk notes. Before that I'd like to give thanks to the organizers who did a\ngreat job. The workshop was well organized, the social events were wonderful (a\nvery relaxing boat trip in the many lakes around Potsdam and a conference\ndinner).\nVideo recordings of all the talks can be found on the program page.\n\nInvited Talks\n\"Late-bound Object Lambda Architectures\" by Ian Piumarta was quite an inspiring\ntalk about VPRI's attempt at writing a flexible and understandable computing\nsystem in 20K lines of code. The talk was lacking a bit in technical details, so\nwhile it was inspiring I couldn't really say much about their implementation.\nApart from that, I disagree with some of their goals, but that's the topic of\nanother blog post.\n\"The Lively Kernel \u2013 A Self-supporting System on a Web Page\" by Dan Ingalls. Dan\nIngalls is one of the inventors of the original Smalltalk and of Squeak. He was\ntalking about his latest work, the attempts of bringing a Squeak-like system to\na web browser using JavaScript and SVG. To get some feel for what exactly The\nLively Kernel is, it is easiest to just try it out (only works in Safari\nand Firefox 3 above Beta 5 though). I guess in a sense the progress of the\nLively Kernel over Squeak is not that great but Dan seems to be having fun.  Dan\nis an incredibly enthusiastic, friendly and positive person, it was really great\nmeeting him. He even seemed to like some of the ideas in SPy.\n\"On Sustaining Self\" by Richard P. Gabriel was a sort of deconstructivist\nmulti-media-show train wreck of a presentation that was a bit too weird for my\ntaste. There was a lot of music, there were sections in the presentation\nwhere Richard discussed with an alter ego, whose part he had recorded in advance\nand mangled with a sound editor. There was a large bit of a documentary\nabout Levittown. Even the introduction and the questions were weird, with Pascal\nConstanza staring down the audience, without saying a word (nobody dared to ask\nquestions). I am not sure I saw the point of the presentation, apart from\ngetting the audience to think, which probably worked. It seems that there are\npeople (e.g. Christian Neukirchen) that liked the presentation, though.\n\n\nResearch Papers\n\"SBCL - A Sanely Bootstrappable Common Lisp by Christophe Rhodes described the\nbootstrapping process of SBCL (Steel Bank Common Lisp). SBCL can be bootstrapped\nby a variety of Common Lisps, not just by itself. SBCL contains a complete\nblueprint of the initial image instead of always getting the new image by\ncarefully mutating the old one. This bootstrapping approach is sort of similar\nto that of PyPy.\n\"Reflection for the Masses\" by Charlotte Herzeel, Pascal Costanza, and Theo\nD'Hondt retraced some of the work of Brian Smith on reflection in Lisp. The\ntalk was not very good, it was way too long (40 min), quite hard to understand\nbecause Charlotte Herzeel was talking in a very low voice. The biggest mistake\nin her talk was in my opinion that she spent too much time explaining a more or\nless standard meta-circular interpreter for Lisp and then running out of time\nwhen she was trying to explain the modifications. I guess it would have been a\nfair assumptions that large parts of the audience know such interpreters, so\nglossing over the details would have been fine. A bit of a pity, since the paper\nseems interesting.\n\"Back to the Future in One Week - Implementing a Smalltalk VM in PyPy\"\nby Carl Friedrich Bolz, Adrian Kuhn, Adrian Lienhard, Nicholas D. Matsakis,\nOscar Nierstrasz, Lukas Renggli, Armin Rigo and Toon Verwaest, the paper with\nthe longest author list. We just made everybody an author who was at the sprint\nin Bern. Our paper had more authors than all the other papers together :-). I\ngave the presentation at the workshop, which went quite well, judging from the\nfeedback I got.\n\"Huemul - A Smalltalk Implementation\" by Guillermo Adri\u00e1n Molina. Huemul is a\nSmalltalk implementation that doesn't contain an interpreter but directly\ncompiles all methods to assembler (and also saves the assembler in the image).\nIn addition, as much functionality (such as threading, GUI) as possible is\ndelegated to libraries instead of reimplementing them in Smalltalk\n(as e.g. Squeak is doing). The approach seems to suffer from the usual problems\nof manually writing a JIT, e.g. the VM seems to segfault pretty often. Also I\ndon't agree with some of the design decisions of the threading scheme, there is\nno automatic locking of objects at all, instead the user code is responsible for\npreventing concurrent accesses from messing up things (which even seems to lead\nto segfaults in the default image).\n\"Are Bytecodes an Atavism?\" by Theo D'Hondt argued that using AST-based\ninterpreters can be as fast as bytecode-based interpreters which he proved by\nwriting two AST-interpreters, one for Pico and one for Scheme. Both of these\nimplementations seem to perform pretty well. Theo seems to have many similar\nviews as PyPy, for example that writing simple straightforward interpreters is\noften preferable than writing complex (JIT-)compilers.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/s3-workshop-potsdam-2008-writeup-6610637452403831794.html"
    },
    {
      "title": "Berlin Sprint Finished",
      "text": "The Berlin sprint is finished, below some notes on what we worked on during\nthe last three days:\n\n\nCamillo worked tirelessly on the gameboy emulator with some occasional input\nby various people. He is making good progress, some test ROMs run now on the\ntranslated emulator. However, the graphics are still not completely working\nfor unclear reasons. Since PyBoy is already taken as a project name, we\nconsidered calling it PyGirl (another name proposition was \"BoyBoy\", but the\nimplementation is not circular enough for that).\n\n\n\n\nOn Monday Armin and Samuele fixed the problem with our multimethods so that\nthe builtin shortcut works again (the builtin shortcut is an optimization\nthat speeds up all operations on builtin non-subclassed types quite a bit).\nAntonio and Holger (who hasn't been on a sprint in a while, great to have you\nback!) worked on writing a conftest file (the plugin mechanism of py.test)\nthat would allow us to run Django tests using py.test, which seems to be not\ncompletely trivial. They also fixed some bugs in PyPy's Python interpreter,\ne.g. related to dictionary subclassing.\nKarl started adding sound support to the RPython SDL-bindings, which will be\nneeded both by the Gameboy emulator and eventually by the SPy VM.\nArmin and Maciek continued the work that Maciek had started a while ago of\nimproving the speed of PyPy's IO operation. In the past, doing IO usually\ninvolved copying lots of memory around, which should have improved now. Armin\nand Maciek improved and then merged the first of the two branches that\ncontained IO improvements, which speeds up IO on non-moving GCs (mostly the\nBoehm GC). Then they continued working on the hybrid-io branch which is\nsupposed improve IO on the hybrid GC (which was partially designed exactly\nfor this).\nToon, Carl Friedrich finished cleaning up the SPy improvement branch and\nfixed all warnings that occur when you translate SPy there. An obscure bug in\nan optimization prevented them from getting working executables, which at\nthis moment blocks the merging of that branch.\n\n\nBy now everybody is home again (except for Anto, who booked his return flight\ntwo days too late, accidentally) and mostly resting. It was a good sprint, with\nsome interesting results and several new people joining. And it was definitely\nthe most unusual sprint location ever :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/berlin-sprint-finished-1597243123548564657.html"
    },
    {
      "title": "Berlin Sprint Day 1 + 2",
      "text": "After having survived the S3-Workshop which took place in Potsdam on Thursday\nand Friday (a blog-post about this will follow later) we are now sitting in the\nc-base in Berlin, happily sprinting. Below are some notes on what progress we\nmade so far:\n\n\nThe Gameboy emulator in RPython that Camillo Bruni is working on for his\nBachelor project at Uni Bern does now translate. It took him (assisted by\nvarious people) a while to figure out the translation errors (essentially\nbecause he wrote nice Python code that passed bound methods around, which the\nRTyper doesn't completely like).  Now that is fixed and the Gameboy emulator\ntranslates and runs a test ROM.  You cannot really see anything yet, because\nthere is no graphics support in RPython.\nTo get graphics support in RPython Armin and Karl started writing SDL\nbindings for RPython, which both the Gameboy emulator and the SPy VM need.\nThey have basic stuff working, probably enough to support the Gameboy\nalready.\nAlexander, Armin, Maciek and Samuele discussed how to approach separate\ncompilation for RPython, which isn't easy because the RPython type analysis\nis a whole-program analysis.\nStephan, Peter and Adrian (at least in the beginning) worked on making PyPy's\nstackless module more complete. They added channel preferences which\nchange details of the scheduling semantics.\nToon, Carl Friedrich and Adrian (a tiny bit) worked on SPy. There is a branch\nthat Toon started a while ago which contains many improvements but is also\nquite unclear in many respects. There was some progress in cleaning that up.\nThis involved implementing the Smalltalk process scheduler (Smalltalk really\nis an OS). There is still quite some work left though. While doing so, we\ndiscovered many funny facts about Squeak's implementation details (most of\nwhich are exposed to the user) in the process. I guess we should collect them\nand blog about them eventually.\nSamuele and Maciek improved the ctypes version of pysqlite that Gerhard\nH\u00e4ring started.\nArmin, Samuele and Maciek found an obscure bug in the interaction between the\nbuiltin-type-shortcut that Armin recently implemented and our multimethod\nimplementation. It's not clear which of the two are to blame, however it\nseems rather unclear how to fix the problem: Armin and Samuele are stuck in a\ndiscussion about how to approach a solution since a while and are hard to\ntalk to.\nStijn Timbermont, a Ph.D. student at the Vrije Universiteit Brussel who is\nvisiting the sprint for two days was first looking at how our GCs are\nimplemented to figure out whether he can use PyPy for some experiments. The\nanswer to that seems to be no. Today he was hacking on a Pico interpreter\n(without knowing too much about Python) and is making some nice progress, it\nseems.\n\n\nWill try to blog more as the sprint progresses.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/berlin-sprint-day-1-2-8761821946764492267.html"
    },
    {
      "title": "General performance improvements",
      "text": "Hi all,\n\nDuring the past two weeks we invested some more efforts on the\nbaseline performance of pypy-c.  Some of the tweaks we did\nwere just new ideas, and others were based on actual\nprofiling.  The net outcome is that we now expect PyPy to be\nin the worst case twice as slow than CPython on real\napplications.  Here are some small-to-medium-size benchmark\nresults.  The number is the execution time, normalized to 1.0\nfor CPython 2.4:\n\n1.90 on templess (a simple templating language)\n1.49 on gadfly (pure Python SQL database)\n1.49 on translate.py (pypy's own translation toolchain)\n1.44 on mako (another templating system)\n1.21 on pystone\n0.78 on richards\n\n\n(This is all without the JIT, as usual.  The JIT is not ready yet.)\n\nYou can build yourself a pypy-c with this kind of speed with\nthe magic command line (gcrootfinder is only for a 32-bit\nLinux machine):\n\n    pypy/translator/goal/translate.py --gc=hybrid --gcrootfinder=asmgcc targetpypystandalone --allworkingmodules --faassen\n\nThe main improvements come from:\n    \nA general shortcut for any operation between built-in objects:\nfor example, a subtraction of two integers or floats now dispatches\ndirectly to the integer or float subtraction code, without looking up\nthe '__sub__' in the class.\nA shortcut for getting attributes out of instances of user classes\nwhen the '__getattribute__' special method is not overridden.\nThe so-called Hybrid Garbage Collector is now a\nthree-generations collector.\n\nMore about our GCs...\nSome profiling showed bad performance in our implementation of\nthe built-in id() -- a trivial function to write in CPython, but a lot\nmore fun when you have a moving GC and your object's real address can\nchange.\nThe bytecode compiler's parser had a very slow linear search\nalgorithm that we replaced with a dictionary lookup.\n\n\nThese benchmarks are doing CPU-intensive operations. You can expect\na similar blog post soon about the I/O performance, as the\nio-improvements branch gets closer to being merged\n:-)  The branch could also improve the speed of\nstring operations, as used e.g. by the templating systems.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/general-performance-improvements-838741900863354293.html"
    },
    {
      "title": "Next Sprint: Berlin, May 17-22nd May",
      "text": "Our next PyPy sprint will take place in the crashed c-base space station, Berlin, Germany, Earth, Solar System.  This is a fully public sprint: newcomers (from all planets) are welcome.  Suggestion of topics (other topics are welcome too):\n\n\nwork on PyPy's JIT generator: we are refactoring parts of the\n    compiling logic, in ways that may also allow generating better\n    machine code for loops (people or aliens with knowledge on\n    compilers and SSA, welcome)\n\nwork on the SPy VM, PyPy's Squeak implementation, particularly the\n    graphics capabilities                                             \n\nwork on PyPy's GameBoy emulator, which also needs graphics support\n                                                                      \ntrying some large pure-Python applications or libraries on PyPy and\n    fixing the resulting bugs. Possibilities are Zope 3, Django and\n    others.\n\n\nFor more information, see the full announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/next-sprint-berlin-may-17-22nd-may-5362899847460267375.html"
    },
    {
      "title": "Google's Summer of Code",
      "text": "PyPy got one proposal accepted for Google's Summer of Code under the Python\nSoftware Foundation's umbrella.  We welcome Bruno Gola into the PyPy\ncommunity. He will work on supporting all Python 2.5 features in PyPy and will\nalso update PyPy's standard library to support the modules that were modified\nor new in Python 2.5.\nRight now PyPy supports only Python 2.4 fully (some Python 2.5 features have\nalready sneaked in, though).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/googles-summer-of-code-4911168632727441622.html"
    },
    {
      "title": "Float operations for JIT",
      "text": "Recently, we taught the JIT x86 backend how to produce code for the x87 floating point coprocessor. This means that JIT is able to nicely speed up float operations (this this is not true for our Python interpreter yet - we did not integrate it yet). This is the first time we started going beyond what is feasible in psyco - it would take a lot of effort to make floats working on top of psyco, way more than it will take on PyPy.\n\nThis work is in very early stage and lives on a jit-hotpath branch, which includes all our recent experiments on JIT compiler generation, including tracing JIT experiments and huge JIT refactoring.\n\nBecause we don't encode the Python's semantics in our JIT (which is really a JIT generator), it is expected that our Python interpreter with a JIT will become fast \"suddenly\", when our JIT generator is good enough. If this point is reached, we  would also get fast interpreters for Smalltalk or JavaScript with relatively low effort.\n\nStay tuned.\n\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/float-operations-for-jit-6499693696246367083.html"
    },
    {
      "title": "Wrapping pyrepl in the readline API",
      "text": "If you translate a pypy-c with --allworkingmodules and start it, you will probably not notice anything strange about its prompt - except when typing multiline statements. You can move the cursor up and continue editing previous lines. And the history is multiline-statements-aware as well. Great experience! Ah, and completion using tab is nice too.\n\nTruth be told, there is nothing new here: it was all done by Michael Hudson's pyrepl many years ago.  We had already included pyrepl in PyPy some time ago.  What is new is a pure Python readline.py which exposes the most important parts of the API of the standard readline module by wrapping pyrepl under the hood, without needing the GNU readline library at all. The PyPy prompt is based on this, benefitting automagically from pyrepl's multiline editing capabilities, with minor tweaks so that the prompt looks much more like CPython's than a regular pyrepl prompt does.\n\nYou can also try and use this multiline prompt with CPython: check out pyrepl at https://codespeak.net/svn/pyrepl/trunk/pyrepl and run the new pythoni1 script.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/wrapping-pyrepl-in-readline-api-362730784820949868.html"
    },
    {
      "title": "Other April's Fools Ideas",
      "text": "While discussing what to post as an April Fool's joke yesterday, we had a\ncouple of other ideas, listed below. Most of them were rejected because they are\ntoo incredible, others because they are too close to our wish list.\n\nquantum computer backend\nPerl6 interpreter in RPython\nRuby backend to allow run \"python on rails\"\nmandatory static typing at app-level, because it's the only way to increase\nperformances\nrewrite PyPy in Haskell, because we discovered that dynamic typing is just\nnot suitable for a project of this size\na C front-end, so that we can interpret the C source of Python C extensions\nand JIT it. This would work by writing an interpreter for LLVM bytecode in\nRPython.\nan elisp backend\na TeX backend (use PyPy for your advanced typesetting needs)\nan SQL JIT backend, pushing remote procedures into the DB engine",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/other-aprils-fools-ideas-955926452383759016.html"
    },
    {
      "title": "Trying to get PyPy to run on Python 3.0",
      "text": "As you surely know, Python 3.0 is coming; recently, they released\nPython 3.0 alpha 3, and the final version is expected around\nSeptember.\nAs suggested by the migration guide (in the PEP 3000), we started by applying\n2to3 to our standard interpreter, which is written in RPython (though\nwe should call it RPython 2.4 now, as opposed to RPython 3.0 -- see\nbelow).\nConverting was not seamless, but most of the resulting bugs were due to the\nnew dict views, str/unicode changes and the missing \"reduce\" built-in.\nAfter forking and refactoring both our interpreter and the 2to3 script,\nthe Python interpreter runs on Python 3.0 alpha 3!\nNext step was to run 2to3 over the whole translation toolchain,\ni.e. the part of PyPy which takes care of analyzing the interpreter in\norder to produce efficient executables; after the good results we got\nwith the standard interpreter, we were confident that it would have\nbeen relatively easy to run 2to3 over it: unfortunately, it was not\n:-(.\nAfter letting 2to3 run for days and days uninterrupted, we decided to\nkill it: we assume that the toolchain is simply too complex to be\nconverted in a reasonable amount of time.\nSo, we needed to think something else; THE great idea we had was to\nturn everything upside-down: if we can't port PyPy to Py3k, we can\nalways port Py3k to PyPy!\nUnder the hood, the 2to3 conversion tool operates as a graph\ntransformer: it takes the graph of your program (in the form of Python\n2.x source file) and returns a transformed graph of the same program\n(in the form of Python 3.0 source file).  Since the entire translation\ntoolchain of PyPy is based on graph transformations, we could reuse it\nto modify the behaviour of the 2to3 tool.  We wrote a general\ngraph-inverter algorithm which, as the name suggests, takes a graph\ntransformation and build the inverse transformation; then, we applied\nthe graph inverter to 2to3, getting something that we called 3to2: it\nis important to underline that 3to2 was built by automatically\nanalysing 2to3 and reversing its operation with only the help of a few\nmanual hints. For this reason and because we are not keeping generated\nfiles under version control, we do not need to maintain this new tool in\nthe Subversion repository.\nOnce we built 3to2, it was relatively easy to pipe its result to our\ninterpreter, getting something that can run Python 3.0 programs.\nPerformance-wise, this approach has the problem of being slower at\nimport time, because it needs to run (automatically) 3to2 every time\nthe source is modified; in the future, we plan to apply our JIT\ntechniques also to this part of the interpreter, trying to mitigate the\nslowdown until it is not noticeable anymore to the final user.\nIn the next weeks, we will work on the transformation (and probably publish\nthe technique as a research paper, with a title like \"Automatic Program\nReversion on Intermediate Languages\").\nUPDATE: In case anybody didn't guess or didn't spot the acronym: The above\nwas an April Fool's joke. Nearly nothing of it is true.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/trying-to-get-pypy-to-run-on-python-30-5082015544752137606.html"
    },
    {
      "title": "Py-Lib 0.9.1 released",
      "text": "The Py-Lib 0.9.1 release is out! The Py-Lib is a very important support\nlibrary that PyPy uses for a lot of things \u2013 most importantly it contains\npy.test, which PyPy uses for testing.\nThis is mostly a bugfix release, with a couple of new features sneaked in.\nMost important changes:\n\nsome new functionality (authentication, export, locking) in py.path's\nSubversion APIs\nnumerous small fixes in py.test's rsession (experimental pluggable session)\nand generative test features\nsome fixes in the py.test core\n\nDownload/Install:   https://codespeak.net/py/0.9.1/download.html\nDocumentation/API:  https://codespeak.net/py/0.9.1/index.html\nUPDATE: the py-lib is now easy-installable with:\n\neasy_install py",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2008/03/py-lib-091-released-1654797401128918376.html"
    },
    {
      "title": "PyPy Summer of Code Participation",
      "text": "As in the last years, PyPy will again participate in Google's Summer of Code\nprogram under the umbrella of the Python Software Foundation. Unfortunately we\nwere a bit disorganized this year, so that our project ideas are only put up\nnow. The list of project ideas of PyPy can be found here.\nAny interested student should mail to our mailing list or just come to the\n#pypy channel on irc.freenode.net to discuss things.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/pypy-summer-of-code-participation-3403842530060519982.html"
    },
    {
      "title": "ctypes configuration tool",
      "text": "As a part of implementing ctypes, we decided to make coding using ctypes better on its own (irrelevant what python interpreter you use). The concrete problem we're trying to solve is to make ctypes code more platform-independent than it is. Say you want to create a ctypes type for size_t: ctypes itself provides no mechanism for doing that, so you need to use a concrete integer type (c_int, c_long, c_short etc.). Your code either becomes platform dependent if you pick one of them or is littered with conditionals for all sorts of platforms. We created a small library, called ctypes_configure (which is actually a variation of something we use somewhere in the PyPy source tree), which tries to solve some platform dependencies by compiling and running small chunks of C code through a C compiler. It's sort of like configure in the Linux world, except for Python using ctypes.\n\nTo install the library, you can just type easy_install ctypes_configure. The code is in an svn repository on codespeak and there is even some documentation and sample code. Also, even though the code lives in the pypy repository, it depends only on pylib, not on the whole of pypy.\n\nThe library is in its early infancy (but we think it is already rather useful). In the future we could add extra features, it might be possible to check whether the argtypes that are attached to  the external functions are consistent with what is in the C headers), so that the following code wouldn't segfault but give a nice error\n\nlibc = ctypes.CDLL(\"libc.so\")\ntime = libc.time\ntime.argtypes = [ctypes.c_double, ctypes.c_double]\ntime(0.0, 0.0)\n\n\nAlso, we plan to add a way to install a package that uses ctypes_configure in such a way that the installed library doesn't need to call the C compiler any more later.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/ctypes-configuration-tool-7414864595600362988.html"
    },
    {
      "title": "Bittorrent on PyPy",
      "text": "Hi all,\n\nBittorrent now runs on PyPy! I tried the no-GUI BitTornado version (btdownloadheadless.py). It behaves correctly and I fixed the last few obvious places which made noticeable pauses.  (However we know that there are I/O performance issues left: we make too many internal copies of the data, e.g. in a file.read() or os.read().)\n\nWe are interested in people trying out other real-world applications that, like the GUI-less Bittorrent, don't have many external dependencies to C extension modules. Please report all the issues to us!\n\nThe current magic command line for creating a pypy-c executable with as many of CPython's modules as possible is:\n\n\n  cd pypy/translator/goal\n  ./translate.py --thread targetpypystandalone.py --allworkingmodules --withmod-_rawffi --faassen\n\n\n(This gives you a thread-aware pypy-c, which requires the Boehm gc library.  The _rawffi module gives you ctypes support but is only tested for Linux at the moment.)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/bittorrent-on-pypy-7984272143557948160.html"
    },
    {
      "title": "As fast as CPython (for carefully taken benchmarks)",
      "text": "Good news everyone. A tuned PyPy compiled to C is nowadays as fast as CPython on the richards benchmark and slightly faster on the gcbench benchmark.\n\nIMPORTANT:  These are very carefully taken benchmarks where we expect pypy to be fast!  PyPy is still quite slower than CPython on other benchmarks and on real-world applications (but we're working on it).  The point of this post is just that for the first time (not counting JIT experiments) we are faster than CPython on *one* example :-)\n\nThe exact times as measured on my notebook (which is a Core Duo machine) are here:\n\nCompiled pypy with options:\n\n\n./translate.py --gcrootfinder=asmgcc --gc=generation targetpypystandalone.py --allworkingmodules --withmod-_rawffi --faassen\n\n(allworkingmodules and withmod-_rawffi are very likely irrelevant to those benchmarks)\n\nCPython version 2.5.1, release.\n\n\nrichards 800ms pypy-c vs 809ms cpython (1% difference)\ngcbench 53700ms pypy-c vs 60215ms cpython (11% difference)\n\nPyPy shines on gcbench, which is mostly just about allocating and freeing many objects.  Our gc is simply better than refcounting, even though we've got shortcomings in other places.\n\n\nAbout richards, there is a catch. We use a method cache optimization, and have an optimization which helps to avoid creating bound methods each time a method is called. This speeds up the benchmark for about 20%. Although method cache was even implemented for CPython, it didn't make its way to the core because some C modules directly modify the dictionary of new-style classes. In PyPy, the greater level of abstraction means that this operation is just illegal.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/as-fast-as-cpython-for-carefully-taken-1984440931984637179.html"
    },
    {
      "title": "Running Pyglet on PyPy",
      "text": "As part of our efforts of making PyPy's Python interpreter usable we put quite some effort into interfacing with external libraries. We were able, in quite a short amount of time (I think beginning really from Leysin sprint, or slightly earlier) to provide a prototype of the ctypes library. It is written in completely normal Python, at applevel, based on a very thin wrapper around the libffi library. This makes development a lot easier, but it makes the resulting ctypes implementation rather slow. The implementation is not complete yet and it will still need quite some effort to make it feature-complete (ctypes has lots of details and special cases and\ndo-what-I-mean magic). Yet another point will be to make it faster, but that's for much later.\n\nThe implementation is good enough to run those parts of Pyglet that don't depend on PIL (which PyPy doesn't have).  Here are a few pictures of running Pyglet demos on top of compiled pypy-c.\n\n\n\nTo compile a version of PyPy that supports ctypes, use this highly sophisticated command line\n\n\n./translate.py --gc=generation ./targetpypystandalone.py --allworkingmodules --withmod-_rawffi\n\nNote: this works on linux only right now.\n\nThe list of missing small ctypes features is quite extensive, but I consider the current implementation to be usable for most common cases. I would love to hear about libraries written in pure python (using ctypes), to run them on top of PyPy and use them as test cases. If someone knows such library, please provide a link.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/running-pyglet-on-pypy-3191536711417589549.html"
    },
    {
      "title": "Python Finalizers Semantics, Part 2: Resurrection",
      "text": "Continuing the last blog post about GC semantics in Python.\nAnother consequence of reference counting is that resurrection is easy to\ndetect. A dead object can resurrect itself if its finalizer stores it into a\nglobally reachable position, like this:\n\nclass C(object):\n    def __init__(self, num):\n        self.num = num\n    def __del__(self):\n        global c\n        if c is None:\n            c = self\nc = C(1)\nwhile c is not None:\n    c = None\n    print \"again\"\n\nThis is an infinite loop in CPython: Every time c is set to None in the\nloop, the __del__ method resets it to the C instance again (note that\nthis is terribly bad programming style, of course. In case anybody was wondering\n:-)). CPython can detect resurrection by checking whether the reference count\nafter the call to __del__ has gotten bigger.\nThere exist even worse examples of perpetual resurrection in particular in\ncombination with the cycle GC. If you want to see a particularly horrible one,\nsee this discussion started by Armin Rigo. In the ensuing thread Tim Peters\nproposes to follow Java's example and call the finalizer of every object at most\nonce.\nIn PyPy the resurrection problem is slightly more complex, since we have GCs\nthat run collection from time to time and don't really get to know at which\nprecise time an object dies. If the GC discovers during a collection that an\nobject is dead, it will call the finalizer after the collection is finished. If\nthe object is then dead at the next collection, the GC does not know whether\nthe object was resurrected by the finalizer and then died in the meantime or\nwhether it was not resurrected. Therefore it seemed sanest to follow Tim's\nsolution and to never call the finalizer of an object a second time, which has\nmany other benefits as well.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/python-finalizers-semantics-part-2-2748812428675325525.html"
    },
    {
      "title": "Python Finalizers Semantics, Part 1",
      "text": "Python's garbage collection semantics is very much historically grown and\nimplementation-driven. Samuele Pedroni therefore likes to call it the \"'there\nis no such thing as too much chocolate'-approach to GC semantics\" :-). In this\ntwo-part post series I am going to talk about the semantics of finalization\n(__del__ methods) in CPython and PyPy.\nThe current behaviour is mostly all a consequence of the fact that CPython uses\nreference counting for garbage collection. The first consequence is that if\nseveral objects die at the same time, their finalizers are called in a\nso-called topological order, which is a feature that some GCs have that\nCPython offers by chance.  This ensures, that in a __del__ method, all the\nattributes of the object didn't get their __del__ called yet. A simple\nexample:\n\nclass B(object):\n    def __init__(self, logfile):\n        self.logfile = logfile\n    def __del__(self):\n        self.logfile.write(\"done doing stuff\")\nb = B(file(\"logfile.txt\", \"w\"))\n\nIf the instance of B dies now, both it and the logfile are dead. They will\nget their __del__``s called and it's important that the file's ``__del__\ngets called second, because otherwise the __del__ of B would try to\nwrite to a closed file.\nThe correct ordering happens completely automatically if you use reference\ncounting: Setting b to None will decref the old value of b. This reduces\nthe reference count of this instance to 0, so the finalizer will be called.\nAfter the __del__ has finished, this object will be freed and all the\nobjects it points to decrefed as well, which decreases the reference count of\nthe file to 0 and call its `` __del__`` as well, which closes the file.\nThe behaviour of PyPy's semispace and generational GCs wasn't very nice so far:\nit just called the finalizers in an essentially random order. Last week Armin\ncame up with a somewhat complicated algorithm that solves this by emulating\nCPython's finalization order, which we subsequently implemented. So PyPy does\nwhat you expect now! The Boehm GC does a topological ordering by default, so it\nwasn't a problem there.\nA small twist on the above is when\nthere is a cycle of objects involving finalizers:\nIn this case a topological ordering is not possible, so that CPython refuses to\nguess the finalization order and puts such cycles into gc.garbage. This\nwould be very hard for PyPy to do, since our GC implementation is essentially\nindependent from the Python interpreter. The same GCs work for our other\ninterpreters after all too. Therefore we decided to break such a cycle at an\narbitrary place, which doesn't sound too insane.  The insane thing is for\na Python program to create a cycle of objects with finalizers and depend\non the order in which the finalizers are called.  Don't do that :-)  (After\nall, CPython wouldn't even call the finalizers in this case.)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/python-finalizers-semantics-part-1-1196956834543115766.html"
    },
    {
      "title": "PyPy presence on various conferences in the near future",
      "text": "Hello! I will have the pleasure of presenting PyPy on various conferences in the near future. They're (in chronological order):\n\n\nStudencki Festiwal Informatyczny in Krakow, POLAND 6-8 March 2008. I think this might be only interesting for polish people (website, in polish)\n\nPycon Chicago, IL, USA. 14-17 March 2008. There should be also a PyPy sprint afterwards, including newbie-friendly tutorial, everybody is welcome to join us!  (Provided that I'll get the US visa, which seems to be non-trivial issue for a polish citizen)\n RuPy, Poznan, POLAND 13-14 April 2008 (website). This is small, but very friendly Ruby and Python conference. Last year was amazing, I can strongly recommend to go there (Poznan is only 2h by train from Berlin also has its own airport).\n\n\nHope to see you at those places!\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/pypy-presence-on-various-conferences-in-6584680808789191759.html"
    },
    {
      "title": "Buildbots and Better Platform Support",
      "text": "In the last days we improved platform-support of PyPy's Python interpreter.\nJean-Paul Calderone has been tirelessly working for some time now on setting up a\nbuildbot for translating and testing PyPy. So far the basic mechanisms are\nworking and the buildbot is running on various machines, including some that\nMichael Schneider (bigdog) lets us use, one of them being a Windows machine,\nthe other one with a 64bit Linux (lots of thanks to those two, you are\nawesome!).\nWhat is still missing is a nice way to visualize the test results to quickly see\nwhich tests have started failing on which platforms. There is a prototype\nalready, which still needs some tweaking.\nThe availability of these machines has triggered some much-needed bug-fixing in\nPyPy to make our Python interpreter work better on Windows and on 64 bit Linux.\nMaciek and Michael Schneider worked on this quite a bit last week, with the\nresult that PyPy supports many more extension modules now on Windows and 64 bit\nLinux. Since we now have the buildbot the hope is that the support also won't\ndisappear soon :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/buildbots-and-better-platform-support-6965497451398110731.html"
    },
    {
      "title": "PyPy Keyboard Heatmap",
      "text": "Today I saw the keyboard heatmap generator on the Blended Technologies\nblog. I threw all the PyPy code at it to see whether the heatmap looks any\ndifferent than normal Python code. It doesn't:\n\nSo now the excuse \"I can't contribute to PyPy because it needs all those special\nPyPy-keys\" isn't working anymore :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/pypy-keyboard-heatmap-4950995633665492453.html"
    },
    {
      "title": "RPython can be faster than C",
      "text": "(yes, C as in language, not c as in speed of light). I looked recently at the great computer language shootout, for some benchmarks and to make some speed comparisons. I use this benchmark, modified it to be rpythonic-enough and compared speeds. The code is here (the only change from the Python version was to create a class instead of tuple, so actually this version is more OO). Also the benchmark is very likely flawed because it favours better GCs :).\nSo, here we go:\n\n\nLanguage:Time of run (for N=14):\nPython version running on Python 2.5.1, distribution25.5s\nPython version running on PyPy with generational GC45.5\nPython with psyco20s\nRPython translated to C using PyPy's generational GC0.42s\ncompiling the Haskell version with GHC 6.6.11.6s\ncompiling the C version with gcc 4.1.2 -O3 -fomit-frame-pointer0.6s\n\n\n\nAlso worth noticing is that when using psyco with the original version (with tuples) it is very fast (2s).\n\nSo, PyPy's Python interpreter is 80% slower than CPython on this (not too horrible), but RPython is 40% faster than gcc here. Cool. The result is mostly due to our GC, which also proves that manual memory-management can be slower than garbage collection in some situations. Please note that this result does not mean that RPython is meant for you. It requires a completely different mindset than the one used to program in Python. Don't say you weren't warned! :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/rpython-can-be-faster-than-c-2559071147541131237.html"
    },
    {
      "title": "PyPy.NET goes Windows Forms",
      "text": "After having spent the last few days on understanding PyPy's JIT,\ntoday I went back hacking the clr module.  As a result, it is now\npossible to import and use external assemblies from pypy-cli,\nincluding Windows Forms\nHere is a screenshot of the result you get by typing the following at\nthe pypy-cli interactive prompt:\n\n>>>> import clr\n>>>> clr.AddReferenceByPartialName(\"System.Windows.Forms\")\n>>>> clr.AddReferenceByPartialName(\"System.Drawing\")\n>>>> from System.Windows.Forms import Application, Form, Label\n>>>> from System.Drawing import Point\n>>>>\n>>>> frm = Form()\n>>>> frm.Text = \"The first pypy-cli Windows Forms app ever\"\n>>>> lbl = Label()\n>>>> lbl.Text = \"Hello World!\"\n>>>> lbl.AutoSize = True\n>>>> lbl.Location = Point(100, 100)\n>>>> frm.Controls.Add(lbl)\n>>>> Application.Run(frm)\n\nUnfortunately at the moment you can't do much more than this, because\nwe still miss support for delegates and so it's not possibile to\nhandle events. Still, it's a step in the right direction :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/pypynet-goes-windows-forms-7031406830502864570.html"
    },
    {
      "title": "Improve .NET Integration",
      "text": "A while ago Amit Regmi, a student from Canada, started working on the\nclr module improvements branch as a university project.\nDuring the sprint Carl Friedrich, Paul and me worked more on it and\nbrought it to a mergeable state.\nIt adds a lot of new features to the clr module, which is the\nmodule that allows integration between pypy-cli (aka PyPy.NET) and\nthe surrounding .NET environment:\n\n\nfull support to generic classes;\na new importer hook, allowing things like from System import\nMath and so on;\n.NET classes that implements IEnumerator are treated\nas Python iterators; e.g. it's is possile to iterate over them\nwith a for loop.\n\n\nThis is an example of a pypy-cli session:\n\n>>>> from System import Math\n>>>> Math.Abs(-42)\n42\n>>>> from System.Collections.Generic import List\n>>>> mylist = List[int]()\n>>>> mylist.Add(42)\n>>>> mylist.Add(43)\n>>>> mylist.Add(\"foo\")\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <interactive>\nTypeError: No overloads for Add could match\n>>>> mylist[0]\n42\n>>>> for item in mylist: print item\n42\n43\n\nThis is still to be considered an alpha version; there are few known\nbugs and probably a lot of unknown ones :-), so don't expect it to\nwork in every occasion. Still, it's a considerable step towards real\nworld :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/improve-net-integration-2239651503641931440.html"
    },
    {
      "title": "Crashing Other People's Compilers",
      "text": "Over the years PyPy has (ab?)used various external software for different\npurposes, and we've discovered bugs in nearly all of them, mostly by pushing them\nto their limits. For example, many compilers are not happy with 200MB of\nsource in one file. The Microsoft C compiler has a limit of 65536 lines of code\nper file and the CLI was raising \"System.InvalidProgramException: Method\npypy.runtime.Constants:.cctor () is too complex.\", where too complex probably\nmeans \"too long\". Just for fun, today we collected all projects we could think of\nin which we found bugs:\n\n\nCPython (lots)\nPyPy and the py-lib (surpise)\nctypes\nTCC (we gave up on it)\nBoehm\nGraphviz\nMono\nLLVM (lots)\nPython.net\nthe Microsoft IL assembler\nMicrosoft's C compiler\nJasmin\nJPype\nnucular\nTwisted\nthe JVM, maybe\npygame or SDL\n\n\nSo one could say that PyPy is really just the most expensive debugging tool\never :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/crashing-other-peoples-compilers-4574453763254909150.html"
    },
    {
      "title": "Leysin Winter Sport Sprint Started",
      "text": "The Leysin sprint has started since yesterday morning in the usual location. The view is spectacular (see photo) the weather mostly sunny. The following people are sprinting:\nMaciej FijalkowskiArmin RigoToby WatsonPaul deGrandisAntonio CuniCarl Friedrich BolzSo it is a rather small sprint.We started working on various features and performance improvements for the high level backends (JVM and .NET) and on implementing ctypes for PyPy. Later this week we plan to spend a few days on the JIT, because Anto and I both need to get into it for our respective university projects.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/leysin-winter-sport-sprint-started-5478612778498579467.html"
    },
    {
      "title": "Finding GC roots: using LLVM or parsing assembler files from GCC",
      "text": "PyPy contains a framework for writing custom Garbage Collectors, and a few simple GCs have been written in this framework. A common issue with all these GCs is how to find all the stack roots, i.e. all the pointers to live GC-managed objects currently stored in local variables, in all the callers of the current function. The current solution is to maintain a custom shadow stack of roots, where all functions push and pop copies of their local variables of type \"GC pointer\". Clearly this is an overhead. Can we remove it?\n\nLLVM has recently grown some support for this. By emitting markers in the LLVM source and with the help of a bit of custom C++ code, we can generate stack maps for the functions compiled by LLVM.  Then, with 100% non-portable code in our framework GC's root finding algorithm, we can walk the machine stack and locate where in each stack frame LLVM stores the GC pointers. (Yes, I mean non-portable: LLVM offers no help for doing that.  Maybe it will at some point, though I didn't manage to explain why this is an issue to people working on  this in LLVM so far...).  I've tried that approach in the llvmgcroot branch.  Over the manually-managed shadow stack, this gives speed improvements which are, very roughly, on the order of 5%.\n\nNote that this prevents some optimizations in LLVM, because it forces it to allocate all local variables of type \"GC pointer\" in the stack; it cannot keep them in registers and it must assume that they can be changed more or less at any time (as moving GCs do). Can we do better?\n\nActually, yes.  We can even do better in the C backend, using a GCC hack.  GCC has this nice extension:\nasm(\"bla\", constrains);\nThis is meant to generate assembler instructions directly from C. Internally, GCC considers the whole asm() as a single regular instruction of its intermediate language; the constrains are expressed in the same way as the constrains for all the prebuilt intermediate language instructions. They express things like input and output operands of the instruction, whether they can live in memory or in registers, whether the whole instruction has side-effects, etc. The nice thing about asm() is that it doesn't kill any optimization whatsoever in GCC - it's your job to make sure that you use the correct constrains.\n\nSo what I've tried in the asmgcroot branch is to use asm() as markers. In this branch, the C backend produces code like this after each function call, for each local variable containing a live GC pointer:\n\nasm(\"/* GCROOT %0 */\" : \"=g\"(localvar) : \"0\"(localvar) : \"memory\");\n\nThis causes GCC to emit the following line in the assembler file it generates:\n\n/* GCROOT register-or-memory-containing-localvar */\n\nI won't go in the details of the asm() line above - the constrains are just enough to make sure that GCC doesn't optimize too much, but don't prevent most optimizations from occurring. For example, the localvar can be in a register.\n\nThe assembler will just ignore the line above; it is a comment. But what we can do is write our own tool parsing the assembler files. This tool locates the /* GCROOT */ comments and follows where the register or memory location in the comment comes from (to do this it must follow the control flow and data flow of the function). This allows it to build a stack map: for each call instruction it knows exactly which registers and frame stack locations contain a live GC pointer. The stack map is then emitted in an extra assembler file that we link with the rest. As with LLVM above, the stack map is then used at run-time by non-portable code written in our GC's stack root tracker.\n\nYes, that's rather insane. But at least, we don't need to modify the assembler file - just read it. If GCC is too clever in its optimizations, the custom parser will get lost and complain cleanly; but I think that it is relatively safe in the sense that GCC optimizations should not be able to make the custom parser produce wrong results.\n\nThe branch is not merged because it's probably too insane to merge (not to mention, it's probably not portable to non-GCC compilers, and it is completely platform-specific). Still, it gives good results, better that the pure LLVM approach - on the order of 10% to 25% speed-ups for pypy-c.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/finding-gc-roots-using-llvm-or-parsing-1980376164990001937.html"
    },
    {
      "title": "Visualizing a Python tokenizer",
      "text": "Armin and me have been working on PyPy's parser and bytecode compiler for the Python language in the last days. Armin implemented several bytecode optimizations that CPython has since a while whereas I tried to refactor our tokenizer and parser (because our existing parser is rather slow and also not very nice code). Armin is mostly done whereas the new parser is not very far yet. What is done, however, is the Python tokenizer. It is implemented in the usual way, by using a set of regular expressions to generate a deterministic finite automaton (DFA). This automaton is then turned into a big function which does the actual tokenization. Of course the picture is not quite as simple for Python, because it is not possible to tokenize Python using only regular expressions. To generate the proper \"indent\" and \"dedent\" tokens it would be necessary to keep state (the previous indentation levels) which a DFA cannot do. This is solved by postprocessing the tokens that the tokenizer produces to turn whitespace tokens into the proper indent and dedent tokens.\nFor debugging purposes I implemented a visualization tool for DFAs using PyPy's pygame-based graph viewer. The graph viewer is able to visualize interactively any graph given in the graph-description language of Graphviz. Looking at the tokenizing DFA for Python is rather instructive, both for understanding how tokenizing works and (maybe) for understanding the Python language. To try it, download the dot file of the DFA and run from a pypy checkout:\n$ python pypy/bin/dotviewer.py tokenizer.dotThe following is a screenshot of the graphviewer:\n\nFor people who don't want do checkout PyPy I generated a (rather big) png for the DFA.\nNext thing I would like to do (apart from actually finishing the parser, of course :-) ) is visualize the Python grammar itself using syntax diagrams or something similar. So far I couldn't really find a program to do that, though.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/visualizing-python-tokenizer-5020282079473796926.html"
    },
    {
      "title": "PyPy Winter Sports Sprint from 12-19th of January in Leysin, Switzerland",
      "text": "The next PyPy sprint will be held in Leysin, Switzerland, for\nthe fifth time.  The overall idea of the sprint is to continue\nworking on making PyPy ready for general use.\nThe proposed topics are: ctypes, JIT, testing, LLVM.  This is\na fully public sprint, so newcomers and other topics are\nwelcome.  And like previous winters, the main side goal is to\nhave fun in winter sports :-) See the sprint announcement\nfor details.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/pypy-winter-sports-sprint-from-12-19th-5592383212609773292.html"
    },
    {
      "title": "(German) Slides of Talk at Python User Group Munich Available",
      "text": "Georg Brandl has put up the slides of the PyPy talk he gave at the Python User Group Munich.  The slides are in German.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/german-slides-of-talk-at-python-user-3715884461725333051.html"
    },
    {
      "title": "Various Performance Improvements",
      "text": "A few days ago, Armin discovered Gnuplot. He wrote a script that turns the results of the nightly benchmark runs into plots (lower is always better, all the numbers of the microbenchmarks are \"times slower than CPython\"). The corresponding microbenchmarks can be found in the repository. Staring at the plots revealed a strange performance regression around the revision 45000. After some investigation Armin found that an mostly unrelated change had disabled our method cache, which caused the regression. This was fixed.\n\nIn addition, Armin did a few other small tweaks in the interpreter main loop, making sure that small bytecodes are inlined into the main loop. This gave another few percent of performance increase. Together with the GC improvements two weeks ago this leads to the fastest non-JIT PyPy ever. Unfortunately \"fastest\" is not really very fast yet in absolute terms, with realistic apps being around 3-4 times slower than CPython. Especially calls (in all its variants) are quite slow, which is something we should look into.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/various-performance-improvements-7027210611565246190.html"
    },
    {
      "title": "Faster implementation of classic classes merged",
      "text": "Old-style classes have so far been a bit neglected by PyPy's Python interpreter. By default, PyPy makes all classes new-style and you have to use a command-line switch (--oldstyle) at startup or at translation time to change that default. Then you would get an pure-Python implementation of classic classes. This implementation was extremely slow (around 20 times slower than classic classes in CPython). In the past we had hoped that we could get away with mostly only supporting new-style classes, however it seems that real-world software seems to rely on them quite a bit, so we decided to offer a better migration path.\n\nA while ago I therefore started a re-implementation of classic classes in RPython to speed them up. This work is now finished, the branch I worked on got merged today. Speed for the old-style class benchmarks was improved greatly and I found quite a number of bugs in the old implementation too. New-style classes are still a bit faster than old-style in PyPy though, and this is unlikely to change.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/faster-implementation-of-classic-1021557618590043616.html"
    },
    {
      "title": "Profiling for fun with valgrind",
      "text": "Recently I've been doing a lot of profiling on the PyPy executables to find speed bottlenecks. Valgrind (the original page seems to be down) is an extremely nice tool for doing this. It has several built-in tools that give you different types of profiles. The callgrind mode provides you with a lot of information including relative call costs. The cachegrind tool gives you less information, but what it gives you (e.g. cache misses) is much more accurate. The obvious choice would be to have a way to combine the results of two profiling runs to have both. In the last days I wrote a script that does this. It's available at my user's svn and has a pretty intuitive command line interface. The combining calculation are not perfect yet, total costs of functions can still be a bit bogus (they can sum up to whatever) but at least the relative figures are good. This means that we can stop looking at two different types of graphs now.\n\nAn awesome tool for analyzing the profile data is kcachegrind.\n\n\n\nWhich also proves that my 12'' display is to small at least for some things :-).\n\n\nUpdate: pygrind is available under the MIT license.",
      "tags": "kcachegrind,profiling,valgrind",
      "url": "https://www.pypy.org/posts/2007/12/profiling-for-fun-with-valgrind-3215121784705288400.html"
    },
    {
      "title": "PyPy Talk at the Python User Group Munich",
      "text": "Tomorrow evening there will be an introductory talk about PyPy at the Python User Group Munich. The talk will be given by CPython and PyPy contributor Georg Brandl and will be in German.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/pypy-talk-at-python-user-group-munich-1952379593354367249.html"
    },
    {
      "title": "PyPy tasks in GHOP",
      "text": "In the latest bunch of tasks that Titus released on Friday for the Google Highly Open Participation Contest there are several that are related to PyPy. Some of them are about presenting PyPy to a technical audience: Task 187, Task 188, Task 189, Task 190.\n\nThen there are some three about Ropes, which are all rather challenging:\nSolving the first three section of the last ICFP contest with PyPy's ropes implementation:Task 248.Implementing nice wrapper classes around PyPy's ropes algorithms to make their use convenient: Task 239.Implementing the Ropes algorithms in C as a CPython extension module: Task 218 (already taken).\nIn addition there is a task to use PyPy's sandboxing features to provide an interactive Python tutorial on a web page: Task 220.\n\nWe're really looking forward to working together with some bright students!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/pypy-tasks-in-ghop-5130253260153218709.html"
    },
    {
      "title": "faster than c",
      "text": "Of course being \"faster than c\" means being faster than light. What did you think it means? :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/faster-than-c-8057790636822502084.html"
    },
    {
      "title": "Good news from the garbage collection front",
      "text": "It seems that we can do better! Armin fixed a bug in our generational garbage collector, which caused variable sized objects (e.g. arrays) to be allocated outside  of the nursery. This resulted in 50% speedup on synthetic benchmarks and about 10-20% on real world ones. Doing some preliminary measures, it seems that we spend roughly 10% of the time in garbage collection, which is good (and there is still some room for improvements!)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/good-news-from-garbage-collection-front-2678138026363485439.html"
    },
    {
      "title": "PyPy Google Tech Talk",
      "text": "The Google Tech Talk that Samuele, Armin, Jacob and Laura gave during the US trip is now on YouTube: https://www.youtube.com/watch?v=GnPmErtqPXk",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-google-tech-talk-9082134238390123890.html"
    },
    {
      "title": "Sprint Pictures",
      "text": "The obligatory sprint picture post...\n\n\n\n\nAlexander Schremmer, Armin Rigo, Maciek Fijalkowski, Antonio Cuni\n\nAnders Chrigstr\u00f6m, Samuele Pedroni, Laura Creighton, Jacob Hall\u00e9n, Carl Friedrich Bolz, Richard Emslie, Maciek Fijalkowski, Armin Rigo\n\nHolger Krekel\n\nWhiteboard with \"real world goals\" dependencies.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/sprint-pictures-3151912856495869652.html"
    },
    {
      "title": "Sprint Discussions: Wrapping External Libraries",
      "text": "A more technical discussion during the sprint was about the next steps for the external module problem (minutes). One of PyPy's biggest problems in becoming more generally useful are C extension modules, which can't work with PyPy's Python interpreter. We already reimplemented many of the more commonly used extension modules in CPython's standard library in Python or RPython. However, there are more missing and there is no way to implement all the extension modules that other people have written.\nWhiteboard after the discussion.\n\nTherefore we need a different approach to this problem. Extension modules are commonly written for two different reasons, one being speed, the other being wrapping non-Python libraries. At the moment we want mostly to approach a solution for the latter problem, because we hope that the JIT will eventually make it possible to not have to write extension modules for speed reasons any more. There are two rough ideas to approach this problem in the near future (there are other, more long-term ideas that I am not describing now): One of them is to add the ctypes module to PyPy's Python interpreter, which would mean re-implementing it since the existing implementation is written in C. The other way would be to work on the existing way to get extensions in that PyPy provides, which are \"mixed modules\". Mixed modules are written in a combination of RPython and normal Python code. To then wrap C libraries you would use rffi, which is the foreign function interface of RPython.The discussion round: Maciek Fijalkowski, Armin Rigo, Richard Emslie, Alexander Schremmer.Both approaches have problems: With ctypes you have no built-in way to query C header files for structure layouts and constants which requires you to hard-wire them, which is highly platform dependant. Mixed modules are not really fun to write, since they need to be RPython and we currently don't have a way to do separate compilation, so you always need to translate PyPy's whole Python interpreter to see whether your module is correct. In the meeting it was decided to first go for a ctypes replacement. The replacement would be written in pure Python, we already have a very thin wrapper around libffi which the new ctypes implementation would use.  The goal to reach would be to get the pygame implementation in ctypes to run on PyPy. To make ctypes more useful in general to write this kind of wrappers, we will probably extract some code that we have already written for PyPy's own usage: it gives a way to write \"imprecise\" declarations (\"a structure with at least fields called x and y which are of some kind of integer type\") and turn them into exact ctypes declarations, internally using the C compiler to inspect the platform headers. After this is done we should approach separate compilation so that developing modules in RPython has a quicker turnaround time. This is somewhat involved to implement for technical reasons. There are ideas how to implement it quickly to make it usable for prototyping, but it's still a lot of work.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/sprint-discussions-wrapping-external-8731011170537270161.html"
    },
    {
      "title": "Sprint Discussions: Releases, Testing",
      "text": "During the sprint we had various discussions about technical issues as well as planning discussions about how we want to go about things. One of them was about the stability of PyPy, how to ensure stability, how to handle releases and approaches to being more \"usable\". I will describe this discussion in this post (there are also minutes of the meeting).\n\n\n\nThe Meetings whiteboard\n\nTesting\n  First we discussed the current situation in terms of testing. PyPy has been extremely testing-oriented from the start, it is being developed almost exclusively in test-driven-development style. To deal with the large number of tests we already have some infrastructure in place:   we run all of PyPy's tests nightly on a Linux machine we translate a PyPy Python interpreter every night and use that to run the CPython compliance tests against it, also on a Linux machine we translate several Python interpreters every night and run benchmarks against them on a PowerPC running Mac OS X   As you can see, we are lacking in the Windows testing area, which is an even worse problem because none of the currently active developers has Windows as his primary OS. We should improve this by finding a Windows machine where the tests are run nightly and where we can log in to try bug-fixes quickly. The latter bit is important, we had a nightly windows test run before (thanks to Scott Dial) but it didn't help, because even if you tried to fix a bug you would have to wait until the next night to see whether it worked. Another very serious problem is that of aggregation: we have these various test runs that all have a web interface to check for errors but there is no easy way to find out which tests failed. You have to go to each page and even some sub-pages to see what needs fixing, which is a tedious process. The idea for solving this is aggregate all the available information into some sort of testing-entry-point page that gives a quick overview of the regressions that happened during the night. It's not clear whether we can achieve that with existing tools (buildbots or whatever), but we will investigate that.\n  Releases\nThe discussion about releases was more on a fundamental and less on a concrete level (especially when it comes to time-frames). We discussed what it means to make a release, because obviously it is more than just taking an SVN revision and putting a tarball of it onto the webpage. During the EU period we were required to make several releases, but those were not really meant to be more than technology previews for the brave adventurers to try. In the future we have the goal to release things that are more stable and hopefully more practically useful. The plan is to use medium-sized Python applications that have a chance to run on top of PyPy because they don't use too many extension modules (web apps being likely candidates) and that have good unit-tests themselves. The first step would be to find some applications that fit this description, fix the bugs that prevents PyPy from running them and from then on run them nightly on one of the testing machines to check for regressions. This would allow us to be more confident when stating that \"PyPy works\". Another thing to keep in mind for releases is the special features that our Python interpreter provides (e.g. the thunk and the taint object space, our stackless features, transparent proxies, sandboxing, special object implementations). Those features are neither tested by the CPython tests nor by any existing applications. Therefore we cannot really be confident that these features work and don't have too many bugs (in fact, the first time somebody really use the become feature of the thunk space in earnest he found a serious bug that is not fixed so far). To get around this problem, we plan to write small-to-medium sized example applications for each of these features (for stackless we can maybe use one of the existing stackless examples). This will hopefully find bugs and will also make it possible to evaluate whether the features make sense from a language design point of view. A minor thing to make releases easier is to be able to not only have the tests be run once a night but also be able to trigger them manually on the release branch before doing the release.Publishing Cool Things\n  Since we decided that the releases we make should be stable and usable, we also discussed how we would go about making new \"cool things\" like features, experiments etc. better known. The consensus was that this blog is probably the best forum for doing this. In addition we discussed having a stabler snapshot of the trunk made to ensure that people wanting to play around with these features don't accidentally get\na broken version.Helping Out\nRight now we are still in cleanup mode (the cleanup sprint is nearly done, but we haven't finished all the cleanups yet), so we won't be able to start on the above things right now. However, they will have a strong focus soon. So if you are interested in trying out to run programs on top of PyPy or writing new ones that use the new features you are most welcome to do so and we will try to fix the bugs or help you doing it (of course some tolerance against frustration is needed when you do that, because the bugs that turn up tend to be obscure). We have not been perfect at this in the past, but this will have to change.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2007/11/sprint-discussions-releases-testing-1126468258904483211.html"
    },
    {
      "title": "Ropes branch merged",
      "text": "This afternoon we merged the ropes branch that I have been working on on the side for a while (also to cut down the number of currently active branches a bit, since we are doing major cleanups right now). It contained a new (optional) implementation of the unicode type using the rope data structure. Ropes essentially use concatenation trees to represent strings. The leaves of the trees contain either byte arrays or arrays of unicode characters.\n\n\nOf course the fact that ropes are used is mostly completely transparent to the user (as usual in the pypy world :) ). Normal and unicode strings are implemented with them, but just from the behavior of these types the user has a hard time noticing. Of course there are significant changes in performance (in both directions).\n\nUsing ropes to implement strings has some interesting effects. The most obvious one is that string concatenation, slicing and repetition is really fast (I suspect that it is amortized O(1),   but haven't proved it). This is probably not helping most existing Python programs because people tend to code in such a way that these operations are not done too often. However, with ropes it is possible to do something like this:\nPython 2.4.1 (pypy 1.0.0 build 48942) on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>> import sys\n>>>> a = \"a\" * sys.maxint\n>>>> hash(a)\n-768146060\n\n\nSo somebody who is targeting a Python implementation that has ropes could write his code in such a way that this is taken into account. Another interesting feature is that ropes try to share as much data as possible with each other, so if you create a large slice of a large string, the slice is not going to take much additional memory.\n\nOne of the most interesting use-cases of ropes are together with unicode. The leaf nodes of a rope unicode string can be either a byte array or an array of unicode characters. This means that a unicode string that uses only characters that are latin-1 or ascii will use one byte of memory per character. If a unicode string contains mostly only unicode characters that are latin-1 and a few that are not, it will still use 1 byte for most of the latin-1 characters. This property also allows really fast encoding and decoding of unicode strings as long as they don't contain non-latin-1 characters (only with certain encodings of course):\n>>>> s = \"a\" * sys.maxint\n>>>> u = s.decode(\"ascii\")\n>>>> u = s.decode(\"latin-1\")\n>>>> u = s.decode(\"utf-8\")\nAgain, encoding and decoding strings that contain a few non-latin-1 characters is again efficient:\n>>>> u = \"a\" * 100000000 + u\"\\uffff\"\n>>>> s = u.encode(\"utf-8\")\n>>>> len(s)\n100000003\nI am not completely certain how useful this behaviour is for real-life applications, but it's kind of cool :-). It saves memory for european languages that contain few non-ascii characters.\n\nOf course there is at least one down-side to all of this, which is that string indexing is not O(1) any longer, because we have to walk down the tree to find the correct leaf where the character is actually in. I have not measured much, but I expect it to be quite fast in practice, because the trees are never deeper than 32 nodes.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/ropes-branch-merged-8782576892496878598.html"
    },
    {
      "title": "PyPy cleanup sprint startup",
      "text": "The following week we will have a sprint in Gothenburg to clean up the PyPy codebase and make it ready for future developments. So far, only a few people are here, the others will arrive this afternoon.\n\nThe \u00c4lvsborgsbron in Gothenburg from the ferry I took to get there.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-cleanup-sprint-startup-4429006224971155209.html"
    },
    {
      "title": "Unicode support in RPython",
      "text": "In the recent days we (Carl Friedrich, Anto and me) implemented native unicode support for RPython. This means that now you can write u'xxxx' directly in your RPython program, as well as unicode(some_string_variable) and most of the unicode methods should work as well. The things that don't work, are operations that require the unicode database (such as .upper() and friends) and encodings (unicode(x, encoding) for example). Right now our python interpreter does not use this at all, but that's the next step.\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/unicode-support-in-rpython-in-recent-1444449848043047640.html"
    },
    {
      "title": "The PyPy Road Show (1): New York and IBM",
      "text": "We're slowly getting adjusted to the jet-lag (except maybe Samuele). Time to blog... The past two days at IBM, in New York, have been quite interesting.  The place is a research center.  Feels University-like, but meetings rooms have no windows and climatization fixed on \"polar\" settings.  The building is of course heated at this time of the year, and then the meeting rooms are climatized... I guess that just doesn't make sense to me. We gave a 1h30 talk to a general audience first.  Then we had a compact schedule of meetings with various people or groups of people.  In the early preparations for this trip we planned to stay only one day, but Martin Hirzel, our host, found too many people that wanted to talk with us :-) I think that both us and most of the people we talked with got interesting things out of the meetings.  On our side, let me point a few highlights. We asked two people that worked on the GCs for the Jikes RVM if reusing them for RPython programs would make sense.  They didn't scream \"you're mad!\", so I guess the answer is yes.  Apparently, it has been done before, too.  I'm still not sure I got this right, but it seems that Microsoft paid someone money to integrate them with Rotor...  Then the real-time garbage-collection guys explained to us the things that we need to take care about when writing a VM: real-time GC needs not only write barriers and read barriers, but pointer-equality-comparison barriers...  They have bad memories of trying to add a posteriori this kind of barrier into existing VMs, so it took us a bit of explaining to make them realize that adding new kinds of barriers is mostly trivial for us (I'm still not 100% sure they got it... bad memories can stick hard). Then we had discussions with JIT people.  Mostly, this allowed us to confirm that Samuele has already got a good idea about what Java JITs like Hotspot can do, and in which kind of situation they work well.  As expected, the most difficult bit for a PyPy-like JIT that would run on top of a JVM would be the promotion.  We discussed approaches like first generating fall-back cases that include some instrumentation logic, and regenerating code with a few promoted values after some time if it seems like it will be a gain.  Replacing a method with a new version is difficult to do in a way that is portable across Java VMs. There are still possible workarounds, but it also means that if we really want to explore this seriously, we should consider experimenting with specifics VMs - e.g. the Jikes RVM gives (or could be adapted to give) hooks to replace methods with new versions of them, which is something that the JVM's own JIT internally does all the time. We showed the taint object space and the sandboxed PyPy to several groups of security people.  I won't say much about it here, beyond the fact that they were generally interested by the fact that the corresponding code is very short and easy to play with.  They are doing a lot on security in Java and... PHP, for web sites.  Someone could write a PHP interpreter (!) in PyPy to get the same kind of results. But as Laura and Samuele put it, there are things in life you do for fun, and things you do for money :-) We're in Vancouver today and tomorrow.  More about this later... Armin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-road-show-1-new-york-and-ibm-7837076523877011699.html"
    },
    {
      "title": "The PyPy Road Show",
      "text": "Armin Rigo, Samuele Pedroni, Laura Creighton and Jacob Hall\u00e9n are on a two-week-trip through the USA and Canada, to present PyPy to various companies and institutions. The next few blog entries will cover our experiences and adventures.\n\nHere is a glimpse of our schedule (all November 2007):\n4th: Chigaco5th-6th:  New York7th-8th:  Vancouver9th-18th: San Francisco and the Bay Area\nNotably, we meet with IBM Research in New York and give a Google Talk in the Bay Area.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-road-show-5790414147905233059.html"
    },
    {
      "title": "First Post",
      "text": "Welcome to the PyPy status blog. After we got a lot of positive feedback about the blog coverage of our Squeak/PyPy sprint in Bern  we decided that having a general PyPy blog sounds like a good idea. We will try to periodically post about what is going on in the PyPy project, cover sprints and other events where PyPyers are present. If you have any wishes about things we should write about, feel free to leave a comment.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/10/first-post-8150793557471983289.html"
    },
    {
      "title": "Search",
      "text": "Search results appear here.",
      "tags": "",
      "url": "https://www.pypy.org/search.html"
    }
  ]
};